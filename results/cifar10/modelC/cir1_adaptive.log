
Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=0 | lr=1.0e-02 | circles=1 
Training: Epoch=0 | Loss: 12.726 |  Acc: 26.652,30.446,37.108,42.066,46.434,48.436,49.220,49.536,% 
Testing: Epoch=0 | Loss: 12.663 |  Acc: 25.570,32.610,36.690,44.480,52.870,57.500,59.280,58.470,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=1 | lr=1.0e-02 | circles=1 
Training: Epoch=1 | Loss: 9.913 |  Acc: 33.058,39.150,49.496,57.144,62.358,65.520,66.500,66.770,% 
Testing: Epoch=1 | Loss: 11.946 |  Acc: 27.090,34.480,45.530,47.650,58.920,61.820,66.200,64.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=2 | lr=1.0e-02 | circles=1 
Training: Epoch=2 | Loss: 8.768 |  Acc: 35.952,43.836,54.240,63.270,68.750,72.036,73.318,73.526,% 
Testing: Epoch=2 | Loss: 11.043 |  Acc: 26.670,40.250,49.640,62.070,64.790,73.310,72.620,72.960,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=3 | lr=1.0e-02 | circles=1 
Training: Epoch=3 | Loss: 7.995 |  Acc: 37.428,46.658,57.350,66.788,73.140,76.974,78.004,78.000,% 
Testing: Epoch=3 | Loss: 10.718 |  Acc: 30.100,42.410,49.470,57.300,68.840,73.780,73.970,72.830,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=4 | lr=1.0e-02 | circles=1 
Training: Epoch=4 | Loss: 7.432 |  Acc: 38.874,48.970,59.170,69.126,76.400,80.012,80.918,80.984,% 
Testing: Epoch=4 | Loss: 10.032 |  Acc: 29.700,40.220,53.890,62.770,73.340,78.170,79.260,79.340,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=5 | lr=1.0e-02 | circles=1 
Training: Epoch=5 | Loss: 7.041 |  Acc: 39.832,49.810,60.860,71.176,78.842,82.120,83.112,83.128,% 
Testing: Epoch=5 | Loss: 10.611 |  Acc: 28.720,42.130,46.010,53.840,70.720,75.390,76.910,76.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=6 | lr=1.0e-02 | circles=1 
Training: Epoch=6 | Loss: 6.710 |  Acc: 40.178,51.158,62.134,72.950,81.006,84.070,84.828,84.938,% 
Testing: Epoch=6 | Loss: 9.747 |  Acc: 30.480,42.280,56.590,63.930,75.890,80.480,79.740,81.480,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=7 | lr=1.0e-02 | circles=1 
Training: Epoch=7 | Loss: 6.462 |  Acc: 41.282,52.154,63.304,74.026,82.020,85.242,86.186,86.244,% 
Testing: Epoch=7 | Loss: 9.746 |  Acc: 30.100,42.000,57.670,70.820,79.030,82.900,84.280,84.390,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=8 | lr=1.0e-02 | circles=1 
Training: Epoch=8 | Loss: 6.213 |  Acc: 42.016,53.284,64.698,75.450,83.226,86.388,87.054,87.152,% 
Testing: Epoch=8 | Loss: 9.512 |  Acc: 30.390,44.830,55.600,69.190,78.010,83.240,84.980,84.430,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=9 | lr=1.0e-02 | circles=1 
Training: Epoch=9 | Loss: 6.037 |  Acc: 42.560,53.478,65.444,76.448,84.050,87.246,88.030,88.186,% 
Testing: Epoch=9 | Loss: 9.636 |  Acc: 32.250,37.190,55.870,70.000,80.130,83.370,83.720,84.380,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=10 | lr=1.0e-02 | circles=1 
Training: Epoch=10 | Loss: 5.871 |  Acc: 43.368,54.596,66.170,77.074,85.106,88.110,88.700,88.872,% 
Testing: Epoch=10 | Loss: 9.336 |  Acc: 35.680,43.640,58.570,69.790,79.840,83.940,84.400,84.750,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=11 | lr=1.0e-02 | circles=1 
Training: Epoch=11 | Loss: 5.724 |  Acc: 43.352,54.810,66.562,77.940,85.666,88.816,89.580,89.682,% 
Testing: Epoch=11 | Loss: 9.427 |  Acc: 35.380,46.540,53.830,72.050,80.700,84.830,84.530,85.540,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=12 | lr=1.0e-02 | circles=1 
Training: Epoch=12 | Loss: 5.607 |  Acc: 43.904,55.394,67.430,78.180,86.048,89.348,90.162,90.356,% 
Testing: Epoch=12 | Loss: 9.273 |  Acc: 32.580,46.610,58.620,72.710,80.830,85.390,85.420,86.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=13 | lr=1.0e-02 | circles=1 
Training: Epoch=13 | Loss: 5.470 |  Acc: 44.130,55.770,68.002,79.074,86.804,90.040,90.774,90.954,% 
Testing: Epoch=13 | Loss: 9.209 |  Acc: 29.100,45.610,59.010,74.380,81.590,85.240,86.410,86.190,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=14 | lr=1.0e-02 | circles=1 
Training: Epoch=14 | Loss: 5.374 |  Acc: 44.706,56.508,68.624,79.460,87.120,90.516,91.132,91.302,% 



==> Preparing data..
Dataset: CIFAR10
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=64, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=64, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=74, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=74, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=138, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=138, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (3): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=138, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=138, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (4): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=266, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=266, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (5): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=266, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=266, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (6): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=522, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=522, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
    (7): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=522, out_features=10, bias=True)
      (linear_bw): Linear(in_features=10, out_features=522, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x10])
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Batch: 0 | Loss: 19.446 | Acc: 10.938,17.969,9.375,8.594,10.938,10.156,10.938,9.375,%
Batch: 20 | Loss: 17.052 | Acc: 12.351,18.527,18.080,19.903,21.912,23.921,24.070,23.661,%
Batch: 40 | Loss: 16.351 | Acc: 15.206,20.503,20.636,23.209,25.419,26.582,26.982,26.829,%
Batch: 60 | Loss: 15.767 | Acc: 17.738,22.439,22.592,26.627,28.471,30.046,30.200,29.739,%
Batch: 80 | Loss: 15.425 | Acc: 19.387,23.630,24.383,28.048,30.218,31.858,32.137,31.597,%
Batch: 100 | Loss: 15.091 | Acc: 20.545,24.482,26.137,29.618,32.201,33.857,34.244,33.756,%
Batch: 120 | Loss: 14.804 | Acc: 21.236,25.187,27.344,30.656,33.742,35.453,35.866,35.511,%
Batch: 140 | Loss: 14.585 | Acc: 21.941,25.826,28.596,31.871,35.073,36.807,37.262,37.051,%
Batch: 160 | Loss: 14.341 | Acc: 22.685,26.650,29.780,33.176,36.661,38.359,38.703,38.650,%
Batch: 180 | Loss: 14.125 | Acc: 23.334,27.085,30.952,34.453,38.009,39.727,40.185,40.198,%
Batch: 200 | Loss: 13.938 | Acc: 23.768,27.732,31.814,35.475,39.090,40.835,41.422,41.468,%
Batch: 220 | Loss: 13.779 | Acc: 24.205,28.072,32.487,36.333,39.914,41.753,42.322,42.509,%
Batch: 240 | Loss: 13.634 | Acc: 24.601,28.388,33.182,37.260,40.823,42.674,43.299,43.432,%
Batch: 260 | Loss: 13.504 | Acc: 24.916,28.550,33.836,37.982,41.724,43.549,44.241,44.403,%
Batch: 280 | Loss: 13.360 | Acc: 25.225,28.762,34.333,38.740,42.546,44.437,45.146,45.376,%
Batch: 300 | Loss: 13.223 | Acc: 25.504,29.098,34.811,39.413,43.381,45.268,46.026,46.252,%
Batch: 320 | Loss: 13.108 | Acc: 25.847,29.442,35.456,40.119,44.164,45.999,46.775,46.970,%
Batch: 340 | Loss: 12.991 | Acc: 26.072,29.713,35.915,40.685,44.987,46.818,47.629,47.888,%
Batch: 360 | Loss: 12.880 | Acc: 26.303,30.014,36.370,41.220,45.607,47.524,48.312,48.604,%
Batch: 380 | Loss: 12.773 | Acc: 26.558,30.266,36.903,41.841,46.217,48.191,48.967,49.268,%
Batch: 0 | Loss: 12.229 | Acc: 28.906,35.156,40.625,44.531,50.000,59.375,62.500,60.156,%
Batch: 20 | Loss: 12.736 | Acc: 25.223,32.961,37.016,43.824,51.749,56.882,58.891,57.515,%
Batch: 40 | Loss: 12.646 | Acc: 24.676,32.241,36.776,44.703,53.049,58.022,59.546,58.403,%
Batch: 60 | Loss: 12.642 | Acc: 25.628,32.390,36.847,44.480,52.946,57.620,59.324,58.478,%

Epoch: 1
Batch: 0 | Loss: 9.955 | Acc: 29.688,37.500,50.000,56.250,67.969,66.406,62.500,67.969,%
Batch: 20 | Loss: 10.481 | Acc: 32.403,36.868,46.391,53.571,58.445,61.756,61.905,63.504,%
Batch: 40 | Loss: 10.579 | Acc: 31.517,36.128,46.037,53.087,58.232,61.280,62.081,62.729,%
Batch: 60 | Loss: 10.500 | Acc: 31.801,36.424,47.195,53.637,58.991,61.732,62.474,62.961,%
Batch: 80 | Loss: 10.485 | Acc: 31.655,36.680,47.251,53.791,59.095,61.719,62.751,63.262,%
Batch: 100 | Loss: 10.429 | Acc: 31.706,36.688,47.857,54.131,59.344,62.376,63.196,63.683,%
Batch: 120 | Loss: 10.380 | Acc: 31.728,36.751,47.850,54.455,59.788,62.771,63.740,64.114,%
Batch: 140 | Loss: 10.342 | Acc: 31.715,36.830,47.917,54.765,59.951,62.954,63.974,64.351,%
Batch: 160 | Loss: 10.304 | Acc: 31.789,36.961,48.035,55.119,60.345,63.218,64.315,64.645,%
Batch: 180 | Loss: 10.251 | Acc: 31.962,37.077,48.299,55.318,60.605,63.575,64.602,64.982,%
Batch: 200 | Loss: 10.204 | Acc: 32.191,37.380,48.472,55.531,60.825,63.872,64.782,65.166,%
Batch: 220 | Loss: 10.176 | Acc: 32.441,37.624,48.547,55.607,60.945,64.006,64.960,65.360,%
Batch: 240 | Loss: 10.150 | Acc: 32.511,37.866,48.632,55.741,60.999,64.062,65.064,65.473,%
Batch: 260 | Loss: 10.118 | Acc: 32.570,38.117,48.791,55.900,61.219,64.308,65.359,65.790,%
Batch: 280 | Loss: 10.102 | Acc: 32.562,38.228,48.813,56.050,61.316,64.438,65.494,65.892,%
Batch: 300 | Loss: 10.068 | Acc: 32.610,38.442,48.900,56.229,61.470,64.649,65.705,66.056,%
Batch: 320 | Loss: 10.032 | Acc: 32.708,38.639,49.026,56.496,61.748,64.863,65.922,66.182,%
Batch: 340 | Loss: 9.996 | Acc: 32.812,38.806,49.242,56.692,61.964,65.041,66.122,66.386,%
Batch: 360 | Loss: 9.961 | Acc: 32.988,39.004,49.316,56.841,62.078,65.227,66.289,66.560,%
Batch: 380 | Loss: 9.931 | Acc: 33.040,39.104,49.428,57.062,62.256,65.404,66.400,66.650,%
Batch: 0 | Loss: 11.686 | Acc: 33.594,30.469,45.312,48.438,58.594,60.938,65.625,67.969,%
Batch: 20 | Loss: 12.016 | Acc: 27.530,33.073,45.201,46.801,57.775,60.900,64.807,64.249,%
Batch: 40 | Loss: 11.930 | Acc: 27.306,33.880,45.998,47.542,59.299,61.719,66.502,65.168,%
Batch: 60 | Loss: 11.931 | Acc: 27.267,34.465,45.722,47.772,59.234,61.796,66.496,65.010,%

Epoch: 2
Batch: 0 | Loss: 8.714 | Acc: 35.938,42.969,53.906,64.844,67.188,69.531,69.531,72.656,%
Batch: 20 | Loss: 9.048 | Acc: 34.598,44.159,54.167,62.835,67.188,70.908,72.210,72.879,%
Batch: 40 | Loss: 9.126 | Acc: 34.527,42.645,52.439,62.024,67.092,70.332,71.684,71.989,%
Batch: 60 | Loss: 9.070 | Acc: 35.156,42.674,52.805,61.962,67.264,70.530,71.862,72.118,%
Batch: 80 | Loss: 9.059 | Acc: 35.166,42.631,53.048,62.027,67.583,70.727,71.933,72.068,%
Batch: 100 | Loss: 9.046 | Acc: 35.087,42.713,53.148,62.330,67.652,70.900,72.269,72.355,%
Batch: 120 | Loss: 9.037 | Acc: 35.130,42.717,53.319,62.326,67.271,70.629,72.107,72.191,%
Batch: 140 | Loss: 9.049 | Acc: 35.184,42.858,53.169,62.168,67.199,70.512,71.919,72.141,%
Batch: 160 | Loss: 9.072 | Acc: 35.108,42.697,53.091,62.049,67.027,70.337,71.642,71.865,%
Batch: 180 | Loss: 9.055 | Acc: 35.199,42.783,53.263,62.181,67.166,70.464,71.806,71.901,%
Batch: 200 | Loss: 9.028 | Acc: 35.308,42.984,53.292,62.278,67.273,70.616,71.980,72.058,%
Batch: 220 | Loss: 8.957 | Acc: 35.506,43.195,53.652,62.592,67.640,70.917,72.324,72.412,%
Batch: 240 | Loss: 8.953 | Acc: 35.565,43.325,53.634,62.529,67.680,70.873,72.316,72.423,%
Batch: 260 | Loss: 8.928 | Acc: 35.659,43.304,53.700,62.698,67.804,70.947,72.357,72.495,%
Batch: 280 | Loss: 8.901 | Acc: 35.640,43.402,53.728,62.792,68.035,71.213,72.584,72.687,%
Batch: 300 | Loss: 8.852 | Acc: 35.719,43.581,53.867,62.991,68.296,71.444,72.854,73.017,%
Batch: 320 | Loss: 8.840 | Acc: 35.772,43.604,53.994,63.048,68.404,71.551,72.924,73.104,%
Batch: 340 | Loss: 8.816 | Acc: 35.848,43.631,54.030,63.151,68.535,71.710,73.073,73.279,%
Batch: 360 | Loss: 8.783 | Acc: 35.927,43.692,54.123,63.218,68.687,71.918,73.210,73.435,%
Batch: 380 | Loss: 8.777 | Acc: 35.950,43.762,54.171,63.209,68.711,71.928,73.228,73.474,%
Batch: 0 | Loss: 10.860 | Acc: 25.781,37.500,44.531,58.594,64.062,74.219,73.438,75.000,%
Batch: 20 | Loss: 11.037 | Acc: 26.600,39.509,49.033,60.677,64.286,73.140,72.433,73.438,%
Batch: 40 | Loss: 10.990 | Acc: 26.505,40.663,50.191,62.157,65.225,73.628,72.828,73.609,%
Batch: 60 | Loss: 11.037 | Acc: 26.614,40.318,49.782,62.039,64.972,73.361,72.579,72.976,%

Epoch: 3
Batch: 0 | Loss: 8.656 | Acc: 32.812,41.406,53.906,67.188,69.531,74.219,73.438,71.094,%
Batch: 20 | Loss: 8.437 | Acc: 36.942,45.126,55.469,65.179,71.540,73.698,75.558,75.037,%
Batch: 40 | Loss: 8.203 | Acc: 37.252,45.922,56.174,66.254,72.447,75.591,76.753,76.734,%
Batch: 60 | Loss: 8.189 | Acc: 37.052,45.863,56.288,66.291,72.029,75.679,76.908,76.716,%
Batch: 80 | Loss: 8.172 | Acc: 36.912,46.026,56.395,66.011,72.078,75.762,76.929,76.890,%
Batch: 100 | Loss: 8.154 | Acc: 37.059,46.163,56.443,65.803,71.921,75.835,77.096,76.980,%
Batch: 120 | Loss: 8.136 | Acc: 37.197,46.094,56.921,65.974,71.991,75.878,77.073,77.047,%
Batch: 140 | Loss: 8.112 | Acc: 36.968,46.138,56.992,65.952,71.980,75.981,77.133,77.122,%
Batch: 160 | Loss: 8.105 | Acc: 36.859,46.142,56.973,66.101,72.205,76.068,77.315,77.295,%
Batch: 180 | Loss: 8.106 | Acc: 36.874,46.275,57.161,66.104,72.194,76.127,77.339,77.318,%
Batch: 200 | Loss: 8.083 | Acc: 36.831,46.370,57.218,66.266,72.439,76.252,77.546,77.511,%
Batch: 220 | Loss: 8.069 | Acc: 36.952,46.479,57.176,66.311,72.479,76.382,77.680,77.630,%
Batch: 240 | Loss: 8.047 | Acc: 36.968,46.538,57.229,66.422,72.582,76.468,77.730,77.655,%
Batch: 260 | Loss: 8.047 | Acc: 37.084,46.594,57.250,66.460,72.584,76.524,77.703,77.688,%
Batch: 280 | Loss: 8.036 | Acc: 37.016,46.519,57.231,66.559,72.692,76.624,77.825,77.814,%
Batch: 300 | Loss: 8.034 | Acc: 37.152,46.595,57.223,66.578,72.833,76.690,77.834,77.824,%
Batch: 320 | Loss: 8.022 | Acc: 37.237,46.646,57.275,66.635,72.968,76.774,77.835,77.855,%
Batch: 340 | Loss: 8.011 | Acc: 37.273,46.660,57.322,66.688,73.046,76.881,77.942,77.955,%
Batch: 360 | Loss: 8.002 | Acc: 37.400,46.713,57.354,66.791,73.067,76.935,77.982,77.993,%
Batch: 380 | Loss: 8.002 | Acc: 37.400,46.678,57.329,66.769,73.091,76.975,77.981,77.996,%
Batch: 0 | Loss: 10.208 | Acc: 29.688,45.312,55.469,64.844,71.875,74.219,84.375,78.906,%
Batch: 20 | Loss: 10.740 | Acc: 28.757,41.778,49.554,55.766,68.936,72.879,74.033,72.805,%
Batch: 40 | Loss: 10.718 | Acc: 29.478,42.569,49.695,57.146,69.017,73.114,73.761,72.847,%
Batch: 60 | Loss: 10.714 | Acc: 30.213,42.405,49.436,57.364,68.904,73.758,74.001,72.836,%

Epoch: 4
Batch: 0 | Loss: 9.681 | Acc: 35.156,46.094,46.094,57.812,64.844,71.094,71.094,71.875,%
Batch: 20 | Loss: 7.687 | Acc: 38.728,48.289,58.073,67.820,74.219,78.199,79.539,79.241,%
Batch: 40 | Loss: 7.573 | Acc: 38.586,48.914,58.460,68.712,74.943,78.792,80.050,80.011,%
Batch: 60 | Loss: 7.553 | Acc: 38.653,48.604,58.350,68.532,75.525,79.162,80.392,80.456,%
Batch: 80 | Loss: 7.553 | Acc: 38.735,48.814,58.507,68.682,75.617,79.331,80.353,80.411,%
Batch: 100 | Loss: 7.550 | Acc: 38.591,48.801,58.772,68.959,75.619,79.270,80.175,80.159,%
Batch: 120 | Loss: 7.574 | Acc: 38.656,48.631,58.678,68.673,75.523,79.190,80.120,80.223,%
Batch: 140 | Loss: 7.590 | Acc: 38.575,48.482,58.461,68.578,75.477,79.095,80.109,80.186,%
Batch: 160 | Loss: 7.610 | Acc: 38.616,48.418,58.511,68.464,75.529,79.091,79.998,80.100,%
Batch: 180 | Loss: 7.586 | Acc: 38.635,48.407,58.533,68.608,75.470,79.113,80.106,80.175,%
Batch: 200 | Loss: 7.563 | Acc: 38.643,48.422,58.675,68.637,75.552,79.171,80.263,80.317,%
Batch: 220 | Loss: 7.562 | Acc: 38.631,48.328,58.640,68.587,75.573,79.200,80.292,80.391,%
Batch: 240 | Loss: 7.538 | Acc: 38.602,48.405,58.808,68.679,75.678,79.305,80.368,80.478,%
Batch: 260 | Loss: 7.524 | Acc: 38.548,48.554,58.845,68.792,75.769,79.418,80.466,80.577,%
Batch: 280 | Loss: 7.504 | Acc: 38.651,48.643,58.955,68.933,75.862,79.504,80.469,80.563,%
Batch: 300 | Loss: 7.481 | Acc: 38.722,48.689,58.980,69.054,76.049,79.630,80.643,80.736,%
Batch: 320 | Loss: 7.468 | Acc: 38.722,48.642,59.025,69.052,76.197,79.812,80.807,80.863,%
Batch: 340 | Loss: 7.455 | Acc: 38.760,48.802,59.105,69.107,76.228,79.878,80.847,80.904,%
Batch: 360 | Loss: 7.439 | Acc: 38.777,48.875,59.120,69.120,76.324,79.936,80.882,80.949,%
Batch: 380 | Loss: 7.433 | Acc: 38.884,48.971,59.195,69.135,76.370,79.997,80.912,80.990,%
Batch: 0 | Loss: 9.460 | Acc: 32.812,41.406,55.469,64.062,74.219,78.906,85.156,86.719,%
Batch: 20 | Loss: 10.050 | Acc: 30.171,39.546,53.497,61.682,72.954,77.381,78.981,79.129,%
Batch: 40 | Loss: 10.030 | Acc: 29.325,40.530,54.516,62.557,73.171,78.106,78.925,79.383,%
Batch: 60 | Loss: 10.026 | Acc: 29.726,40.433,54.380,62.769,73.348,78.356,79.355,79.419,%

Epoch: 5
Batch: 0 | Loss: 6.584 | Acc: 37.500,51.562,61.719,72.656,83.594,85.938,85.938,85.938,%
Batch: 20 | Loss: 7.212 | Acc: 39.062,47.991,58.519,69.531,78.460,81.808,83.222,83.482,%
Batch: 40 | Loss: 7.142 | Acc: 39.520,48.133,59.566,70.084,78.830,82.298,83.117,83.556,%
Batch: 60 | Loss: 7.110 | Acc: 39.857,48.809,60.143,70.556,78.586,82.249,83.145,83.363,%
Batch: 80 | Loss: 7.084 | Acc: 39.853,49.093,60.291,70.563,78.733,82.292,83.005,83.324,%
Batch: 100 | Loss: 7.125 | Acc: 39.759,49.126,60.187,70.305,78.458,82.070,82.898,83.176,%
Batch: 120 | Loss: 7.109 | Acc: 40.063,49.522,60.421,70.384,78.519,82.064,82.922,83.097,%
Batch: 140 | Loss: 7.088 | Acc: 39.966,49.357,60.483,70.551,78.784,82.181,83.139,83.295,%
Batch: 160 | Loss: 7.065 | Acc: 39.975,49.617,60.811,70.769,78.867,82.230,83.133,83.254,%
Batch: 180 | Loss: 7.059 | Acc: 39.939,49.612,60.769,70.705,78.798,82.230,83.201,83.309,%
Batch: 200 | Loss: 7.067 | Acc: 40.023,49.549,60.693,70.682,78.692,82.222,83.186,83.263,%
Batch: 220 | Loss: 7.053 | Acc: 40.074,49.650,60.743,70.779,78.807,82.197,83.215,83.307,%
Batch: 240 | Loss: 7.068 | Acc: 40.019,49.605,60.827,70.815,78.676,82.031,83.010,83.072,%
Batch: 260 | Loss: 7.072 | Acc: 39.883,49.641,60.647,70.723,78.601,81.995,83.037,83.127,%
Batch: 280 | Loss: 7.059 | Acc: 39.805,49.644,60.737,70.846,78.701,82.073,83.060,83.177,%
Batch: 300 | Loss: 7.055 | Acc: 39.797,49.691,60.803,70.943,78.696,82.021,82.955,83.069,%
Batch: 320 | Loss: 7.038 | Acc: 39.773,49.786,60.896,71.082,78.816,82.085,83.041,83.139,%
Batch: 340 | Loss: 7.043 | Acc: 39.731,49.739,60.864,71.059,78.773,82.061,83.042,83.120,%
Batch: 360 | Loss: 7.025 | Acc: 39.868,49.818,60.946,71.200,78.887,82.155,83.133,83.209,%
Batch: 380 | Loss: 7.038 | Acc: 39.827,49.842,60.896,71.168,78.867,82.154,83.116,83.155,%
Batch: 0 | Loss: 10.205 | Acc: 38.281,50.000,50.000,60.156,70.312,75.000,78.125,75.781,%
Batch: 20 | Loss: 10.609 | Acc: 29.725,41.815,46.540,52.790,70.908,75.595,77.009,76.600,%
Batch: 40 | Loss: 10.599 | Acc: 29.230,42.492,46.246,53.811,71.246,75.591,77.172,76.734,%
Batch: 60 | Loss: 10.609 | Acc: 29.201,41.944,46.171,53.765,71.055,75.538,77.011,76.498,%

Epoch: 6
Batch: 0 | Loss: 7.368 | Acc: 39.062,50.000,64.844,64.062,75.781,82.031,83.594,82.812,%
Batch: 20 | Loss: 6.897 | Acc: 39.100,50.186,61.012,71.949,80.729,84.598,85.417,85.342,%
Batch: 40 | Loss: 6.798 | Acc: 39.139,49.886,61.204,72.275,81.174,84.604,85.347,85.537,%
Batch: 60 | Loss: 6.779 | Acc: 39.472,50.282,61.463,72.464,80.943,84.132,84.810,84.913,%
Batch: 80 | Loss: 6.825 | Acc: 39.361,50.000,61.584,72.280,80.874,83.922,84.597,84.626,%
Batch: 100 | Loss: 6.795 | Acc: 39.341,50.170,61.788,72.478,81.064,83.957,84.808,84.653,%
Batch: 120 | Loss: 6.757 | Acc: 39.534,50.523,62.067,72.747,81.244,84.168,84.975,84.866,%
Batch: 140 | Loss: 6.761 | Acc: 39.389,50.449,61.846,72.817,81.200,84.098,84.835,84.802,%
Batch: 160 | Loss: 6.758 | Acc: 39.543,50.476,61.796,72.845,81.114,84.001,84.652,84.734,%
Batch: 180 | Loss: 6.756 | Acc: 39.671,50.587,61.779,72.855,81.121,84.025,84.643,84.776,%
Batch: 200 | Loss: 6.734 | Acc: 39.863,50.824,61.940,73.022,81.164,84.052,84.694,84.826,%
Batch: 220 | Loss: 6.728 | Acc: 39.879,50.909,62.086,72.996,81.056,83.990,84.725,84.831,%
Batch: 240 | Loss: 6.743 | Acc: 39.983,50.966,62.011,72.909,81.000,83.876,84.628,84.735,%
Batch: 260 | Loss: 6.752 | Acc: 39.981,50.976,61.982,72.833,80.927,83.848,84.594,84.701,%
Batch: 280 | Loss: 6.760 | Acc: 39.902,50.831,61.886,72.837,80.839,83.799,84.520,84.664,%
Batch: 300 | Loss: 6.742 | Acc: 39.971,50.914,61.877,72.895,80.967,83.913,84.614,84.803,%
Batch: 320 | Loss: 6.738 | Acc: 40.026,50.995,61.979,72.870,80.946,83.969,84.652,84.811,%
Batch: 340 | Loss: 6.734 | Acc: 39.984,51.036,62.049,72.885,80.941,84.004,84.684,84.840,%
Batch: 360 | Loss: 6.726 | Acc: 39.991,51.041,62.061,72.925,81.023,84.100,84.821,84.944,%
Batch: 380 | Loss: 6.720 | Acc: 40.061,51.038,62.061,72.929,80.977,84.078,84.820,84.933,%
Batch: 0 | Loss: 9.137 | Acc: 32.031,42.969,54.688,67.188,76.562,83.594,83.594,86.719,%
Batch: 20 | Loss: 9.740 | Acc: 31.176,41.741,56.882,62.872,76.004,81.027,80.320,81.808,%
Batch: 40 | Loss: 9.705 | Acc: 30.850,42.511,57.260,63.948,76.277,81.288,80.488,82.336,%
Batch: 60 | Loss: 9.736 | Acc: 30.930,42.380,56.698,63.768,75.973,80.751,80.085,81.749,%

Epoch: 7
Batch: 0 | Loss: 6.518 | Acc: 38.281,51.562,59.375,69.531,78.906,85.156,84.375,85.938,%
Batch: 20 | Loss: 6.412 | Acc: 40.737,52.418,62.612,73.772,82.143,85.491,86.347,86.235,%
Batch: 40 | Loss: 6.522 | Acc: 40.987,51.715,62.691,72.580,82.146,85.309,86.357,86.109,%
Batch: 60 | Loss: 6.496 | Acc: 40.856,51.755,62.513,72.759,82.339,85.528,86.578,86.194,%
Batch: 80 | Loss: 6.514 | Acc: 41.069,51.553,62.442,73.196,82.321,85.262,86.323,85.995,%
Batch: 100 | Loss: 6.495 | Acc: 40.772,51.346,62.740,73.368,82.418,85.497,86.471,86.139,%
Batch: 120 | Loss: 6.497 | Acc: 40.941,51.763,63.081,73.509,82.102,85.305,86.183,85.938,%
Batch: 140 | Loss: 6.489 | Acc: 40.996,51.646,63.165,73.582,82.020,85.256,86.176,85.976,%
Batch: 160 | Loss: 6.481 | Acc: 40.979,51.689,63.252,73.646,81.915,85.176,86.001,85.894,%
Batch: 180 | Loss: 6.490 | Acc: 40.953,51.571,62.970,73.627,81.828,85.087,85.950,85.894,%
Batch: 200 | Loss: 6.480 | Acc: 41.010,51.438,62.920,73.675,81.856,85.164,86.023,85.988,%
Batch: 220 | Loss: 6.492 | Acc: 41.014,51.410,62.832,73.646,81.706,85.008,85.927,85.923,%
Batch: 240 | Loss: 6.486 | Acc: 41.076,51.575,62.954,73.742,81.772,85.082,85.938,85.918,%
Batch: 260 | Loss: 6.469 | Acc: 41.200,51.697,63.027,73.824,81.894,85.165,86.066,86.033,%
Batch: 280 | Loss: 6.465 | Acc: 41.156,51.813,63.123,73.832,81.942,85.231,86.107,86.099,%
Batch: 300 | Loss: 6.471 | Acc: 41.105,51.905,63.162,73.931,81.948,85.221,86.114,86.158,%
Batch: 320 | Loss: 6.470 | Acc: 41.192,51.988,63.130,73.922,81.992,85.293,86.159,86.222,%
Batch: 340 | Loss: 6.462 | Acc: 41.138,52.050,63.180,73.960,82.059,85.287,86.192,86.249,%
Batch: 360 | Loss: 6.457 | Acc: 41.222,52.138,63.275,74.080,82.064,85.310,86.240,86.277,%
Batch: 380 | Loss: 6.456 | Acc: 41.224,52.159,63.312,74.051,82.062,85.300,86.237,86.274,%
Batch: 0 | Loss: 9.525 | Acc: 31.250,43.750,55.469,76.562,82.812,84.375,89.062,88.281,%
Batch: 20 | Loss: 9.756 | Acc: 30.543,41.555,57.292,69.903,78.795,82.775,84.747,84.561,%
Batch: 40 | Loss: 9.746 | Acc: 30.469,42.435,58.613,70.598,78.963,82.774,84.642,84.699,%
Batch: 60 | Loss: 9.751 | Acc: 30.379,42.546,57.710,70.735,78.957,82.851,84.375,84.388,%

Epoch: 8
Batch: 0 | Loss: 5.777 | Acc: 48.438,57.031,59.375,76.562,82.812,89.844,92.188,92.188,%
Batch: 20 | Loss: 6.118 | Acc: 41.481,53.051,65.402,76.339,83.668,86.756,88.430,88.281,%
Batch: 40 | Loss: 6.321 | Acc: 41.368,52.401,64.215,75.800,82.946,85.995,87.195,87.405,%
Batch: 60 | Loss: 6.266 | Acc: 41.816,52.369,64.050,75.922,83.056,86.130,87.205,87.321,%
Batch: 80 | Loss: 6.248 | Acc: 42.033,52.623,64.439,75.993,82.976,86.092,87.076,87.269,%
Batch: 100 | Loss: 6.228 | Acc: 41.986,52.808,64.542,75.859,83.099,86.239,87.198,87.392,%
Batch: 120 | Loss: 6.209 | Acc: 41.923,53.002,64.418,75.710,83.045,86.241,87.138,87.364,%
Batch: 140 | Loss: 6.231 | Acc: 41.700,52.970,64.362,75.443,82.973,86.159,87.007,87.206,%
Batch: 160 | Loss: 6.217 | Acc: 41.600,53.106,64.543,75.437,83.031,86.238,87.092,87.253,%
Batch: 180 | Loss: 6.229 | Acc: 41.799,53.108,64.438,75.445,83.059,86.179,86.999,87.172,%
Batch: 200 | Loss: 6.233 | Acc: 41.865,53.183,64.416,75.167,83.158,86.229,87.014,87.154,%
Batch: 220 | Loss: 6.234 | Acc: 41.795,53.132,64.469,75.194,83.162,86.227,87.005,87.125,%
Batch: 240 | Loss: 6.233 | Acc: 41.938,53.115,64.507,75.133,83.159,86.275,86.968,87.095,%
Batch: 260 | Loss: 6.246 | Acc: 41.996,53.026,64.350,75.078,83.073,86.216,86.931,87.033,%
Batch: 280 | Loss: 6.236 | Acc: 42.043,53.094,64.385,75.153,83.021,86.268,87.019,87.116,%
Batch: 300 | Loss: 6.241 | Acc: 42.050,53.203,64.470,75.197,83.044,86.215,86.950,87.028,%
Batch: 320 | Loss: 6.238 | Acc: 41.995,53.213,64.471,75.190,83.039,86.239,86.945,87.040,%
Batch: 340 | Loss: 6.246 | Acc: 41.974,53.150,64.509,75.115,83.058,86.224,86.916,86.991,%
Batch: 360 | Loss: 6.237 | Acc: 42.012,53.196,64.523,75.201,83.081,86.292,86.965,87.056,%
Batch: 380 | Loss: 6.223 | Acc: 41.993,53.219,64.649,75.344,83.163,86.350,86.989,87.086,%
Batch: 0 | Loss: 9.086 | Acc: 31.250,45.312,49.219,75.781,82.812,87.500,92.188,89.062,%
Batch: 20 | Loss: 9.582 | Acc: 30.208,44.754,54.427,68.341,77.827,82.701,84.003,83.259,%
Batch: 40 | Loss: 9.510 | Acc: 30.221,45.065,55.316,69.131,77.820,83.327,84.775,84.108,%
Batch: 60 | Loss: 9.513 | Acc: 30.520,45.300,55.443,69.198,78.151,83.414,84.951,84.490,%

Epoch: 9
Batch: 0 | Loss: 7.159 | Acc: 41.406,51.562,52.344,66.406,75.000,78.906,81.250,80.469,%
Batch: 20 | Loss: 6.078 | Acc: 41.406,52.418,65.365,76.153,83.966,87.388,88.170,88.356,%
Batch: 40 | Loss: 6.065 | Acc: 41.787,53.049,65.796,75.991,84.013,87.671,88.167,88.357,%
Batch: 60 | Loss: 6.032 | Acc: 42.239,52.959,65.599,75.768,84.260,87.884,88.422,88.704,%
Batch: 80 | Loss: 5.977 | Acc: 42.573,53.029,65.577,76.157,84.520,87.886,88.465,88.725,%
Batch: 100 | Loss: 6.013 | Acc: 42.404,52.939,65.354,76.006,84.545,87.647,88.281,88.560,%
Batch: 120 | Loss: 5.987 | Acc: 42.446,53.164,65.573,76.362,84.659,87.823,88.385,88.630,%
Batch: 140 | Loss: 5.989 | Acc: 42.470,53.330,65.703,76.441,84.591,87.855,88.486,88.686,%
Batch: 160 | Loss: 6.007 | Acc: 42.430,53.227,65.717,76.422,84.399,87.728,88.364,88.602,%
Batch: 180 | Loss: 6.024 | Acc: 42.356,53.168,65.573,76.291,84.284,87.465,88.247,88.506,%
Batch: 200 | Loss: 6.015 | Acc: 42.296,53.401,65.679,76.489,84.391,87.442,88.196,88.429,%
Batch: 220 | Loss: 6.014 | Acc: 42.396,53.397,65.607,76.545,84.368,87.504,88.214,88.462,%
Batch: 240 | Loss: 6.008 | Acc: 42.311,53.436,65.573,76.640,84.362,87.523,88.220,88.456,%
Batch: 260 | Loss: 6.034 | Acc: 42.388,53.409,65.350,76.446,84.189,87.371,88.105,88.347,%
Batch: 280 | Loss: 6.045 | Acc: 42.388,53.275,65.339,76.451,84.097,87.358,88.059,88.320,%
Batch: 300 | Loss: 6.048 | Acc: 42.447,53.398,65.373,76.389,84.001,87.347,88.045,88.279,%
Batch: 320 | Loss: 6.048 | Acc: 42.421,53.432,65.384,76.348,84.046,87.344,88.043,88.264,%
Batch: 340 | Loss: 6.041 | Acc: 42.476,53.482,65.412,76.386,84.107,87.337,88.052,88.279,%
Batch: 360 | Loss: 6.044 | Acc: 42.439,53.421,65.367,76.385,84.089,87.314,88.041,88.236,%
Batch: 380 | Loss: 6.038 | Acc: 42.518,53.486,65.424,76.429,84.067,87.268,88.039,88.212,%
Batch: 0 | Loss: 9.168 | Acc: 31.250,38.281,53.125,75.000,86.719,86.719,89.062,88.281,%
Batch: 20 | Loss: 9.644 | Acc: 32.068,36.161,54.353,69.531,80.320,83.445,83.705,84.152,%
Batch: 40 | Loss: 9.610 | Acc: 32.317,36.890,56.117,70.408,80.469,83.670,84.165,84.432,%
Batch: 60 | Loss: 9.640 | Acc: 32.339,37.334,55.738,69.890,80.085,83.478,83.799,84.401,%

Epoch: 10
Batch: 0 | Loss: 6.611 | Acc: 40.625,48.438,58.594,67.188,83.594,85.938,86.719,85.156,%
Batch: 20 | Loss: 5.931 | Acc: 44.234,53.497,63.690,76.302,85.082,88.281,88.653,88.430,%
Batch: 40 | Loss: 5.952 | Acc: 42.912,53.030,64.653,76.905,85.671,88.205,88.948,88.948,%
Batch: 60 | Loss: 5.927 | Acc: 43.238,53.343,65.228,77.690,85.592,88.115,88.665,89.024,%
Batch: 80 | Loss: 5.908 | Acc: 43.605,53.877,65.577,77.479,85.320,88.011,88.812,89.188,%
Batch: 100 | Loss: 5.901 | Acc: 43.611,54.069,65.300,77.614,85.373,88.173,88.916,89.217,%
Batch: 120 | Loss: 5.895 | Acc: 43.769,54.229,65.444,77.473,85.279,88.146,88.888,89.172,%
Batch: 140 | Loss: 5.898 | Acc: 43.390,54.449,65.653,77.366,85.245,88.248,88.930,89.157,%
Batch: 160 | Loss: 5.904 | Acc: 43.391,54.581,65.800,77.135,85.195,88.145,88.796,88.999,%
Batch: 180 | Loss: 5.894 | Acc: 43.323,54.554,65.854,77.093,85.139,88.212,88.791,89.015,%
Batch: 200 | Loss: 5.890 | Acc: 43.264,54.571,65.963,76.951,85.125,88.157,88.767,88.989,%
Batch: 220 | Loss: 5.892 | Acc: 43.195,54.514,65.954,77.043,85.132,88.172,88.773,89.013,%
Batch: 240 | Loss: 5.892 | Acc: 43.179,54.529,65.969,77.117,85.130,88.178,88.761,88.962,%
Batch: 260 | Loss: 5.857 | Acc: 43.295,54.729,66.230,77.227,85.207,88.290,88.892,89.086,%
Batch: 280 | Loss: 5.860 | Acc: 43.380,54.754,66.164,77.180,85.176,88.240,88.854,89.057,%
Batch: 300 | Loss: 5.855 | Acc: 43.529,54.809,66.175,77.240,85.203,88.224,88.855,89.037,%
Batch: 320 | Loss: 5.847 | Acc: 43.606,54.848,66.309,77.315,85.251,88.199,88.795,88.968,%
Batch: 340 | Loss: 5.847 | Acc: 43.562,54.820,66.276,77.270,85.259,88.210,88.776,88.934,%
Batch: 360 | Loss: 5.848 | Acc: 43.544,54.811,66.285,77.201,85.243,88.190,88.744,88.920,%
Batch: 380 | Loss: 5.866 | Acc: 43.412,54.667,66.205,77.100,85.121,88.136,88.708,88.898,%
Batch: 0 | Loss: 8.795 | Acc: 34.375,45.312,60.938,75.000,89.062,92.188,92.969,89.844,%
Batch: 20 | Loss: 9.405 | Acc: 35.007,42.746,57.812,68.192,79.501,83.222,83.743,84.115,%
Batch: 40 | Loss: 9.362 | Acc: 35.766,43.236,58.689,68.845,79.688,83.479,84.356,84.546,%
Batch: 60 | Loss: 9.332 | Acc: 35.976,43.507,58.427,69.493,79.764,84.029,84.759,84.977,%

Epoch: 11
Batch: 0 | Loss: 5.930 | Acc: 38.281,54.688,70.312,80.469,86.719,87.500,89.062,89.062,%
Batch: 20 | Loss: 5.781 | Acc: 42.746,54.167,66.741,78.311,85.342,88.281,89.397,89.397,%
Batch: 40 | Loss: 5.800 | Acc: 42.702,54.688,66.101,77.915,85.480,88.567,89.463,89.768,%
Batch: 60 | Loss: 5.711 | Acc: 42.879,54.649,66.573,78.330,85.822,89.062,89.933,90.087,%
Batch: 80 | Loss: 5.697 | Acc: 42.882,54.572,66.319,78.270,85.957,89.140,90.056,90.123,%
Batch: 100 | Loss: 5.702 | Acc: 42.922,54.262,66.422,77.963,86.115,89.186,89.882,90.045,%
Batch: 120 | Loss: 5.704 | Acc: 42.820,54.442,66.665,77.738,86.067,89.172,89.786,89.921,%
Batch: 140 | Loss: 5.680 | Acc: 42.880,54.671,66.539,77.970,86.165,89.334,89.999,90.110,%
Batch: 160 | Loss: 5.675 | Acc: 43.012,54.678,66.629,77.984,86.170,89.252,89.951,90.120,%
Batch: 180 | Loss: 5.682 | Acc: 43.116,54.700,66.518,78.060,86.024,89.110,89.896,90.029,%
Batch: 200 | Loss: 5.703 | Acc: 43.008,54.680,66.527,78.039,85.860,88.880,89.754,89.933,%
Batch: 220 | Loss: 5.705 | Acc: 43.156,54.677,66.548,78.044,85.803,88.861,89.734,89.904,%
Batch: 240 | Loss: 5.705 | Acc: 43.150,54.720,66.552,78.070,85.869,88.884,89.727,89.831,%
Batch: 260 | Loss: 5.704 | Acc: 43.112,54.759,66.622,78.131,85.857,88.886,89.736,89.844,%
Batch: 280 | Loss: 5.706 | Acc: 43.144,54.793,66.648,78.067,85.804,88.879,89.744,89.852,%
Batch: 300 | Loss: 5.701 | Acc: 43.246,54.882,66.627,78.104,85.831,88.928,89.724,89.815,%
Batch: 320 | Loss: 5.710 | Acc: 43.234,54.865,66.599,78.028,85.723,88.873,89.688,89.798,%
Batch: 340 | Loss: 5.711 | Acc: 43.267,54.791,66.548,78.033,85.729,88.888,89.638,89.720,%
Batch: 360 | Loss: 5.717 | Acc: 43.174,54.824,66.532,77.999,85.738,88.883,89.647,89.716,%
Batch: 380 | Loss: 5.720 | Acc: 43.276,54.813,66.568,77.984,85.720,88.859,89.628,89.717,%
Batch: 0 | Loss: 9.112 | Acc: 35.938,47.656,57.812,80.469,85.156,86.719,90.625,88.281,%
Batch: 20 | Loss: 9.519 | Acc: 34.859,45.610,53.795,71.280,79.762,83.891,84.152,84.710,%
Batch: 40 | Loss: 9.425 | Acc: 35.595,46.627,54.135,71.704,80.221,84.794,84.680,85.404,%
Batch: 60 | Loss: 9.431 | Acc: 35.502,46.542,53.548,71.862,80.571,84.900,84.644,85.540,%

Epoch: 12
Batch: 0 | Loss: 6.043 | Acc: 46.875,53.125,62.500,75.000,82.031,88.281,89.062,87.500,%
Batch: 20 | Loss: 5.651 | Acc: 44.829,54.948,67.820,78.869,84.375,88.988,90.439,90.365,%
Batch: 40 | Loss: 5.691 | Acc: 44.207,54.421,67.035,77.858,85.309,89.082,90.358,90.415,%
Batch: 60 | Loss: 5.702 | Acc: 43.609,53.881,66.995,77.651,85.515,89.114,90.241,90.164,%
Batch: 80 | Loss: 5.604 | Acc: 43.943,54.678,67.612,78.241,85.909,89.390,90.394,90.365,%
Batch: 100 | Loss: 5.587 | Acc: 43.889,55.020,67.435,78.419,86.061,89.480,90.362,90.401,%
Batch: 120 | Loss: 5.609 | Acc: 43.640,55.114,67.452,78.383,85.989,89.527,90.367,90.431,%
Batch: 140 | Loss: 5.594 | Acc: 43.822,55.236,67.398,78.480,86.148,89.578,90.392,90.448,%
Batch: 160 | Loss: 5.575 | Acc: 44.017,55.085,67.493,78.523,86.340,89.587,90.421,90.460,%
Batch: 180 | Loss: 5.576 | Acc: 44.005,55.365,67.589,78.475,86.248,89.555,90.405,90.474,%
Batch: 200 | Loss: 5.583 | Acc: 43.991,55.271,67.495,78.378,86.241,89.560,90.372,90.505,%
Batch: 220 | Loss: 5.580 | Acc: 43.923,55.080,67.385,78.387,86.312,89.582,90.399,90.590,%
Batch: 240 | Loss: 5.576 | Acc: 43.808,55.206,67.376,78.436,86.310,89.601,90.405,90.593,%
Batch: 260 | Loss: 5.583 | Acc: 43.840,55.217,67.388,78.394,86.279,89.547,90.332,90.505,%
Batch: 280 | Loss: 5.577 | Acc: 43.931,55.363,67.507,78.470,86.307,89.516,90.325,90.511,%
Batch: 300 | Loss: 5.586 | Acc: 43.888,55.422,67.442,78.408,86.200,89.434,90.298,90.498,%
Batch: 320 | Loss: 5.591 | Acc: 43.894,55.364,67.377,78.337,86.183,89.449,90.292,90.503,%
Batch: 340 | Loss: 5.594 | Acc: 43.897,55.409,67.341,78.228,86.137,89.452,90.249,90.465,%
Batch: 360 | Loss: 5.589 | Acc: 43.988,55.477,67.436,78.268,86.108,89.452,90.238,90.452,%
Batch: 380 | Loss: 5.596 | Acc: 43.949,55.432,67.462,78.207,86.095,89.405,90.215,90.414,%
Batch: 0 | Loss: 8.880 | Acc: 34.375,47.656,60.938,77.344,86.719,90.625,89.062,89.062,%
Batch: 20 | Loss: 9.330 | Acc: 32.664,46.838,58.891,71.057,80.469,84.970,85.231,85.863,%
Batch: 40 | Loss: 9.269 | Acc: 32.965,46.970,58.975,71.932,80.488,84.870,85.442,85.957,%
Batch: 60 | Loss: 9.277 | Acc: 32.838,46.939,58.402,72.323,80.827,85.207,85.412,85.938,%

Epoch: 13
Batch: 0 | Loss: 5.462 | Acc: 40.625,58.594,69.531,78.125,88.281,90.625,91.406,91.406,%
Batch: 20 | Loss: 5.391 | Acc: 44.606,56.101,69.643,78.311,87.202,90.327,90.811,90.737,%
Batch: 40 | Loss: 5.434 | Acc: 44.284,55.755,68.998,78.525,87.309,90.282,90.892,91.025,%
Batch: 60 | Loss: 5.407 | Acc: 44.390,56.032,68.507,79.047,87.321,90.471,91.009,91.112,%
Batch: 80 | Loss: 5.462 | Acc: 43.846,55.189,68.355,78.810,87.249,90.210,91.040,91.088,%
Batch: 100 | Loss: 5.431 | Acc: 43.881,55.391,68.448,78.844,87.183,90.385,91.190,91.321,%
Batch: 120 | Loss: 5.414 | Acc: 43.685,55.417,68.414,79.029,87.203,90.386,91.290,91.439,%
Batch: 140 | Loss: 5.400 | Acc: 43.778,55.574,68.384,79.172,87.273,90.431,91.323,91.534,%
Batch: 160 | Loss: 5.406 | Acc: 43.993,55.546,68.265,79.071,87.199,90.445,91.319,91.494,%
Batch: 180 | Loss: 5.432 | Acc: 43.987,55.616,68.103,79.057,87.073,90.310,91.177,91.363,%
Batch: 200 | Loss: 5.433 | Acc: 44.007,55.636,68.097,78.996,87.100,90.322,91.177,91.340,%
Batch: 220 | Loss: 5.449 | Acc: 43.955,55.547,67.976,78.935,86.910,90.240,91.046,91.233,%
Batch: 240 | Loss: 5.465 | Acc: 44.032,55.563,67.904,78.851,86.852,90.187,90.965,91.153,%
Batch: 260 | Loss: 5.448 | Acc: 44.160,55.687,68.118,78.951,86.859,90.233,90.966,91.155,%
Batch: 280 | Loss: 5.456 | Acc: 44.014,55.591,68.041,78.959,86.816,90.222,90.950,91.139,%
Batch: 300 | Loss: 5.458 | Acc: 44.036,55.645,68.052,79.005,86.807,90.199,90.936,91.110,%
Batch: 320 | Loss: 5.469 | Acc: 44.025,55.622,67.981,78.994,86.814,90.102,90.822,90.993,%
Batch: 340 | Loss: 5.459 | Acc: 44.167,55.716,68.024,79.080,86.859,90.126,90.836,91.031,%
Batch: 360 | Loss: 5.471 | Acc: 44.148,55.752,68.012,79.021,86.786,90.058,90.776,90.971,%
Batch: 380 | Loss: 5.472 | Acc: 44.160,55.758,68.010,79.081,86.780,90.036,90.762,90.951,%
Batch: 0 | Loss: 8.910 | Acc: 34.375,48.438,56.250,76.562,85.156,86.719,90.625,85.156,%
Batch: 20 | Loss: 9.278 | Acc: 29.390,44.792,58.519,74.107,81.473,84.561,86.644,85.826,%
Batch: 40 | Loss: 9.196 | Acc: 29.211,45.655,59.546,74.543,81.498,85.175,86.719,86.319,%
Batch: 60 | Loss: 9.206 | Acc: 29.380,45.671,58.568,74.257,81.660,85.310,86.424,86.258,%

Epoch: 14
Batch: 0 | Loss: 4.707 | Acc: 46.875,61.719,71.094,79.688,89.844,92.969,92.188,92.969,%
Batch: 20 | Loss: 5.379 | Acc: 44.048,56.287,68.824,78.832,86.235,90.588,91.741,91.964,%
Batch: 40 | Loss: 5.322 | Acc: 44.836,57.127,68.502,79.249,86.890,90.949,91.845,91.959,%
Batch: 60 | Loss: 5.252 | Acc: 45.325,56.839,69.198,79.675,87.654,91.445,92.098,92.200,%
Batch: 80 | Loss: 5.308 | Acc: 44.975,56.636,68.818,79.350,87.172,91.020,91.705,91.898,%
Batch: 100 | Loss: 5.306 | Acc: 44.887,56.575,68.758,79.664,87.252,91.074,91.677,91.824,%
Batch: 120 | Loss: 5.313 | Acc: 44.596,56.418,68.640,79.565,87.203,91.045,91.645,91.826,%
Batch: 140 | Loss: 5.322 | Acc: 44.459,56.400,68.628,79.632,87.350,91.135,91.678,91.877,%
Batch: 160 | Loss: 5.315 | Acc: 44.439,56.444,68.852,79.615,87.393,91.071,91.620,91.848,%
Batch: 180 | Loss: 5.318 | Acc: 44.415,56.449,68.884,79.610,87.345,90.988,91.540,91.747,%
Batch: 200 | Loss: 5.323 | Acc: 44.578,56.545,68.758,79.505,87.290,90.932,91.519,91.741,%
Batch: 220 | Loss: 5.337 | Acc: 44.627,56.614,68.655,79.334,87.161,90.844,91.480,91.707,%
Batch: 240 | Loss: 5.342 | Acc: 44.768,56.581,68.620,79.315,87.211,90.794,91.397,91.585,%
Batch: 260 | Loss: 5.340 | Acc: 44.816,56.552,68.597,79.343,87.198,90.766,91.394,91.559,%
Batch: 280 | Loss: 5.350 | Acc: 44.781,56.489,68.497,79.307,87.139,90.683,91.292,91.451,%
Batch: 300 | Loss: 5.351 | Acc: 44.850,56.507,68.628,79.350,87.087,90.599,91.214,91.385,%
Batch: 320 | Loss: 5.362 | Acc: 44.731,56.381,68.499,79.332,87.050,90.559,91.202,91.367,%
Batch: 340 | Loss: 5.360 | Acc: 44.728,56.431,68.530,79.422,87.099,90.625,91.250,91.422,%
Batch: 360 | Loss: 5.366 | Acc: 44.724,56.447,68.562,79.434,87.145,90.610,91.229,91.398,%
Batch: 380 | Loss: 5.370 | Acc: 44.749,56.492,68.621,79.468,87.108,90.529,91.142,91.308,%
Batch: 0 | Loss: 9.337 | Acc: 35.938,46.875,59.375,76.562,78.125,85.156,85.938,87.500,%
Batch: 20 | Loss: 9.553 | Acc: 33.110,46.875,60.417,71.875,79.762,85.007,85.900,86.161,%
Traceback (most recent call last):
  File "main.py", line 259, in <module>
    main_cifar(args)
  File "main.py", line 249, in main_cifar
    test(epoch)
  File "main.py", line 193, in test
    test_loss += to_python_float(loss.data)
  File "main.py", line 113, in to_python_float
    return t.item()
KeyboardInterrupt
