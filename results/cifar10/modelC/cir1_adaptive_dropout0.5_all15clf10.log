Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=0 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=0 | Loss: 4.886 |  Acc: 33.080,42.172,46.916,% 
Testing: Epoch=0 | Loss: 4.154 |  Acc: 39.880,51.980,59.560,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=1 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=1 | Loss: 3.793 |  Acc: 44.616,57.334,63.852,% 
Testing: Epoch=1 | Loss: 3.854 |  Acc: 41.720,56.400,62.970,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=2 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=2 | Loss: 3.343 |  Acc: 49.650,63.236,70.294,% 
Testing: Epoch=2 | Loss: 3.378 |  Acc: 46.170,63.340,70.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=3 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=3 | Loss: 3.025 |  Acc: 52.720,67.430,74.788,% 
Testing: Epoch=3 | Loss: 2.993 |  Acc: 51.380,68.150,75.490,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=4 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=4 | Loss: 2.788 |  Acc: 55.232,70.370,78.070,% 
Testing: Epoch=4 | Loss: 2.859 |  Acc: 51.550,70.980,78.060,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=5 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=5 | Loss: 2.574 |  Acc: 57.314,73.364,81.098,% 
Testing: Epoch=5 | Loss: 2.697 |  Acc: 54.580,71.760,80.370,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=6 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=6 | Loss: 2.422 |  Acc: 58.652,75.366,82.774,% 
Testing: Epoch=6 | Loss: 2.617 |  Acc: 54.480,72.720,80.840,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=7 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=7 | Loss: 2.283 |  Acc: 60.426,77.356,84.486,% 
Testing: Epoch=7 | Loss: 2.453 |  Acc: 56.950,75.180,83.530,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=8 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=8 | Loss: 2.200 |  Acc: 61.090,78.692,85.446,% 
Testing: Epoch=8 | Loss: 2.374 |  Acc: 57.690,77.260,83.180,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=9 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=9 | Loss: 2.086 |  Acc: 62.496,80.078,86.740,% 
Testing: Epoch=9 | Loss: 2.316 |  Acc: 58.020,78.440,84.540,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=10 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=10 | Loss: 2.017 |  Acc: 63.002,81.252,87.396,% 
Testing: Epoch=10 | Loss: 2.159 |  Acc: 61.120,79.480,86.160,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=11 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=11 | Loss: 1.934 |  Acc: 63.902,82.252,88.298,% 
Testing: Epoch=11 | Loss: 2.252 |  Acc: 58.790,78.690,85.520,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=12 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=12 | Loss: 1.876 |  Acc: 64.664,83.188,88.870,% 
Testing: Epoch=12 | Loss: 2.121 |  Acc: 61.760,79.600,86.630,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=13 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=13 | Loss: 1.821 |  Acc: 65.262,83.696,89.458,% 
Testing: Epoch=13 | Loss: 2.063 |  Acc: 61.670,80.940,86.920,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=14 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=14 | Loss: 1.789 |  Acc: 65.768,84.116,90.032,% 
Testing: Epoch=14 | Loss: 1.996 |  Acc: 63.040,81.690,87.110,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=15 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=15 | Loss: 1.905 |  Acc: 64.006,82.938,88.674,% 
Testing: Epoch=15 | Loss: 2.091 |  Acc: 62.520,81.020,85.950,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=16 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=16 | Loss: 1.852 |  Acc: 64.806,83.860,89.200,% 
Testing: Epoch=16 | Loss: 2.030 |  Acc: 63.570,81.800,86.320,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=17 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=17 | Loss: 1.829 |  Acc: 64.796,84.376,89.522,% 
Testing: Epoch=17 | Loss: 2.009 |  Acc: 63.780,82.060,86.700,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=18 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=18 | Loss: 1.802 |  Acc: 65.476,84.512,89.744,% 
Testing: Epoch=18 | Loss: 1.999 |  Acc: 63.800,82.120,86.510,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=19 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=19 | Loss: 1.783 |  Acc: 65.604,84.602,89.738,% 
Testing: Epoch=19 | Loss: 1.980 |  Acc: 64.300,82.700,86.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=20 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=20 | Loss: 1.784 |  Acc: 65.628,84.690,89.876,% 
Testing: Epoch=20 | Loss: 1.970 |  Acc: 64.330,82.540,86.740,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=21 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=21 | Loss: 1.763 |  Acc: 66.066,84.910,90.024,% 
Testing: Epoch=21 | Loss: 1.967 |  Acc: 64.510,82.300,86.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=22 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=22 | Loss: 1.762 |  Acc: 65.840,84.966,90.268,% 
Testing: Epoch=22 | Loss: 1.954 |  Acc: 64.640,82.790,87.010,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=23 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=23 | Loss: 1.763 |  Acc: 66.062,84.870,89.858,% 
Testing: Epoch=23 | Loss: 1.952 |  Acc: 64.850,82.660,86.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=24 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=24 | Loss: 1.754 |  Acc: 66.128,84.968,89.962,% 
Testing: Epoch=24 | Loss: 1.944 |  Acc: 64.830,82.650,87.180,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=25 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=25 | Loss: 2.018 |  Acc: 63.606,81.270,86.318,% 
Testing: Epoch=25 | Loss: 2.222 |  Acc: 61.740,78.290,83.050,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=26 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=26 | Loss: 1.854 |  Acc: 65.254,83.484,88.512,% 
Testing: Epoch=26 | Loss: 2.113 |  Acc: 63.510,78.670,85.710,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=27 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=27 | Loss: 1.753 |  Acc: 66.562,84.690,89.580,% 
Testing: Epoch=27 | Loss: 1.952 |  Acc: 63.830,81.690,87.310,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=28 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=28 | Loss: 1.703 |  Acc: 66.848,85.338,90.364,% 
Testing: Epoch=28 | Loss: 1.969 |  Acc: 63.720,81.990,86.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=29 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=29 | Loss: 1.643 |  Acc: 67.764,86.002,90.958,% 
Testing: Epoch=29 | Loss: 1.850 |  Acc: 65.910,83.750,87.400,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=30 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=30 | Loss: 1.602 |  Acc: 68.078,86.452,91.524,% 
Testing: Epoch=30 | Loss: 1.893 |  Acc: 63.340,83.910,88.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=31 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=31 | Loss: 1.566 |  Acc: 68.658,87.028,92.036,% 
Testing: Epoch=31 | Loss: 1.969 |  Acc: 62.510,82.430,88.100,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=32 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=32 | Loss: 1.526 |  Acc: 68.852,87.540,92.742,% 
Testing: Epoch=32 | Loss: 2.027 |  Acc: 62.770,80.740,87.210,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=33 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=33 | Loss: 1.503 |  Acc: 69.218,87.658,92.852,% 
Testing: Epoch=33 | Loss: 1.820 |  Acc: 65.000,83.610,89.190,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=34 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=34 | Loss: 1.469 |  Acc: 69.794,88.270,93.276,% 
Testing: Epoch=34 | Loss: 1.784 |  Acc: 68.200,84.260,87.500,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=35 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=35 | Loss: 1.461 |  Acc: 69.880,88.156,93.288,% 
Testing: Epoch=35 | Loss: 1.795 |  Acc: 65.560,84.150,88.490,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=36 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=36 | Loss: 1.426 |  Acc: 70.250,88.680,93.634,% 
Testing: Epoch=36 | Loss: 1.927 |  Acc: 64.670,82.430,87.300,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=37 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=37 | Loss: 1.410 |  Acc: 70.680,88.820,93.838,% 
Testing: Epoch=37 | Loss: 1.716 |  Acc: 67.620,85.610,89.500,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=38 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=38 | Loss: 1.406 |  Acc: 70.664,88.934,93.848,% 
Testing: Epoch=38 | Loss: 1.705 |  Acc: 67.600,84.630,89.490,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=39 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=39 | Loss: 1.370 |  Acc: 70.960,89.406,94.322,% 
Testing: Epoch=39 | Loss: 1.789 |  Acc: 65.890,84.230,88.630,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=40 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=40 | Loss: 1.521 |  Acc: 69.272,87.882,92.318,% 
Testing: Epoch=40 | Loss: 1.771 |  Acc: 67.090,84.860,87.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=41 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=41 | Loss: 1.486 |  Acc: 69.760,88.160,92.910,% 
Testing: Epoch=41 | Loss: 1.739 |  Acc: 67.990,85.170,88.510,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=42 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=42 | Loss: 1.454 |  Acc: 69.940,88.508,93.208,% 
Testing: Epoch=42 | Loss: 1.723 |  Acc: 68.230,85.250,88.730,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=43 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=43 | Loss: 1.436 |  Acc: 70.442,88.962,93.462,% 
Testing: Epoch=43 | Loss: 1.697 |  Acc: 68.290,85.460,88.760,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=44 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=44 | Loss: 1.431 |  Acc: 70.522,88.916,93.514,% 
Testing: Epoch=44 | Loss: 1.698 |  Acc: 68.610,85.370,89.050,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=45 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=45 | Loss: 1.412 |  Acc: 70.898,88.980,93.550,% 
Testing: Epoch=45 | Loss: 1.695 |  Acc: 68.680,85.100,89.030,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=46 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=46 | Loss: 1.420 |  Acc: 70.570,88.964,93.388,% 
Testing: Epoch=46 | Loss: 1.674 |  Acc: 68.970,85.680,89.160,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=47 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=47 | Loss: 1.406 |  Acc: 70.842,89.232,93.676,% 
Testing: Epoch=47 | Loss: 1.680 |  Acc: 69.100,85.590,89.100,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=48 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=48 | Loss: 1.396 |  Acc: 71.082,89.172,93.708,% 
Testing: Epoch=48 | Loss: 1.661 |  Acc: 69.000,85.990,89.190,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=49 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=49 | Loss: 1.386 |  Acc: 71.400,89.296,93.734,% 
Testing: Epoch=49 | Loss: 1.667 |  Acc: 68.440,85.690,89.350,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=50 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=50 | Loss: 1.790 |  Acc: 66.826,84.020,88.884,% 
Testing: Epoch=50 | Loss: 1.947 |  Acc: 66.380,82.230,86.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=51 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=51 | Loss: 1.577 |  Acc: 69.038,86.538,91.292,% 
Testing: Epoch=51 | Loss: 1.749 |  Acc: 68.730,85.130,88.650,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=52 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=52 | Loss: 1.494 |  Acc: 69.780,87.644,92.330,% 
Testing: Epoch=52 | Loss: 1.774 |  Acc: 67.360,84.490,89.040,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=53 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=53 | Loss: 1.465 |  Acc: 70.254,88.120,92.866,% 
Testing: Epoch=53 | Loss: 1.851 |  Acc: 64.260,84.020,88.560,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=54 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=54 | Loss: 1.417 |  Acc: 70.734,88.724,93.388,% 
Testing: Epoch=54 | Loss: 1.845 |  Acc: 65.090,84.330,88.500,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=55 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=55 | Loss: 1.379 |  Acc: 71.308,89.214,93.836,% 
Testing: Epoch=55 | Loss: 1.727 |  Acc: 68.300,84.950,89.180,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=56 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=56 | Loss: 1.350 |  Acc: 71.394,89.614,94.252,% 
Testing: Epoch=56 | Loss: 1.719 |  Acc: 67.960,84.540,88.730,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=57 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=57 | Loss: 1.337 |  Acc: 72.026,89.402,94.466,% 
Testing: Epoch=57 | Loss: 1.714 |  Acc: 70.100,84.500,88.700,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=58 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=58 | Loss: 1.323 |  Acc: 71.860,89.876,94.634,% 
Testing: Epoch=58 | Loss: 1.605 |  Acc: 70.100,86.110,90.080,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=59 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=59 | Loss: 1.299 |  Acc: 72.382,89.912,94.788,% 
Testing: Epoch=59 | Loss: 1.817 |  Acc: 66.570,83.370,88.700,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=60 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=60 | Loss: 1.287 |  Acc: 72.514,90.128,94.878,% 
Testing: Epoch=60 | Loss: 1.839 |  Acc: 66.450,82.390,88.890,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=61 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=61 | Loss: 1.277 |  Acc: 72.684,90.218,95.050,% 
Testing: Epoch=61 | Loss: 1.628 |  Acc: 69.440,85.980,89.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=62 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=62 | Loss: 1.266 |  Acc: 72.590,90.408,95.090,% 
Testing: Epoch=62 | Loss: 1.737 |  Acc: 67.820,84.340,88.720,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=63 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=63 | Loss: 1.254 |  Acc: 73.038,90.562,95.224,% 
Testing: Epoch=63 | Loss: 1.665 |  Acc: 69.360,85.130,89.230,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=64 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=64 | Loss: 1.241 |  Acc: 73.146,90.680,95.392,% 
Testing: Epoch=64 | Loss: 1.780 |  Acc: 67.270,83.070,89.350,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=65 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=65 | Loss: 1.532 |  Acc: 69.882,87.498,92.024,% 
Testing: Epoch=65 | Loss: 1.743 |  Acc: 69.100,85.070,87.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=66 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=66 | Loss: 1.455 |  Acc: 71.142,88.378,92.758,% 
Testing: Epoch=66 | Loss: 1.696 |  Acc: 70.090,85.570,88.540,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=67 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=67 | Loss: 1.426 |  Acc: 71.306,88.604,93.078,% 
Testing: Epoch=67 | Loss: 1.680 |  Acc: 69.760,85.830,88.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=68 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=68 | Loss: 1.404 |  Acc: 71.838,88.956,93.522,% 
Testing: Epoch=68 | Loss: 1.666 |  Acc: 70.190,85.780,88.800,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=69 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=69 | Loss: 1.388 |  Acc: 71.900,89.142,93.468,% 
Testing: Epoch=69 | Loss: 1.658 |  Acc: 70.280,85.610,89.030,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=70 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=70 | Loss: 1.390 |  Acc: 71.968,89.148,93.430,% 
Testing: Epoch=70 | Loss: 1.658 |  Acc: 70.150,85.960,89.140,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=71 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=71 | Loss: 1.385 |  Acc: 72.002,89.042,93.764,% 
Testing: Epoch=71 | Loss: 1.640 |  Acc: 70.420,85.930,89.250,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=72 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=72 | Loss: 1.369 |  Acc: 72.098,89.456,93.788,% 
Testing: Epoch=72 | Loss: 1.629 |  Acc: 70.890,86.050,89.220,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=73 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=73 | Loss: 1.371 |  Acc: 72.136,89.248,93.676,% 
Testing: Epoch=73 | Loss: 1.627 |  Acc: 70.210,86.060,89.530,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=74 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=74 | Loss: 1.365 |  Acc: 72.342,89.422,93.880,% 
Testing: Epoch=74 | Loss: 1.623 |  Acc: 70.770,86.250,89.530,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=75 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=75 | Loss: 1.671 |  Acc: 68.604,85.106,89.718,% 
Testing: Epoch=75 | Loss: 1.939 |  Acc: 66.040,82.800,86.380,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=76 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=76 | Loss: 1.471 |  Acc: 70.858,87.896,92.208,% 
Testing: Epoch=76 | Loss: 1.759 |  Acc: 65.890,84.830,88.580,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=77 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=77 | Loss: 1.397 |  Acc: 71.560,88.590,93.120,% 
Testing: Epoch=77 | Loss: 1.732 |  Acc: 67.140,86.000,89.160,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=78 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=78 | Loss: 1.364 |  Acc: 72.204,89.170,93.632,% 
Testing: Epoch=78 | Loss: 1.768 |  Acc: 67.830,84.460,88.560,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=79 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=79 | Loss: 1.316 |  Acc: 72.802,89.648,94.226,% 
Testing: Epoch=79 | Loss: 1.738 |  Acc: 67.900,83.960,89.390,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=80 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=80 | Loss: 1.305 |  Acc: 72.904,89.780,94.278,% 
Testing: Epoch=80 | Loss: 2.199 |  Acc: 59.430,78.290,84.650,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=81 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=81 | Loss: 1.267 |  Acc: 73.492,90.340,94.666,% 
Testing: Epoch=81 | Loss: 1.740 |  Acc: 69.130,84.720,88.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=82 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=82 | Loss: 1.257 |  Acc: 73.612,90.386,94.806,% 
Testing: Epoch=82 | Loss: 1.642 |  Acc: 69.500,85.740,89.680,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=83 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=83 | Loss: 1.229 |  Acc: 73.832,90.740,95.176,% 
Testing: Epoch=83 | Loss: 1.630 |  Acc: 70.060,86.360,89.750,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=84 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=84 | Loss: 1.213 |  Acc: 74.112,90.962,95.418,% 
Testing: Epoch=84 | Loss: 1.736 |  Acc: 65.080,85.190,89.720,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=85 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=85 | Loss: 1.218 |  Acc: 73.866,90.788,95.424,% 
Testing: Epoch=85 | Loss: 1.644 |  Acc: 70.450,85.570,89.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=86 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=86 | Loss: 1.190 |  Acc: 74.284,91.128,95.698,% 
Testing: Epoch=86 | Loss: 1.653 |  Acc: 69.260,85.490,89.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=87 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=87 | Loss: 1.181 |  Acc: 74.430,91.292,95.692,% 
Testing: Epoch=87 | Loss: 1.616 |  Acc: 69.570,85.590,89.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=88 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=88 | Loss: 1.178 |  Acc: 74.630,91.306,95.804,% 
Testing: Epoch=88 | Loss: 1.599 |  Acc: 70.890,85.850,90.360,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=89 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=89 | Loss: 1.152 |  Acc: 74.748,91.526,96.140,% 
Testing: Epoch=89 | Loss: 1.541 |  Acc: 69.880,87.330,90.280,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=90 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=90 | Loss: 1.344 |  Acc: 72.088,89.532,94.078,% 
Testing: Epoch=90 | Loss: 1.637 |  Acc: 70.350,86.470,89.430,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=91 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=91 | Loss: 1.305 |  Acc: 72.788,89.938,94.558,% 
Testing: Epoch=91 | Loss: 1.608 |  Acc: 70.890,86.650,89.490,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=92 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=92 | Loss: 1.276 |  Acc: 73.248,90.316,94.792,% 
Testing: Epoch=92 | Loss: 1.589 |  Acc: 71.070,86.790,89.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=93 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=93 | Loss: 1.258 |  Acc: 73.296,90.628,95.044,% 
Testing: Epoch=93 | Loss: 1.581 |  Acc: 71.130,87.030,90.070,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=94 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=94 | Loss: 1.252 |  Acc: 73.530,90.706,95.126,% 
Testing: Epoch=94 | Loss: 1.570 |  Acc: 71.190,87.240,90.110,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=95 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=95 | Loss: 1.235 |  Acc: 74.000,90.676,95.062,% 
Testing: Epoch=95 | Loss: 1.555 |  Acc: 71.330,87.580,90.220,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=96 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=96 | Loss: 1.244 |  Acc: 73.862,90.854,95.182,% 
Testing: Epoch=96 | Loss: 1.563 |  Acc: 70.980,87.210,90.270,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=97 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=97 | Loss: 1.236 |  Acc: 73.928,90.868,95.328,% 
Testing: Epoch=97 | Loss: 1.550 |  Acc: 71.450,87.340,90.140,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=98 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=98 | Loss: 1.227 |  Acc: 73.904,91.008,95.364,% 
Testing: Epoch=98 | Loss: 1.544 |  Acc: 71.600,87.460,90.420,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=99 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=99 | Loss: 1.229 |  Acc: 73.980,90.946,95.300,% 
Testing: Epoch=99 | Loss: 1.545 |  Acc: 71.620,87.180,90.380,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=100 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=100 | Loss: 1.644 |  Acc: 69.282,85.436,89.890,% 
Testing: Epoch=100 | Loss: 2.013 |  Acc: 63.800,81.200,85.810,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=101 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=101 | Loss: 1.409 |  Acc: 71.916,88.270,92.774,% 
Testing: Epoch=101 | Loss: 1.808 |  Acc: 67.140,83.850,88.470,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=102 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=102 | Loss: 1.327 |  Acc: 73.180,89.432,93.682,% 
Testing: Epoch=102 | Loss: 1.847 |  Acc: 68.900,82.000,87.440,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=103 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=103 | Loss: 1.287 |  Acc: 73.604,89.910,94.284,% 
Testing: Epoch=103 | Loss: 1.597 |  Acc: 70.230,86.000,89.550,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=104 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=104 | Loss: 1.265 |  Acc: 73.624,90.212,94.532,% 
Testing: Epoch=104 | Loss: 1.941 |  Acc: 63.910,80.560,88.420,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=105 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=105 | Loss: 1.237 |  Acc: 73.804,90.542,94.988,% 
Testing: Epoch=105 | Loss: 1.630 |  Acc: 69.090,85.560,90.180,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=106 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=106 | Loss: 1.200 |  Acc: 74.512,91.020,95.324,% 
Testing: Epoch=106 | Loss: 1.695 |  Acc: 68.160,85.210,88.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=107 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=107 | Loss: 1.200 |  Acc: 74.466,90.978,95.212,% 
Testing: Epoch=107 | Loss: 1.493 |  Acc: 73.030,87.120,90.740,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=108 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=108 | Loss: 1.188 |  Acc: 75.022,91.000,95.470,% 
Testing: Epoch=108 | Loss: 1.604 |  Acc: 69.830,86.360,90.110,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=109 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=109 | Loss: 1.158 |  Acc: 75.002,91.338,95.786,% 
Testing: Epoch=109 | Loss: 1.809 |  Acc: 66.560,84.320,88.050,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=110 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=110 | Loss: 1.165 |  Acc: 74.888,91.440,95.914,% 
Testing: Epoch=110 | Loss: 1.595 |  Acc: 69.360,86.090,89.860,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=111 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=111 | Loss: 1.160 |  Acc: 75.196,91.536,95.900,% 
Testing: Epoch=111 | Loss: 1.560 |  Acc: 70.230,87.120,90.710,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=112 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=112 | Loss: 1.130 |  Acc: 75.410,91.848,96.146,% 
Testing: Epoch=112 | Loss: 1.693 |  Acc: 67.900,84.650,89.340,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=113 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=113 | Loss: 1.127 |  Acc: 75.446,91.888,96.186,% 
Testing: Epoch=113 | Loss: 1.692 |  Acc: 68.710,85.420,89.600,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=114 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=114 | Loss: 1.124 |  Acc: 75.328,91.672,96.182,% 
Testing: Epoch=114 | Loss: 1.966 |  Acc: 64.910,81.020,87.420,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=115 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=115 | Loss: 1.286 |  Acc: 72.756,90.056,94.798,% 
Testing: Epoch=115 | Loss: 1.568 |  Acc: 71.510,87.080,90.440,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=116 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=116 | Loss: 1.228 |  Acc: 73.500,90.838,95.300,% 
Testing: Epoch=116 | Loss: 1.553 |  Acc: 71.690,87.010,90.610,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=117 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=117 | Loss: 1.224 |  Acc: 74.112,90.954,95.398,% 
Testing: Epoch=117 | Loss: 1.539 |  Acc: 71.730,87.070,90.870,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=118 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=118 | Loss: 1.204 |  Acc: 74.412,91.218,95.598,% 
Testing: Epoch=118 | Loss: 1.538 |  Acc: 71.410,87.040,90.800,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=119 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=119 | Loss: 1.196 |  Acc: 74.428,91.240,95.790,% 
Testing: Epoch=119 | Loss: 1.515 |  Acc: 72.120,87.230,91.120,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=120 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=120 | Loss: 1.199 |  Acc: 74.340,91.408,95.606,% 
Testing: Epoch=120 | Loss: 1.504 |  Acc: 72.140,87.630,91.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=121 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=121 | Loss: 1.168 |  Acc: 74.978,91.420,95.956,% 
Testing: Epoch=121 | Loss: 1.507 |  Acc: 72.220,87.280,90.820,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=122 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=122 | Loss: 1.178 |  Acc: 74.880,91.378,95.804,% 
Testing: Epoch=122 | Loss: 1.513 |  Acc: 72.000,87.470,91.060,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=123 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=123 | Loss: 1.169 |  Acc: 74.682,91.438,95.910,% 
Testing: Epoch=123 | Loss: 1.520 |  Acc: 71.900,87.180,91.140,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=124 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=124 | Loss: 1.167 |  Acc: 75.090,91.340,95.852,% 
Testing: Epoch=124 | Loss: 1.510 |  Acc: 72.120,87.640,91.080,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=125 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=125 | Loss: 1.591 |  Acc: 70.458,86.234,90.520,% 
Testing: Epoch=125 | Loss: 1.980 |  Acc: 65.670,80.070,85.630,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=126 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=126 | Loss: 1.386 |  Acc: 72.572,88.666,92.960,% 
Testing: Epoch=126 | Loss: 1.695 |  Acc: 66.610,86.210,89.720,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=127 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=127 | Loss: 1.299 |  Acc: 73.402,89.712,94.184,% 
Testing: Epoch=127 | Loss: 1.723 |  Acc: 69.700,85.100,88.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=128 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=128 | Loss: 1.258 |  Acc: 74.060,90.170,94.550,% 
Testing: Epoch=128 | Loss: 1.681 |  Acc: 67.880,85.340,88.840,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=129 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=129 | Loss: 1.237 |  Acc: 74.258,90.358,94.766,% 
Testing: Epoch=129 | Loss: 1.595 |  Acc: 70.570,85.410,89.220,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=130 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=130 | Loss: 1.184 |  Acc: 74.940,91.050,95.308,% 
Testing: Epoch=130 | Loss: 1.723 |  Acc: 68.130,85.350,87.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=131 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=131 | Loss: 1.188 |  Acc: 74.972,91.072,95.414,% 
Testing: Epoch=131 | Loss: 1.512 |  Acc: 72.790,87.060,90.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=132 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=132 | Loss: 1.159 |  Acc: 75.346,91.420,95.744,% 
Testing: Epoch=132 | Loss: 1.542 |  Acc: 72.000,87.010,90.590,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=133 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=133 | Loss: 1.146 |  Acc: 75.250,91.402,95.778,% 
Testing: Epoch=133 | Loss: 1.715 |  Acc: 69.910,84.750,89.140,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=134 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=134 | Loss: 1.149 |  Acc: 75.648,91.506,95.748,% 
Testing: Epoch=134 | Loss: 1.536 |  Acc: 70.450,87.510,90.970,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=135 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=135 | Loss: 1.112 |  Acc: 75.928,92.064,96.394,% 
Testing: Epoch=135 | Loss: 1.471 |  Acc: 73.340,87.400,90.500,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=136 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=136 | Loss: 1.120 |  Acc: 75.780,91.752,96.004,% 
Testing: Epoch=136 | Loss: 1.596 |  Acc: 72.280,85.330,89.400,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=137 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=137 | Loss: 1.117 |  Acc: 75.876,91.794,96.268,% 
Testing: Epoch=137 | Loss: 1.691 |  Acc: 68.890,84.860,88.360,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=138 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=138 | Loss: 1.119 |  Acc: 75.916,91.756,96.104,% 
Testing: Epoch=138 | Loss: 1.506 |  Acc: 72.850,87.050,90.280,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=139 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=139 | Loss: 1.103 |  Acc: 76.250,91.890,96.184,% 
Testing: Epoch=139 | Loss: 1.529 |  Acc: 72.960,87.310,89.870,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=140 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=140 | Loss: 1.362 |  Acc: 72.006,89.704,93.648,% 
Testing: Epoch=140 | Loss: 1.633 |  Acc: 70.540,86.110,89.260,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=141 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=141 | Loss: 1.304 |  Acc: 73.428,90.008,94.162,% 
Testing: Epoch=141 | Loss: 1.597 |  Acc: 71.160,86.880,89.550,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=142 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=142 | Loss: 1.280 |  Acc: 73.590,90.266,94.468,% 
Testing: Epoch=142 | Loss: 1.596 |  Acc: 71.700,86.520,89.620,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=143 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=143 | Loss: 1.267 |  Acc: 73.956,90.542,94.714,% 
Testing: Epoch=143 | Loss: 1.571 |  Acc: 71.670,86.860,89.740,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=144 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=144 | Loss: 1.244 |  Acc: 74.238,90.644,94.868,% 
Testing: Epoch=144 | Loss: 1.560 |  Acc: 71.790,87.000,89.740,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=145 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=145 | Loss: 1.236 |  Acc: 74.498,90.744,94.856,% 
Testing: Epoch=145 | Loss: 1.565 |  Acc: 72.180,86.830,89.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=146 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=146 | Loss: 1.253 |  Acc: 74.106,90.790,94.858,% 
Testing: Epoch=146 | Loss: 1.549 |  Acc: 72.130,86.980,90.040,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=147 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=147 | Loss: 1.244 |  Acc: 74.388,90.668,94.912,% 
Testing: Epoch=147 | Loss: 1.550 |  Acc: 71.850,87.310,90.170,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=148 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=148 | Loss: 1.237 |  Acc: 74.252,90.748,94.918,% 
Testing: Epoch=148 | Loss: 1.541 |  Acc: 72.360,87.120,90.040,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=149 | lr=1.0e-02 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=149 | Loss: 1.240 |  Acc: 74.028,90.764,94.782,% 
Testing: Epoch=149 | Loss: 1.533 |  Acc: 72.540,87.420,90.040,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=150 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=150 | Loss: 0.923 |  Acc: 77.970,94.576,98.268,% 
Testing: Epoch=150 | Loss: 1.237 |  Acc: 76.110,90.180,93.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=151 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=151 | Loss: 0.844 |  Acc: 79.170,95.724,99.124,% 
Testing: Epoch=151 | Loss: 1.208 |  Acc: 76.990,90.520,93.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=152 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=152 | Loss: 0.814 |  Acc: 79.656,95.980,99.318,% 
Testing: Epoch=152 | Loss: 1.219 |  Acc: 76.680,90.190,93.350,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=153 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=153 | Loss: 0.797 |  Acc: 79.772,96.284,99.444,% 
Testing: Epoch=153 | Loss: 1.203 |  Acc: 76.920,90.530,93.450,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=154 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=154 | Loss: 0.780 |  Acc: 79.996,96.504,99.550,% 
Testing: Epoch=154 | Loss: 1.215 |  Acc: 76.740,90.350,93.220,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=155 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=155 | Loss: 0.770 |  Acc: 80.078,96.616,99.542,% 
Testing: Epoch=155 | Loss: 1.185 |  Acc: 77.530,90.710,93.560,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=156 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=156 | Loss: 0.771 |  Acc: 80.222,96.676,99.642,% 
Testing: Epoch=156 | Loss: 1.194 |  Acc: 77.750,90.480,93.210,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=157 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=157 | Loss: 0.748 |  Acc: 80.438,96.908,99.702,% 
Testing: Epoch=157 | Loss: 1.183 |  Acc: 77.410,90.620,93.700,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=158 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=158 | Loss: 0.752 |  Acc: 80.294,96.816,99.636,% 
Testing: Epoch=158 | Loss: 1.201 |  Acc: 77.580,90.590,93.470,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=159 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=159 | Loss: 0.744 |  Acc: 80.532,96.960,99.680,% 
Testing: Epoch=159 | Loss: 1.198 |  Acc: 77.460,90.680,93.320,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=160 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=160 | Loss: 0.739 |  Acc: 80.612,96.936,99.736,% 
Testing: Epoch=160 | Loss: 1.198 |  Acc: 77.630,90.700,93.180,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=161 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=161 | Loss: 0.744 |  Acc: 80.726,97.038,99.676,% 
Testing: Epoch=161 | Loss: 1.183 |  Acc: 78.460,90.750,93.280,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=162 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=162 | Loss: 0.736 |  Acc: 80.940,96.984,99.742,% 
Testing: Epoch=162 | Loss: 1.185 |  Acc: 77.150,90.650,93.570,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=163 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=163 | Loss: 0.730 |  Acc: 80.878,97.328,99.742,% 
Testing: Epoch=163 | Loss: 1.193 |  Acc: 76.990,90.610,93.550,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=164 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=164 | Loss: 0.737 |  Acc: 80.828,97.206,99.750,% 
Testing: Epoch=164 | Loss: 1.177 |  Acc: 77.290,90.830,93.630,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=165 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=165 | Loss: 0.737 |  Acc: 80.258,97.218,99.724,% 
Testing: Epoch=165 | Loss: 1.195 |  Acc: 77.300,90.470,93.510,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=166 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=166 | Loss: 0.735 |  Acc: 80.392,97.382,99.760,% 
Testing: Epoch=166 | Loss: 1.191 |  Acc: 77.410,90.460,93.470,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=167 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=167 | Loss: 0.738 |  Acc: 80.594,97.256,99.782,% 
Testing: Epoch=167 | Loss: 1.187 |  Acc: 77.580,90.670,93.610,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=168 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=168 | Loss: 0.738 |  Acc: 80.604,97.372,99.748,% 
Testing: Epoch=168 | Loss: 1.186 |  Acc: 77.610,90.630,93.530,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=169 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=169 | Loss: 0.738 |  Acc: 80.486,97.290,99.748,% 
Testing: Epoch=169 | Loss: 1.186 |  Acc: 77.480,90.450,93.630,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=170 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=170 | Loss: 0.730 |  Acc: 80.476,97.324,99.776,% 
Testing: Epoch=170 | Loss: 1.188 |  Acc: 77.540,90.450,93.520,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=171 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=171 | Loss: 0.719 |  Acc: 80.698,97.408,99.762,% 
Testing: Epoch=171 | Loss: 1.185 |  Acc: 77.600,90.470,93.560,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=172 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=172 | Loss: 0.726 |  Acc: 80.750,97.416,99.778,% 
Testing: Epoch=172 | Loss: 1.185 |  Acc: 77.630,90.450,93.680,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=173 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=173 | Loss: 0.732 |  Acc: 80.692,97.308,99.744,% 
Testing: Epoch=173 | Loss: 1.184 |  Acc: 77.680,90.500,93.520,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=174 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=174 | Loss: 0.738 |  Acc: 80.710,97.302,99.716,% 
Testing: Epoch=174 | Loss: 1.186 |  Acc: 77.620,90.390,93.550,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=175 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=175 | Loss: 0.731 |  Acc: 80.960,96.964,99.698,% 
Testing: Epoch=175 | Loss: 1.202 |  Acc: 77.370,90.200,93.250,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=176 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=176 | Loss: 0.732 |  Acc: 80.738,97.094,99.672,% 
Testing: Epoch=176 | Loss: 1.200 |  Acc: 78.010,90.190,93.040,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=177 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=177 | Loss: 0.725 |  Acc: 81.006,97.270,99.694,% 
Testing: Epoch=177 | Loss: 1.200 |  Acc: 77.270,90.360,93.350,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=178 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=178 | Loss: 0.730 |  Acc: 80.888,97.384,99.744,% 
Testing: Epoch=178 | Loss: 1.203 |  Acc: 77.370,89.840,93.290,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=179 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=179 | Loss: 0.715 |  Acc: 81.212,97.148,99.728,% 
Testing: Epoch=179 | Loss: 1.194 |  Acc: 77.800,90.550,93.230,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=180 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=180 | Loss: 0.720 |  Acc: 81.330,97.278,99.706,% 
Testing: Epoch=180 | Loss: 1.188 |  Acc: 77.670,90.610,93.620,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=181 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=181 | Loss: 0.718 |  Acc: 81.088,97.294,99.748,% 
Testing: Epoch=181 | Loss: 1.187 |  Acc: 77.680,90.300,93.550,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=182 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=182 | Loss: 0.708 |  Acc: 81.302,97.460,99.768,% 
Testing: Epoch=182 | Loss: 1.192 |  Acc: 77.580,90.470,93.530,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=183 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=183 | Loss: 0.715 |  Acc: 81.294,97.384,99.732,% 
Testing: Epoch=183 | Loss: 1.190 |  Acc: 77.150,90.450,93.360,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=184 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=184 | Loss: 0.709 |  Acc: 81.500,97.578,99.804,% 
Testing: Epoch=184 | Loss: 1.171 |  Acc: 78.380,90.410,93.400,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=185 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=185 | Loss: 0.707 |  Acc: 81.432,97.526,99.778,% 
Testing: Epoch=185 | Loss: 1.207 |  Acc: 77.770,89.940,93.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=186 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=186 | Loss: 0.716 |  Acc: 81.192,97.524,99.748,% 
Testing: Epoch=186 | Loss: 1.236 |  Acc: 77.140,90.090,93.150,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=187 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=187 | Loss: 0.704 |  Acc: 81.388,97.582,99.756,% 
Testing: Epoch=187 | Loss: 1.204 |  Acc: 77.750,89.940,93.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=188 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=188 | Loss: 0.701 |  Acc: 81.438,97.504,99.728,% 
Testing: Epoch=188 | Loss: 1.205 |  Acc: 77.470,90.420,93.130,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=189 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=189 | Loss: 0.710 |  Acc: 81.460,97.422,99.762,% 
Testing: Epoch=189 | Loss: 1.232 |  Acc: 76.950,89.900,93.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=190 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=190 | Loss: 0.747 |  Acc: 80.264,97.130,99.624,% 
Testing: Epoch=190 | Loss: 1.243 |  Acc: 76.860,89.620,92.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=191 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=191 | Loss: 0.741 |  Acc: 80.392,97.258,99.674,% 
Testing: Epoch=191 | Loss: 1.238 |  Acc: 77.150,89.820,92.820,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=192 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=192 | Loss: 0.743 |  Acc: 80.624,97.208,99.660,% 
Testing: Epoch=192 | Loss: 1.237 |  Acc: 77.110,89.870,93.050,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=193 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=193 | Loss: 0.738 |  Acc: 80.602,97.274,99.662,% 
Testing: Epoch=193 | Loss: 1.233 |  Acc: 77.260,89.970,92.930,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=194 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=194 | Loss: 0.727 |  Acc: 80.836,97.330,99.678,% 
Testing: Epoch=194 | Loss: 1.234 |  Acc: 77.310,89.770,92.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=195 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=195 | Loss: 0.739 |  Acc: 80.630,97.244,99.682,% 
Testing: Epoch=195 | Loss: 1.232 |  Acc: 77.350,89.790,92.860,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=196 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=196 | Loss: 0.728 |  Acc: 80.830,97.300,99.698,% 
Testing: Epoch=196 | Loss: 1.230 |  Acc: 77.330,89.930,92.930,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=197 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=197 | Loss: 0.730 |  Acc: 80.764,97.274,99.678,% 
Testing: Epoch=197 | Loss: 1.226 |  Acc: 77.380,89.880,93.070,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=198 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=198 | Loss: 0.729 |  Acc: 80.970,97.296,99.636,% 
Testing: Epoch=198 | Loss: 1.228 |  Acc: 77.310,89.960,92.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=199 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=199 | Loss: 0.733 |  Acc: 80.806,97.304,99.690,% 
Testing: Epoch=199 | Loss: 1.225 |  Acc: 77.510,89.910,92.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=200 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=200 | Loss: 0.712 |  Acc: 81.268,97.312,99.664,% 
Testing: Epoch=200 | Loss: 1.192 |  Acc: 78.090,90.080,93.180,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=201 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=201 | Loss: 0.717 |  Acc: 81.164,97.408,99.716,% 
Testing: Epoch=201 | Loss: 1.214 |  Acc: 77.670,89.720,93.030,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=202 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=202 | Loss: 0.712 |  Acc: 81.456,97.308,99.714,% 
Testing: Epoch=202 | Loss: 1.218 |  Acc: 77.780,90.220,92.960,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=203 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=203 | Loss: 0.702 |  Acc: 81.556,97.496,99.728,% 
Testing: Epoch=203 | Loss: 1.195 |  Acc: 78.380,90.180,92.950,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=204 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=204 | Loss: 0.708 |  Acc: 81.398,97.436,99.738,% 
Testing: Epoch=204 | Loss: 1.208 |  Acc: 78.050,90.210,93.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=205 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=205 | Loss: 0.694 |  Acc: 81.764,97.448,99.686,% 
Testing: Epoch=205 | Loss: 1.191 |  Acc: 78.220,90.240,93.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=206 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=206 | Loss: 0.704 |  Acc: 81.592,97.530,99.772,% 
Testing: Epoch=206 | Loss: 1.199 |  Acc: 78.240,90.470,93.070,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=207 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=207 | Loss: 0.703 |  Acc: 81.628,97.454,99.712,% 
Testing: Epoch=207 | Loss: 1.219 |  Acc: 78.090,89.870,92.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=208 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=208 | Loss: 0.700 |  Acc: 81.700,97.484,99.712,% 
Testing: Epoch=208 | Loss: 1.227 |  Acc: 77.670,90.320,93.070,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=209 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=209 | Loss: 0.694 |  Acc: 81.750,97.524,99.722,% 
Testing: Epoch=209 | Loss: 1.210 |  Acc: 77.670,90.230,93.240,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=210 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=210 | Loss: 0.695 |  Acc: 81.730,97.646,99.716,% 
Testing: Epoch=210 | Loss: 1.198 |  Acc: 78.150,90.330,92.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=211 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=211 | Loss: 0.698 |  Acc: 81.770,97.552,99.732,% 
Testing: Epoch=211 | Loss: 1.239 |  Acc: 76.530,90.140,93.200,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=212 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=212 | Loss: 0.694 |  Acc: 81.922,97.492,99.708,% 
Testing: Epoch=212 | Loss: 1.235 |  Acc: 77.440,90.090,93.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=213 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=213 | Loss: 0.693 |  Acc: 81.988,97.646,99.768,% 
Testing: Epoch=213 | Loss: 1.218 |  Acc: 77.230,90.040,93.110,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=214 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=214 | Loss: 0.696 |  Acc: 81.782,97.488,99.674,% 
Testing: Epoch=214 | Loss: 1.236 |  Acc: 76.870,89.800,92.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=215 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=215 | Loss: 0.712 |  Acc: 81.246,97.316,99.632,% 
Testing: Epoch=215 | Loss: 1.224 |  Acc: 77.250,89.900,93.060,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=216 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=216 | Loss: 0.710 |  Acc: 81.344,97.276,99.686,% 
Testing: Epoch=216 | Loss: 1.220 |  Acc: 77.300,90.060,93.030,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=217 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=217 | Loss: 0.708 |  Acc: 81.396,97.464,99.650,% 
Testing: Epoch=217 | Loss: 1.218 |  Acc: 77.460,90.010,93.020,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=218 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=218 | Loss: 0.715 |  Acc: 81.292,97.428,99.662,% 
Testing: Epoch=218 | Loss: 1.218 |  Acc: 77.540,89.970,92.950,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=219 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=219 | Loss: 0.702 |  Acc: 81.468,97.360,99.694,% 
Testing: Epoch=219 | Loss: 1.215 |  Acc: 77.590,89.960,92.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=220 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=220 | Loss: 0.701 |  Acc: 81.468,97.422,99.644,% 
Testing: Epoch=220 | Loss: 1.217 |  Acc: 77.500,89.870,92.950,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=221 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=221 | Loss: 0.707 |  Acc: 81.450,97.416,99.688,% 
Testing: Epoch=221 | Loss: 1.210 |  Acc: 77.720,90.050,93.160,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=222 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=222 | Loss: 0.698 |  Acc: 81.602,97.396,99.662,% 
Testing: Epoch=222 | Loss: 1.209 |  Acc: 77.770,90.090,93.130,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=223 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=223 | Loss: 0.697 |  Acc: 81.764,97.418,99.652,% 
Testing: Epoch=223 | Loss: 1.210 |  Acc: 77.760,90.100,92.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=224 | lr=1.0e-03 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=224 | Loss: 0.701 |  Acc: 81.554,97.452,99.698,% 
Testing: Epoch=224 | Loss: 1.210 |  Acc: 77.600,90.160,93.090,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=225 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=225 | Loss: 0.644 |  Acc: 82.908,98.140,99.834,% 
Testing: Epoch=225 | Loss: 1.132 |  Acc: 79.220,90.920,93.720,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=226 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=226 | Loss: 0.638 |  Acc: 83.248,98.508,99.896,% 
Testing: Epoch=226 | Loss: 1.121 |  Acc: 79.570,91.210,93.710,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=227 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=227 | Loss: 0.626 |  Acc: 83.190,98.652,99.942,% 
Testing: Epoch=227 | Loss: 1.119 |  Acc: 79.880,91.150,93.820,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=228 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=228 | Loss: 0.618 |  Acc: 83.512,98.652,99.918,% 
Testing: Epoch=228 | Loss: 1.119 |  Acc: 79.800,91.200,93.830,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=229 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=229 | Loss: 0.609 |  Acc: 83.524,98.654,99.924,% 
Testing: Epoch=229 | Loss: 1.118 |  Acc: 79.720,91.060,93.790,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=230 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=230 | Loss: 0.617 |  Acc: 83.446,98.628,99.954,% 
Testing: Epoch=230 | Loss: 1.119 |  Acc: 79.690,91.230,93.760,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=231 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=231 | Loss: 0.611 |  Acc: 83.536,98.626,99.938,% 
Testing: Epoch=231 | Loss: 1.114 |  Acc: 79.900,91.250,93.850,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=232 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=232 | Loss: 0.626 |  Acc: 83.476,98.728,99.946,% 
Testing: Epoch=232 | Loss: 1.110 |  Acc: 79.990,91.140,93.890,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=233 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=233 | Loss: 0.621 |  Acc: 83.356,98.670,99.934,% 
Testing: Epoch=233 | Loss: 1.117 |  Acc: 79.780,91.150,93.770,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=234 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=234 | Loss: 0.603 |  Acc: 83.726,98.828,99.934,% 
Testing: Epoch=234 | Loss: 1.118 |  Acc: 79.790,91.140,93.730,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=235 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=235 | Loss: 0.608 |  Acc: 83.572,98.734,99.942,% 
Testing: Epoch=235 | Loss: 1.109 |  Acc: 79.930,91.180,93.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=236 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=236 | Loss: 0.612 |  Acc: 83.602,98.752,99.954,% 
Testing: Epoch=236 | Loss: 1.114 |  Acc: 79.740,91.160,93.920,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=237 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=237 | Loss: 0.608 |  Acc: 83.532,98.834,99.954,% 
Testing: Epoch=237 | Loss: 1.110 |  Acc: 79.930,91.270,93.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=238 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=238 | Loss: 0.608 |  Acc: 83.652,98.834,99.964,% 
Testing: Epoch=238 | Loss: 1.117 |  Acc: 79.870,91.030,93.890,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=239 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=239 | Loss: 0.594 |  Acc: 83.840,98.810,99.948,% 
Testing: Epoch=239 | Loss: 1.112 |  Acc: 80.140,91.100,93.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=240 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=240 | Loss: 0.601 |  Acc: 83.706,98.834,99.946,% 
Testing: Epoch=240 | Loss: 1.113 |  Acc: 80.050,91.220,93.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=241 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=241 | Loss: 0.599 |  Acc: 83.846,98.872,99.960,% 
Testing: Epoch=241 | Loss: 1.116 |  Acc: 80.140,91.200,93.800,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=242 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=242 | Loss: 0.603 |  Acc: 83.702,98.992,99.962,% 
Testing: Epoch=242 | Loss: 1.112 |  Acc: 80.220,91.260,93.860,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=243 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=243 | Loss: 0.605 |  Acc: 83.760,98.846,99.946,% 
Testing: Epoch=243 | Loss: 1.117 |  Acc: 80.070,91.200,93.930,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=244 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=244 | Loss: 0.605 |  Acc: 83.872,98.826,99.974,% 
Testing: Epoch=244 | Loss: 1.115 |  Acc: 80.180,91.190,93.770,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=245 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=245 | Loss: 0.601 |  Acc: 83.874,98.902,99.952,% 
Testing: Epoch=245 | Loss: 1.114 |  Acc: 80.190,91.230,93.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=246 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=246 | Loss: 0.596 |  Acc: 83.968,98.934,99.948,% 
Testing: Epoch=246 | Loss: 1.111 |  Acc: 80.130,91.220,93.970,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=247 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=247 | Loss: 0.592 |  Acc: 83.988,98.914,99.938,% 
Testing: Epoch=247 | Loss: 1.111 |  Acc: 80.170,91.150,93.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=248 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=248 | Loss: 0.605 |  Acc: 83.838,98.888,99.940,% 
Testing: Epoch=248 | Loss: 1.114 |  Acc: 80.230,91.180,93.780,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=249 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=249 | Loss: 0.604 |  Acc: 83.914,98.876,99.940,% 
Testing: Epoch=249 | Loss: 1.112 |  Acc: 80.290,91.160,93.990,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=250 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=250 | Loss: 0.610 |  Acc: 83.644,98.778,99.946,% 
Testing: Epoch=250 | Loss: 1.114 |  Acc: 80.140,91.040,93.870,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=251 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=251 | Loss: 0.602 |  Acc: 83.826,98.824,99.930,% 
Testing: Epoch=251 | Loss: 1.113 |  Acc: 80.150,91.130,93.770,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=252 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=252 | Loss: 0.598 |  Acc: 83.758,98.872,99.944,% 
Testing: Epoch=252 | Loss: 1.109 |  Acc: 80.080,91.100,93.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=253 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=253 | Loss: 0.602 |  Acc: 83.994,98.840,99.952,% 
Testing: Epoch=253 | Loss: 1.110 |  Acc: 80.320,91.110,94.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=254 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=254 | Loss: 0.598 |  Acc: 83.740,98.878,99.942,% 
Testing: Epoch=254 | Loss: 1.111 |  Acc: 79.890,91.220,93.840,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=255 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=255 | Loss: 0.592 |  Acc: 83.980,98.804,99.966,% 
Testing: Epoch=255 | Loss: 1.118 |  Acc: 79.970,91.050,93.830,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=256 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=256 | Loss: 0.602 |  Acc: 83.828,98.842,99.954,% 
Testing: Epoch=256 | Loss: 1.118 |  Acc: 79.880,91.040,93.680,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=257 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=257 | Loss: 0.600 |  Acc: 84.052,98.842,99.954,% 
Testing: Epoch=257 | Loss: 1.108 |  Acc: 80.180,91.290,93.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=258 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=258 | Loss: 0.594 |  Acc: 83.962,98.890,99.966,% 
Testing: Epoch=258 | Loss: 1.109 |  Acc: 80.020,91.030,93.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=259 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=259 | Loss: 0.597 |  Acc: 83.916,98.954,99.948,% 
Testing: Epoch=259 | Loss: 1.112 |  Acc: 80.010,91.050,93.790,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=260 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=260 | Loss: 0.593 |  Acc: 83.780,98.882,99.970,% 
Testing: Epoch=260 | Loss: 1.110 |  Acc: 80.090,91.240,93.790,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=261 | lr=1.0e-04 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=261 | Loss: 0.594 |  Acc: 84.016,98.946,99.976,% 
Testing: Epoch=261 | Loss: 1.109 |  Acc: 80.040,91.040,93.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=262 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=262 | Loss: 0.582 |  Acc: 84.144,98.972,99.964,% 
Testing: Epoch=262 | Loss: 1.108 |  Acc: 80.150,91.150,93.890,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=263 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=263 | Loss: 0.591 |  Acc: 84.162,98.916,99.954,% 
Testing: Epoch=263 | Loss: 1.107 |  Acc: 80.250,91.230,93.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=264 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=264 | Loss: 0.597 |  Acc: 84.092,98.960,99.970,% 
Testing: Epoch=264 | Loss: 1.107 |  Acc: 80.260,91.260,93.960,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=265 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=265 | Loss: 0.593 |  Acc: 84.266,98.978,99.972,% 
Testing: Epoch=265 | Loss: 1.107 |  Acc: 80.190,91.220,93.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=266 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=266 | Loss: 0.594 |  Acc: 84.176,98.926,99.954,% 
Testing: Epoch=266 | Loss: 1.109 |  Acc: 80.200,91.180,93.920,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=267 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=267 | Loss: 0.589 |  Acc: 84.208,98.910,99.966,% 
Testing: Epoch=267 | Loss: 1.106 |  Acc: 80.030,91.210,93.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=268 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=268 | Loss: 0.589 |  Acc: 84.026,99.008,99.976,% 
Testing: Epoch=268 | Loss: 1.107 |  Acc: 80.210,91.030,93.930,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=269 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=269 | Loss: 0.588 |  Acc: 84.228,98.994,99.978,% 
Testing: Epoch=269 | Loss: 1.110 |  Acc: 80.080,91.160,93.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=270 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=270 | Loss: 0.581 |  Acc: 84.166,98.984,99.974,% 
Testing: Epoch=270 | Loss: 1.108 |  Acc: 80.110,91.120,93.890,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=271 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=271 | Loss: 0.586 |  Acc: 84.288,98.940,99.968,% 
Testing: Epoch=271 | Loss: 1.114 |  Acc: 80.130,91.050,93.820,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=272 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=272 | Loss: 0.590 |  Acc: 84.174,98.972,99.954,% 
Testing: Epoch=272 | Loss: 1.107 |  Acc: 80.220,91.210,93.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=273 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=273 | Loss: 0.597 |  Acc: 84.080,99.054,99.962,% 
Testing: Epoch=273 | Loss: 1.109 |  Acc: 80.070,91.140,93.840,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=274 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=274 | Loss: 0.587 |  Acc: 84.240,98.946,99.968,% 
Testing: Epoch=274 | Loss: 1.112 |  Acc: 80.040,91.120,93.820,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=275 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=275 | Loss: 0.586 |  Acc: 84.242,99.012,99.968,% 
Testing: Epoch=275 | Loss: 1.104 |  Acc: 80.250,91.180,93.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=276 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=276 | Loss: 0.591 |  Acc: 84.002,98.908,99.946,% 
Testing: Epoch=276 | Loss: 1.108 |  Acc: 80.380,91.170,93.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=277 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=277 | Loss: 0.596 |  Acc: 84.072,98.968,99.970,% 
Testing: Epoch=277 | Loss: 1.108 |  Acc: 80.320,91.010,93.860,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=278 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=278 | Loss: 0.590 |  Acc: 84.282,98.934,99.968,% 
Testing: Epoch=278 | Loss: 1.109 |  Acc: 80.140,91.160,93.860,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=279 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=279 | Loss: 0.581 |  Acc: 84.182,98.992,99.962,% 
Testing: Epoch=279 | Loss: 1.108 |  Acc: 80.360,91.170,93.920,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=280 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=280 | Loss: 0.587 |  Acc: 84.282,98.940,99.970,% 
Testing: Epoch=280 | Loss: 1.103 |  Acc: 80.170,91.180,93.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=281 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=281 | Loss: 0.595 |  Acc: 84.168,98.898,99.970,% 
Testing: Epoch=281 | Loss: 1.104 |  Acc: 80.230,91.230,94.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=282 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=282 | Loss: 0.580 |  Acc: 84.244,99.052,99.972,% 
Testing: Epoch=282 | Loss: 1.107 |  Acc: 80.140,91.080,93.900,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=283 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=283 | Loss: 0.588 |  Acc: 84.258,98.928,99.970,% 
Testing: Epoch=283 | Loss: 1.103 |  Acc: 80.260,91.140,93.920,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=284 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=284 | Loss: 0.583 |  Acc: 84.176,98.882,99.938,% 
Testing: Epoch=284 | Loss: 1.107 |  Acc: 80.290,91.230,93.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=285 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=285 | Loss: 0.584 |  Acc: 84.364,99.006,99.960,% 
Testing: Epoch=285 | Loss: 1.106 |  Acc: 80.190,91.100,93.870,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=286 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=286 | Loss: 0.580 |  Acc: 84.294,99.042,99.954,% 
Testing: Epoch=286 | Loss: 1.108 |  Acc: 80.290,91.170,93.960,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=287 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=287 | Loss: 0.593 |  Acc: 84.198,98.936,99.970,% 
Testing: Epoch=287 | Loss: 1.106 |  Acc: 80.300,91.180,93.880,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=288 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=288 | Loss: 0.601 |  Acc: 84.032,98.980,99.976,% 
Testing: Epoch=288 | Loss: 1.107 |  Acc: 80.230,91.190,93.980,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=289 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=289 | Loss: 0.592 |  Acc: 84.176,98.984,99.968,% 
Testing: Epoch=289 | Loss: 1.104 |  Acc: 80.080,91.080,93.930,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=290 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=290 | Loss: 0.588 |  Acc: 84.292,98.924,99.960,% 
Testing: Epoch=290 | Loss: 1.107 |  Acc: 80.300,91.180,93.860,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=291 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=291 | Loss: 0.581 |  Acc: 84.178,98.898,99.948,% 
Testing: Epoch=291 | Loss: 1.104 |  Acc: 80.230,91.180,93.930,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=292 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=292 | Loss: 0.589 |  Acc: 84.110,99.054,99.972,% 
Testing: Epoch=292 | Loss: 1.103 |  Acc: 80.160,91.140,93.960,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=293 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=293 | Loss: 0.579 |  Acc: 84.308,98.928,99.960,% 
Testing: Epoch=293 | Loss: 1.104 |  Acc: 80.180,91.160,94.070,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=294 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=294 | Loss: 0.576 |  Acc: 84.452,98.994,99.960,% 
Testing: Epoch=294 | Loss: 1.105 |  Acc: 80.250,91.190,93.910,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=295 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=295 | Loss: 0.581 |  Acc: 84.216,98.928,99.966,% 
Testing: Epoch=295 | Loss: 1.109 |  Acc: 80.180,91.140,93.820,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=296 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=296 | Loss: 0.573 |  Acc: 84.320,98.934,99.970,% 
Testing: Epoch=296 | Loss: 1.104 |  Acc: 80.180,91.170,94.000,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=297 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=297 | Loss: 0.579 |  Acc: 84.294,98.966,99.962,% 
Testing: Epoch=297 | Loss: 1.107 |  Acc: 80.170,91.150,93.940,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=298 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=298 | Loss: 0.589 |  Acc: 84.168,98.992,99.974,% 
Testing: Epoch=298 | Loss: 1.106 |  Acc: 80.280,91.110,93.920,% 

Training Setting: backend=modelC | dataset=cifar10 | adaptive=1 | batch_size=128 | epoch=299 | lr=1.0e-05 | circles=1 | dropout=0.50 | avg=0
Training: Epoch=299 | Loss: 0.574 |  Acc: 84.186,98.994,99.980,% 
Testing: Epoch=299 | Loss: 1.105 |  Acc: 80.240,91.110,93.980,% 



Epoch: 219
Batch: 0 | Loss: 0.670 | Acc: 82.812,97.656,100.000,%
Batch: 20 | Loss: 0.701 | Acc: 81.696,97.024,99.554,%
Batch: 40 | Loss: 0.704 | Acc: 81.288,97.104,99.638,%
Batch: 60 | Loss: 0.707 | Acc: 81.404,97.336,99.641,%
Batch: 80 | Loss: 0.710 | Acc: 81.375,97.425,99.595,%
Batch: 100 | Loss: 0.699 | Acc: 81.559,97.440,99.636,%
Batch: 120 | Loss: 0.701 | Acc: 81.399,97.430,99.626,%
Batch: 140 | Loss: 0.703 | Acc: 81.377,97.318,99.629,%
Batch: 160 | Loss: 0.701 | Acc: 81.352,97.385,99.651,%
Batch: 180 | Loss: 0.704 | Acc: 81.306,97.328,99.642,%
Batch: 200 | Loss: 0.705 | Acc: 81.234,97.283,99.650,%
Batch: 220 | Loss: 0.705 | Acc: 81.211,97.317,99.657,%
Batch: 240 | Loss: 0.705 | Acc: 81.282,97.313,99.653,%
Batch: 260 | Loss: 0.707 | Acc: 81.196,97.285,99.668,%
Batch: 280 | Loss: 0.704 | Acc: 81.283,97.311,99.686,%
Batch: 300 | Loss: 0.704 | Acc: 81.294,97.290,99.678,%
Batch: 320 | Loss: 0.703 | Acc: 81.325,97.335,99.676,%
Batch: 340 | Loss: 0.704 | Acc: 81.413,97.333,99.684,%
Batch: 360 | Loss: 0.705 | Acc: 81.421,97.351,99.695,%
Batch: 380 | Loss: 0.703 | Acc: 81.453,97.357,99.697,%
Batch: 0 | Loss: 1.195 | Acc: 77.344,92.188,92.188,%
Batch: 20 | Loss: 1.245 | Acc: 77.232,89.844,93.266,%
Batch: 40 | Loss: 1.228 | Acc: 77.382,90.015,92.988,%
Batch: 60 | Loss: 1.221 | Acc: 77.561,89.908,92.905,%

Epoch: 220
Batch: 0 | Loss: 0.859 | Acc: 77.344,96.875,100.000,%
Batch: 20 | Loss: 0.694 | Acc: 81.622,97.656,99.702,%
Batch: 40 | Loss: 0.684 | Acc: 82.107,97.294,99.600,%
Batch: 60 | Loss: 0.676 | Acc: 82.057,97.336,99.654,%
Batch: 80 | Loss: 0.676 | Acc: 81.993,97.348,99.614,%
Batch: 100 | Loss: 0.686 | Acc: 81.791,97.339,99.652,%
Batch: 120 | Loss: 0.685 | Acc: 81.786,97.275,99.638,%
Batch: 140 | Loss: 0.685 | Acc: 81.693,97.318,99.629,%
Batch: 160 | Loss: 0.694 | Acc: 81.628,97.302,99.622,%
Batch: 180 | Loss: 0.697 | Acc: 81.569,97.367,99.612,%
Batch: 200 | Loss: 0.695 | Acc: 81.755,97.376,99.611,%
Batch: 220 | Loss: 0.697 | Acc: 81.649,97.352,99.625,%
Batch: 240 | Loss: 0.694 | Acc: 81.704,97.368,99.621,%
Batch: 260 | Loss: 0.697 | Acc: 81.684,97.423,99.632,%
Batch: 280 | Loss: 0.693 | Acc: 81.667,97.456,99.641,%
Batch: 300 | Loss: 0.695 | Acc: 81.678,97.438,99.655,%
Batch: 320 | Loss: 0.697 | Acc: 81.625,97.457,99.654,%
Batch: 340 | Loss: 0.697 | Acc: 81.628,97.466,99.643,%
Batch: 360 | Loss: 0.700 | Acc: 81.514,97.427,99.636,%
Batch: 380 | Loss: 0.700 | Acc: 81.498,97.427,99.639,%
Batch: 0 | Loss: 1.180 | Acc: 78.125,91.406,92.969,%
Batch: 20 | Loss: 1.247 | Acc: 77.344,89.807,93.155,%
Batch: 40 | Loss: 1.230 | Acc: 77.306,90.015,92.969,%
Batch: 60 | Loss: 1.223 | Acc: 77.382,89.857,92.943,%

Epoch: 221
Batch: 0 | Loss: 0.792 | Acc: 81.250,98.438,100.000,%
Batch: 20 | Loss: 0.671 | Acc: 81.808,97.842,99.740,%
Batch: 40 | Loss: 0.701 | Acc: 81.822,97.466,99.733,%
Batch: 60 | Loss: 0.708 | Acc: 81.519,97.387,99.680,%
Batch: 80 | Loss: 0.703 | Acc: 81.433,97.367,99.701,%
Batch: 100 | Loss: 0.704 | Acc: 81.351,97.471,99.745,%
Batch: 120 | Loss: 0.698 | Acc: 81.489,97.488,99.729,%
Batch: 140 | Loss: 0.700 | Acc: 81.427,97.479,99.740,%
Batch: 160 | Loss: 0.700 | Acc: 81.386,97.443,99.704,%
Batch: 180 | Loss: 0.702 | Acc: 81.332,97.453,99.706,%
Batch: 200 | Loss: 0.706 | Acc: 81.308,97.439,99.720,%
Batch: 220 | Loss: 0.707 | Acc: 81.353,97.384,99.700,%
Batch: 240 | Loss: 0.708 | Acc: 81.412,97.403,99.699,%
Batch: 260 | Loss: 0.707 | Acc: 81.370,97.399,99.695,%
Batch: 280 | Loss: 0.706 | Acc: 81.422,97.392,99.689,%
Batch: 300 | Loss: 0.709 | Acc: 81.403,97.384,99.699,%
Batch: 320 | Loss: 0.705 | Acc: 81.489,97.427,99.706,%
Batch: 340 | Loss: 0.704 | Acc: 81.500,97.439,99.702,%
Batch: 360 | Loss: 0.703 | Acc: 81.529,97.433,99.699,%
Batch: 380 | Loss: 0.707 | Acc: 81.447,97.435,99.694,%
Batch: 0 | Loss: 1.169 | Acc: 78.125,91.406,92.969,%
Batch: 20 | Loss: 1.240 | Acc: 77.567,90.030,93.229,%
Batch: 40 | Loss: 1.225 | Acc: 77.477,90.130,93.140,%
Batch: 60 | Loss: 1.216 | Acc: 77.613,90.061,93.097,%

Epoch: 222
Batch: 0 | Loss: 0.663 | Acc: 84.375,97.656,99.219,%
Batch: 20 | Loss: 0.671 | Acc: 82.106,97.507,99.665,%
Batch: 40 | Loss: 0.690 | Acc: 81.993,97.885,99.619,%
Batch: 60 | Loss: 0.697 | Acc: 81.327,97.746,99.705,%
Batch: 80 | Loss: 0.698 | Acc: 81.327,97.791,99.711,%
Batch: 100 | Loss: 0.698 | Acc: 81.699,97.695,99.683,%
Batch: 120 | Loss: 0.701 | Acc: 81.760,97.605,99.671,%
Batch: 140 | Loss: 0.692 | Acc: 81.965,97.629,99.690,%
Batch: 160 | Loss: 0.693 | Acc: 81.837,97.525,99.709,%
Batch: 180 | Loss: 0.692 | Acc: 81.764,97.509,99.689,%
Batch: 200 | Loss: 0.689 | Acc: 81.810,97.516,99.677,%
Batch: 220 | Loss: 0.691 | Acc: 81.731,97.487,99.682,%
Batch: 240 | Loss: 0.689 | Acc: 81.707,97.475,99.656,%
Batch: 260 | Loss: 0.686 | Acc: 81.687,97.492,99.659,%
Batch: 280 | Loss: 0.687 | Acc: 81.714,97.495,99.644,%
Batch: 300 | Loss: 0.691 | Acc: 81.704,97.475,99.652,%
Batch: 320 | Loss: 0.691 | Acc: 81.683,97.466,99.657,%
Batch: 340 | Loss: 0.695 | Acc: 81.651,97.455,99.656,%
Batch: 360 | Loss: 0.697 | Acc: 81.627,97.444,99.665,%
Batch: 380 | Loss: 0.697 | Acc: 81.662,97.400,99.668,%
Batch: 0 | Loss: 1.166 | Acc: 78.125,91.406,92.188,%
Batch: 20 | Loss: 1.237 | Acc: 77.827,90.104,93.155,%
Batch: 40 | Loss: 1.223 | Acc: 77.630,90.149,93.140,%
Batch: 60 | Loss: 1.215 | Acc: 77.728,90.061,93.046,%

Epoch: 223
Batch: 0 | Loss: 0.842 | Acc: 76.562,95.312,98.438,%
Batch: 20 | Loss: 0.704 | Acc: 80.804,97.321,99.591,%
Batch: 40 | Loss: 0.698 | Acc: 81.364,97.466,99.581,%
Batch: 60 | Loss: 0.712 | Acc: 81.263,97.439,99.654,%
Batch: 80 | Loss: 0.711 | Acc: 81.240,97.512,99.662,%
Batch: 100 | Loss: 0.713 | Acc: 81.412,97.432,99.683,%
Batch: 120 | Loss: 0.703 | Acc: 81.553,97.559,99.664,%
Batch: 140 | Loss: 0.705 | Acc: 81.533,97.501,99.634,%
Batch: 160 | Loss: 0.702 | Acc: 81.357,97.506,99.651,%
Batch: 180 | Loss: 0.702 | Acc: 81.345,97.505,99.650,%
Batch: 200 | Loss: 0.701 | Acc: 81.370,97.466,99.646,%
Batch: 220 | Loss: 0.702 | Acc: 81.501,97.448,99.650,%
Batch: 240 | Loss: 0.700 | Acc: 81.620,97.478,99.653,%
Batch: 260 | Loss: 0.699 | Acc: 81.669,97.486,99.659,%
Batch: 280 | Loss: 0.698 | Acc: 81.706,97.473,99.647,%
Batch: 300 | Loss: 0.699 | Acc: 81.733,97.438,99.642,%
Batch: 320 | Loss: 0.701 | Acc: 81.661,97.425,99.635,%
Batch: 340 | Loss: 0.701 | Acc: 81.713,97.452,99.643,%
Batch: 360 | Loss: 0.700 | Acc: 81.674,97.453,99.645,%
Batch: 380 | Loss: 0.695 | Acc: 81.779,97.447,99.662,%
Batch: 0 | Loss: 1.173 | Acc: 78.906,91.406,92.969,%
Batch: 20 | Loss: 1.238 | Acc: 77.827,90.104,93.043,%
Batch: 40 | Loss: 1.224 | Acc: 77.611,90.263,92.931,%
Batch: 60 | Loss: 1.215 | Acc: 77.715,90.087,92.866,%

Epoch: 224
Batch: 0 | Loss: 0.706 | Acc: 83.594,96.094,100.000,%
Batch: 20 | Loss: 0.711 | Acc: 81.213,97.731,99.851,%
Batch: 40 | Loss: 0.719 | Acc: 81.040,97.618,99.714,%
Batch: 60 | Loss: 0.725 | Acc: 80.904,97.515,99.705,%
Batch: 80 | Loss: 0.721 | Acc: 81.047,97.560,99.691,%
Batch: 100 | Loss: 0.721 | Acc: 81.002,97.641,99.714,%
Batch: 120 | Loss: 0.716 | Acc: 81.173,97.534,99.709,%
Batch: 140 | Loss: 0.711 | Acc: 81.139,97.518,99.690,%
Batch: 160 | Loss: 0.710 | Acc: 81.177,97.486,99.694,%
Batch: 180 | Loss: 0.711 | Acc: 81.254,97.488,99.702,%
Batch: 200 | Loss: 0.706 | Acc: 81.386,97.458,99.708,%
Batch: 220 | Loss: 0.707 | Acc: 81.356,97.405,99.710,%
Batch: 240 | Loss: 0.708 | Acc: 81.467,97.384,99.702,%
Batch: 260 | Loss: 0.706 | Acc: 81.439,97.411,99.698,%
Batch: 280 | Loss: 0.709 | Acc: 81.303,97.395,99.694,%
Batch: 300 | Loss: 0.710 | Acc: 81.362,97.415,99.691,%
Batch: 320 | Loss: 0.711 | Acc: 81.423,97.410,99.688,%
Batch: 340 | Loss: 0.709 | Acc: 81.429,97.400,99.691,%
Batch: 360 | Loss: 0.703 | Acc: 81.529,97.412,99.699,%
Batch: 380 | Loss: 0.701 | Acc: 81.553,97.433,99.699,%
Batch: 0 | Loss: 1.169 | Acc: 78.125,91.406,91.406,%
Batch: 20 | Loss: 1.236 | Acc: 77.716,90.030,93.229,%
Batch: 40 | Loss: 1.222 | Acc: 77.572,90.187,93.026,%
Batch: 60 | Loss: 1.215 | Acc: 77.664,90.138,92.994,%

Epoch: 225
Batch: 0 | Loss: 0.444 | Acc: 85.938,98.438,100.000,%
Batch: 20 | Loss: 0.710 | Acc: 81.585,98.028,99.628,%
Batch: 40 | Loss: 0.695 | Acc: 82.069,97.828,99.714,%
Batch: 60 | Loss: 0.685 | Acc: 82.147,97.861,99.782,%
Batch: 80 | Loss: 0.671 | Acc: 82.205,97.926,99.778,%
Batch: 100 | Loss: 0.663 | Acc: 82.627,97.896,99.783,%
Batch: 120 | Loss: 0.666 | Acc: 82.574,97.915,99.793,%
Batch: 140 | Loss: 0.665 | Acc: 82.574,97.961,99.806,%
Batch: 160 | Loss: 0.662 | Acc: 82.609,98.006,99.801,%
Batch: 180 | Loss: 0.656 | Acc: 82.761,98.062,99.810,%
Batch: 200 | Loss: 0.654 | Acc: 82.762,98.076,99.810,%
Batch: 220 | Loss: 0.651 | Acc: 82.841,98.105,99.806,%
Batch: 240 | Loss: 0.650 | Acc: 82.822,98.100,99.809,%
Batch: 260 | Loss: 0.650 | Acc: 82.812,98.069,99.799,%
Batch: 280 | Loss: 0.652 | Acc: 82.840,98.073,99.811,%
Batch: 300 | Loss: 0.651 | Acc: 82.841,98.092,99.818,%
Batch: 320 | Loss: 0.649 | Acc: 82.849,98.119,99.822,%
Batch: 340 | Loss: 0.648 | Acc: 82.863,98.110,99.833,%
Batch: 360 | Loss: 0.646 | Acc: 82.854,98.126,99.836,%
Batch: 380 | Loss: 0.644 | Acc: 82.897,98.134,99.834,%
Batch: 0 | Loss: 1.094 | Acc: 78.125,92.969,92.188,%
Batch: 20 | Loss: 1.159 | Acc: 79.315,90.885,93.601,%
Batch: 40 | Loss: 1.149 | Acc: 79.097,90.873,93.617,%
Batch: 60 | Loss: 1.137 | Acc: 79.162,90.996,93.686,%

Epoch: 226
Batch: 0 | Loss: 0.875 | Acc: 76.562,96.094,100.000,%
Batch: 20 | Loss: 0.639 | Acc: 83.519,97.768,99.777,%
Batch: 40 | Loss: 0.648 | Acc: 83.346,98.209,99.886,%
Batch: 60 | Loss: 0.643 | Acc: 83.671,98.322,99.898,%
Batch: 80 | Loss: 0.634 | Acc: 83.555,98.389,99.904,%
Batch: 100 | Loss: 0.637 | Acc: 83.609,98.391,99.899,%
Batch: 120 | Loss: 0.631 | Acc: 83.478,98.412,99.890,%
Batch: 140 | Loss: 0.625 | Acc: 83.533,98.482,99.900,%
Batch: 160 | Loss: 0.630 | Acc: 83.322,98.438,99.898,%
Batch: 180 | Loss: 0.637 | Acc: 83.205,98.407,99.896,%
Batch: 200 | Loss: 0.639 | Acc: 83.120,98.379,99.895,%
Batch: 220 | Loss: 0.632 | Acc: 83.304,98.423,99.897,%
Batch: 240 | Loss: 0.634 | Acc: 83.283,98.441,99.896,%
Batch: 260 | Loss: 0.635 | Acc: 83.309,98.464,99.898,%
Batch: 280 | Loss: 0.635 | Acc: 83.271,98.457,99.897,%
Batch: 300 | Loss: 0.637 | Acc: 83.215,98.471,99.901,%
Batch: 320 | Loss: 0.636 | Acc: 83.290,98.491,99.900,%
Batch: 340 | Loss: 0.638 | Acc: 83.246,98.506,99.904,%
Batch: 360 | Loss: 0.635 | Acc: 83.332,98.524,99.894,%
Batch: 380 | Loss: 0.637 | Acc: 83.286,98.522,99.897,%
Batch: 0 | Loss: 1.051 | Acc: 80.469,92.188,92.188,%
Batch: 20 | Loss: 1.142 | Acc: 79.613,91.183,93.899,%
Batch: 40 | Loss: 1.135 | Acc: 79.707,91.197,93.731,%
Batch: 60 | Loss: 1.125 | Acc: 79.585,91.355,93.712,%

Epoch: 227
Batch: 0 | Loss: 0.631 | Acc: 83.594,98.438,100.000,%
Batch: 20 | Loss: 0.633 | Acc: 83.185,99.033,99.926,%
Batch: 40 | Loss: 0.642 | Acc: 83.518,99.066,99.962,%
Batch: 60 | Loss: 0.649 | Acc: 83.338,98.745,99.949,%
Batch: 80 | Loss: 0.644 | Acc: 83.324,98.688,99.961,%
Batch: 100 | Loss: 0.632 | Acc: 83.369,98.700,99.946,%
Batch: 120 | Loss: 0.623 | Acc: 83.536,98.709,99.955,%
Batch: 140 | Loss: 0.622 | Acc: 83.494,98.709,99.956,%
Batch: 160 | Loss: 0.622 | Acc: 83.468,98.700,99.956,%
Batch: 180 | Loss: 0.621 | Acc: 83.499,98.714,99.944,%
Batch: 200 | Loss: 0.616 | Acc: 83.563,98.713,99.946,%
Batch: 220 | Loss: 0.618 | Acc: 83.512,98.717,99.951,%
Batch: 240 | Loss: 0.621 | Acc: 83.451,98.674,99.948,%
Batch: 260 | Loss: 0.620 | Acc: 83.408,98.668,99.943,%
Batch: 280 | Loss: 0.622 | Acc: 83.338,98.632,99.939,%
Batch: 300 | Loss: 0.623 | Acc: 83.332,98.658,99.935,%
Batch: 320 | Loss: 0.623 | Acc: 83.263,98.683,99.937,%
Batch: 340 | Loss: 0.626 | Acc: 83.200,98.687,99.936,%
Batch: 360 | Loss: 0.627 | Acc: 83.198,98.654,99.939,%
Batch: 380 | Loss: 0.627 | Acc: 83.182,98.649,99.941,%
Batch: 0 | Loss: 1.063 | Acc: 78.906,92.969,92.969,%
Batch: 20 | Loss: 1.145 | Acc: 80.060,91.034,93.899,%
Batch: 40 | Loss: 1.137 | Acc: 79.992,91.044,93.769,%
Batch: 60 | Loss: 1.124 | Acc: 79.828,91.291,93.801,%

Epoch: 228
Batch: 0 | Loss: 0.697 | Acc: 80.469,96.094,100.000,%
Batch: 20 | Loss: 0.587 | Acc: 84.375,98.996,100.000,%
Batch: 40 | Loss: 0.596 | Acc: 84.223,98.819,99.943,%
Batch: 60 | Loss: 0.606 | Acc: 84.157,98.719,99.923,%
Batch: 80 | Loss: 0.611 | Acc: 83.748,98.727,99.923,%
Batch: 100 | Loss: 0.611 | Acc: 83.880,98.685,99.930,%
Batch: 120 | Loss: 0.611 | Acc: 83.858,98.644,99.935,%
Batch: 140 | Loss: 0.622 | Acc: 83.660,98.604,99.939,%
Batch: 160 | Loss: 0.624 | Acc: 83.657,98.641,99.932,%
Batch: 180 | Loss: 0.624 | Acc: 83.706,98.658,99.935,%
Batch: 200 | Loss: 0.623 | Acc: 83.730,98.640,99.922,%
Batch: 220 | Loss: 0.625 | Acc: 83.647,98.674,99.929,%
Batch: 240 | Loss: 0.627 | Acc: 83.490,98.651,99.929,%
Batch: 260 | Loss: 0.624 | Acc: 83.483,98.653,99.922,%
Batch: 280 | Loss: 0.624 | Acc: 83.505,98.657,99.919,%
Batch: 300 | Loss: 0.624 | Acc: 83.513,98.653,99.920,%
Batch: 320 | Loss: 0.621 | Acc: 83.562,98.666,99.917,%
Batch: 340 | Loss: 0.621 | Acc: 83.520,98.655,99.915,%
Batch: 360 | Loss: 0.621 | Acc: 83.473,98.641,99.911,%
Batch: 380 | Loss: 0.618 | Acc: 83.516,98.661,99.916,%
Batch: 0 | Loss: 1.046 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.145 | Acc: 79.948,91.183,93.750,%
Batch: 40 | Loss: 1.136 | Acc: 79.973,91.178,93.750,%
Batch: 60 | Loss: 1.123 | Acc: 79.956,91.381,93.801,%

Epoch: 229
Batch: 0 | Loss: 0.664 | Acc: 85.156,100.000,100.000,%
Batch: 20 | Loss: 0.579 | Acc: 83.966,99.033,99.888,%
Batch: 40 | Loss: 0.596 | Acc: 84.184,98.685,99.943,%
Batch: 60 | Loss: 0.590 | Acc: 84.132,98.604,99.962,%
Batch: 80 | Loss: 0.577 | Acc: 84.433,98.534,99.923,%
Batch: 100 | Loss: 0.589 | Acc: 84.081,98.554,99.907,%
Batch: 120 | Loss: 0.594 | Acc: 83.904,98.592,99.910,%
Batch: 140 | Loss: 0.589 | Acc: 83.937,98.582,99.906,%
Batch: 160 | Loss: 0.590 | Acc: 83.909,98.622,99.898,%
Batch: 180 | Loss: 0.595 | Acc: 83.797,98.632,99.909,%
Batch: 200 | Loss: 0.597 | Acc: 83.792,98.675,99.914,%
Batch: 220 | Loss: 0.598 | Acc: 83.774,98.646,99.919,%
Batch: 240 | Loss: 0.601 | Acc: 83.756,98.622,99.922,%
Batch: 260 | Loss: 0.603 | Acc: 83.710,98.614,99.922,%
Batch: 280 | Loss: 0.607 | Acc: 83.652,98.610,99.922,%
Batch: 300 | Loss: 0.607 | Acc: 83.589,98.637,99.925,%
Batch: 320 | Loss: 0.605 | Acc: 83.579,98.661,99.922,%
Batch: 340 | Loss: 0.606 | Acc: 83.575,98.662,99.924,%
Batch: 360 | Loss: 0.606 | Acc: 83.522,98.643,99.926,%
Batch: 380 | Loss: 0.608 | Acc: 83.536,98.651,99.926,%
Batch: 0 | Loss: 1.011 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.141 | Acc: 79.836,91.071,93.750,%
Batch: 40 | Loss: 1.134 | Acc: 79.916,91.101,93.750,%
Batch: 60 | Loss: 1.122 | Acc: 79.726,91.227,93.776,%

Epoch: 230
Batch: 0 | Loss: 0.615 | Acc: 85.156,96.875,100.000,%
Batch: 20 | Loss: 0.578 | Acc: 85.119,98.586,99.926,%
Batch: 40 | Loss: 0.592 | Acc: 84.413,98.552,99.905,%
Batch: 60 | Loss: 0.588 | Acc: 84.196,98.489,99.898,%
Batch: 80 | Loss: 0.593 | Acc: 83.825,98.640,99.894,%
Batch: 100 | Loss: 0.596 | Acc: 83.895,98.623,99.907,%
Batch: 120 | Loss: 0.611 | Acc: 83.620,98.528,99.923,%
Batch: 140 | Loss: 0.613 | Acc: 83.544,98.526,99.934,%
Batch: 160 | Loss: 0.616 | Acc: 83.448,98.505,99.937,%
Batch: 180 | Loss: 0.623 | Acc: 83.412,98.446,99.931,%
Batch: 200 | Loss: 0.618 | Acc: 83.532,98.461,99.934,%
Batch: 220 | Loss: 0.619 | Acc: 83.520,98.505,99.940,%
Batch: 240 | Loss: 0.621 | Acc: 83.448,98.557,99.942,%
Batch: 260 | Loss: 0.623 | Acc: 83.444,98.563,99.946,%
Batch: 280 | Loss: 0.621 | Acc: 83.569,98.596,99.947,%
Batch: 300 | Loss: 0.623 | Acc: 83.469,98.593,99.948,%
Batch: 320 | Loss: 0.622 | Acc: 83.436,98.586,99.949,%
Batch: 340 | Loss: 0.620 | Acc: 83.456,98.589,99.950,%
Batch: 360 | Loss: 0.619 | Acc: 83.468,98.608,99.950,%
Batch: 380 | Loss: 0.618 | Acc: 83.469,98.614,99.953,%
Batch: 0 | Loss: 1.019 | Acc: 78.906,92.969,92.188,%
Batch: 20 | Loss: 1.144 | Acc: 79.390,91.332,93.824,%
Batch: 40 | Loss: 1.135 | Acc: 79.745,91.235,93.674,%
Batch: 60 | Loss: 1.123 | Acc: 79.623,91.329,93.712,%

Epoch: 231
Batch: 0 | Loss: 0.671 | Acc: 81.250,98.438,100.000,%
Batch: 20 | Loss: 0.596 | Acc: 84.301,98.735,100.000,%
Batch: 40 | Loss: 0.615 | Acc: 83.403,98.838,100.000,%
Batch: 60 | Loss: 0.631 | Acc: 83.158,98.732,99.974,%
Batch: 80 | Loss: 0.611 | Acc: 83.410,98.785,99.971,%
Batch: 100 | Loss: 0.623 | Acc: 83.130,98.739,99.977,%
Batch: 120 | Loss: 0.619 | Acc: 83.226,98.754,99.968,%
Batch: 140 | Loss: 0.612 | Acc: 83.372,98.720,99.945,%
Batch: 160 | Loss: 0.607 | Acc: 83.400,98.748,99.951,%
Batch: 180 | Loss: 0.609 | Acc: 83.469,98.692,99.948,%
Batch: 200 | Loss: 0.605 | Acc: 83.508,98.698,99.934,%
Batch: 220 | Loss: 0.603 | Acc: 83.569,98.671,99.940,%
Batch: 240 | Loss: 0.607 | Acc: 83.552,98.677,99.929,%
Batch: 260 | Loss: 0.607 | Acc: 83.540,98.662,99.925,%
Batch: 280 | Loss: 0.607 | Acc: 83.521,98.629,99.928,%
Batch: 300 | Loss: 0.609 | Acc: 83.526,98.635,99.930,%
Batch: 320 | Loss: 0.608 | Acc: 83.533,98.640,99.934,%
Batch: 340 | Loss: 0.608 | Acc: 83.603,98.644,99.936,%
Batch: 360 | Loss: 0.609 | Acc: 83.576,98.634,99.937,%
Batch: 380 | Loss: 0.611 | Acc: 83.540,98.622,99.938,%
Batch: 0 | Loss: 1.026 | Acc: 78.125,93.750,92.188,%
Batch: 20 | Loss: 1.138 | Acc: 79.911,91.220,93.936,%
Batch: 40 | Loss: 1.130 | Acc: 80.050,91.197,93.788,%
Batch: 60 | Loss: 1.119 | Acc: 79.956,91.329,93.827,%

Epoch: 232
Batch: 0 | Loss: 0.753 | Acc: 81.250,97.656,100.000,%
Batch: 20 | Loss: 0.669 | Acc: 82.143,98.103,99.851,%
Batch: 40 | Loss: 0.643 | Acc: 83.060,98.476,99.905,%
Batch: 60 | Loss: 0.629 | Acc: 83.120,98.617,99.936,%
Batch: 80 | Loss: 0.637 | Acc: 83.150,98.659,99.932,%
Batch: 100 | Loss: 0.628 | Acc: 83.493,98.662,99.946,%
Batch: 120 | Loss: 0.626 | Acc: 83.581,98.709,99.935,%
Batch: 140 | Loss: 0.626 | Acc: 83.522,98.726,99.939,%
Batch: 160 | Loss: 0.624 | Acc: 83.531,98.719,99.947,%
Batch: 180 | Loss: 0.619 | Acc: 83.658,98.744,99.944,%
Batch: 200 | Loss: 0.628 | Acc: 83.547,98.721,99.942,%
Batch: 220 | Loss: 0.629 | Acc: 83.456,98.710,99.943,%
Batch: 240 | Loss: 0.630 | Acc: 83.377,98.694,99.948,%
Batch: 260 | Loss: 0.626 | Acc: 83.507,98.716,99.943,%
Batch: 280 | Loss: 0.627 | Acc: 83.452,98.718,99.942,%
Batch: 300 | Loss: 0.623 | Acc: 83.461,98.733,99.945,%
Batch: 320 | Loss: 0.623 | Acc: 83.504,98.747,99.949,%
Batch: 340 | Loss: 0.623 | Acc: 83.520,98.731,99.943,%
Batch: 360 | Loss: 0.624 | Acc: 83.503,98.727,99.944,%
Batch: 380 | Loss: 0.624 | Acc: 83.469,98.735,99.945,%
Batch: 0 | Loss: 1.005 | Acc: 78.906,92.969,92.188,%
Batch: 20 | Loss: 1.131 | Acc: 80.022,91.109,93.750,%
Batch: 40 | Loss: 1.124 | Acc: 80.069,91.159,93.788,%
Batch: 60 | Loss: 1.114 | Acc: 80.020,91.265,93.840,%

Epoch: 233
Batch: 0 | Loss: 0.394 | Acc: 85.938,100.000,100.000,%
Batch: 20 | Loss: 0.577 | Acc: 84.673,99.479,99.926,%
Batch: 40 | Loss: 0.608 | Acc: 84.146,99.028,99.924,%
Batch: 60 | Loss: 0.607 | Acc: 83.645,98.963,99.923,%
Batch: 80 | Loss: 0.612 | Acc: 83.642,98.862,99.932,%
Batch: 100 | Loss: 0.615 | Acc: 83.485,98.824,99.930,%
Batch: 120 | Loss: 0.620 | Acc: 83.310,98.760,99.929,%
Batch: 140 | Loss: 0.620 | Acc: 83.272,98.726,99.934,%
Batch: 160 | Loss: 0.623 | Acc: 83.196,98.738,99.937,%
Batch: 180 | Loss: 0.622 | Acc: 83.223,98.766,99.940,%
Batch: 200 | Loss: 0.619 | Acc: 83.256,98.764,99.938,%
Batch: 220 | Loss: 0.622 | Acc: 83.219,98.763,99.940,%
Batch: 240 | Loss: 0.622 | Acc: 83.276,98.771,99.942,%
Batch: 260 | Loss: 0.620 | Acc: 83.318,98.758,99.946,%
Batch: 280 | Loss: 0.621 | Acc: 83.280,98.754,99.939,%
Batch: 300 | Loss: 0.622 | Acc: 83.303,98.726,99.940,%
Batch: 320 | Loss: 0.621 | Acc: 83.377,98.698,99.942,%
Batch: 340 | Loss: 0.619 | Acc: 83.447,98.701,99.936,%
Batch: 360 | Loss: 0.619 | Acc: 83.395,98.680,99.935,%
Batch: 380 | Loss: 0.620 | Acc: 83.360,98.677,99.934,%
Batch: 0 | Loss: 1.020 | Acc: 76.562,92.969,92.188,%
Batch: 20 | Loss: 1.144 | Acc: 79.948,90.997,93.490,%
Batch: 40 | Loss: 1.136 | Acc: 79.954,91.063,93.674,%
Batch: 60 | Loss: 1.124 | Acc: 79.752,91.201,93.712,%

Epoch: 234
Batch: 0 | Loss: 0.582 | Acc: 82.812,96.094,100.000,%
Batch: 20 | Loss: 0.656 | Acc: 83.073,98.549,99.926,%
Batch: 40 | Loss: 0.626 | Acc: 83.327,98.761,99.943,%
Batch: 60 | Loss: 0.621 | Acc: 83.607,98.719,99.898,%
Batch: 80 | Loss: 0.609 | Acc: 83.922,98.775,99.894,%
Batch: 100 | Loss: 0.608 | Acc: 83.764,98.770,99.876,%
Batch: 120 | Loss: 0.602 | Acc: 83.581,98.786,99.877,%
Batch: 140 | Loss: 0.601 | Acc: 83.677,98.781,99.884,%
Batch: 160 | Loss: 0.604 | Acc: 83.662,98.734,99.884,%
Batch: 180 | Loss: 0.608 | Acc: 83.482,98.783,99.896,%
Batch: 200 | Loss: 0.610 | Acc: 83.539,98.768,99.899,%
Batch: 220 | Loss: 0.612 | Acc: 83.580,98.784,99.905,%
Batch: 240 | Loss: 0.611 | Acc: 83.626,98.807,99.909,%
Batch: 260 | Loss: 0.608 | Acc: 83.666,98.788,99.913,%
Batch: 280 | Loss: 0.610 | Acc: 83.663,98.793,99.917,%
Batch: 300 | Loss: 0.609 | Acc: 83.705,98.803,99.914,%
Batch: 320 | Loss: 0.606 | Acc: 83.740,98.822,99.920,%
Batch: 340 | Loss: 0.604 | Acc: 83.734,98.827,99.924,%
Batch: 360 | Loss: 0.604 | Acc: 83.741,98.825,99.929,%
Batch: 380 | Loss: 0.604 | Acc: 83.719,98.829,99.932,%
Batch: 0 | Loss: 1.011 | Acc: 79.688,92.188,92.969,%
Batch: 20 | Loss: 1.141 | Acc: 79.725,91.183,93.564,%
Batch: 40 | Loss: 1.134 | Acc: 79.802,91.216,93.598,%
Batch: 60 | Loss: 1.124 | Acc: 79.841,91.265,93.648,%

Epoch: 235
Batch: 0 | Loss: 0.568 | Acc: 85.156,96.875,100.000,%
Batch: 20 | Loss: 0.576 | Acc: 85.082,98.847,100.000,%
Batch: 40 | Loss: 0.580 | Acc: 84.737,98.666,100.000,%
Batch: 60 | Loss: 0.601 | Acc: 83.799,98.655,99.987,%
Batch: 80 | Loss: 0.606 | Acc: 83.951,98.727,99.971,%
Batch: 100 | Loss: 0.608 | Acc: 83.857,98.716,99.954,%
Batch: 120 | Loss: 0.606 | Acc: 83.781,98.747,99.961,%
Batch: 140 | Loss: 0.607 | Acc: 83.588,98.781,99.961,%
Batch: 160 | Loss: 0.608 | Acc: 83.434,98.777,99.966,%
Batch: 180 | Loss: 0.612 | Acc: 83.248,98.744,99.965,%
Batch: 200 | Loss: 0.611 | Acc: 83.271,98.713,99.961,%
Batch: 220 | Loss: 0.609 | Acc: 83.382,98.706,99.954,%
Batch: 240 | Loss: 0.609 | Acc: 83.467,98.703,99.938,%
Batch: 260 | Loss: 0.612 | Acc: 83.408,98.710,99.943,%
Batch: 280 | Loss: 0.611 | Acc: 83.471,98.699,99.947,%
Batch: 300 | Loss: 0.607 | Acc: 83.555,98.707,99.943,%
Batch: 320 | Loss: 0.606 | Acc: 83.550,98.703,99.937,%
Batch: 340 | Loss: 0.607 | Acc: 83.553,98.708,99.938,%
Batch: 360 | Loss: 0.606 | Acc: 83.594,98.732,99.942,%
Batch: 380 | Loss: 0.608 | Acc: 83.559,98.741,99.943,%
Batch: 0 | Loss: 1.006 | Acc: 76.562,92.969,92.188,%
Batch: 20 | Loss: 1.132 | Acc: 79.762,91.406,93.713,%
Batch: 40 | Loss: 1.125 | Acc: 79.821,91.273,93.883,%
Batch: 60 | Loss: 1.114 | Acc: 79.905,91.342,93.904,%

Epoch: 236
Batch: 0 | Loss: 0.612 | Acc: 82.031,99.219,100.000,%
Batch: 20 | Loss: 0.573 | Acc: 84.412,98.698,99.888,%
Batch: 40 | Loss: 0.573 | Acc: 84.318,98.780,99.886,%
Batch: 60 | Loss: 0.593 | Acc: 83.709,98.809,99.910,%
Batch: 80 | Loss: 0.600 | Acc: 83.584,98.746,99.913,%
Batch: 100 | Loss: 0.608 | Acc: 83.555,98.770,99.923,%
Batch: 120 | Loss: 0.607 | Acc: 83.523,98.786,99.923,%
Batch: 140 | Loss: 0.610 | Acc: 83.400,98.798,99.934,%
Batch: 160 | Loss: 0.615 | Acc: 83.273,98.763,99.932,%
Batch: 180 | Loss: 0.615 | Acc: 83.248,98.740,99.935,%
Batch: 200 | Loss: 0.618 | Acc: 83.337,98.725,99.942,%
Batch: 220 | Loss: 0.615 | Acc: 83.435,98.745,99.940,%
Batch: 240 | Loss: 0.616 | Acc: 83.445,98.729,99.945,%
Batch: 260 | Loss: 0.617 | Acc: 83.459,98.746,99.949,%
Batch: 280 | Loss: 0.615 | Acc: 83.496,98.741,99.953,%
Batch: 300 | Loss: 0.613 | Acc: 83.503,98.752,99.953,%
Batch: 320 | Loss: 0.609 | Acc: 83.640,98.771,99.951,%
Batch: 340 | Loss: 0.610 | Acc: 83.626,98.767,99.954,%
Batch: 360 | Loss: 0.610 | Acc: 83.652,98.762,99.957,%
Batch: 380 | Loss: 0.614 | Acc: 83.581,98.745,99.955,%
Batch: 0 | Loss: 0.997 | Acc: 78.906,92.188,93.750,%
Batch: 20 | Loss: 1.139 | Acc: 79.874,91.183,93.713,%
Batch: 40 | Loss: 1.133 | Acc: 79.764,91.216,93.826,%
Batch: 60 | Loss: 1.119 | Acc: 79.675,91.342,93.878,%

Epoch: 237
Batch: 0 | Loss: 0.724 | Acc: 77.344,99.219,99.219,%
Batch: 20 | Loss: 0.595 | Acc: 84.040,98.475,99.926,%
Batch: 40 | Loss: 0.614 | Acc: 83.746,98.666,99.924,%
Batch: 60 | Loss: 0.582 | Acc: 83.901,98.873,99.949,%
Batch: 80 | Loss: 0.594 | Acc: 83.729,98.804,99.942,%
Batch: 100 | Loss: 0.596 | Acc: 83.764,98.817,99.938,%
Batch: 120 | Loss: 0.596 | Acc: 83.884,98.844,99.948,%
Batch: 140 | Loss: 0.606 | Acc: 83.716,98.853,99.956,%
Batch: 160 | Loss: 0.604 | Acc: 83.754,98.831,99.961,%
Batch: 180 | Loss: 0.603 | Acc: 83.715,98.843,99.957,%
Batch: 200 | Loss: 0.604 | Acc: 83.753,98.861,99.957,%
Batch: 220 | Loss: 0.606 | Acc: 83.710,98.826,99.951,%
Batch: 240 | Loss: 0.603 | Acc: 83.740,98.836,99.948,%
Batch: 260 | Loss: 0.602 | Acc: 83.719,98.839,99.952,%
Batch: 280 | Loss: 0.602 | Acc: 83.744,98.824,99.950,%
Batch: 300 | Loss: 0.605 | Acc: 83.700,98.832,99.951,%
Batch: 320 | Loss: 0.605 | Acc: 83.662,98.832,99.946,%
Batch: 340 | Loss: 0.606 | Acc: 83.656,98.845,99.950,%
Batch: 360 | Loss: 0.609 | Acc: 83.587,98.827,99.952,%
Batch: 380 | Loss: 0.610 | Acc: 83.577,98.827,99.953,%
Batch: 0 | Loss: 1.012 | Acc: 80.469,92.969,92.188,%
Batch: 20 | Loss: 1.136 | Acc: 80.060,91.146,93.862,%
Batch: 40 | Loss: 1.129 | Acc: 79.992,91.139,93.845,%
Batch: 60 | Loss: 1.115 | Acc: 80.033,91.355,93.852,%

Epoch: 238
Batch: 0 | Loss: 0.487 | Acc: 84.375,100.000,100.000,%
Batch: 20 | Loss: 0.594 | Acc: 84.524,99.070,99.963,%
Batch: 40 | Loss: 0.595 | Acc: 84.223,98.780,99.962,%
Batch: 60 | Loss: 0.597 | Acc: 84.029,98.886,99.974,%
Batch: 80 | Loss: 0.614 | Acc: 83.709,98.862,99.981,%
Batch: 100 | Loss: 0.619 | Acc: 83.656,98.847,99.985,%
Batch: 120 | Loss: 0.620 | Acc: 83.639,98.838,99.987,%
Batch: 140 | Loss: 0.617 | Acc: 83.594,98.859,99.983,%
Batch: 160 | Loss: 0.613 | Acc: 83.560,98.889,99.985,%
Batch: 180 | Loss: 0.612 | Acc: 83.594,98.895,99.987,%
Batch: 200 | Loss: 0.616 | Acc: 83.512,98.877,99.977,%
Batch: 220 | Loss: 0.612 | Acc: 83.629,98.855,99.972,%
Batch: 240 | Loss: 0.613 | Acc: 83.597,98.833,99.968,%
Batch: 260 | Loss: 0.609 | Acc: 83.699,98.845,99.970,%
Batch: 280 | Loss: 0.612 | Acc: 83.577,98.810,99.969,%
Batch: 300 | Loss: 0.612 | Acc: 83.602,98.832,99.971,%
Batch: 320 | Loss: 0.610 | Acc: 83.611,98.844,99.973,%
Batch: 340 | Loss: 0.611 | Acc: 83.621,98.861,99.973,%
Batch: 360 | Loss: 0.609 | Acc: 83.628,98.857,99.972,%
Batch: 380 | Loss: 0.608 | Acc: 83.645,98.837,99.963,%
Batch: 0 | Loss: 0.998 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.134 | Acc: 79.762,91.109,94.122,%
Batch: 40 | Loss: 1.131 | Acc: 79.878,91.063,93.883,%
Batch: 60 | Loss: 1.120 | Acc: 79.918,91.150,93.878,%

Epoch: 239
Batch: 0 | Loss: 0.682 | Acc: 81.250,99.219,100.000,%
Batch: 20 | Loss: 0.666 | Acc: 83.482,98.475,99.851,%
Batch: 40 | Loss: 0.625 | Acc: 83.861,98.457,99.867,%
Batch: 60 | Loss: 0.610 | Acc: 83.991,98.719,99.910,%
Batch: 80 | Loss: 0.603 | Acc: 84.298,98.736,99.932,%
Batch: 100 | Loss: 0.589 | Acc: 84.305,98.809,99.946,%
Batch: 120 | Loss: 0.588 | Acc: 84.123,98.909,99.955,%
Batch: 140 | Loss: 0.586 | Acc: 84.159,98.975,99.956,%
Batch: 160 | Loss: 0.590 | Acc: 84.084,98.942,99.942,%
Batch: 180 | Loss: 0.589 | Acc: 84.060,98.917,99.944,%
Batch: 200 | Loss: 0.591 | Acc: 84.095,98.865,99.942,%
Batch: 220 | Loss: 0.594 | Acc: 83.919,98.851,99.947,%
Batch: 240 | Loss: 0.594 | Acc: 83.934,98.836,99.951,%
Batch: 260 | Loss: 0.593 | Acc: 83.854,98.821,99.955,%
Batch: 280 | Loss: 0.594 | Acc: 83.844,98.824,99.956,%
Batch: 300 | Loss: 0.594 | Acc: 83.866,98.798,99.951,%
Batch: 320 | Loss: 0.597 | Acc: 83.762,98.793,99.946,%
Batch: 340 | Loss: 0.597 | Acc: 83.766,98.774,99.950,%
Batch: 360 | Loss: 0.595 | Acc: 83.873,98.808,99.952,%
Batch: 380 | Loss: 0.595 | Acc: 83.854,98.811,99.949,%
Batch: 0 | Loss: 1.003 | Acc: 79.688,91.406,92.188,%
Batch: 20 | Loss: 1.132 | Acc: 80.283,90.960,93.899,%
Batch: 40 | Loss: 1.129 | Acc: 80.335,90.968,93.883,%
Batch: 60 | Loss: 1.118 | Acc: 80.289,91.227,93.891,%

Epoch: 240
Batch: 0 | Loss: 0.556 | Acc: 85.938,97.656,100.000,%
Batch: 20 | Loss: 0.587 | Acc: 83.631,99.033,100.000,%
Batch: 40 | Loss: 0.611 | Acc: 83.746,98.685,100.000,%
Batch: 60 | Loss: 0.623 | Acc: 83.619,98.668,99.987,%
Batch: 80 | Loss: 0.615 | Acc: 83.661,98.727,99.971,%
Batch: 100 | Loss: 0.614 | Acc: 83.702,98.770,99.969,%
Batch: 120 | Loss: 0.610 | Acc: 83.820,98.735,99.961,%
Batch: 140 | Loss: 0.605 | Acc: 83.832,98.831,99.961,%
Batch: 160 | Loss: 0.603 | Acc: 83.788,98.860,99.966,%
Batch: 180 | Loss: 0.599 | Acc: 83.762,98.830,99.961,%
Batch: 200 | Loss: 0.600 | Acc: 83.780,98.869,99.961,%
Batch: 220 | Loss: 0.601 | Acc: 83.824,98.855,99.958,%
Batch: 240 | Loss: 0.601 | Acc: 83.837,98.830,99.961,%
Batch: 260 | Loss: 0.601 | Acc: 83.797,98.839,99.961,%
Batch: 280 | Loss: 0.602 | Acc: 83.763,98.838,99.958,%
Batch: 300 | Loss: 0.599 | Acc: 83.820,98.832,99.951,%
Batch: 320 | Loss: 0.600 | Acc: 83.779,98.834,99.954,%
Batch: 340 | Loss: 0.599 | Acc: 83.729,98.825,99.947,%
Batch: 360 | Loss: 0.600 | Acc: 83.724,98.849,99.948,%
Batch: 380 | Loss: 0.600 | Acc: 83.700,98.852,99.951,%
Batch: 0 | Loss: 1.009 | Acc: 78.125,92.969,92.188,%
Batch: 20 | Loss: 1.135 | Acc: 80.208,91.220,93.824,%
Batch: 40 | Loss: 1.131 | Acc: 80.069,91.216,93.960,%
Batch: 60 | Loss: 1.118 | Acc: 80.020,91.304,93.981,%

Epoch: 241
Batch: 0 | Loss: 0.702 | Acc: 81.250,99.219,100.000,%
Batch: 20 | Loss: 0.626 | Acc: 84.189,98.996,99.926,%
Batch: 40 | Loss: 0.600 | Acc: 83.994,99.123,99.962,%
Batch: 60 | Loss: 0.594 | Acc: 84.221,98.937,99.962,%
Batch: 80 | Loss: 0.586 | Acc: 84.404,98.978,99.961,%
Batch: 100 | Loss: 0.586 | Acc: 84.360,98.963,99.954,%
Batch: 120 | Loss: 0.592 | Acc: 84.130,98.889,99.948,%
Batch: 140 | Loss: 0.588 | Acc: 84.214,98.870,99.956,%
Batch: 160 | Loss: 0.596 | Acc: 84.103,98.860,99.956,%
Batch: 180 | Loss: 0.595 | Acc: 84.064,98.869,99.953,%
Batch: 200 | Loss: 0.597 | Acc: 84.072,98.842,99.957,%
Batch: 220 | Loss: 0.599 | Acc: 84.021,98.812,99.954,%
Batch: 240 | Loss: 0.599 | Acc: 84.038,98.797,99.951,%
Batch: 260 | Loss: 0.599 | Acc: 83.965,98.827,99.955,%
Batch: 280 | Loss: 0.600 | Acc: 83.958,98.804,99.956,%
Batch: 300 | Loss: 0.597 | Acc: 83.955,98.829,99.956,%
Batch: 320 | Loss: 0.597 | Acc: 83.900,98.832,99.959,%
Batch: 340 | Loss: 0.598 | Acc: 83.960,98.866,99.959,%
Batch: 360 | Loss: 0.601 | Acc: 83.890,98.864,99.959,%
Batch: 380 | Loss: 0.600 | Acc: 83.862,98.882,99.959,%
Batch: 0 | Loss: 1.004 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.138 | Acc: 80.394,91.220,93.936,%
Batch: 40 | Loss: 1.134 | Acc: 80.297,91.139,93.807,%
Batch: 60 | Loss: 1.123 | Acc: 80.123,91.355,93.763,%

Epoch: 242
Batch: 0 | Loss: 0.390 | Acc: 88.281,99.219,100.000,%
Batch: 20 | Loss: 0.551 | Acc: 83.966,99.144,99.888,%
Batch: 40 | Loss: 0.580 | Acc: 83.670,99.295,99.924,%
Batch: 60 | Loss: 0.566 | Acc: 83.888,99.180,99.936,%
Batch: 80 | Loss: 0.578 | Acc: 83.864,99.113,99.952,%
Batch: 100 | Loss: 0.577 | Acc: 83.841,99.072,99.961,%
Batch: 120 | Loss: 0.577 | Acc: 83.858,99.019,99.968,%
Batch: 140 | Loss: 0.586 | Acc: 83.716,98.947,99.967,%
Batch: 160 | Loss: 0.597 | Acc: 83.545,98.947,99.961,%
Batch: 180 | Loss: 0.598 | Acc: 83.551,98.951,99.957,%
Batch: 200 | Loss: 0.600 | Acc: 83.625,98.943,99.961,%
Batch: 220 | Loss: 0.597 | Acc: 83.735,98.968,99.961,%
Batch: 240 | Loss: 0.598 | Acc: 83.668,98.969,99.964,%
Batch: 260 | Loss: 0.600 | Acc: 83.648,98.976,99.964,%
Batch: 280 | Loss: 0.601 | Acc: 83.585,98.980,99.967,%
Batch: 300 | Loss: 0.602 | Acc: 83.651,98.996,99.964,%
Batch: 320 | Loss: 0.599 | Acc: 83.667,98.995,99.961,%
Batch: 340 | Loss: 0.598 | Acc: 83.690,98.976,99.963,%
Batch: 360 | Loss: 0.603 | Acc: 83.644,98.976,99.965,%
Batch: 380 | Loss: 0.603 | Acc: 83.678,98.993,99.963,%
Batch: 0 | Loss: 1.000 | Acc: 78.906,92.969,92.188,%
Batch: 20 | Loss: 1.134 | Acc: 80.469,91.332,93.824,%
Batch: 40 | Loss: 1.129 | Acc: 80.354,91.216,93.902,%
Batch: 60 | Loss: 1.118 | Acc: 80.200,91.419,93.814,%

Epoch: 243
Batch: 0 | Loss: 0.506 | Acc: 85.938,98.438,100.000,%
Batch: 20 | Loss: 0.618 | Acc: 83.408,98.735,99.963,%
Batch: 40 | Loss: 0.619 | Acc: 83.841,98.800,99.924,%
Batch: 60 | Loss: 0.616 | Acc: 83.696,98.847,99.936,%
Batch: 80 | Loss: 0.613 | Acc: 83.738,98.833,99.942,%
Batch: 100 | Loss: 0.615 | Acc: 83.702,98.832,99.946,%
Batch: 120 | Loss: 0.611 | Acc: 83.600,98.844,99.942,%
Batch: 140 | Loss: 0.613 | Acc: 83.461,98.842,99.945,%
Batch: 160 | Loss: 0.610 | Acc: 83.579,98.855,99.951,%
Batch: 180 | Loss: 0.613 | Acc: 83.551,98.822,99.944,%
Batch: 200 | Loss: 0.614 | Acc: 83.532,98.826,99.946,%
Batch: 220 | Loss: 0.613 | Acc: 83.597,98.826,99.936,%
Batch: 240 | Loss: 0.616 | Acc: 83.519,98.814,99.942,%
Batch: 260 | Loss: 0.612 | Acc: 83.615,98.833,99.943,%
Batch: 280 | Loss: 0.613 | Acc: 83.658,98.821,99.947,%
Batch: 300 | Loss: 0.611 | Acc: 83.690,98.840,99.945,%
Batch: 320 | Loss: 0.611 | Acc: 83.728,98.851,99.949,%
Batch: 340 | Loss: 0.608 | Acc: 83.727,98.873,99.950,%
Batch: 360 | Loss: 0.608 | Acc: 83.717,98.862,99.950,%
Batch: 380 | Loss: 0.604 | Acc: 83.799,98.854,99.947,%
Batch: 0 | Loss: 1.020 | Acc: 77.344,92.188,92.188,%
Batch: 20 | Loss: 1.136 | Acc: 80.432,91.443,93.862,%
Batch: 40 | Loss: 1.132 | Acc: 80.316,91.235,93.902,%
Batch: 60 | Loss: 1.123 | Acc: 80.187,91.393,93.878,%

Epoch: 244
Batch: 0 | Loss: 0.449 | Acc: 84.375,97.656,100.000,%
Batch: 20 | Loss: 0.597 | Acc: 84.412,98.512,100.000,%
Batch: 40 | Loss: 0.608 | Acc: 84.451,98.819,99.981,%
Batch: 60 | Loss: 0.622 | Acc: 83.747,98.822,99.962,%
Batch: 80 | Loss: 0.615 | Acc: 83.729,98.823,99.942,%
Batch: 100 | Loss: 0.613 | Acc: 83.663,98.840,99.954,%
Batch: 120 | Loss: 0.604 | Acc: 83.723,98.831,99.955,%
Batch: 140 | Loss: 0.602 | Acc: 83.826,98.848,99.961,%
Batch: 160 | Loss: 0.598 | Acc: 83.919,98.845,99.966,%
Batch: 180 | Loss: 0.604 | Acc: 83.814,98.830,99.961,%
Batch: 200 | Loss: 0.601 | Acc: 83.858,98.853,99.965,%
Batch: 220 | Loss: 0.602 | Acc: 83.862,98.840,99.968,%
Batch: 240 | Loss: 0.602 | Acc: 83.947,98.836,99.971,%
Batch: 260 | Loss: 0.600 | Acc: 84.001,98.833,99.973,%
Batch: 280 | Loss: 0.601 | Acc: 84.014,98.824,99.969,%
Batch: 300 | Loss: 0.602 | Acc: 84.025,98.848,99.969,%
Batch: 320 | Loss: 0.604 | Acc: 83.954,98.846,99.971,%
Batch: 340 | Loss: 0.605 | Acc: 83.944,98.813,99.973,%
Batch: 360 | Loss: 0.604 | Acc: 83.949,98.821,99.974,%
Batch: 380 | Loss: 0.604 | Acc: 83.871,98.821,99.973,%
Batch: 0 | Loss: 0.999 | Acc: 78.906,92.969,92.969,%
Batch: 20 | Loss: 1.136 | Acc: 80.171,91.257,93.824,%
Batch: 40 | Loss: 1.132 | Acc: 80.335,91.197,93.750,%
Batch: 60 | Loss: 1.120 | Acc: 80.136,91.355,93.699,%

Epoch: 245
Batch: 0 | Loss: 0.715 | Acc: 80.469,100.000,100.000,%
Batch: 20 | Loss: 0.617 | Acc: 83.854,98.698,99.963,%
Batch: 40 | Loss: 0.589 | Acc: 84.775,98.819,99.981,%
Batch: 60 | Loss: 0.591 | Acc: 84.516,98.770,99.974,%
Batch: 80 | Loss: 0.598 | Acc: 84.510,98.765,99.961,%
Batch: 100 | Loss: 0.585 | Acc: 84.592,98.817,99.969,%
Batch: 120 | Loss: 0.582 | Acc: 84.685,98.838,99.974,%
Batch: 140 | Loss: 0.585 | Acc: 84.342,98.881,99.978,%
Batch: 160 | Loss: 0.592 | Acc: 84.225,98.860,99.966,%
Batch: 180 | Loss: 0.597 | Acc: 84.112,98.886,99.970,%
Batch: 200 | Loss: 0.603 | Acc: 83.912,98.908,99.973,%
Batch: 220 | Loss: 0.606 | Acc: 83.788,98.904,99.972,%
Batch: 240 | Loss: 0.607 | Acc: 83.766,98.917,99.971,%
Batch: 260 | Loss: 0.604 | Acc: 83.878,98.940,99.973,%
Batch: 280 | Loss: 0.605 | Acc: 83.863,98.935,99.969,%
Batch: 300 | Loss: 0.602 | Acc: 83.957,98.925,99.971,%
Batch: 320 | Loss: 0.598 | Acc: 83.976,98.922,99.966,%
Batch: 340 | Loss: 0.600 | Acc: 83.912,98.932,99.968,%
Batch: 360 | Loss: 0.600 | Acc: 83.892,98.909,99.959,%
Batch: 380 | Loss: 0.601 | Acc: 83.893,98.897,99.953,%
Batch: 0 | Loss: 1.007 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.134 | Acc: 80.246,91.257,93.899,%
Batch: 40 | Loss: 1.130 | Acc: 80.316,91.292,93.826,%
Batch: 60 | Loss: 1.120 | Acc: 80.187,91.393,93.865,%

Epoch: 246
Batch: 0 | Loss: 0.890 | Acc: 76.562,99.219,100.000,%
Batch: 20 | Loss: 0.585 | Acc: 84.747,98.772,99.963,%
Batch: 40 | Loss: 0.584 | Acc: 84.585,98.895,99.943,%
Batch: 60 | Loss: 0.600 | Acc: 84.055,99.014,99.949,%
Batch: 80 | Loss: 0.591 | Acc: 84.028,99.064,99.942,%
Batch: 100 | Loss: 0.584 | Acc: 84.174,99.033,99.923,%
Batch: 120 | Loss: 0.581 | Acc: 84.123,98.986,99.929,%
Batch: 140 | Loss: 0.584 | Acc: 84.142,98.986,99.928,%
Batch: 160 | Loss: 0.581 | Acc: 84.297,99.000,99.932,%
Batch: 180 | Loss: 0.581 | Acc: 84.388,98.981,99.931,%
Batch: 200 | Loss: 0.587 | Acc: 84.297,98.997,99.934,%
Batch: 220 | Loss: 0.585 | Acc: 84.350,98.996,99.940,%
Batch: 240 | Loss: 0.585 | Acc: 84.320,98.963,99.945,%
Batch: 260 | Loss: 0.584 | Acc: 84.273,98.964,99.946,%
Batch: 280 | Loss: 0.586 | Acc: 84.161,98.946,99.939,%
Batch: 300 | Loss: 0.587 | Acc: 84.188,98.931,99.938,%
Batch: 320 | Loss: 0.589 | Acc: 84.175,98.924,99.942,%
Batch: 340 | Loss: 0.590 | Acc: 84.144,98.921,99.943,%
Batch: 360 | Loss: 0.590 | Acc: 84.148,98.942,99.946,%
Batch: 380 | Loss: 0.594 | Acc: 84.055,98.938,99.949,%
Batch: 0 | Loss: 1.003 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.134 | Acc: 80.357,91.220,93.899,%
Batch: 40 | Loss: 1.130 | Acc: 80.183,91.178,93.921,%
Batch: 60 | Loss: 1.117 | Acc: 80.136,91.368,93.916,%

Epoch: 247
Batch: 0 | Loss: 0.641 | Acc: 81.250,99.219,100.000,%
Batch: 20 | Loss: 0.620 | Acc: 83.817,99.107,100.000,%
Batch: 40 | Loss: 0.596 | Acc: 83.975,99.085,100.000,%
Batch: 60 | Loss: 0.611 | Acc: 83.952,98.860,100.000,%
Batch: 80 | Loss: 0.600 | Acc: 84.144,98.881,99.990,%
Batch: 100 | Loss: 0.603 | Acc: 84.143,98.878,99.985,%
Batch: 120 | Loss: 0.597 | Acc: 84.162,98.838,99.968,%
Batch: 140 | Loss: 0.596 | Acc: 84.081,98.853,99.945,%
Batch: 160 | Loss: 0.598 | Acc: 83.909,98.865,99.947,%
Batch: 180 | Loss: 0.599 | Acc: 83.991,98.873,99.948,%
Batch: 200 | Loss: 0.601 | Acc: 83.843,98.881,99.934,%
Batch: 220 | Loss: 0.600 | Acc: 83.827,98.894,99.933,%
Batch: 240 | Loss: 0.602 | Acc: 83.824,98.895,99.932,%
Batch: 260 | Loss: 0.602 | Acc: 83.914,98.886,99.931,%
Batch: 280 | Loss: 0.602 | Acc: 83.886,98.882,99.933,%
Batch: 300 | Loss: 0.600 | Acc: 83.822,98.889,99.933,%
Batch: 320 | Loss: 0.597 | Acc: 83.861,98.893,99.934,%
Batch: 340 | Loss: 0.594 | Acc: 83.898,98.893,99.936,%
Batch: 360 | Loss: 0.593 | Acc: 83.925,98.903,99.939,%
Batch: 380 | Loss: 0.593 | Acc: 83.944,98.913,99.936,%
Batch: 0 | Loss: 0.999 | Acc: 78.906,92.188,92.188,%
Batch: 20 | Loss: 1.131 | Acc: 80.506,91.220,93.936,%
Batch: 40 | Loss: 1.127 | Acc: 80.316,91.159,93.883,%
Batch: 60 | Loss: 1.116 | Acc: 80.187,91.304,93.852,%

Epoch: 248
Batch: 0 | Loss: 0.606 | Acc: 85.938,100.000,100.000,%
Batch: 20 | Loss: 0.637 | Acc: 83.185,99.256,99.963,%
Batch: 40 | Loss: 0.627 | Acc: 83.136,99.028,99.924,%
Batch: 60 | Loss: 0.605 | Acc: 83.683,98.924,99.910,%
Batch: 80 | Loss: 0.608 | Acc: 83.584,98.891,99.913,%
Batch: 100 | Loss: 0.602 | Acc: 83.733,98.894,99.907,%
Batch: 120 | Loss: 0.593 | Acc: 83.904,98.928,99.916,%
Batch: 140 | Loss: 0.599 | Acc: 83.987,98.925,99.928,%
Batch: 160 | Loss: 0.603 | Acc: 83.865,98.884,99.927,%
Batch: 180 | Loss: 0.597 | Acc: 84.012,98.912,99.931,%
Batch: 200 | Loss: 0.597 | Acc: 84.002,98.900,99.926,%
Batch: 220 | Loss: 0.600 | Acc: 83.933,98.858,99.929,%
Batch: 240 | Loss: 0.604 | Acc: 83.843,98.872,99.935,%
Batch: 260 | Loss: 0.602 | Acc: 83.881,98.842,99.937,%
Batch: 280 | Loss: 0.604 | Acc: 83.838,98.855,99.942,%
Batch: 300 | Loss: 0.603 | Acc: 83.864,98.863,99.945,%
Batch: 320 | Loss: 0.608 | Acc: 83.701,98.868,99.946,%
Batch: 340 | Loss: 0.608 | Acc: 83.724,98.882,99.943,%
Batch: 360 | Loss: 0.607 | Acc: 83.752,98.879,99.942,%
Batch: 380 | Loss: 0.604 | Acc: 83.834,98.887,99.938,%
Batch: 0 | Loss: 1.004 | Acc: 78.906,92.188,92.188,%
Batch: 20 | Loss: 1.136 | Acc: 80.506,91.183,93.750,%
Batch: 40 | Loss: 1.132 | Acc: 80.278,91.101,93.636,%
Batch: 60 | Loss: 1.120 | Acc: 80.225,91.278,93.673,%

Epoch: 249
Batch: 0 | Loss: 0.697 | Acc: 85.938,100.000,100.000,%
Batch: 20 | Loss: 0.618 | Acc: 83.519,98.958,100.000,%
Batch: 40 | Loss: 0.600 | Acc: 83.651,98.838,99.924,%
Batch: 60 | Loss: 0.599 | Acc: 83.299,98.937,99.910,%
Batch: 80 | Loss: 0.612 | Acc: 83.353,98.929,99.923,%
Batch: 100 | Loss: 0.624 | Acc: 83.338,98.902,99.938,%
Batch: 120 | Loss: 0.623 | Acc: 83.342,98.870,99.935,%
Batch: 140 | Loss: 0.616 | Acc: 83.566,98.908,99.934,%
Batch: 160 | Loss: 0.613 | Acc: 83.545,98.962,99.937,%
Batch: 180 | Loss: 0.609 | Acc: 83.706,98.943,99.940,%
Batch: 200 | Loss: 0.609 | Acc: 83.811,98.939,99.938,%
Batch: 220 | Loss: 0.608 | Acc: 83.845,98.943,99.936,%
Batch: 240 | Loss: 0.610 | Acc: 83.775,98.937,99.938,%
Batch: 260 | Loss: 0.610 | Acc: 83.737,98.910,99.937,%
Batch: 280 | Loss: 0.607 | Acc: 83.819,98.882,99.936,%
Batch: 300 | Loss: 0.609 | Acc: 83.749,98.894,99.933,%
Batch: 320 | Loss: 0.609 | Acc: 83.776,98.873,99.937,%
Batch: 340 | Loss: 0.607 | Acc: 83.855,98.887,99.940,%
Batch: 360 | Loss: 0.606 | Acc: 83.847,98.872,99.939,%
Batch: 380 | Loss: 0.604 | Acc: 83.885,98.868,99.938,%
Batch: 0 | Loss: 0.997 | Acc: 78.906,92.969,92.188,%
Batch: 20 | Loss: 1.134 | Acc: 80.469,91.109,93.936,%
Batch: 40 | Loss: 1.130 | Acc: 80.354,91.006,93.979,%
Batch: 60 | Loss: 1.118 | Acc: 80.277,91.278,93.981,%

Epoch: 250
Batch: 0 | Loss: 0.658 | Acc: 85.156,99.219,100.000,%
Batch: 20 | Loss: 0.623 | Acc: 83.854,99.033,99.926,%
Batch: 40 | Loss: 0.599 | Acc: 84.089,98.914,99.924,%
Batch: 60 | Loss: 0.579 | Acc: 84.285,98.860,99.949,%
Batch: 80 | Loss: 0.587 | Acc: 84.221,98.910,99.942,%
Batch: 100 | Loss: 0.595 | Acc: 83.934,98.886,99.954,%
Batch: 120 | Loss: 0.600 | Acc: 83.826,98.844,99.955,%
Batch: 140 | Loss: 0.607 | Acc: 83.527,98.870,99.961,%
Batch: 160 | Loss: 0.609 | Acc: 83.497,98.860,99.966,%
Batch: 180 | Loss: 0.612 | Acc: 83.387,98.856,99.970,%
Batch: 200 | Loss: 0.612 | Acc: 83.427,98.853,99.961,%
Batch: 220 | Loss: 0.611 | Acc: 83.491,98.844,99.961,%
Batch: 240 | Loss: 0.609 | Acc: 83.454,98.849,99.955,%
Batch: 260 | Loss: 0.610 | Acc: 83.480,98.839,99.955,%
Batch: 280 | Loss: 0.611 | Acc: 83.483,98.838,99.950,%
Batch: 300 | Loss: 0.607 | Acc: 83.550,98.840,99.948,%
Batch: 320 | Loss: 0.609 | Acc: 83.533,98.795,99.951,%
Batch: 340 | Loss: 0.610 | Acc: 83.532,98.788,99.947,%
Batch: 360 | Loss: 0.608 | Acc: 83.607,98.803,99.948,%
Batch: 380 | Loss: 0.608 | Acc: 83.668,98.778,99.945,%
Batch: 0 | Loss: 0.988 | Acc: 78.906,92.188,92.188,%
Batch: 20 | Loss: 1.133 | Acc: 80.357,91.146,93.824,%
Batch: 40 | Loss: 1.129 | Acc: 80.221,91.044,93.921,%
Batch: 60 | Loss: 1.119 | Acc: 80.161,91.137,93.878,%

Epoch: 251
Batch: 0 | Loss: 0.660 | Acc: 82.031,100.000,100.000,%
Batch: 20 | Loss: 0.626 | Acc: 83.705,98.549,100.000,%
Batch: 40 | Loss: 0.597 | Acc: 84.261,98.800,99.943,%
Batch: 60 | Loss: 0.591 | Acc: 84.285,98.847,99.936,%
Batch: 80 | Loss: 0.599 | Acc: 84.230,98.920,99.942,%
Batch: 100 | Loss: 0.605 | Acc: 84.073,98.832,99.946,%
Batch: 120 | Loss: 0.597 | Acc: 84.162,98.922,99.955,%
Batch: 140 | Loss: 0.597 | Acc: 84.236,98.920,99.945,%
Batch: 160 | Loss: 0.597 | Acc: 84.113,98.865,99.951,%
Batch: 180 | Loss: 0.595 | Acc: 84.086,98.899,99.953,%
Batch: 200 | Loss: 0.598 | Acc: 84.083,98.892,99.946,%
Batch: 220 | Loss: 0.598 | Acc: 84.004,98.901,99.940,%
Batch: 240 | Loss: 0.596 | Acc: 84.048,98.872,99.932,%
Batch: 260 | Loss: 0.597 | Acc: 83.989,98.863,99.934,%
Batch: 280 | Loss: 0.597 | Acc: 83.883,98.882,99.936,%
Batch: 300 | Loss: 0.600 | Acc: 83.820,98.871,99.930,%
Batch: 320 | Loss: 0.601 | Acc: 83.820,98.876,99.932,%
Batch: 340 | Loss: 0.600 | Acc: 83.839,98.857,99.927,%
Batch: 360 | Loss: 0.602 | Acc: 83.799,98.840,99.929,%
Batch: 380 | Loss: 0.601 | Acc: 83.842,98.831,99.928,%
Batch: 0 | Loss: 1.009 | Acc: 78.125,91.406,92.188,%
Batch: 20 | Loss: 1.139 | Acc: 79.948,90.997,93.862,%
Batch: 40 | Loss: 1.131 | Acc: 80.107,91.120,93.788,%
Batch: 60 | Loss: 1.121 | Acc: 80.174,91.201,93.776,%

Epoch: 252
Batch: 0 | Loss: 0.457 | Acc: 83.594,98.438,100.000,%
Batch: 20 | Loss: 0.598 | Acc: 83.631,99.033,100.000,%
Batch: 40 | Loss: 0.579 | Acc: 84.280,99.066,99.981,%
Batch: 60 | Loss: 0.582 | Acc: 84.209,98.847,99.974,%
Batch: 80 | Loss: 0.590 | Acc: 84.172,98.891,99.981,%
Batch: 100 | Loss: 0.585 | Acc: 84.189,98.832,99.985,%
Batch: 120 | Loss: 0.590 | Acc: 84.110,98.818,99.981,%
Batch: 140 | Loss: 0.595 | Acc: 83.854,98.836,99.978,%
Batch: 160 | Loss: 0.594 | Acc: 83.807,98.855,99.976,%
Batch: 180 | Loss: 0.595 | Acc: 83.684,98.843,99.965,%
Batch: 200 | Loss: 0.594 | Acc: 83.745,98.850,99.961,%
Batch: 220 | Loss: 0.591 | Acc: 83.778,98.855,99.958,%
Batch: 240 | Loss: 0.590 | Acc: 83.775,98.862,99.958,%
Batch: 260 | Loss: 0.590 | Acc: 83.836,98.881,99.958,%
Batch: 280 | Loss: 0.593 | Acc: 83.808,98.871,99.956,%
Batch: 300 | Loss: 0.593 | Acc: 83.749,98.863,99.958,%
Batch: 320 | Loss: 0.595 | Acc: 83.803,98.868,99.956,%
Batch: 340 | Loss: 0.594 | Acc: 83.823,98.868,99.954,%
Batch: 360 | Loss: 0.596 | Acc: 83.760,98.866,99.946,%
Batch: 380 | Loss: 0.598 | Acc: 83.733,98.876,99.943,%
Batch: 0 | Loss: 1.002 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.133 | Acc: 80.022,91.071,93.862,%
Batch: 40 | Loss: 1.127 | Acc: 79.992,91.159,93.921,%
Batch: 60 | Loss: 1.116 | Acc: 80.161,91.278,93.891,%

Epoch: 253
Batch: 0 | Loss: 0.519 | Acc: 86.719,97.656,100.000,%
Batch: 20 | Loss: 0.624 | Acc: 84.561,98.958,100.000,%
Batch: 40 | Loss: 0.598 | Acc: 84.604,98.819,99.962,%
Batch: 60 | Loss: 0.599 | Acc: 84.477,98.719,99.974,%
Batch: 80 | Loss: 0.604 | Acc: 84.346,98.727,99.971,%
Batch: 100 | Loss: 0.602 | Acc: 84.120,98.731,99.961,%
Batch: 120 | Loss: 0.613 | Acc: 84.097,98.722,99.968,%
Batch: 140 | Loss: 0.605 | Acc: 84.076,98.770,99.967,%
Batch: 160 | Loss: 0.607 | Acc: 84.040,98.816,99.971,%
Batch: 180 | Loss: 0.606 | Acc: 83.991,98.826,99.970,%
Batch: 200 | Loss: 0.602 | Acc: 83.959,98.853,99.961,%
Batch: 220 | Loss: 0.604 | Acc: 83.951,98.844,99.954,%
Batch: 240 | Loss: 0.605 | Acc: 83.918,98.843,99.951,%
Batch: 260 | Loss: 0.605 | Acc: 83.875,98.809,99.952,%
Batch: 280 | Loss: 0.609 | Acc: 83.816,98.804,99.956,%
Batch: 300 | Loss: 0.607 | Acc: 83.908,98.822,99.958,%
Batch: 320 | Loss: 0.603 | Acc: 84.000,98.834,99.954,%
Batch: 340 | Loss: 0.602 | Acc: 84.034,98.838,99.954,%
Batch: 360 | Loss: 0.603 | Acc: 84.001,98.857,99.957,%
Batch: 380 | Loss: 0.603 | Acc: 83.961,98.850,99.955,%
Batch: 0 | Loss: 1.003 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.130 | Acc: 80.432,91.369,93.750,%
Batch: 40 | Loss: 1.122 | Acc: 80.354,91.197,93.845,%
Batch: 60 | Loss: 1.116 | Acc: 80.366,91.201,93.929,%

Epoch: 254
Batch: 0 | Loss: 0.632 | Acc: 80.469,99.219,99.219,%
Batch: 20 | Loss: 0.603 | Acc: 83.110,98.921,99.851,%
Batch: 40 | Loss: 0.612 | Acc: 83.365,98.876,99.924,%
Batch: 60 | Loss: 0.599 | Acc: 83.491,98.899,99.910,%
Batch: 80 | Loss: 0.593 | Acc: 83.517,98.939,99.932,%
Batch: 100 | Loss: 0.594 | Acc: 83.601,98.933,99.923,%
Batch: 120 | Loss: 0.596 | Acc: 83.555,98.896,99.935,%
Batch: 140 | Loss: 0.603 | Acc: 83.494,98.908,99.939,%
Batch: 160 | Loss: 0.597 | Acc: 83.618,98.889,99.937,%
Batch: 180 | Loss: 0.597 | Acc: 83.671,98.895,99.935,%
Batch: 200 | Loss: 0.593 | Acc: 83.745,98.884,99.938,%
Batch: 220 | Loss: 0.592 | Acc: 83.820,98.865,99.940,%
Batch: 240 | Loss: 0.595 | Acc: 83.869,98.843,99.942,%
Batch: 260 | Loss: 0.596 | Acc: 83.767,98.860,99.943,%
Batch: 280 | Loss: 0.594 | Acc: 83.783,98.857,99.942,%
Batch: 300 | Loss: 0.596 | Acc: 83.773,98.871,99.945,%
Batch: 320 | Loss: 0.599 | Acc: 83.725,98.888,99.944,%
Batch: 340 | Loss: 0.600 | Acc: 83.685,98.877,99.945,%
Batch: 360 | Loss: 0.600 | Acc: 83.665,98.883,99.948,%
Batch: 380 | Loss: 0.599 | Acc: 83.707,98.878,99.947,%
Batch: 0 | Loss: 0.999 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.136 | Acc: 79.911,91.034,93.787,%
Batch: 40 | Loss: 1.129 | Acc: 79.973,91.044,93.845,%
Batch: 60 | Loss: 1.119 | Acc: 79.918,91.214,93.788,%

Epoch: 255
Batch: 0 | Loss: 0.669 | Acc: 78.906,100.000,100.000,%
Batch: 20 | Loss: 0.578 | Acc: 84.077,99.070,100.000,%
Batch: 40 | Loss: 0.577 | Acc: 84.546,98.800,99.962,%
Batch: 60 | Loss: 0.581 | Acc: 84.260,98.975,99.974,%
Batch: 80 | Loss: 0.587 | Acc: 84.250,98.949,99.961,%
Batch: 100 | Loss: 0.587 | Acc: 84.097,98.840,99.969,%
Batch: 120 | Loss: 0.585 | Acc: 84.072,98.825,99.961,%
Batch: 140 | Loss: 0.590 | Acc: 83.976,98.842,99.956,%
Batch: 160 | Loss: 0.593 | Acc: 84.016,98.840,99.951,%
Batch: 180 | Loss: 0.594 | Acc: 83.943,98.843,99.957,%
Batch: 200 | Loss: 0.591 | Acc: 84.021,98.861,99.957,%
Batch: 220 | Loss: 0.591 | Acc: 84.057,98.848,99.958,%
Batch: 240 | Loss: 0.589 | Acc: 84.093,98.823,99.961,%
Batch: 260 | Loss: 0.592 | Acc: 84.061,98.830,99.961,%
Batch: 280 | Loss: 0.593 | Acc: 84.027,98.824,99.964,%
Batch: 300 | Loss: 0.591 | Acc: 84.053,98.832,99.966,%
Batch: 320 | Loss: 0.590 | Acc: 84.046,98.820,99.966,%
Batch: 340 | Loss: 0.592 | Acc: 84.061,98.832,99.966,%
Batch: 360 | Loss: 0.591 | Acc: 84.059,98.825,99.965,%
Batch: 380 | Loss: 0.591 | Acc: 84.035,98.800,99.965,%
Batch: 0 | Loss: 1.004 | Acc: 78.906,90.625,93.750,%
Batch: 20 | Loss: 1.139 | Acc: 79.799,91.071,93.973,%
Batch: 40 | Loss: 1.132 | Acc: 80.011,91.101,94.017,%
Batch: 60 | Loss: 1.123 | Acc: 80.020,91.201,93.840,%

Epoch: 256
Batch: 0 | Loss: 0.608 | Acc: 85.938,100.000,100.000,%
Batch: 20 | Loss: 0.587 | Acc: 84.598,98.958,99.926,%
Batch: 40 | Loss: 0.601 | Acc: 83.956,98.800,99.924,%
Batch: 60 | Loss: 0.610 | Acc: 84.221,98.758,99.949,%
Batch: 80 | Loss: 0.600 | Acc: 84.269,98.765,99.961,%
Batch: 100 | Loss: 0.603 | Acc: 84.042,98.832,99.961,%
Batch: 120 | Loss: 0.605 | Acc: 83.975,98.818,99.961,%
Batch: 140 | Loss: 0.609 | Acc: 83.937,98.897,99.967,%
Batch: 160 | Loss: 0.610 | Acc: 83.715,98.840,99.956,%
Batch: 180 | Loss: 0.607 | Acc: 83.797,98.835,99.953,%
Batch: 200 | Loss: 0.608 | Acc: 83.780,98.822,99.957,%
Batch: 220 | Loss: 0.607 | Acc: 83.827,98.791,99.958,%
Batch: 240 | Loss: 0.605 | Acc: 83.840,98.801,99.948,%
Batch: 260 | Loss: 0.603 | Acc: 83.854,98.812,99.949,%
Batch: 280 | Loss: 0.602 | Acc: 83.836,98.816,99.947,%
Batch: 300 | Loss: 0.601 | Acc: 83.840,98.796,99.951,%
Batch: 320 | Loss: 0.600 | Acc: 83.832,98.783,99.949,%
Batch: 340 | Loss: 0.601 | Acc: 83.880,98.804,99.952,%
Batch: 360 | Loss: 0.601 | Acc: 83.793,98.818,99.952,%
Batch: 380 | Loss: 0.601 | Acc: 83.836,98.841,99.955,%
Batch: 0 | Loss: 0.986 | Acc: 81.250,93.750,92.188,%
Batch: 20 | Loss: 1.147 | Acc: 79.948,91.146,93.452,%
Batch: 40 | Loss: 1.137 | Acc: 79.783,91.082,93.750,%
Batch: 60 | Loss: 1.125 | Acc: 79.918,91.176,93.724,%

Epoch: 257
Batch: 0 | Loss: 0.680 | Acc: 84.375,99.219,100.000,%
Batch: 20 | Loss: 0.599 | Acc: 83.743,99.070,100.000,%
Batch: 40 | Loss: 0.601 | Acc: 84.356,98.914,99.981,%
Batch: 60 | Loss: 0.594 | Acc: 84.375,98.886,99.936,%
Batch: 80 | Loss: 0.580 | Acc: 84.751,98.872,99.942,%
Batch: 100 | Loss: 0.584 | Acc: 84.638,98.894,99.954,%
Batch: 120 | Loss: 0.593 | Acc: 84.446,98.864,99.955,%
Batch: 140 | Loss: 0.586 | Acc: 84.574,98.881,99.961,%
Batch: 160 | Loss: 0.585 | Acc: 84.564,98.918,99.961,%
Batch: 180 | Loss: 0.587 | Acc: 84.509,98.869,99.957,%
Batch: 200 | Loss: 0.597 | Acc: 84.255,98.811,99.961,%
Batch: 220 | Loss: 0.599 | Acc: 84.195,98.791,99.961,%
Batch: 240 | Loss: 0.600 | Acc: 84.103,98.804,99.958,%
Batch: 260 | Loss: 0.596 | Acc: 84.145,98.812,99.958,%
Batch: 280 | Loss: 0.599 | Acc: 84.105,98.832,99.958,%
Batch: 300 | Loss: 0.599 | Acc: 84.118,98.850,99.961,%
Batch: 320 | Loss: 0.598 | Acc: 84.151,98.863,99.959,%
Batch: 340 | Loss: 0.600 | Acc: 84.112,98.854,99.959,%
Batch: 360 | Loss: 0.601 | Acc: 84.081,98.844,99.955,%
Batch: 380 | Loss: 0.601 | Acc: 84.018,98.831,99.953,%
Batch: 0 | Loss: 0.987 | Acc: 78.906,93.750,94.531,%
Batch: 20 | Loss: 1.134 | Acc: 80.171,91.220,93.750,%
Batch: 40 | Loss: 1.125 | Acc: 80.354,91.273,93.960,%
Batch: 60 | Loss: 1.114 | Acc: 80.315,91.470,93.955,%

Epoch: 258
Batch: 0 | Loss: 0.456 | Acc: 86.719,98.438,100.000,%
Batch: 20 | Loss: 0.598 | Acc: 83.966,99.330,99.963,%
Batch: 40 | Loss: 0.584 | Acc: 84.261,99.276,99.924,%
Batch: 60 | Loss: 0.599 | Acc: 84.157,99.232,99.936,%
Batch: 80 | Loss: 0.599 | Acc: 84.259,99.199,99.952,%
Batch: 100 | Loss: 0.601 | Acc: 84.042,99.203,99.961,%
Batch: 120 | Loss: 0.599 | Acc: 84.168,99.154,99.961,%
Batch: 140 | Loss: 0.600 | Acc: 83.943,99.025,99.967,%
Batch: 160 | Loss: 0.597 | Acc: 83.938,99.034,99.966,%
Batch: 180 | Loss: 0.599 | Acc: 83.874,99.012,99.965,%
Batch: 200 | Loss: 0.599 | Acc: 83.862,98.970,99.957,%
Batch: 220 | Loss: 0.596 | Acc: 83.859,98.936,99.961,%
Batch: 240 | Loss: 0.597 | Acc: 83.873,98.924,99.961,%
Batch: 260 | Loss: 0.595 | Acc: 83.788,98.922,99.964,%
Batch: 280 | Loss: 0.595 | Acc: 83.752,98.927,99.961,%
Batch: 300 | Loss: 0.596 | Acc: 83.807,98.925,99.961,%
Batch: 320 | Loss: 0.594 | Acc: 83.827,98.927,99.963,%
Batch: 340 | Loss: 0.595 | Acc: 83.830,98.909,99.966,%
Batch: 360 | Loss: 0.595 | Acc: 83.832,98.894,99.963,%
Batch: 380 | Loss: 0.596 | Acc: 83.891,98.889,99.965,%
Batch: 0 | Loss: 0.986 | Acc: 79.688,92.969,92.188,%
Batch: 20 | Loss: 1.134 | Acc: 80.134,90.997,93.638,%
Batch: 40 | Loss: 1.127 | Acc: 80.164,90.968,93.731,%
Batch: 60 | Loss: 1.115 | Acc: 80.123,91.124,93.788,%

Epoch: 259
Batch: 0 | Loss: 0.616 | Acc: 84.375,99.219,100.000,%
Batch: 20 | Loss: 0.582 | Acc: 85.454,98.958,99.926,%
Batch: 40 | Loss: 0.601 | Acc: 84.356,98.819,99.924,%
Batch: 60 | Loss: 0.612 | Acc: 83.927,98.886,99.949,%
Batch: 80 | Loss: 0.603 | Acc: 83.806,98.939,99.961,%
Batch: 100 | Loss: 0.608 | Acc: 83.764,98.925,99.954,%
Batch: 120 | Loss: 0.604 | Acc: 83.884,98.973,99.955,%
Batch: 140 | Loss: 0.604 | Acc: 83.826,98.942,99.945,%
Batch: 160 | Loss: 0.599 | Acc: 83.977,98.957,99.942,%
Batch: 180 | Loss: 0.598 | Acc: 83.874,98.977,99.948,%
Batch: 200 | Loss: 0.595 | Acc: 83.940,98.989,99.953,%
Batch: 220 | Loss: 0.598 | Acc: 83.781,99.000,99.958,%
Batch: 240 | Loss: 0.600 | Acc: 83.749,99.005,99.961,%
Batch: 260 | Loss: 0.602 | Acc: 83.639,98.985,99.958,%
Batch: 280 | Loss: 0.601 | Acc: 83.702,98.985,99.953,%
Batch: 300 | Loss: 0.601 | Acc: 83.755,98.985,99.956,%
Batch: 320 | Loss: 0.597 | Acc: 83.849,98.973,99.954,%
Batch: 340 | Loss: 0.599 | Acc: 83.823,98.971,99.952,%
Batch: 360 | Loss: 0.597 | Acc: 83.838,98.974,99.946,%
Batch: 380 | Loss: 0.597 | Acc: 83.889,98.960,99.949,%
Batch: 0 | Loss: 0.973 | Acc: 78.906,92.969,94.531,%
Batch: 20 | Loss: 1.134 | Acc: 79.948,90.885,93.787,%
Batch: 40 | Loss: 1.128 | Acc: 80.050,90.911,93.864,%
Batch: 60 | Loss: 1.118 | Acc: 80.008,91.112,93.750,%

Epoch: 260
Batch: 0 | Loss: 0.468 | Acc: 82.812,100.000,100.000,%
Batch: 20 | Loss: 0.603 | Acc: 83.296,98.698,99.963,%
Batch: 40 | Loss: 0.605 | Acc: 83.727,98.723,99.962,%
Batch: 60 | Loss: 0.594 | Acc: 83.888,98.796,99.974,%
Batch: 80 | Loss: 0.603 | Acc: 83.652,98.804,99.971,%
Batch: 100 | Loss: 0.592 | Acc: 83.795,98.847,99.977,%
Batch: 120 | Loss: 0.587 | Acc: 83.891,98.831,99.981,%
Batch: 140 | Loss: 0.586 | Acc: 83.915,98.798,99.983,%
Batch: 160 | Loss: 0.591 | Acc: 83.885,98.826,99.985,%
Batch: 180 | Loss: 0.592 | Acc: 83.909,98.822,99.987,%
Batch: 200 | Loss: 0.595 | Acc: 83.808,98.834,99.984,%
Batch: 220 | Loss: 0.594 | Acc: 83.866,98.837,99.986,%
Batch: 240 | Loss: 0.597 | Acc: 83.779,98.817,99.977,%
Batch: 260 | Loss: 0.597 | Acc: 83.731,98.833,99.979,%
Batch: 280 | Loss: 0.596 | Acc: 83.763,98.830,99.978,%
Batch: 300 | Loss: 0.597 | Acc: 83.672,98.845,99.977,%
Batch: 320 | Loss: 0.596 | Acc: 83.698,98.842,99.976,%
Batch: 340 | Loss: 0.594 | Acc: 83.722,98.868,99.977,%
Batch: 360 | Loss: 0.593 | Acc: 83.750,98.875,99.976,%
Batch: 380 | Loss: 0.594 | Acc: 83.780,98.876,99.973,%
Batch: 0 | Loss: 0.999 | Acc: 78.125,92.969,92.969,%
Batch: 20 | Loss: 1.135 | Acc: 80.246,91.034,93.676,%
Batch: 40 | Loss: 1.128 | Acc: 80.145,91.216,93.769,%
Batch: 60 | Loss: 1.116 | Acc: 80.213,91.342,93.776,%

Epoch: 261
Batch: 0 | Loss: 0.468 | Acc: 86.719,98.438,100.000,%
Batch: 20 | Loss: 0.587 | Acc: 83.817,98.810,99.963,%
Batch: 40 | Loss: 0.605 | Acc: 83.689,98.933,99.981,%
Batch: 60 | Loss: 0.611 | Acc: 83.811,98.899,99.974,%
Batch: 80 | Loss: 0.602 | Acc: 83.980,98.920,99.971,%
Batch: 100 | Loss: 0.608 | Acc: 83.996,98.963,99.969,%
Batch: 120 | Loss: 0.599 | Acc: 84.020,98.986,99.974,%
Batch: 140 | Loss: 0.599 | Acc: 84.054,99.003,99.978,%
Batch: 160 | Loss: 0.597 | Acc: 84.050,98.996,99.981,%
Batch: 180 | Loss: 0.599 | Acc: 83.926,98.960,99.978,%
Batch: 200 | Loss: 0.595 | Acc: 84.010,98.989,99.981,%
Batch: 220 | Loss: 0.600 | Acc: 83.933,98.961,99.982,%
Batch: 240 | Loss: 0.596 | Acc: 83.999,98.959,99.981,%
Batch: 260 | Loss: 0.595 | Acc: 84.046,98.961,99.982,%
Batch: 280 | Loss: 0.594 | Acc: 84.044,98.988,99.983,%
Batch: 300 | Loss: 0.594 | Acc: 84.077,98.996,99.982,%
Batch: 320 | Loss: 0.593 | Acc: 84.063,99.009,99.983,%
Batch: 340 | Loss: 0.592 | Acc: 84.059,99.001,99.977,%
Batch: 360 | Loss: 0.590 | Acc: 84.120,99.020,99.976,%
Batch: 380 | Loss: 0.592 | Acc: 84.051,98.962,99.975,%
Batch: 0 | Loss: 0.972 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.134 | Acc: 79.836,90.997,93.862,%
Batch: 40 | Loss: 1.127 | Acc: 79.973,91.063,93.921,%
Batch: 60 | Loss: 1.114 | Acc: 80.059,91.137,93.929,%

Epoch: 262
Batch: 0 | Loss: 0.631 | Acc: 84.375,100.000,100.000,%
Batch: 20 | Loss: 0.626 | Acc: 83.296,98.698,100.000,%
Batch: 40 | Loss: 0.595 | Acc: 83.746,98.838,99.943,%
Batch: 60 | Loss: 0.596 | Acc: 83.824,99.001,99.962,%
Batch: 80 | Loss: 0.595 | Acc: 84.018,98.987,99.971,%
Batch: 100 | Loss: 0.596 | Acc: 83.926,98.994,99.977,%
Batch: 120 | Loss: 0.602 | Acc: 83.878,98.993,99.968,%
Batch: 140 | Loss: 0.600 | Acc: 84.026,98.997,99.956,%
Batch: 160 | Loss: 0.598 | Acc: 84.098,99.015,99.961,%
Batch: 180 | Loss: 0.598 | Acc: 84.125,99.020,99.961,%
Batch: 200 | Loss: 0.599 | Acc: 84.072,98.997,99.965,%
Batch: 220 | Loss: 0.597 | Acc: 84.099,98.993,99.968,%
Batch: 240 | Loss: 0.596 | Acc: 84.106,98.979,99.968,%
Batch: 260 | Loss: 0.593 | Acc: 84.145,98.985,99.970,%
Batch: 280 | Loss: 0.588 | Acc: 84.169,98.991,99.967,%
Batch: 300 | Loss: 0.588 | Acc: 84.084,98.980,99.966,%
Batch: 320 | Loss: 0.589 | Acc: 84.078,98.949,99.968,%
Batch: 340 | Loss: 0.588 | Acc: 84.045,98.974,99.970,%
Batch: 360 | Loss: 0.584 | Acc: 84.098,98.959,99.965,%
Batch: 380 | Loss: 0.582 | Acc: 84.129,98.971,99.965,%
Batch: 0 | Loss: 0.976 | Acc: 77.344,92.188,92.188,%
Batch: 20 | Loss: 1.136 | Acc: 80.060,91.109,93.527,%
Batch: 40 | Loss: 1.127 | Acc: 80.240,91.159,93.788,%
Batch: 60 | Loss: 1.115 | Acc: 80.238,91.214,93.840,%

Epoch: 263
Batch: 0 | Loss: 0.609 | Acc: 81.250,97.656,100.000,%
Batch: 20 | Loss: 0.548 | Acc: 84.487,98.698,99.963,%
Batch: 40 | Loss: 0.568 | Acc: 84.165,98.590,99.924,%
Batch: 60 | Loss: 0.583 | Acc: 83.901,98.566,99.923,%
Batch: 80 | Loss: 0.582 | Acc: 83.902,98.727,99.913,%
Batch: 100 | Loss: 0.581 | Acc: 83.996,98.793,99.930,%
Batch: 120 | Loss: 0.585 | Acc: 84.091,98.760,99.942,%
Batch: 140 | Loss: 0.587 | Acc: 84.004,98.770,99.950,%
Batch: 160 | Loss: 0.585 | Acc: 83.948,98.821,99.951,%
Batch: 180 | Loss: 0.586 | Acc: 83.935,98.835,99.957,%
Batch: 200 | Loss: 0.589 | Acc: 84.072,98.834,99.957,%
Batch: 220 | Loss: 0.592 | Acc: 84.103,98.787,99.951,%
Batch: 240 | Loss: 0.594 | Acc: 83.989,98.817,99.948,%
Batch: 260 | Loss: 0.593 | Acc: 84.121,98.869,99.949,%
Batch: 280 | Loss: 0.591 | Acc: 84.169,98.868,99.950,%
Batch: 300 | Loss: 0.591 | Acc: 84.134,98.868,99.948,%
Batch: 320 | Loss: 0.593 | Acc: 84.100,98.885,99.951,%
Batch: 340 | Loss: 0.593 | Acc: 84.121,98.893,99.954,%
Batch: 360 | Loss: 0.592 | Acc: 84.102,98.911,99.957,%
Batch: 380 | Loss: 0.593 | Acc: 84.117,98.909,99.953,%
Batch: 0 | Loss: 0.982 | Acc: 78.125,92.969,92.969,%
Batch: 20 | Loss: 1.134 | Acc: 80.097,91.034,93.676,%
Batch: 40 | Loss: 1.126 | Acc: 80.221,91.139,93.921,%
Batch: 60 | Loss: 1.114 | Acc: 80.289,91.304,93.904,%

Epoch: 264
Batch: 0 | Loss: 0.596 | Acc: 82.031,99.219,100.000,%
Batch: 20 | Loss: 0.621 | Acc: 83.445,98.996,100.000,%
Batch: 40 | Loss: 0.579 | Acc: 84.242,99.009,99.962,%
Batch: 60 | Loss: 0.590 | Acc: 83.863,98.988,99.974,%
Batch: 80 | Loss: 0.590 | Acc: 84.018,98.920,99.981,%
Batch: 100 | Loss: 0.587 | Acc: 83.988,98.971,99.977,%
Batch: 120 | Loss: 0.591 | Acc: 84.007,98.980,99.974,%
Batch: 140 | Loss: 0.595 | Acc: 83.876,98.975,99.972,%
Batch: 160 | Loss: 0.599 | Acc: 83.929,99.039,99.976,%
Batch: 180 | Loss: 0.595 | Acc: 84.077,99.033,99.978,%
Batch: 200 | Loss: 0.599 | Acc: 84.049,98.997,99.977,%
Batch: 220 | Loss: 0.598 | Acc: 83.997,98.993,99.972,%
Batch: 240 | Loss: 0.600 | Acc: 83.957,98.969,99.974,%
Batch: 260 | Loss: 0.603 | Acc: 83.920,98.922,99.970,%
Batch: 280 | Loss: 0.602 | Acc: 83.911,98.966,99.972,%
Batch: 300 | Loss: 0.600 | Acc: 84.004,98.959,99.971,%
Batch: 320 | Loss: 0.599 | Acc: 83.981,98.951,99.968,%
Batch: 340 | Loss: 0.601 | Acc: 84.013,98.962,99.968,%
Batch: 360 | Loss: 0.600 | Acc: 83.981,98.955,99.968,%
Batch: 380 | Loss: 0.598 | Acc: 84.084,98.956,99.969,%
Batch: 0 | Loss: 0.978 | Acc: 78.906,92.188,93.750,%
Batch: 20 | Loss: 1.132 | Acc: 80.246,91.332,93.936,%
Batch: 40 | Loss: 1.126 | Acc: 80.259,91.311,93.998,%
Batch: 60 | Loss: 1.113 | Acc: 80.315,91.317,93.993,%

Epoch: 265
Batch: 0 | Loss: 0.787 | Acc: 85.156,96.094,100.000,%
Batch: 20 | Loss: 0.617 | Acc: 85.565,99.070,99.963,%
Batch: 40 | Loss: 0.576 | Acc: 86.052,99.066,99.943,%
Batch: 60 | Loss: 0.601 | Acc: 85.361,99.039,99.949,%
Batch: 80 | Loss: 0.597 | Acc: 85.253,99.007,99.961,%
Batch: 100 | Loss: 0.583 | Acc: 85.234,99.049,99.969,%
Batch: 120 | Loss: 0.587 | Acc: 84.924,98.954,99.974,%
Batch: 140 | Loss: 0.587 | Acc: 84.730,99.003,99.978,%
Batch: 160 | Loss: 0.592 | Acc: 84.501,99.005,99.981,%
Batch: 180 | Loss: 0.593 | Acc: 84.289,99.029,99.978,%
Batch: 200 | Loss: 0.593 | Acc: 84.274,99.024,99.977,%
Batch: 220 | Loss: 0.595 | Acc: 84.255,99.035,99.975,%
Batch: 240 | Loss: 0.595 | Acc: 84.171,99.008,99.974,%
Batch: 260 | Loss: 0.595 | Acc: 84.234,98.988,99.976,%
Batch: 280 | Loss: 0.593 | Acc: 84.217,98.988,99.978,%
Batch: 300 | Loss: 0.593 | Acc: 84.253,98.996,99.977,%
Batch: 320 | Loss: 0.594 | Acc: 84.192,98.983,99.973,%
Batch: 340 | Loss: 0.594 | Acc: 84.231,98.980,99.975,%
Batch: 360 | Loss: 0.594 | Acc: 84.200,98.972,99.976,%
Batch: 380 | Loss: 0.592 | Acc: 84.277,98.983,99.971,%
Batch: 0 | Loss: 0.996 | Acc: 78.906,92.188,94.531,%
Batch: 20 | Loss: 1.133 | Acc: 80.208,91.257,93.862,%
Batch: 40 | Loss: 1.126 | Acc: 80.297,91.216,93.998,%
Batch: 60 | Loss: 1.112 | Acc: 80.264,91.278,93.968,%

Epoch: 266
Batch: 0 | Loss: 0.719 | Acc: 84.375,97.656,100.000,%
Batch: 20 | Loss: 0.607 | Acc: 83.371,98.698,100.000,%
Batch: 40 | Loss: 0.614 | Acc: 82.717,98.628,99.981,%
Batch: 60 | Loss: 0.609 | Acc: 83.017,98.745,99.987,%
Batch: 80 | Loss: 0.602 | Acc: 83.343,98.775,99.990,%
Batch: 100 | Loss: 0.595 | Acc: 83.648,98.801,99.992,%
Batch: 120 | Loss: 0.589 | Acc: 83.858,98.838,99.987,%
Batch: 140 | Loss: 0.592 | Acc: 83.898,98.825,99.989,%
Batch: 160 | Loss: 0.585 | Acc: 84.103,98.860,99.976,%
Batch: 180 | Loss: 0.587 | Acc: 84.237,98.865,99.974,%
Batch: 200 | Loss: 0.585 | Acc: 84.239,98.869,99.977,%
Batch: 220 | Loss: 0.583 | Acc: 84.326,98.908,99.972,%
Batch: 240 | Loss: 0.583 | Acc: 84.411,98.895,99.974,%
Batch: 260 | Loss: 0.586 | Acc: 84.333,98.892,99.964,%
Batch: 280 | Loss: 0.587 | Acc: 84.292,98.924,99.961,%
Batch: 300 | Loss: 0.585 | Acc: 84.380,98.938,99.961,%
Batch: 320 | Loss: 0.589 | Acc: 84.319,98.917,99.959,%
Batch: 340 | Loss: 0.590 | Acc: 84.329,98.928,99.956,%
Batch: 360 | Loss: 0.590 | Acc: 84.254,98.922,99.952,%
Batch: 380 | Loss: 0.593 | Acc: 84.178,98.926,99.955,%
Batch: 0 | Loss: 0.979 | Acc: 78.125,91.406,92.188,%
Batch: 20 | Loss: 1.135 | Acc: 80.060,90.997,93.564,%
Batch: 40 | Loss: 1.128 | Acc: 80.202,91.178,93.807,%
Batch: 60 | Loss: 1.116 | Acc: 80.149,91.265,93.827,%

Epoch: 267
Batch: 0 | Loss: 0.632 | Acc: 82.031,100.000,100.000,%
Batch: 20 | Loss: 0.604 | Acc: 84.487,98.847,99.963,%
Batch: 40 | Loss: 0.592 | Acc: 84.470,98.742,99.962,%
Batch: 60 | Loss: 0.591 | Acc: 84.119,98.796,99.974,%
Batch: 80 | Loss: 0.596 | Acc: 84.057,98.785,99.981,%
Batch: 100 | Loss: 0.593 | Acc: 84.166,98.847,99.977,%
Batch: 120 | Loss: 0.600 | Acc: 84.084,98.838,99.974,%
Batch: 140 | Loss: 0.596 | Acc: 84.242,98.892,99.978,%
Batch: 160 | Loss: 0.593 | Acc: 84.307,98.835,99.981,%
Batch: 180 | Loss: 0.586 | Acc: 84.414,98.852,99.974,%
Batch: 200 | Loss: 0.586 | Acc: 84.282,98.888,99.977,%
Batch: 220 | Loss: 0.590 | Acc: 84.241,98.833,99.979,%
Batch: 240 | Loss: 0.588 | Acc: 84.333,98.875,99.977,%
Batch: 260 | Loss: 0.587 | Acc: 84.381,98.875,99.976,%
Batch: 280 | Loss: 0.587 | Acc: 84.378,98.891,99.975,%
Batch: 300 | Loss: 0.585 | Acc: 84.362,98.920,99.971,%
Batch: 320 | Loss: 0.585 | Acc: 84.358,98.919,99.973,%
Batch: 340 | Loss: 0.588 | Acc: 84.221,98.903,99.970,%
Batch: 360 | Loss: 0.589 | Acc: 84.236,98.916,99.970,%
Batch: 380 | Loss: 0.589 | Acc: 84.213,98.901,99.967,%
Batch: 0 | Loss: 0.982 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.132 | Acc: 80.246,90.997,93.676,%
Batch: 40 | Loss: 1.125 | Acc: 80.183,91.139,93.845,%
Batch: 60 | Loss: 1.112 | Acc: 80.123,91.304,93.904,%

Epoch: 268
Batch: 0 | Loss: 0.536 | Acc: 85.938,100.000,100.000,%
Batch: 20 | Loss: 0.561 | Acc: 85.603,98.847,99.963,%
Batch: 40 | Loss: 0.577 | Acc: 85.061,98.914,99.962,%
Batch: 60 | Loss: 0.580 | Acc: 84.593,98.937,99.962,%
Batch: 80 | Loss: 0.581 | Acc: 84.587,98.997,99.971,%
Batch: 100 | Loss: 0.585 | Acc: 84.421,99.018,99.969,%
Batch: 120 | Loss: 0.589 | Acc: 84.207,99.032,99.968,%
Batch: 140 | Loss: 0.593 | Acc: 84.081,99.003,99.972,%
Batch: 160 | Loss: 0.593 | Acc: 84.084,98.962,99.976,%
Batch: 180 | Loss: 0.599 | Acc: 83.974,98.938,99.978,%
Batch: 200 | Loss: 0.596 | Acc: 84.025,98.947,99.981,%
Batch: 220 | Loss: 0.595 | Acc: 83.958,98.950,99.982,%
Batch: 240 | Loss: 0.595 | Acc: 83.941,98.933,99.981,%
Batch: 260 | Loss: 0.595 | Acc: 84.025,98.991,99.982,%
Batch: 280 | Loss: 0.591 | Acc: 84.094,98.996,99.983,%
Batch: 300 | Loss: 0.589 | Acc: 84.134,99.006,99.982,%
Batch: 320 | Loss: 0.587 | Acc: 84.110,99.024,99.981,%
Batch: 340 | Loss: 0.586 | Acc: 84.130,99.017,99.979,%
Batch: 360 | Loss: 0.586 | Acc: 84.130,99.009,99.976,%
Batch: 380 | Loss: 0.588 | Acc: 84.074,99.003,99.975,%
Batch: 0 | Loss: 0.972 | Acc: 78.906,92.969,93.750,%
Batch: 20 | Loss: 1.129 | Acc: 80.283,90.885,93.750,%
Batch: 40 | Loss: 1.123 | Acc: 80.393,90.968,93.979,%
Batch: 60 | Loss: 1.112 | Acc: 80.366,91.112,93.929,%

Epoch: 269
Batch: 0 | Loss: 0.576 | Acc: 85.938,96.094,100.000,%
Batch: 20 | Loss: 0.589 | Acc: 84.226,98.512,99.926,%
Batch: 40 | Loss: 0.588 | Acc: 84.261,98.704,99.943,%
Batch: 60 | Loss: 0.593 | Acc: 84.170,98.796,99.962,%
Batch: 80 | Loss: 0.590 | Acc: 84.201,98.881,99.961,%
Batch: 100 | Loss: 0.585 | Acc: 84.244,98.909,99.961,%
Batch: 120 | Loss: 0.595 | Acc: 84.149,98.851,99.961,%
Batch: 140 | Loss: 0.591 | Acc: 84.187,98.903,99.961,%
Batch: 160 | Loss: 0.588 | Acc: 84.244,98.923,99.966,%
Batch: 180 | Loss: 0.589 | Acc: 84.172,98.930,99.970,%
Batch: 200 | Loss: 0.589 | Acc: 84.216,98.939,99.973,%
Batch: 220 | Loss: 0.589 | Acc: 84.318,98.936,99.975,%
Batch: 240 | Loss: 0.587 | Acc: 84.378,98.937,99.977,%
Batch: 260 | Loss: 0.587 | Acc: 84.345,98.934,99.979,%
Batch: 280 | Loss: 0.590 | Acc: 84.317,98.916,99.978,%
Batch: 300 | Loss: 0.589 | Acc: 84.380,98.931,99.977,%
Batch: 320 | Loss: 0.589 | Acc: 84.348,98.953,99.978,%
Batch: 340 | Loss: 0.586 | Acc: 84.357,98.971,99.979,%
Batch: 360 | Loss: 0.585 | Acc: 84.321,98.976,99.978,%
Batch: 380 | Loss: 0.586 | Acc: 84.254,98.991,99.979,%
Batch: 0 | Loss: 0.978 | Acc: 79.688,92.188,92.188,%
Batch: 20 | Loss: 1.135 | Acc: 79.985,91.183,93.638,%
Batch: 40 | Loss: 1.127 | Acc: 80.183,91.216,93.921,%
Batch: 60 | Loss: 1.116 | Acc: 80.200,91.240,93.865,%

Epoch: 270
Batch: 0 | Loss: 0.591 | Acc: 84.375,99.219,100.000,%
Batch: 20 | Loss: 0.606 | Acc: 83.519,99.219,99.963,%
Batch: 40 | Loss: 0.588 | Acc: 83.937,99.200,99.981,%
Batch: 60 | Loss: 0.579 | Acc: 84.080,99.257,99.987,%
Batch: 80 | Loss: 0.591 | Acc: 83.845,99.238,99.981,%
Batch: 100 | Loss: 0.585 | Acc: 84.058,99.211,99.969,%
Batch: 120 | Loss: 0.588 | Acc: 83.917,99.070,99.968,%
Batch: 140 | Loss: 0.586 | Acc: 84.037,99.091,99.961,%
Batch: 160 | Loss: 0.582 | Acc: 84.074,99.093,99.961,%
Batch: 180 | Loss: 0.587 | Acc: 83.969,99.072,99.957,%
Batch: 200 | Loss: 0.585 | Acc: 84.083,99.079,99.953,%
Batch: 220 | Loss: 0.587 | Acc: 84.082,99.060,99.954,%
Batch: 240 | Loss: 0.586 | Acc: 84.022,99.063,99.958,%
Batch: 260 | Loss: 0.584 | Acc: 84.052,99.024,99.961,%
Batch: 280 | Loss: 0.586 | Acc: 84.075,99.032,99.964,%
Batch: 300 | Loss: 0.586 | Acc: 84.173,99.003,99.966,%
Batch: 320 | Loss: 0.585 | Acc: 84.173,98.995,99.968,%
Batch: 340 | Loss: 0.583 | Acc: 84.173,98.985,99.970,%
Batch: 360 | Loss: 0.579 | Acc: 84.176,99.007,99.972,%
Batch: 380 | Loss: 0.580 | Acc: 84.168,99.003,99.973,%
Batch: 0 | Loss: 0.985 | Acc: 79.688,92.188,92.969,%
Batch: 20 | Loss: 1.133 | Acc: 80.208,91.071,93.713,%
Batch: 40 | Loss: 1.126 | Acc: 80.259,91.101,93.883,%
Batch: 60 | Loss: 1.114 | Acc: 80.213,91.201,93.840,%

Epoch: 271
Batch: 0 | Loss: 0.606 | Acc: 80.469,97.656,100.000,%
Batch: 20 | Loss: 0.577 | Acc: 83.891,98.735,100.000,%
Batch: 40 | Loss: 0.576 | Acc: 83.918,98.857,99.981,%
Batch: 60 | Loss: 0.574 | Acc: 84.247,98.809,99.987,%
Batch: 80 | Loss: 0.581 | Acc: 84.095,98.843,99.981,%
Batch: 100 | Loss: 0.583 | Acc: 83.919,98.855,99.977,%
Batch: 120 | Loss: 0.582 | Acc: 84.013,98.818,99.968,%
Batch: 140 | Loss: 0.587 | Acc: 83.882,98.836,99.961,%
Batch: 160 | Loss: 0.583 | Acc: 84.089,98.889,99.966,%
Batch: 180 | Loss: 0.584 | Acc: 84.107,98.943,99.970,%
Batch: 200 | Loss: 0.586 | Acc: 84.107,98.916,99.969,%
Batch: 220 | Loss: 0.584 | Acc: 84.202,98.901,99.972,%
Batch: 240 | Loss: 0.585 | Acc: 84.187,98.888,99.971,%
Batch: 260 | Loss: 0.586 | Acc: 84.168,98.889,99.973,%
Batch: 280 | Loss: 0.584 | Acc: 84.194,98.868,99.967,%
Batch: 300 | Loss: 0.584 | Acc: 84.201,98.892,99.969,%
Batch: 320 | Loss: 0.586 | Acc: 84.168,98.917,99.968,%
Batch: 340 | Loss: 0.589 | Acc: 84.146,98.919,99.970,%
Batch: 360 | Loss: 0.587 | Acc: 84.180,98.916,99.965,%
Batch: 380 | Loss: 0.586 | Acc: 84.254,98.926,99.967,%
Batch: 0 | Loss: 0.985 | Acc: 79.688,92.188,92.969,%
Batch: 20 | Loss: 1.140 | Acc: 80.060,91.034,93.824,%
Batch: 40 | Loss: 1.132 | Acc: 80.240,91.082,93.864,%
Batch: 60 | Loss: 1.122 | Acc: 80.174,91.150,93.801,%

Epoch: 272
Batch: 0 | Loss: 0.660 | Acc: 79.688,99.219,100.000,%
Batch: 20 | Loss: 0.643 | Acc: 83.594,99.107,99.926,%
Batch: 40 | Loss: 0.644 | Acc: 83.365,99.066,99.962,%
Batch: 60 | Loss: 0.620 | Acc: 83.645,98.988,99.962,%
Batch: 80 | Loss: 0.615 | Acc: 83.565,99.007,99.942,%
Batch: 100 | Loss: 0.611 | Acc: 83.516,99.056,99.938,%
Batch: 120 | Loss: 0.605 | Acc: 83.736,98.999,99.948,%
Batch: 140 | Loss: 0.596 | Acc: 83.815,98.997,99.956,%
Batch: 160 | Loss: 0.594 | Acc: 83.768,98.971,99.951,%
Batch: 180 | Loss: 0.592 | Acc: 83.779,98.977,99.957,%
Batch: 200 | Loss: 0.586 | Acc: 83.881,98.982,99.949,%
Batch: 220 | Loss: 0.589 | Acc: 83.919,98.978,99.951,%
Batch: 240 | Loss: 0.587 | Acc: 83.986,98.982,99.951,%
Batch: 260 | Loss: 0.585 | Acc: 84.064,98.985,99.955,%
Batch: 280 | Loss: 0.586 | Acc: 84.055,98.994,99.947,%
Batch: 300 | Loss: 0.588 | Acc: 84.084,98.977,99.945,%
Batch: 320 | Loss: 0.586 | Acc: 84.124,98.946,99.946,%
Batch: 340 | Loss: 0.589 | Acc: 84.079,98.946,99.947,%
Batch: 360 | Loss: 0.588 | Acc: 84.139,98.959,99.950,%
Batch: 380 | Loss: 0.590 | Acc: 84.137,98.967,99.953,%
Batch: 0 | Loss: 0.968 | Acc: 78.906,93.750,95.312,%
Batch: 20 | Loss: 1.131 | Acc: 80.097,91.332,93.787,%
Batch: 40 | Loss: 1.125 | Acc: 80.259,91.311,93.960,%
Batch: 60 | Loss: 1.113 | Acc: 80.277,91.329,93.955,%

Epoch: 273
Batch: 0 | Loss: 0.580 | Acc: 80.469,97.656,100.000,%
Batch: 20 | Loss: 0.604 | Acc: 84.115,99.144,99.963,%
Batch: 40 | Loss: 0.595 | Acc: 84.356,99.085,99.924,%
Batch: 60 | Loss: 0.583 | Acc: 84.593,99.129,99.936,%
Batch: 80 | Loss: 0.601 | Acc: 84.375,99.132,99.932,%
Batch: 100 | Loss: 0.604 | Acc: 84.298,99.087,99.930,%
Batch: 120 | Loss: 0.609 | Acc: 84.091,99.096,99.935,%
Batch: 140 | Loss: 0.608 | Acc: 83.965,99.097,99.939,%
Batch: 160 | Loss: 0.599 | Acc: 84.089,99.078,99.947,%
Batch: 180 | Loss: 0.599 | Acc: 83.995,99.068,99.948,%
Batch: 200 | Loss: 0.599 | Acc: 84.095,99.063,99.946,%
Batch: 220 | Loss: 0.600 | Acc: 83.990,99.053,99.951,%
Batch: 240 | Loss: 0.603 | Acc: 83.934,99.057,99.955,%
Batch: 260 | Loss: 0.601 | Acc: 84.007,99.042,99.955,%
Batch: 280 | Loss: 0.600 | Acc: 84.022,99.063,99.958,%
Batch: 300 | Loss: 0.598 | Acc: 83.975,99.058,99.961,%
Batch: 320 | Loss: 0.597 | Acc: 84.039,99.056,99.961,%
Batch: 340 | Loss: 0.598 | Acc: 84.011,99.052,99.959,%
Batch: 360 | Loss: 0.597 | Acc: 84.018,99.048,99.961,%
Batch: 380 | Loss: 0.597 | Acc: 84.057,99.059,99.963,%
Batch: 0 | Loss: 0.971 | Acc: 78.906,92.969,93.750,%
Batch: 20 | Loss: 1.133 | Acc: 80.208,91.109,93.638,%
Batch: 40 | Loss: 1.126 | Acc: 80.278,91.120,93.845,%
Batch: 60 | Loss: 1.114 | Acc: 80.264,91.214,93.840,%

Epoch: 274
Batch: 0 | Loss: 0.666 | Acc: 78.125,96.094,99.219,%
Batch: 20 | Loss: 0.575 | Acc: 84.487,98.735,99.963,%
Batch: 40 | Loss: 0.591 | Acc: 83.803,98.704,99.943,%
Batch: 60 | Loss: 0.580 | Acc: 84.157,98.783,99.962,%
Batch: 80 | Loss: 0.593 | Acc: 83.816,98.833,99.971,%
Batch: 100 | Loss: 0.593 | Acc: 84.104,98.902,99.969,%
Batch: 120 | Loss: 0.593 | Acc: 84.046,98.935,99.974,%
Batch: 140 | Loss: 0.593 | Acc: 84.048,98.881,99.972,%
Batch: 160 | Loss: 0.599 | Acc: 83.933,98.874,99.976,%
Batch: 180 | Loss: 0.596 | Acc: 83.991,98.917,99.978,%
Batch: 200 | Loss: 0.592 | Acc: 84.080,98.908,99.973,%
Batch: 220 | Loss: 0.594 | Acc: 84.170,98.929,99.972,%
Batch: 240 | Loss: 0.594 | Acc: 84.077,98.914,99.971,%
Batch: 260 | Loss: 0.594 | Acc: 84.097,98.919,99.973,%
Batch: 280 | Loss: 0.592 | Acc: 84.155,98.957,99.975,%
Batch: 300 | Loss: 0.590 | Acc: 84.222,98.967,99.969,%
Batch: 320 | Loss: 0.590 | Acc: 84.239,98.946,99.968,%
Batch: 340 | Loss: 0.589 | Acc: 84.263,98.935,99.968,%
Batch: 360 | Loss: 0.589 | Acc: 84.202,98.946,99.965,%
Batch: 380 | Loss: 0.586 | Acc: 84.242,98.944,99.967,%
Batch: 0 | Loss: 0.988 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.140 | Acc: 79.985,90.997,93.750,%
Batch: 40 | Loss: 1.132 | Acc: 80.126,91.063,93.788,%
Batch: 60 | Loss: 1.120 | Acc: 80.110,91.189,93.750,%

Epoch: 275
Batch: 0 | Loss: 0.655 | Acc: 86.719,100.000,100.000,%
Batch: 20 | Loss: 0.590 | Acc: 84.970,99.293,100.000,%
Batch: 40 | Loss: 0.585 | Acc: 85.271,99.009,100.000,%
Batch: 60 | Loss: 0.574 | Acc: 85.207,98.873,99.974,%
Batch: 80 | Loss: 0.578 | Acc: 84.934,98.968,99.971,%
Batch: 100 | Loss: 0.584 | Acc: 84.568,98.956,99.969,%
Batch: 120 | Loss: 0.589 | Acc: 84.388,98.986,99.974,%
Batch: 140 | Loss: 0.592 | Acc: 84.342,98.953,99.978,%
Batch: 160 | Loss: 0.594 | Acc: 84.254,98.981,99.971,%
Batch: 180 | Loss: 0.595 | Acc: 84.090,98.947,99.974,%
Batch: 200 | Loss: 0.594 | Acc: 84.006,98.927,99.969,%
Batch: 220 | Loss: 0.596 | Acc: 83.993,98.936,99.968,%
Batch: 240 | Loss: 0.596 | Acc: 84.009,98.959,99.968,%
Batch: 260 | Loss: 0.594 | Acc: 84.058,98.964,99.967,%
Batch: 280 | Loss: 0.592 | Acc: 84.072,98.974,99.969,%
Batch: 300 | Loss: 0.590 | Acc: 84.115,98.988,99.971,%
Batch: 320 | Loss: 0.592 | Acc: 84.132,99.007,99.973,%
Batch: 340 | Loss: 0.591 | Acc: 84.144,99.029,99.970,%
Batch: 360 | Loss: 0.590 | Acc: 84.195,99.005,99.972,%
Batch: 380 | Loss: 0.588 | Acc: 84.225,99.001,99.971,%
Batch: 0 | Loss: 0.978 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.129 | Acc: 80.320,91.034,93.601,%
Batch: 40 | Loss: 1.122 | Acc: 80.316,91.139,93.864,%
Batch: 60 | Loss: 1.111 | Acc: 80.302,91.253,93.852,%

Epoch: 276
Batch: 0 | Loss: 0.627 | Acc: 85.938,96.875,100.000,%
Batch: 20 | Loss: 0.629 | Acc: 83.780,98.661,99.926,%
Batch: 40 | Loss: 0.622 | Acc: 83.765,98.819,99.962,%
Batch: 60 | Loss: 0.615 | Acc: 84.042,98.783,99.949,%
Batch: 80 | Loss: 0.613 | Acc: 83.854,98.852,99.942,%
Batch: 100 | Loss: 0.605 | Acc: 83.849,98.871,99.946,%
Batch: 120 | Loss: 0.599 | Acc: 83.904,98.896,99.948,%
Batch: 140 | Loss: 0.593 | Acc: 84.015,98.881,99.956,%
Batch: 160 | Loss: 0.594 | Acc: 83.904,98.908,99.951,%
Batch: 180 | Loss: 0.594 | Acc: 83.835,98.895,99.957,%
Batch: 200 | Loss: 0.593 | Acc: 83.839,98.916,99.953,%
Batch: 220 | Loss: 0.585 | Acc: 84.014,98.904,99.951,%
Batch: 240 | Loss: 0.586 | Acc: 84.025,98.878,99.948,%
Batch: 260 | Loss: 0.588 | Acc: 84.037,98.907,99.946,%
Batch: 280 | Loss: 0.588 | Acc: 84.019,98.899,99.950,%
Batch: 300 | Loss: 0.585 | Acc: 84.069,98.907,99.951,%
Batch: 320 | Loss: 0.586 | Acc: 84.051,98.912,99.942,%
Batch: 340 | Loss: 0.590 | Acc: 84.006,98.907,99.943,%
Batch: 360 | Loss: 0.589 | Acc: 84.029,98.924,99.946,%
Batch: 380 | Loss: 0.591 | Acc: 83.981,98.919,99.945,%
Batch: 0 | Loss: 0.960 | Acc: 78.906,92.969,93.750,%
Batch: 20 | Loss: 1.134 | Acc: 80.283,91.071,93.713,%
Batch: 40 | Loss: 1.127 | Acc: 80.373,91.159,93.921,%
Batch: 60 | Loss: 1.114 | Acc: 80.366,91.240,93.878,%

Epoch: 277
Batch: 0 | Loss: 0.762 | Acc: 82.031,99.219,100.000,%
Batch: 20 | Loss: 0.580 | Acc: 84.152,98.884,100.000,%
Batch: 40 | Loss: 0.598 | Acc: 83.403,98.685,99.981,%
Batch: 60 | Loss: 0.609 | Acc: 83.658,98.694,99.974,%
Batch: 80 | Loss: 0.602 | Acc: 84.047,98.804,99.971,%
Batch: 100 | Loss: 0.605 | Acc: 84.019,98.755,99.977,%
Batch: 120 | Loss: 0.603 | Acc: 83.994,98.806,99.974,%
Batch: 140 | Loss: 0.600 | Acc: 84.148,98.820,99.978,%
Batch: 160 | Loss: 0.598 | Acc: 84.137,98.845,99.966,%
Batch: 180 | Loss: 0.603 | Acc: 84.038,98.873,99.965,%
Batch: 200 | Loss: 0.597 | Acc: 84.083,98.908,99.969,%
Batch: 220 | Loss: 0.597 | Acc: 84.099,98.939,99.972,%
Batch: 240 | Loss: 0.600 | Acc: 84.031,98.976,99.974,%
Batch: 260 | Loss: 0.600 | Acc: 83.992,98.955,99.976,%
Batch: 280 | Loss: 0.600 | Acc: 83.975,98.957,99.978,%
Batch: 300 | Loss: 0.602 | Acc: 84.012,98.941,99.974,%
Batch: 320 | Loss: 0.601 | Acc: 83.976,98.956,99.971,%
Batch: 340 | Loss: 0.601 | Acc: 83.979,98.971,99.970,%
Batch: 360 | Loss: 0.596 | Acc: 84.046,98.985,99.972,%
Batch: 380 | Loss: 0.595 | Acc: 84.086,98.983,99.971,%
Batch: 0 | Loss: 0.977 | Acc: 78.906,92.188,93.750,%
Batch: 20 | Loss: 1.131 | Acc: 80.283,90.960,94.010,%
Batch: 40 | Loss: 1.124 | Acc: 80.316,91.025,93.941,%
Batch: 60 | Loss: 1.113 | Acc: 80.341,91.073,93.904,%

Epoch: 278
Batch: 0 | Loss: 0.896 | Acc: 74.219,96.094,100.000,%
Batch: 20 | Loss: 0.600 | Acc: 83.668,99.107,99.963,%
Batch: 40 | Loss: 0.594 | Acc: 83.746,99.143,99.981,%
Batch: 60 | Loss: 0.585 | Acc: 84.426,99.116,99.987,%
Batch: 80 | Loss: 0.591 | Acc: 84.259,99.064,99.990,%
Batch: 100 | Loss: 0.585 | Acc: 84.344,98.963,99.985,%
Batch: 120 | Loss: 0.584 | Acc: 84.298,98.986,99.987,%
Batch: 140 | Loss: 0.583 | Acc: 84.441,99.025,99.983,%
Batch: 160 | Loss: 0.584 | Acc: 84.589,98.991,99.976,%
Batch: 180 | Loss: 0.591 | Acc: 84.362,98.981,99.974,%
Batch: 200 | Loss: 0.593 | Acc: 84.328,98.970,99.973,%
Batch: 220 | Loss: 0.589 | Acc: 84.258,98.950,99.968,%
Batch: 240 | Loss: 0.584 | Acc: 84.401,98.921,99.961,%
Batch: 260 | Loss: 0.586 | Acc: 84.327,98.919,99.961,%
Batch: 280 | Loss: 0.589 | Acc: 84.300,98.938,99.961,%
Batch: 300 | Loss: 0.590 | Acc: 84.357,98.954,99.961,%
Batch: 320 | Loss: 0.587 | Acc: 84.443,98.980,99.963,%
Batch: 340 | Loss: 0.591 | Acc: 84.320,98.946,99.966,%
Batch: 360 | Loss: 0.590 | Acc: 84.293,98.946,99.968,%
Batch: 380 | Loss: 0.591 | Acc: 84.293,98.944,99.967,%
Batch: 0 | Loss: 0.978 | Acc: 78.906,92.188,92.188,%
Batch: 20 | Loss: 1.136 | Acc: 80.208,91.071,93.527,%
Batch: 40 | Loss: 1.127 | Acc: 80.316,91.101,93.788,%
Batch: 60 | Loss: 1.116 | Acc: 80.187,91.291,93.827,%

Epoch: 279
Batch: 0 | Loss: 0.311 | Acc: 90.625,100.000,100.000,%
Batch: 20 | Loss: 0.607 | Acc: 83.631,99.107,100.000,%
Batch: 40 | Loss: 0.607 | Acc: 83.460,99.162,100.000,%
Batch: 60 | Loss: 0.606 | Acc: 83.402,99.091,99.987,%
Batch: 80 | Loss: 0.602 | Acc: 83.709,99.074,99.981,%
Batch: 100 | Loss: 0.597 | Acc: 83.895,99.041,99.985,%
Batch: 120 | Loss: 0.589 | Acc: 84.001,99.051,99.961,%
Batch: 140 | Loss: 0.581 | Acc: 84.065,99.053,99.967,%
Batch: 160 | Loss: 0.580 | Acc: 84.064,99.005,99.971,%
Batch: 180 | Loss: 0.582 | Acc: 84.077,99.007,99.970,%
Batch: 200 | Loss: 0.585 | Acc: 84.060,99.048,99.965,%
Batch: 220 | Loss: 0.585 | Acc: 84.007,99.060,99.958,%
Batch: 240 | Loss: 0.582 | Acc: 84.015,99.021,99.958,%
Batch: 260 | Loss: 0.585 | Acc: 84.022,98.970,99.961,%
Batch: 280 | Loss: 0.585 | Acc: 84.078,98.966,99.958,%
Batch: 300 | Loss: 0.582 | Acc: 84.141,98.980,99.961,%
Batch: 320 | Loss: 0.582 | Acc: 84.098,98.983,99.963,%
Batch: 340 | Loss: 0.582 | Acc: 84.144,98.983,99.959,%
Batch: 360 | Loss: 0.580 | Acc: 84.185,99.005,99.961,%
Batch: 380 | Loss: 0.581 | Acc: 84.211,98.983,99.961,%
Batch: 0 | Loss: 0.973 | Acc: 78.906,92.188,92.188,%
Batch: 20 | Loss: 1.132 | Acc: 80.357,90.923,93.676,%
Batch: 40 | Loss: 1.126 | Acc: 80.316,91.120,93.845,%
Batch: 60 | Loss: 1.115 | Acc: 80.379,91.278,93.916,%

Epoch: 280
Batch: 0 | Loss: 0.672 | Acc: 78.906,100.000,100.000,%
Batch: 20 | Loss: 0.610 | Acc: 83.891,99.107,99.963,%
Batch: 40 | Loss: 0.605 | Acc: 83.746,98.819,99.981,%
Batch: 60 | Loss: 0.587 | Acc: 84.004,98.822,99.974,%
Batch: 80 | Loss: 0.589 | Acc: 84.230,98.814,99.981,%
Batch: 100 | Loss: 0.595 | Acc: 83.988,98.894,99.977,%
Batch: 120 | Loss: 0.594 | Acc: 84.188,98.928,99.974,%
Batch: 140 | Loss: 0.593 | Acc: 84.292,98.925,99.978,%
Batch: 160 | Loss: 0.594 | Acc: 84.234,98.942,99.976,%
Batch: 180 | Loss: 0.592 | Acc: 84.237,98.912,99.974,%
Batch: 200 | Loss: 0.592 | Acc: 84.181,98.916,99.969,%
Batch: 220 | Loss: 0.593 | Acc: 84.152,98.918,99.965,%
Batch: 240 | Loss: 0.589 | Acc: 84.190,98.930,99.968,%
Batch: 260 | Loss: 0.592 | Acc: 84.130,98.943,99.970,%
Batch: 280 | Loss: 0.587 | Acc: 84.211,98.918,99.972,%
Batch: 300 | Loss: 0.584 | Acc: 84.279,98.902,99.974,%
Batch: 320 | Loss: 0.583 | Acc: 84.283,98.900,99.973,%
Batch: 340 | Loss: 0.581 | Acc: 84.370,98.905,99.973,%
Batch: 360 | Loss: 0.584 | Acc: 84.336,98.903,99.968,%
Batch: 380 | Loss: 0.587 | Acc: 84.289,98.926,99.969,%
Batch: 0 | Loss: 0.991 | Acc: 78.125,92.969,93.750,%
Batch: 20 | Loss: 1.125 | Acc: 80.357,91.183,93.713,%
Batch: 40 | Loss: 1.120 | Acc: 80.393,91.254,93.960,%
Batch: 60 | Loss: 1.109 | Acc: 80.264,91.304,93.904,%

Epoch: 281
Batch: 0 | Loss: 0.665 | Acc: 88.281,98.438,100.000,%
Batch: 20 | Loss: 0.630 | Acc: 83.185,99.107,99.963,%
Batch: 40 | Loss: 0.602 | Acc: 83.918,99.028,99.981,%
Batch: 60 | Loss: 0.592 | Acc: 84.183,99.065,99.987,%
Batch: 80 | Loss: 0.591 | Acc: 84.269,98.939,99.990,%
Batch: 100 | Loss: 0.589 | Acc: 84.120,98.933,99.992,%
Batch: 120 | Loss: 0.586 | Acc: 84.201,98.928,99.974,%
Batch: 140 | Loss: 0.583 | Acc: 84.303,98.848,99.972,%
Batch: 160 | Loss: 0.588 | Acc: 84.259,98.860,99.966,%
Batch: 180 | Loss: 0.584 | Acc: 84.319,98.860,99.961,%
Batch: 200 | Loss: 0.583 | Acc: 84.383,98.861,99.965,%
Batch: 220 | Loss: 0.581 | Acc: 84.396,98.911,99.965,%
Batch: 240 | Loss: 0.585 | Acc: 84.463,98.908,99.964,%
Batch: 260 | Loss: 0.589 | Acc: 84.426,98.892,99.967,%
Batch: 280 | Loss: 0.593 | Acc: 84.333,98.910,99.969,%
Batch: 300 | Loss: 0.595 | Acc: 84.261,98.871,99.971,%
Batch: 320 | Loss: 0.598 | Acc: 84.180,98.868,99.971,%
Batch: 340 | Loss: 0.597 | Acc: 84.155,98.871,99.970,%
Batch: 360 | Loss: 0.595 | Acc: 84.198,98.888,99.970,%
Batch: 380 | Loss: 0.596 | Acc: 84.170,98.882,99.969,%
Batch: 0 | Loss: 0.991 | Acc: 78.906,92.969,92.969,%
Batch: 20 | Loss: 1.126 | Acc: 80.320,91.257,93.564,%
Batch: 40 | Loss: 1.120 | Acc: 80.393,91.254,93.883,%
Batch: 60 | Loss: 1.110 | Acc: 80.366,91.317,93.968,%

Epoch: 282
Batch: 0 | Loss: 0.580 | Acc: 84.375,99.219,100.000,%
Batch: 20 | Loss: 0.614 | Acc: 83.557,98.772,99.963,%
Batch: 40 | Loss: 0.582 | Acc: 84.184,99.104,99.962,%
Batch: 60 | Loss: 0.577 | Acc: 84.388,99.052,99.962,%
Batch: 80 | Loss: 0.575 | Acc: 84.327,98.939,99.961,%
Batch: 100 | Loss: 0.569 | Acc: 84.259,99.025,99.969,%
Batch: 120 | Loss: 0.570 | Acc: 84.317,99.032,99.974,%
Batch: 140 | Loss: 0.573 | Acc: 84.292,99.058,99.978,%
Batch: 160 | Loss: 0.569 | Acc: 84.220,99.049,99.971,%
Batch: 180 | Loss: 0.571 | Acc: 84.194,99.003,99.974,%
Batch: 200 | Loss: 0.577 | Acc: 84.126,99.001,99.969,%
Batch: 220 | Loss: 0.576 | Acc: 84.170,99.024,99.968,%
Batch: 240 | Loss: 0.577 | Acc: 84.103,99.018,99.971,%
Batch: 260 | Loss: 0.578 | Acc: 84.097,98.988,99.967,%
Batch: 280 | Loss: 0.574 | Acc: 84.136,99.019,99.967,%
Batch: 300 | Loss: 0.576 | Acc: 84.149,99.016,99.966,%
Batch: 320 | Loss: 0.576 | Acc: 84.171,99.029,99.968,%
Batch: 340 | Loss: 0.576 | Acc: 84.183,99.042,99.970,%
Batch: 360 | Loss: 0.576 | Acc: 84.236,99.054,99.972,%
Batch: 380 | Loss: 0.578 | Acc: 84.234,99.051,99.973,%
Batch: 0 | Loss: 0.984 | Acc: 78.125,92.188,92.188,%
Batch: 20 | Loss: 1.131 | Acc: 80.171,91.071,93.676,%
Batch: 40 | Loss: 1.123 | Acc: 80.297,91.139,93.864,%
Batch: 60 | Loss: 1.113 | Acc: 80.225,91.176,93.827,%

Epoch: 283
Batch: 0 | Loss: 0.623 | Acc: 85.156,99.219,100.000,%
Batch: 20 | Loss: 0.622 | Acc: 83.929,98.921,100.000,%
Batch: 40 | Loss: 0.616 | Acc: 83.803,98.895,99.962,%
Batch: 60 | Loss: 0.602 | Acc: 83.991,98.975,99.949,%
Batch: 80 | Loss: 0.600 | Acc: 84.240,99.055,99.961,%
Batch: 100 | Loss: 0.594 | Acc: 84.282,99.025,99.954,%
Batch: 120 | Loss: 0.603 | Acc: 84.091,99.019,99.961,%
Batch: 140 | Loss: 0.600 | Acc: 84.076,98.947,99.967,%
Batch: 160 | Loss: 0.597 | Acc: 84.234,98.976,99.966,%
Batch: 180 | Loss: 0.591 | Acc: 84.323,98.994,99.965,%
Batch: 200 | Loss: 0.588 | Acc: 84.352,98.997,99.969,%
Batch: 220 | Loss: 0.589 | Acc: 84.361,98.968,99.972,%
Batch: 240 | Loss: 0.588 | Acc: 84.326,98.946,99.971,%
Batch: 260 | Loss: 0.593 | Acc: 84.180,98.952,99.973,%
Batch: 280 | Loss: 0.592 | Acc: 84.228,98.932,99.975,%
Batch: 300 | Loss: 0.592 | Acc: 84.248,98.931,99.969,%
Batch: 320 | Loss: 0.593 | Acc: 84.227,98.927,99.968,%
Batch: 340 | Loss: 0.591 | Acc: 84.233,98.932,99.970,%
Batch: 360 | Loss: 0.589 | Acc: 84.282,98.929,99.968,%
Batch: 380 | Loss: 0.589 | Acc: 84.254,98.938,99.969,%
Batch: 0 | Loss: 0.982 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.127 | Acc: 80.283,91.034,93.713,%
Batch: 40 | Loss: 1.120 | Acc: 80.373,91.101,93.979,%
Batch: 60 | Loss: 1.109 | Acc: 80.328,91.253,93.904,%

Epoch: 284
Batch: 0 | Loss: 0.691 | Acc: 78.906,100.000,100.000,%
Batch: 20 | Loss: 0.572 | Acc: 84.524,98.921,99.963,%
Batch: 40 | Loss: 0.573 | Acc: 84.889,98.742,99.905,%
Batch: 60 | Loss: 0.575 | Acc: 84.734,98.809,99.923,%
Batch: 80 | Loss: 0.576 | Acc: 84.828,98.881,99.932,%
Batch: 100 | Loss: 0.579 | Acc: 84.746,98.855,99.946,%
Batch: 120 | Loss: 0.586 | Acc: 84.452,98.928,99.955,%
Batch: 140 | Loss: 0.588 | Acc: 84.286,98.831,99.956,%
Batch: 160 | Loss: 0.587 | Acc: 84.215,98.782,99.951,%
Batch: 180 | Loss: 0.585 | Acc: 84.215,98.787,99.957,%
Batch: 200 | Loss: 0.588 | Acc: 84.099,98.815,99.961,%
Batch: 220 | Loss: 0.586 | Acc: 84.075,98.819,99.954,%
Batch: 240 | Loss: 0.582 | Acc: 84.187,98.839,99.955,%
Batch: 260 | Loss: 0.584 | Acc: 84.112,98.821,99.952,%
Batch: 280 | Loss: 0.583 | Acc: 84.155,98.843,99.947,%
Batch: 300 | Loss: 0.583 | Acc: 84.097,98.837,99.943,%
Batch: 320 | Loss: 0.585 | Acc: 84.022,98.856,99.932,%
Batch: 340 | Loss: 0.585 | Acc: 84.102,98.884,99.934,%
Batch: 360 | Loss: 0.584 | Acc: 84.178,98.890,99.935,%
Batch: 380 | Loss: 0.583 | Acc: 84.195,98.887,99.936,%
Batch: 0 | Loss: 0.974 | Acc: 78.906,92.969,92.969,%
Batch: 20 | Loss: 1.131 | Acc: 80.208,91.257,93.824,%
Batch: 40 | Loss: 1.125 | Acc: 80.278,91.216,94.017,%
Batch: 60 | Loss: 1.112 | Acc: 80.315,91.342,93.955,%

Epoch: 285
Batch: 0 | Loss: 0.508 | Acc: 82.812,99.219,100.000,%
Batch: 20 | Loss: 0.550 | Acc: 84.933,98.884,100.000,%
Batch: 40 | Loss: 0.573 | Acc: 84.832,98.895,99.962,%
Batch: 60 | Loss: 0.570 | Acc: 84.951,98.886,99.974,%
Batch: 80 | Loss: 0.561 | Acc: 84.838,98.997,99.961,%
Batch: 100 | Loss: 0.563 | Acc: 84.901,99.018,99.961,%
Batch: 120 | Loss: 0.565 | Acc: 84.737,99.006,99.968,%
Batch: 140 | Loss: 0.568 | Acc: 84.630,99.041,99.967,%
Batch: 160 | Loss: 0.570 | Acc: 84.647,99.039,99.966,%
Batch: 180 | Loss: 0.569 | Acc: 84.599,99.059,99.970,%
Batch: 200 | Loss: 0.572 | Acc: 84.581,99.017,99.969,%
Batch: 220 | Loss: 0.577 | Acc: 84.492,99.035,99.972,%
Batch: 240 | Loss: 0.578 | Acc: 84.430,99.021,99.971,%
Batch: 260 | Loss: 0.579 | Acc: 84.390,99.015,99.970,%
Batch: 280 | Loss: 0.583 | Acc: 84.331,99.016,99.961,%
Batch: 300 | Loss: 0.585 | Acc: 84.308,99.019,99.961,%
Batch: 320 | Loss: 0.583 | Acc: 84.351,99.009,99.963,%
Batch: 340 | Loss: 0.585 | Acc: 84.382,99.013,99.961,%
Batch: 360 | Loss: 0.582 | Acc: 84.351,99.026,99.959,%
Batch: 380 | Loss: 0.583 | Acc: 84.346,99.003,99.959,%
Batch: 0 | Loss: 0.968 | Acc: 79.688,92.969,92.969,%
Batch: 20 | Loss: 1.130 | Acc: 80.283,91.146,93.787,%
Batch: 40 | Loss: 1.123 | Acc: 80.221,91.197,93.941,%
Batch: 60 | Loss: 1.112 | Acc: 80.225,91.227,93.891,%

Epoch: 286
Batch: 0 | Loss: 0.674 | Acc: 85.938,99.219,100.000,%
Batch: 20 | Loss: 0.560 | Acc: 84.896,98.958,99.926,%
Batch: 40 | Loss: 0.579 | Acc: 84.489,98.914,99.886,%
Batch: 60 | Loss: 0.571 | Acc: 84.593,99.065,99.910,%
Batch: 80 | Loss: 0.577 | Acc: 84.317,99.055,99.932,%
Batch: 100 | Loss: 0.587 | Acc: 84.073,98.994,99.923,%
Batch: 120 | Loss: 0.589 | Acc: 84.130,99.006,99.929,%
Batch: 140 | Loss: 0.592 | Acc: 84.153,98.975,99.928,%
Batch: 160 | Loss: 0.587 | Acc: 84.293,98.996,99.937,%
Batch: 180 | Loss: 0.587 | Acc: 84.276,98.964,99.944,%
Batch: 200 | Loss: 0.588 | Acc: 84.200,98.989,99.949,%
Batch: 220 | Loss: 0.587 | Acc: 84.131,98.985,99.951,%
Batch: 240 | Loss: 0.587 | Acc: 84.142,98.989,99.942,%
Batch: 260 | Loss: 0.585 | Acc: 84.180,99.006,99.943,%
Batch: 280 | Loss: 0.584 | Acc: 84.228,98.996,99.944,%
Batch: 300 | Loss: 0.580 | Acc: 84.300,99.019,99.948,%
Batch: 320 | Loss: 0.580 | Acc: 84.324,99.017,99.946,%
Batch: 340 | Loss: 0.581 | Acc: 84.295,99.033,99.950,%
Batch: 360 | Loss: 0.579 | Acc: 84.323,99.033,99.952,%
Batch: 380 | Loss: 0.580 | Acc: 84.316,99.040,99.953,%
Batch: 0 | Loss: 0.981 | Acc: 79.688,92.969,93.750,%
Batch: 20 | Loss: 1.132 | Acc: 80.432,91.034,93.936,%
Batch: 40 | Loss: 1.125 | Acc: 80.373,91.101,94.017,%
Batch: 60 | Loss: 1.114 | Acc: 80.353,91.278,94.006,%

Epoch: 287
Batch: 0 | Loss: 0.568 | Acc: 89.062,99.219,100.000,%
Batch: 20 | Loss: 0.596 | Acc: 84.970,98.958,100.000,%
Batch: 40 | Loss: 0.595 | Acc: 85.061,99.009,99.962,%
Batch: 60 | Loss: 0.592 | Acc: 84.657,98.886,99.962,%
Batch: 80 | Loss: 0.581 | Acc: 84.848,98.920,99.961,%
Batch: 100 | Loss: 0.582 | Acc: 84.723,98.878,99.954,%
Batch: 120 | Loss: 0.594 | Acc: 84.298,98.773,99.955,%
Batch: 140 | Loss: 0.600 | Acc: 84.092,98.792,99.956,%
Batch: 160 | Loss: 0.596 | Acc: 84.229,98.826,99.961,%
Batch: 180 | Loss: 0.595 | Acc: 84.211,98.865,99.961,%
Batch: 200 | Loss: 0.592 | Acc: 84.258,98.853,99.961,%
Batch: 220 | Loss: 0.590 | Acc: 84.393,98.865,99.965,%
Batch: 240 | Loss: 0.592 | Acc: 84.226,98.882,99.964,%
Batch: 260 | Loss: 0.593 | Acc: 84.165,98.878,99.964,%
Batch: 280 | Loss: 0.594 | Acc: 84.155,98.910,99.967,%
Batch: 300 | Loss: 0.594 | Acc: 84.128,98.910,99.966,%
Batch: 320 | Loss: 0.593 | Acc: 84.129,98.924,99.966,%
Batch: 340 | Loss: 0.592 | Acc: 84.164,98.932,99.968,%
Batch: 360 | Loss: 0.592 | Acc: 84.230,98.920,99.970,%
Batch: 380 | Loss: 0.593 | Acc: 84.190,98.932,99.971,%
Batch: 0 | Loss: 0.992 | Acc: 78.906,92.188,92.969,%
Batch: 20 | Loss: 1.129 | Acc: 80.394,91.220,93.713,%
Batch: 40 | Loss: 1.123 | Acc: 80.393,91.197,93.902,%
Batch: 60 | Loss: 1.112 | Acc: 80.405,91.304,93.865,%

Epoch: 288
Batch: 0 | Loss: 0.610 | Acc: 85.156,100.000,100.000,%
Batch: 20 | Loss: 0.529 | Acc: 85.045,98.958,100.000,%
Batch: 40 | Loss: 0.588 | Acc: 84.204,98.952,99.981,%
Batch: 60 | Loss: 0.601 | Acc: 84.144,98.975,99.987,%
Batch: 80 | Loss: 0.597 | Acc: 84.230,98.900,99.990,%
Batch: 100 | Loss: 0.595 | Acc: 84.282,98.925,99.977,%
Batch: 120 | Loss: 0.602 | Acc: 84.052,98.851,99.968,%
Batch: 140 | Loss: 0.606 | Acc: 84.148,98.853,99.972,%
Batch: 160 | Loss: 0.603 | Acc: 84.225,98.860,99.971,%
Batch: 180 | Loss: 0.600 | Acc: 84.237,98.891,99.970,%
Batch: 200 | Loss: 0.599 | Acc: 84.220,98.896,99.973,%
Batch: 220 | Loss: 0.598 | Acc: 84.156,98.901,99.972,%
Batch: 240 | Loss: 0.596 | Acc: 84.093,98.911,99.974,%
Batch: 260 | Loss: 0.594 | Acc: 84.159,98.931,99.976,%
Batch: 280 | Loss: 0.594 | Acc: 84.180,98.944,99.978,%
Batch: 300 | Loss: 0.596 | Acc: 84.152,98.951,99.977,%
Batch: 320 | Loss: 0.597 | Acc: 84.119,98.956,99.978,%
Batch: 340 | Loss: 0.598 | Acc: 84.079,98.958,99.979,%
Batch: 360 | Loss: 0.600 | Acc: 84.074,98.961,99.981,%
Batch: 380 | Loss: 0.598 | Acc: 84.049,98.977,99.977,%
Batch: 0 | Loss: 0.974 | Acc: 78.125,92.188,93.750,%
Batch: 20 | Loss: 1.128 | Acc: 80.171,91.257,93.750,%
Batch: 40 | Loss: 1.122 | Acc: 80.259,91.216,93.921,%
Batch: 60 | Loss: 1.111 | Acc: 80.328,91.317,93.942,%

Epoch: 289
Batch: 0 | Loss: 0.535 | Acc: 78.125,99.219,100.000,%
Batch: 20 | Loss: 0.574 | Acc: 84.561,99.368,99.963,%
Batch: 40 | Loss: 0.578 | Acc: 84.813,99.314,99.962,%
Batch: 60 | Loss: 0.586 | Acc: 84.631,99.168,99.962,%
Batch: 80 | Loss: 0.586 | Acc: 84.481,99.132,99.961,%
Batch: 100 | Loss: 0.590 | Acc: 84.367,99.118,99.954,%
Batch: 120 | Loss: 0.591 | Acc: 84.323,99.077,99.961,%
Batch: 140 | Loss: 0.591 | Acc: 84.259,99.053,99.967,%
Batch: 160 | Loss: 0.590 | Acc: 84.356,99.063,99.966,%
Batch: 180 | Loss: 0.591 | Acc: 84.228,99.029,99.965,%
Batch: 200 | Loss: 0.592 | Acc: 84.192,99.009,99.965,%
Batch: 220 | Loss: 0.594 | Acc: 84.120,99.024,99.968,%
Batch: 240 | Loss: 0.595 | Acc: 84.168,99.011,99.964,%
Batch: 260 | Loss: 0.594 | Acc: 84.204,98.985,99.967,%
Batch: 280 | Loss: 0.593 | Acc: 84.214,98.988,99.964,%
Batch: 300 | Loss: 0.592 | Acc: 84.152,99.011,99.964,%
Batch: 320 | Loss: 0.591 | Acc: 84.166,99.026,99.966,%
Batch: 340 | Loss: 0.591 | Acc: 84.137,99.003,99.966,%
Batch: 360 | Loss: 0.593 | Acc: 84.117,98.985,99.968,%
Batch: 380 | Loss: 0.593 | Acc: 84.160,98.983,99.969,%
Batch: 0 | Loss: 0.983 | Acc: 77.344,92.188,92.969,%
Batch: 20 | Loss: 1.128 | Acc: 80.432,90.997,93.601,%
Batch: 40 | Loss: 1.122 | Acc: 80.278,91.101,93.826,%
Batch: 60 | Loss: 1.109 | Acc: 80.251,91.240,93.865,%

Epoch: 290
Batch: 0 | Loss: 0.586 | Acc: 85.156,96.094,100.000,%
Batch: 20 | Loss: 0.603 | Acc: 84.115,99.144,99.963,%
Batch: 40 | Loss: 0.582 | Acc: 84.794,99.066,99.981,%
Batch: 60 | Loss: 0.587 | Acc: 84.580,99.065,99.987,%
Batch: 80 | Loss: 0.591 | Acc: 84.394,99.026,99.990,%
Batch: 100 | Loss: 0.599 | Acc: 84.213,98.994,99.992,%
Batch: 120 | Loss: 0.605 | Acc: 84.033,98.948,99.987,%
Batch: 140 | Loss: 0.603 | Acc: 84.192,98.980,99.983,%
Batch: 160 | Loss: 0.598 | Acc: 84.171,98.966,99.985,%
Batch: 180 | Loss: 0.591 | Acc: 84.289,98.921,99.965,%
Batch: 200 | Loss: 0.591 | Acc: 84.192,98.966,99.969,%
Batch: 220 | Loss: 0.586 | Acc: 84.280,98.964,99.968,%
Batch: 240 | Loss: 0.585 | Acc: 84.365,98.950,99.964,%
Batch: 260 | Loss: 0.587 | Acc: 84.258,98.928,99.964,%
Batch: 280 | Loss: 0.587 | Acc: 84.342,98.932,99.967,%
Batch: 300 | Loss: 0.588 | Acc: 84.333,98.925,99.966,%
Batch: 320 | Loss: 0.588 | Acc: 84.312,98.934,99.968,%
Batch: 340 | Loss: 0.586 | Acc: 84.368,98.935,99.968,%
Batch: 360 | Loss: 0.588 | Acc: 84.325,98.944,99.961,%
Batch: 380 | Loss: 0.586 | Acc: 84.324,98.930,99.961,%
Batch: 0 | Loss: 0.983 | Acc: 78.906,92.969,92.188,%
Batch: 20 | Loss: 1.133 | Acc: 80.171,91.183,93.713,%
Batch: 40 | Loss: 1.125 | Acc: 80.183,91.235,93.845,%
Batch: 60 | Loss: 1.113 | Acc: 80.328,91.304,93.891,%

Epoch: 291
Batch: 0 | Loss: 0.684 | Acc: 82.031,99.219,100.000,%
Batch: 20 | Loss: 0.593 | Acc: 83.371,98.624,100.000,%
Batch: 40 | Loss: 0.593 | Acc: 83.689,98.857,99.981,%
Batch: 60 | Loss: 0.578 | Acc: 84.260,98.937,99.987,%
Batch: 80 | Loss: 0.569 | Acc: 84.385,98.920,99.990,%
Batch: 100 | Loss: 0.572 | Acc: 84.398,98.925,99.954,%
Batch: 120 | Loss: 0.574 | Acc: 84.465,98.909,99.955,%
Batch: 140 | Loss: 0.571 | Acc: 84.525,98.886,99.950,%
Batch: 160 | Loss: 0.579 | Acc: 84.467,98.918,99.951,%
Batch: 180 | Loss: 0.581 | Acc: 84.496,98.904,99.944,%
Batch: 200 | Loss: 0.583 | Acc: 84.410,98.931,99.946,%
Batch: 220 | Loss: 0.583 | Acc: 84.396,98.954,99.947,%
Batch: 240 | Loss: 0.581 | Acc: 84.352,98.914,99.945,%
Batch: 260 | Loss: 0.581 | Acc: 84.288,98.901,99.949,%
Batch: 280 | Loss: 0.583 | Acc: 84.253,98.910,99.953,%
Batch: 300 | Loss: 0.584 | Acc: 84.227,98.881,99.948,%
Batch: 320 | Loss: 0.582 | Acc: 84.202,98.890,99.949,%
Batch: 340 | Loss: 0.579 | Acc: 84.263,98.891,99.950,%
Batch: 360 | Loss: 0.581 | Acc: 84.206,98.901,99.948,%
Batch: 380 | Loss: 0.583 | Acc: 84.156,98.893,99.947,%
Batch: 0 | Loss: 0.976 | Acc: 78.906,92.969,92.969,%
Batch: 20 | Loss: 1.128 | Acc: 80.171,91.146,93.824,%
Batch: 40 | Loss: 1.122 | Acc: 80.221,91.139,93.960,%
Batch: 60 | Loss: 1.110 | Acc: 80.238,91.304,93.916,%

Epoch: 292
Batch: 0 | Loss: 0.572 | Acc: 84.375,97.656,100.000,%
Batch: 20 | Loss: 0.556 | Acc: 85.268,99.107,99.963,%
Batch: 40 | Loss: 0.575 | Acc: 85.042,99.085,99.981,%
Batch: 60 | Loss: 0.566 | Acc: 84.541,99.039,99.974,%
Batch: 80 | Loss: 0.576 | Acc: 84.471,99.007,99.981,%
Batch: 100 | Loss: 0.579 | Acc: 84.537,98.956,99.977,%
Batch: 120 | Loss: 0.586 | Acc: 84.259,99.012,99.981,%
Batch: 140 | Loss: 0.587 | Acc: 84.159,99.025,99.978,%
Batch: 160 | Loss: 0.585 | Acc: 84.370,99.030,99.971,%
Batch: 180 | Loss: 0.584 | Acc: 84.315,99.059,99.970,%
Batch: 200 | Loss: 0.586 | Acc: 84.274,99.075,99.969,%
Batch: 220 | Loss: 0.585 | Acc: 84.287,99.060,99.972,%
Batch: 240 | Loss: 0.587 | Acc: 84.122,99.050,99.971,%
Batch: 260 | Loss: 0.589 | Acc: 84.097,99.054,99.970,%
Batch: 280 | Loss: 0.590 | Acc: 84.119,99.060,99.969,%
Batch: 300 | Loss: 0.587 | Acc: 84.110,99.063,99.969,%
Batch: 320 | Loss: 0.586 | Acc: 84.154,99.046,99.971,%
Batch: 340 | Loss: 0.585 | Acc: 84.164,99.049,99.973,%
Batch: 360 | Loss: 0.585 | Acc: 84.137,99.059,99.972,%
Batch: 380 | Loss: 0.589 | Acc: 84.121,99.057,99.971,%
Batch: 0 | Loss: 0.976 | Acc: 78.906,92.969,92.969,%
Batch: 20 | Loss: 1.125 | Acc: 80.357,91.183,93.638,%
Batch: 40 | Loss: 1.120 | Acc: 80.278,91.178,93.845,%
Batch: 60 | Loss: 1.109 | Acc: 80.264,91.265,93.916,%

Epoch: 293
Batch: 0 | Loss: 0.565 | Acc: 89.062,98.438,100.000,%
Batch: 20 | Loss: 0.558 | Acc: 84.412,99.070,99.926,%
Batch: 40 | Loss: 0.567 | Acc: 84.546,99.104,99.943,%
Batch: 60 | Loss: 0.583 | Acc: 84.388,98.847,99.936,%
Batch: 80 | Loss: 0.578 | Acc: 84.568,98.862,99.952,%
Batch: 100 | Loss: 0.578 | Acc: 84.383,98.933,99.961,%
Batch: 120 | Loss: 0.580 | Acc: 84.285,98.915,99.948,%
Batch: 140 | Loss: 0.574 | Acc: 84.536,98.892,99.945,%
Batch: 160 | Loss: 0.579 | Acc: 84.555,98.913,99.951,%
Batch: 180 | Loss: 0.576 | Acc: 84.500,98.930,99.957,%
Batch: 200 | Loss: 0.574 | Acc: 84.593,98.927,99.961,%
Batch: 220 | Loss: 0.574 | Acc: 84.474,98.954,99.965,%
Batch: 240 | Loss: 0.577 | Acc: 84.430,98.940,99.961,%
Batch: 260 | Loss: 0.580 | Acc: 84.303,98.928,99.961,%
Batch: 280 | Loss: 0.582 | Acc: 84.286,98.946,99.964,%
Batch: 300 | Loss: 0.583 | Acc: 84.250,98.902,99.958,%
Batch: 320 | Loss: 0.581 | Acc: 84.273,98.924,99.956,%
Batch: 340 | Loss: 0.581 | Acc: 84.244,98.916,99.959,%
Batch: 360 | Loss: 0.580 | Acc: 84.278,98.901,99.959,%
Batch: 380 | Loss: 0.579 | Acc: 84.301,98.923,99.959,%
Batch: 0 | Loss: 0.975 | Acc: 78.906,92.969,94.531,%
Batch: 20 | Loss: 1.125 | Acc: 80.283,91.257,93.862,%
Batch: 40 | Loss: 1.120 | Acc: 80.145,91.159,94.036,%
Batch: 60 | Loss: 1.108 | Acc: 80.187,91.329,94.070,%

Epoch: 294
Batch: 0 | Loss: 0.532 | Acc: 85.156,99.219,99.219,%
Batch: 20 | Loss: 0.550 | Acc: 84.598,98.772,99.926,%
Batch: 40 | Loss: 0.552 | Acc: 85.194,99.028,99.962,%
Batch: 60 | Loss: 0.554 | Acc: 84.926,99.091,99.936,%
Batch: 80 | Loss: 0.564 | Acc: 84.944,99.132,99.942,%
Batch: 100 | Loss: 0.563 | Acc: 84.901,99.072,99.954,%
Batch: 120 | Loss: 0.571 | Acc: 84.762,99.038,99.955,%
Batch: 140 | Loss: 0.576 | Acc: 84.691,99.069,99.961,%
Batch: 160 | Loss: 0.575 | Acc: 84.671,99.059,99.966,%
Batch: 180 | Loss: 0.571 | Acc: 84.716,99.076,99.970,%
Batch: 200 | Loss: 0.569 | Acc: 84.787,99.087,99.965,%
Batch: 220 | Loss: 0.563 | Acc: 84.835,99.084,99.968,%
Batch: 240 | Loss: 0.567 | Acc: 84.702,99.066,99.971,%
Batch: 260 | Loss: 0.572 | Acc: 84.686,99.051,99.964,%
Batch: 280 | Loss: 0.570 | Acc: 84.647,99.046,99.961,%
Batch: 300 | Loss: 0.575 | Acc: 84.546,99.006,99.964,%
Batch: 320 | Loss: 0.576 | Acc: 84.465,99.002,99.961,%
Batch: 340 | Loss: 0.575 | Acc: 84.478,98.990,99.963,%
Batch: 360 | Loss: 0.574 | Acc: 84.453,98.981,99.961,%
Batch: 380 | Loss: 0.574 | Acc: 84.488,99.008,99.961,%
Batch: 0 | Loss: 0.968 | Acc: 78.906,92.969,93.750,%
Batch: 20 | Loss: 1.127 | Acc: 80.283,91.220,93.862,%
Batch: 40 | Loss: 1.122 | Acc: 80.240,91.216,94.017,%
Batch: 60 | Loss: 1.110 | Acc: 80.289,91.329,93.929,%

Epoch: 295
Batch: 0 | Loss: 0.658 | Acc: 83.594,99.219,100.000,%
Batch: 20 | Loss: 0.622 | Acc: 83.594,98.884,99.963,%
Batch: 40 | Loss: 0.589 | Acc: 84.013,98.952,99.981,%
Batch: 60 | Loss: 0.578 | Acc: 84.209,99.052,99.962,%
Batch: 80 | Loss: 0.571 | Acc: 84.317,98.987,99.971,%
Batch: 100 | Loss: 0.576 | Acc: 84.143,99.002,99.969,%
Batch: 120 | Loss: 0.577 | Acc: 84.072,98.941,99.968,%
Batch: 140 | Loss: 0.576 | Acc: 84.098,98.964,99.972,%
Batch: 160 | Loss: 0.579 | Acc: 84.055,98.957,99.971,%
Batch: 180 | Loss: 0.577 | Acc: 84.133,98.960,99.970,%
Batch: 200 | Loss: 0.576 | Acc: 84.095,98.962,99.969,%
Batch: 220 | Loss: 0.576 | Acc: 84.223,98.989,99.965,%
Batch: 240 | Loss: 0.578 | Acc: 84.268,99.005,99.968,%
Batch: 260 | Loss: 0.574 | Acc: 84.363,98.985,99.970,%
Batch: 280 | Loss: 0.577 | Acc: 84.303,98.963,99.969,%
Batch: 300 | Loss: 0.581 | Acc: 84.269,98.959,99.971,%
Batch: 320 | Loss: 0.581 | Acc: 84.253,98.958,99.968,%
Batch: 340 | Loss: 0.580 | Acc: 84.290,98.939,99.966,%
Batch: 360 | Loss: 0.579 | Acc: 84.343,98.957,99.963,%
Batch: 380 | Loss: 0.581 | Acc: 84.285,98.946,99.965,%
Batch: 0 | Loss: 0.979 | Acc: 79.688,91.406,92.188,%
Batch: 20 | Loss: 1.131 | Acc: 80.022,91.146,93.787,%
Batch: 40 | Loss: 1.124 | Acc: 80.259,91.120,93.864,%
Batch: 60 | Loss: 1.114 | Acc: 80.277,91.253,93.814,%

Epoch: 296
Batch: 0 | Loss: 0.567 | Acc: 85.938,100.000,100.000,%
Batch: 20 | Loss: 0.554 | Acc: 84.003,98.996,100.000,%
Batch: 40 | Loss: 0.565 | Acc: 84.165,99.047,100.000,%
Batch: 60 | Loss: 0.575 | Acc: 84.196,98.924,99.949,%
Batch: 80 | Loss: 0.578 | Acc: 84.124,98.843,99.952,%
Batch: 100 | Loss: 0.582 | Acc: 84.189,98.909,99.961,%
Batch: 120 | Loss: 0.579 | Acc: 84.369,98.877,99.961,%
Batch: 140 | Loss: 0.579 | Acc: 84.325,98.848,99.956,%
Batch: 160 | Loss: 0.574 | Acc: 84.351,98.840,99.956,%
Batch: 180 | Loss: 0.576 | Acc: 84.371,98.848,99.957,%
Batch: 200 | Loss: 0.574 | Acc: 84.554,98.884,99.961,%
Batch: 220 | Loss: 0.573 | Acc: 84.591,98.915,99.961,%
Batch: 240 | Loss: 0.569 | Acc: 84.599,98.927,99.964,%
Batch: 260 | Loss: 0.562 | Acc: 84.707,98.952,99.967,%
Batch: 280 | Loss: 0.565 | Acc: 84.597,98.930,99.967,%
Batch: 300 | Loss: 0.566 | Acc: 84.492,98.897,99.969,%
Batch: 320 | Loss: 0.570 | Acc: 84.370,98.915,99.971,%
Batch: 340 | Loss: 0.571 | Acc: 84.334,98.928,99.973,%
Batch: 360 | Loss: 0.572 | Acc: 84.343,98.922,99.970,%
Batch: 380 | Loss: 0.571 | Acc: 84.369,98.934,99.969,%
Batch: 0 | Loss: 0.978 | Acc: 78.906,92.188,93.750,%
Batch: 20 | Loss: 1.127 | Acc: 80.171,91.220,93.862,%
Batch: 40 | Loss: 1.121 | Acc: 80.221,91.254,94.036,%
Batch: 60 | Loss: 1.109 | Acc: 80.225,91.368,93.993,%

Epoch: 297
Batch: 0 | Loss: 0.768 | Acc: 82.812,98.438,100.000,%
Batch: 20 | Loss: 0.576 | Acc: 83.929,99.107,99.926,%
Batch: 40 | Loss: 0.592 | Acc: 83.708,98.838,99.943,%
Batch: 60 | Loss: 0.583 | Acc: 83.952,98.770,99.962,%
Batch: 80 | Loss: 0.587 | Acc: 83.951,98.785,99.971,%
Batch: 100 | Loss: 0.594 | Acc: 83.996,98.786,99.969,%
Batch: 120 | Loss: 0.592 | Acc: 83.975,98.883,99.961,%
Batch: 140 | Loss: 0.584 | Acc: 84.092,98.903,99.956,%
Batch: 160 | Loss: 0.577 | Acc: 84.181,98.937,99.956,%
Batch: 180 | Loss: 0.577 | Acc: 84.280,98.930,99.957,%
Batch: 200 | Loss: 0.577 | Acc: 84.387,98.931,99.953,%
Batch: 220 | Loss: 0.577 | Acc: 84.403,98.932,99.951,%
Batch: 240 | Loss: 0.581 | Acc: 84.300,98.921,99.951,%
Batch: 260 | Loss: 0.581 | Acc: 84.354,98.904,99.952,%
Batch: 280 | Loss: 0.580 | Acc: 84.358,98.916,99.956,%
Batch: 300 | Loss: 0.578 | Acc: 84.328,98.936,99.958,%
Batch: 320 | Loss: 0.579 | Acc: 84.270,98.939,99.961,%
Batch: 340 | Loss: 0.581 | Acc: 84.265,98.944,99.961,%
Batch: 360 | Loss: 0.578 | Acc: 84.306,98.955,99.963,%
Batch: 380 | Loss: 0.578 | Acc: 84.307,98.960,99.963,%
Batch: 0 | Loss: 0.992 | Acc: 78.125,92.188,92.969,%
Batch: 20 | Loss: 1.132 | Acc: 80.320,91.183,93.824,%
Batch: 40 | Loss: 1.125 | Acc: 80.259,91.178,93.998,%
Batch: 60 | Loss: 1.113 | Acc: 80.238,91.278,93.942,%

Epoch: 298
Batch: 0 | Loss: 0.847 | Acc: 78.125,97.656,100.000,%
Batch: 20 | Loss: 0.604 | Acc: 82.961,98.958,100.000,%
Batch: 40 | Loss: 0.597 | Acc: 84.051,98.876,100.000,%
Batch: 60 | Loss: 0.609 | Acc: 83.940,98.860,100.000,%
Batch: 80 | Loss: 0.607 | Acc: 83.922,98.862,99.990,%
Batch: 100 | Loss: 0.607 | Acc: 83.919,98.886,99.992,%
Batch: 120 | Loss: 0.603 | Acc: 83.813,98.915,99.987,%
Batch: 140 | Loss: 0.604 | Acc: 83.771,98.958,99.983,%
Batch: 160 | Loss: 0.603 | Acc: 83.836,98.947,99.971,%
Batch: 180 | Loss: 0.592 | Acc: 84.021,98.960,99.974,%
Batch: 200 | Loss: 0.592 | Acc: 84.010,98.962,99.977,%
Batch: 220 | Loss: 0.591 | Acc: 84.117,98.993,99.975,%
Batch: 240 | Loss: 0.593 | Acc: 84.051,99.011,99.974,%
Batch: 260 | Loss: 0.592 | Acc: 84.076,99.021,99.970,%
Batch: 280 | Loss: 0.592 | Acc: 84.114,99.013,99.972,%
Batch: 300 | Loss: 0.594 | Acc: 84.082,99.009,99.971,%
Batch: 320 | Loss: 0.596 | Acc: 84.105,98.990,99.973,%
Batch: 340 | Loss: 0.596 | Acc: 84.079,98.992,99.975,%
Batch: 360 | Loss: 0.594 | Acc: 84.070,98.985,99.976,%
Batch: 380 | Loss: 0.591 | Acc: 84.117,98.989,99.975,%
Batch: 0 | Loss: 0.980 | Acc: 78.125,92.188,92.969,%
Batch: 20 | Loss: 1.128 | Acc: 80.320,91.146,93.750,%
Batch: 40 | Loss: 1.121 | Acc: 80.335,91.101,93.921,%
Batch: 60 | Loss: 1.111 | Acc: 80.405,91.227,93.916,%

Epoch: 299
Batch: 0 | Loss: 0.611 | Acc: 85.156,99.219,100.000,%
Batch: 20 | Loss: 0.588 | Acc: 84.561,98.810,100.000,%
Batch: 40 | Loss: 0.585 | Acc: 84.680,99.028,99.981,%
Batch: 60 | Loss: 0.561 | Acc: 84.670,99.014,99.987,%
Batch: 80 | Loss: 0.556 | Acc: 84.597,99.093,99.990,%
Batch: 100 | Loss: 0.559 | Acc: 84.522,99.087,99.992,%
Batch: 120 | Loss: 0.559 | Acc: 84.562,99.083,99.987,%
Batch: 140 | Loss: 0.563 | Acc: 84.342,99.014,99.989,%
Batch: 160 | Loss: 0.567 | Acc: 84.220,98.986,99.990,%
Batch: 180 | Loss: 0.573 | Acc: 84.146,98.973,99.991,%
Batch: 200 | Loss: 0.572 | Acc: 84.231,98.958,99.988,%
Batch: 220 | Loss: 0.569 | Acc: 84.329,98.957,99.982,%
Batch: 240 | Loss: 0.571 | Acc: 84.297,98.972,99.984,%
Batch: 260 | Loss: 0.569 | Acc: 84.321,99.015,99.979,%
Batch: 280 | Loss: 0.572 | Acc: 84.286,99.016,99.981,%
Batch: 300 | Loss: 0.572 | Acc: 84.204,99.029,99.982,%
Batch: 320 | Loss: 0.568 | Acc: 84.287,99.024,99.983,%
Batch: 340 | Loss: 0.570 | Acc: 84.276,98.997,99.982,%
Batch: 360 | Loss: 0.574 | Acc: 84.224,98.996,99.978,%
Batch: 380 | Loss: 0.573 | Acc: 84.209,98.995,99.979,%
Batch: 0 | Loss: 0.975 | Acc: 78.125,91.406,94.531,%
Batch: 20 | Loss: 1.127 | Acc: 80.506,91.109,93.936,%
Batch: 40 | Loss: 1.120 | Acc: 80.393,91.120,94.055,%
Batch: 60 | Loss: 1.109 | Acc: 80.353,91.253,93.955,%

