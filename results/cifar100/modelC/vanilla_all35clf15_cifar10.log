==> Preparing data..
Dataset: CIFAR10
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=128, out_features=10, bias=True)
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=256, out_features=10, bias=True)
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=512, out_features=10, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Train all parameters

Epoch: 0
Batch: 0 | Loss: 7.043 | Acc: 7.031,14.062,9.375,% | Adaptive Acc: 9.375% | clf_exit: 0.000 0.000 1.000
Batch: 20 | Loss: 6.438 | Acc: 16.257,20.796,23.400,% | Adaptive Acc: 23.475% | clf_exit: 0.003 0.006 0.991
Batch: 40 | Loss: 6.179 | Acc: 19.474,24.409,26.677,% | Adaptive Acc: 26.829% | clf_exit: 0.006 0.007 0.987
Batch: 60 | Loss: 5.978 | Acc: 22.170,26.319,29.419,% | Adaptive Acc: 29.559% | clf_exit: 0.005 0.015 0.980
Batch: 80 | Loss: 5.868 | Acc: 23.563,27.768,30.999,% | Adaptive Acc: 31.105% | clf_exit: 0.008 0.024 0.969
Batch: 100 | Loss: 5.765 | Acc: 24.590,29.278,32.619,% | Adaptive Acc: 32.689% | clf_exit: 0.009 0.029 0.962
Batch: 120 | Loss: 5.658 | Acc: 25.510,30.779,34.472,% | Adaptive Acc: 34.504% | clf_exit: 0.010 0.039 0.951
Batch: 140 | Loss: 5.560 | Acc: 26.535,31.948,36.026,% | Adaptive Acc: 36.004% | clf_exit: 0.011 0.047 0.942
Batch: 160 | Loss: 5.482 | Acc: 27.121,33.055,37.340,% | Adaptive Acc: 37.272% | clf_exit: 0.013 0.057 0.929
Batch: 180 | Loss: 5.411 | Acc: 27.810,34.146,38.562,% | Adaptive Acc: 38.493% | clf_exit: 0.015 0.063 0.922
Batch: 200 | Loss: 5.349 | Acc: 28.312,35.090,39.572,% | Adaptive Acc: 39.451% | clf_exit: 0.016 0.071 0.913
Batch: 220 | Loss: 5.287 | Acc: 28.751,36.111,40.590,% | Adaptive Acc: 40.459% | clf_exit: 0.018 0.080 0.903
Batch: 240 | Loss: 5.233 | Acc: 29.133,36.897,41.461,% | Adaptive Acc: 41.283% | clf_exit: 0.019 0.088 0.893
Batch: 260 | Loss: 5.178 | Acc: 29.559,37.677,42.385,% | Adaptive Acc: 42.167% | clf_exit: 0.021 0.096 0.883
Batch: 280 | Loss: 5.131 | Acc: 29.924,38.379,43.188,% | Adaptive Acc: 42.941% | clf_exit: 0.023 0.105 0.872
Batch: 300 | Loss: 5.086 | Acc: 30.295,39.062,43.885,% | Adaptive Acc: 43.623% | clf_exit: 0.023 0.115 0.862
Batch: 320 | Loss: 5.038 | Acc: 30.763,39.771,44.658,% | Adaptive Acc: 44.363% | clf_exit: 0.025 0.126 0.850
Batch: 340 | Loss: 4.996 | Acc: 31.147,40.371,45.315,% | Adaptive Acc: 44.969% | clf_exit: 0.025 0.135 0.839
Batch: 360 | Loss: 4.956 | Acc: 31.343,40.902,45.953,% | Adaptive Acc: 45.577% | clf_exit: 0.026 0.144 0.830
Batch: 380 | Loss: 4.914 | Acc: 31.746,41.535,46.666,% | Adaptive Acc: 46.235% | clf_exit: 0.027 0.153 0.819
Batch: 0 | Loss: 4.239 | Acc: 35.156,41.406,54.688,% | Adaptive Acc: 48.438% | clf_exit: 0.086 0.422 0.492
Batch: 20 | Loss: 4.266 | Acc: 37.946,50.186,55.469,% | Adaptive Acc: 53.906% | clf_exit: 0.100 0.394 0.506
Batch: 40 | Loss: 4.221 | Acc: 37.633,51.334,56.498,% | Adaptive Acc: 54.668% | clf_exit: 0.097 0.406 0.497
Batch: 60 | Loss: 4.220 | Acc: 38.089,51.434,56.199,% | Adaptive Acc: 54.367% | clf_exit: 0.098 0.400 0.502
Train all parameters

Epoch: 1
Batch: 0 | Loss: 3.911 | Acc: 42.969,57.812,59.375,% | Adaptive Acc: 56.250% | clf_exit: 0.070 0.383 0.547
Batch: 20 | Loss: 4.132 | Acc: 39.583,53.534,57.217,% | Adaptive Acc: 56.176% | clf_exit: 0.051 0.346 0.604
Batch: 40 | Loss: 4.090 | Acc: 39.291,53.601,58.670,% | Adaptive Acc: 57.527% | clf_exit: 0.056 0.353 0.591
Batch: 60 | Loss: 4.084 | Acc: 39.370,53.496,59.221,% | Adaptive Acc: 57.723% | clf_exit: 0.063 0.351 0.585
Batch: 80 | Loss: 4.045 | Acc: 39.439,54.090,59.915,% | Adaptive Acc: 58.314% | clf_exit: 0.064 0.350 0.586
Batch: 100 | Loss: 4.029 | Acc: 39.318,54.533,60.574,% | Adaptive Acc: 59.011% | clf_exit: 0.065 0.352 0.583
Batch: 120 | Loss: 4.008 | Acc: 39.495,54.759,60.808,% | Adaptive Acc: 59.252% | clf_exit: 0.069 0.353 0.577
Batch: 140 | Loss: 3.983 | Acc: 39.955,55.325,61.115,% | Adaptive Acc: 59.597% | clf_exit: 0.071 0.358 0.571
Batch: 160 | Loss: 3.985 | Acc: 39.878,55.284,61.122,% | Adaptive Acc: 59.627% | clf_exit: 0.072 0.361 0.567
Batch: 180 | Loss: 3.969 | Acc: 40.016,55.447,61.356,% | Adaptive Acc: 59.807% | clf_exit: 0.074 0.362 0.564
Batch: 200 | Loss: 3.952 | Acc: 40.306,55.826,61.614,% | Adaptive Acc: 60.102% | clf_exit: 0.073 0.367 0.559
Batch: 220 | Loss: 3.938 | Acc: 40.590,55.974,61.818,% | Adaptive Acc: 60.223% | clf_exit: 0.077 0.369 0.555
Batch: 240 | Loss: 3.919 | Acc: 40.790,56.101,61.998,% | Adaptive Acc: 60.406% | clf_exit: 0.079 0.371 0.550
Batch: 260 | Loss: 3.902 | Acc: 41.011,56.334,62.225,% | Adaptive Acc: 60.635% | clf_exit: 0.080 0.374 0.546
Batch: 280 | Loss: 3.886 | Acc: 41.234,56.553,62.486,% | Adaptive Acc: 60.885% | clf_exit: 0.082 0.376 0.542
Batch: 300 | Loss: 3.874 | Acc: 41.414,56.657,62.645,% | Adaptive Acc: 61.005% | clf_exit: 0.085 0.377 0.538
Batch: 320 | Loss: 3.857 | Acc: 41.642,56.829,62.880,% | Adaptive Acc: 61.244% | clf_exit: 0.086 0.380 0.534
Batch: 340 | Loss: 3.841 | Acc: 41.796,57.020,63.146,% | Adaptive Acc: 61.517% | clf_exit: 0.088 0.382 0.531
Batch: 360 | Loss: 3.826 | Acc: 42.075,57.213,63.402,% | Adaptive Acc: 61.764% | clf_exit: 0.089 0.383 0.528
Batch: 380 | Loss: 3.810 | Acc: 42.333,57.495,63.704,% | Adaptive Acc: 61.987% | clf_exit: 0.091 0.385 0.524
Batch: 0 | Loss: 3.469 | Acc: 45.312,55.469,67.188,% | Adaptive Acc: 59.375% | clf_exit: 0.172 0.461 0.367
Batch: 20 | Loss: 3.630 | Acc: 45.275,59.040,65.551,% | Adaptive Acc: 62.798% | clf_exit: 0.176 0.453 0.371
Batch: 40 | Loss: 3.550 | Acc: 45.960,60.690,66.749,% | Adaptive Acc: 63.377% | clf_exit: 0.168 0.472 0.360
Batch: 60 | Loss: 3.550 | Acc: 46.030,60.438,66.637,% | Adaptive Acc: 63.358% | clf_exit: 0.172 0.464 0.364
Train all parameters

Epoch: 2
Batch: 0 | Loss: 3.649 | Acc: 40.625,63.281,64.062,% | Adaptive Acc: 64.844% | clf_exit: 0.102 0.484 0.414
Batch: 20 | Loss: 3.433 | Acc: 46.615,62.723,69.792,% | Adaptive Acc: 66.332% | clf_exit: 0.150 0.422 0.427
Batch: 40 | Loss: 3.475 | Acc: 46.627,62.043,68.540,% | Adaptive Acc: 65.625% | clf_exit: 0.142 0.416 0.441
Batch: 60 | Loss: 3.483 | Acc: 46.555,61.744,68.238,% | Adaptive Acc: 65.241% | clf_exit: 0.144 0.417 0.439
Batch: 80 | Loss: 3.488 | Acc: 46.518,61.863,68.451,% | Adaptive Acc: 65.741% | clf_exit: 0.143 0.419 0.438
Batch: 100 | Loss: 3.484 | Acc: 46.566,61.881,68.263,% | Adaptive Acc: 65.517% | clf_exit: 0.148 0.417 0.435
Batch: 120 | Loss: 3.472 | Acc: 46.894,62.151,68.266,% | Adaptive Acc: 65.586% | clf_exit: 0.149 0.420 0.431
Batch: 140 | Loss: 3.462 | Acc: 47.291,62.306,68.495,% | Adaptive Acc: 65.830% | clf_exit: 0.153 0.417 0.429
Batch: 160 | Loss: 3.439 | Acc: 47.574,62.558,68.891,% | Adaptive Acc: 66.135% | clf_exit: 0.157 0.417 0.426
Batch: 180 | Loss: 3.435 | Acc: 47.583,62.582,68.979,% | Adaptive Acc: 66.212% | clf_exit: 0.159 0.419 0.422
Batch: 200 | Loss: 3.428 | Acc: 47.606,62.593,69.088,% | Adaptive Acc: 66.290% | clf_exit: 0.163 0.415 0.422
Batch: 220 | Loss: 3.414 | Acc: 47.794,62.730,69.319,% | Adaptive Acc: 66.509% | clf_exit: 0.165 0.416 0.419
Batch: 240 | Loss: 3.407 | Acc: 47.909,62.876,69.460,% | Adaptive Acc: 66.627% | clf_exit: 0.167 0.417 0.416
Batch: 260 | Loss: 3.394 | Acc: 48.003,62.961,69.588,% | Adaptive Acc: 66.655% | clf_exit: 0.170 0.416 0.414
Batch: 280 | Loss: 3.382 | Acc: 48.179,63.167,69.662,% | Adaptive Acc: 66.768% | clf_exit: 0.173 0.416 0.411
Batch: 300 | Loss: 3.369 | Acc: 48.321,63.273,69.806,% | Adaptive Acc: 66.858% | clf_exit: 0.177 0.416 0.407
Batch: 320 | Loss: 3.356 | Acc: 48.469,63.478,70.052,% | Adaptive Acc: 67.051% | clf_exit: 0.179 0.416 0.405
Batch: 340 | Loss: 3.347 | Acc: 48.598,63.554,70.141,% | Adaptive Acc: 67.142% | clf_exit: 0.181 0.416 0.403
Batch: 360 | Loss: 3.341 | Acc: 48.537,63.589,70.243,% | Adaptive Acc: 67.151% | clf_exit: 0.183 0.415 0.402
Batch: 380 | Loss: 3.331 | Acc: 48.661,63.739,70.374,% | Adaptive Acc: 67.243% | clf_exit: 0.183 0.417 0.400
Batch: 0 | Loss: 3.229 | Acc: 46.875,62.500,71.094,% | Adaptive Acc: 67.969% | clf_exit: 0.336 0.344 0.320
Batch: 20 | Loss: 3.454 | Acc: 44.978,61.719,70.164,% | Adaptive Acc: 64.621% | clf_exit: 0.318 0.398 0.284
Batch: 40 | Loss: 3.383 | Acc: 46.189,62.481,70.979,% | Adaptive Acc: 64.996% | clf_exit: 0.313 0.415 0.272
Batch: 60 | Loss: 3.374 | Acc: 46.452,62.628,70.710,% | Adaptive Acc: 65.087% | clf_exit: 0.313 0.405 0.281
Train all parameters

Epoch: 3
Batch: 0 | Loss: 3.072 | Acc: 51.562,65.625,78.125,% | Adaptive Acc: 74.219% | clf_exit: 0.211 0.406 0.383
Batch: 20 | Loss: 3.070 | Acc: 51.079,66.369,74.256,% | Adaptive Acc: 70.685% | clf_exit: 0.223 0.438 0.339
Batch: 40 | Loss: 3.083 | Acc: 50.781,66.521,74.200,% | Adaptive Acc: 70.446% | clf_exit: 0.229 0.424 0.347
Batch: 60 | Loss: 3.059 | Acc: 51.230,67.098,74.411,% | Adaptive Acc: 70.684% | clf_exit: 0.233 0.423 0.343
Batch: 80 | Loss: 3.050 | Acc: 51.389,67.390,74.769,% | Adaptive Acc: 70.920% | clf_exit: 0.242 0.418 0.341
Batch: 100 | Loss: 3.050 | Acc: 51.431,67.195,74.536,% | Adaptive Acc: 70.560% | clf_exit: 0.246 0.417 0.337
Batch: 120 | Loss: 3.044 | Acc: 51.472,67.588,74.768,% | Adaptive Acc: 70.713% | clf_exit: 0.250 0.417 0.333
Batch: 140 | Loss: 3.040 | Acc: 51.507,67.559,74.839,% | Adaptive Acc: 70.734% | clf_exit: 0.252 0.415 0.332
Batch: 160 | Loss: 3.037 | Acc: 51.645,67.537,74.680,% | Adaptive Acc: 70.667% | clf_exit: 0.253 0.414 0.333
Batch: 180 | Loss: 3.042 | Acc: 51.627,67.550,74.663,% | Adaptive Acc: 70.507% | clf_exit: 0.254 0.412 0.334
Batch: 200 | Loss: 3.042 | Acc: 51.632,67.510,74.646,% | Adaptive Acc: 70.464% | clf_exit: 0.256 0.409 0.335
Batch: 220 | Loss: 3.043 | Acc: 51.545,67.453,74.611,% | Adaptive Acc: 70.422% | clf_exit: 0.258 0.408 0.333
Batch: 240 | Loss: 3.040 | Acc: 51.598,67.447,74.643,% | Adaptive Acc: 70.471% | clf_exit: 0.260 0.407 0.333
Batch: 260 | Loss: 3.045 | Acc: 51.700,67.334,74.509,% | Adaptive Acc: 70.393% | clf_exit: 0.261 0.405 0.333
Batch: 280 | Loss: 3.036 | Acc: 51.846,67.468,74.636,% | Adaptive Acc: 70.513% | clf_exit: 0.264 0.405 0.331
Batch: 300 | Loss: 3.031 | Acc: 51.897,67.530,74.689,% | Adaptive Acc: 70.567% | clf_exit: 0.266 0.403 0.331
Batch: 320 | Loss: 3.023 | Acc: 52.020,67.633,74.805,% | Adaptive Acc: 70.653% | clf_exit: 0.268 0.402 0.330
Batch: 340 | Loss: 3.017 | Acc: 52.133,67.767,74.901,% | Adaptive Acc: 70.727% | clf_exit: 0.270 0.402 0.328
Batch: 360 | Loss: 3.015 | Acc: 52.093,67.783,74.918,% | Adaptive Acc: 70.711% | clf_exit: 0.271 0.401 0.328
Batch: 380 | Loss: 3.009 | Acc: 52.174,67.844,75.029,% | Adaptive Acc: 70.782% | clf_exit: 0.273 0.400 0.327
Batch: 0 | Loss: 3.008 | Acc: 45.312,64.062,69.531,% | Adaptive Acc: 60.156% | clf_exit: 0.438 0.328 0.234
Batch: 20 | Loss: 3.318 | Acc: 46.912,64.509,70.871,% | Adaptive Acc: 61.012% | clf_exit: 0.460 0.341 0.199
Batch: 40 | Loss: 3.267 | Acc: 46.951,65.720,71.989,% | Adaptive Acc: 61.719% | clf_exit: 0.460 0.343 0.197
Batch: 60 | Loss: 3.266 | Acc: 46.593,65.779,71.734,% | Adaptive Acc: 61.796% | clf_exit: 0.456 0.341 0.203
Train all parameters

Epoch: 4
Batch: 0 | Loss: 3.113 | Acc: 49.219,67.188,76.562,% | Adaptive Acc: 73.438% | clf_exit: 0.320 0.352 0.328
Batch: 20 | Loss: 2.853 | Acc: 54.018,70.796,77.083,% | Adaptive Acc: 73.698% | clf_exit: 0.314 0.389 0.298
Batch: 40 | Loss: 2.853 | Acc: 53.735,70.655,76.982,% | Adaptive Acc: 72.847% | clf_exit: 0.312 0.392 0.296
Batch: 60 | Loss: 2.850 | Acc: 53.919,70.645,76.947,% | Adaptive Acc: 72.707% | clf_exit: 0.314 0.393 0.293
Batch: 80 | Loss: 2.844 | Acc: 53.578,70.573,77.151,% | Adaptive Acc: 72.541% | clf_exit: 0.318 0.389 0.293
Batch: 100 | Loss: 2.832 | Acc: 53.806,70.390,77.243,% | Adaptive Acc: 72.556% | clf_exit: 0.320 0.391 0.289
Batch: 120 | Loss: 2.831 | Acc: 53.913,70.364,77.085,% | Adaptive Acc: 72.379% | clf_exit: 0.324 0.391 0.285
Batch: 140 | Loss: 2.828 | Acc: 53.901,70.307,77.272,% | Adaptive Acc: 72.407% | clf_exit: 0.324 0.389 0.287
Batch: 160 | Loss: 2.819 | Acc: 54.052,70.429,77.407,% | Adaptive Acc: 72.554% | clf_exit: 0.326 0.392 0.283
Batch: 180 | Loss: 2.816 | Acc: 53.975,70.571,77.400,% | Adaptive Acc: 72.501% | clf_exit: 0.329 0.389 0.282
Batch: 200 | Loss: 2.810 | Acc: 54.073,70.623,77.441,% | Adaptive Acc: 72.474% | clf_exit: 0.332 0.387 0.281
Batch: 220 | Loss: 2.809 | Acc: 54.034,70.666,77.492,% | Adaptive Acc: 72.398% | clf_exit: 0.333 0.388 0.280
Batch: 240 | Loss: 2.807 | Acc: 53.935,70.653,77.516,% | Adaptive Acc: 72.413% | clf_exit: 0.334 0.387 0.279
Batch: 260 | Loss: 2.806 | Acc: 54.002,70.600,77.559,% | Adaptive Acc: 72.435% | clf_exit: 0.333 0.388 0.279
Batch: 280 | Loss: 2.798 | Acc: 54.034,70.705,77.719,% | Adaptive Acc: 72.517% | clf_exit: 0.335 0.387 0.278
Batch: 300 | Loss: 2.791 | Acc: 54.132,70.878,77.858,% | Adaptive Acc: 72.700% | clf_exit: 0.336 0.388 0.277
Batch: 320 | Loss: 2.783 | Acc: 54.232,70.948,77.960,% | Adaptive Acc: 72.814% | clf_exit: 0.338 0.387 0.275
Batch: 340 | Loss: 2.776 | Acc: 54.339,71.036,78.093,% | Adaptive Acc: 72.885% | clf_exit: 0.339 0.388 0.273
Batch: 360 | Loss: 2.763 | Acc: 54.579,71.208,78.222,% | Adaptive Acc: 73.048% | clf_exit: 0.341 0.387 0.271
Batch: 380 | Loss: 2.757 | Acc: 54.612,71.278,78.273,% | Adaptive Acc: 73.025% | clf_exit: 0.344 0.387 0.269
Batch: 0 | Loss: 2.635 | Acc: 51.562,70.312,76.562,% | Adaptive Acc: 71.875% | clf_exit: 0.414 0.359 0.227
Batch: 20 | Loss: 2.913 | Acc: 51.562,69.531,77.046,% | Adaptive Acc: 69.420% | clf_exit: 0.428 0.384 0.188
Batch: 40 | Loss: 2.861 | Acc: 51.944,70.027,77.725,% | Adaptive Acc: 69.341% | clf_exit: 0.439 0.379 0.182
Batch: 60 | Loss: 2.855 | Acc: 51.793,70.146,77.856,% | Adaptive Acc: 69.608% | clf_exit: 0.433 0.379 0.188
Train all parameters

Epoch: 5
Batch: 0 | Loss: 2.998 | Acc: 55.469,71.875,76.562,% | Adaptive Acc: 70.312% | clf_exit: 0.391 0.367 0.242
Batch: 20 | Loss: 2.623 | Acc: 55.580,73.996,81.101,% | Adaptive Acc: 75.446% | clf_exit: 0.381 0.376 0.243
Batch: 40 | Loss: 2.613 | Acc: 56.021,73.780,80.640,% | Adaptive Acc: 75.019% | clf_exit: 0.376 0.374 0.250
Batch: 60 | Loss: 2.639 | Acc: 55.955,72.964,80.136,% | Adaptive Acc: 74.539% | clf_exit: 0.378 0.373 0.249
Batch: 80 | Loss: 2.620 | Acc: 56.346,73.457,80.353,% | Adaptive Acc: 74.778% | clf_exit: 0.379 0.375 0.246
Batch: 100 | Loss: 2.603 | Acc: 56.428,73.554,80.330,% | Adaptive Acc: 74.729% | clf_exit: 0.380 0.377 0.243
Batch: 120 | Loss: 2.614 | Acc: 56.256,73.212,80.456,% | Adaptive Acc: 74.684% | clf_exit: 0.380 0.376 0.244
Batch: 140 | Loss: 2.600 | Acc: 56.316,73.343,80.629,% | Adaptive Acc: 74.756% | clf_exit: 0.383 0.375 0.242
Batch: 160 | Loss: 2.591 | Acc: 56.478,73.413,80.677,% | Adaptive Acc: 74.728% | clf_exit: 0.387 0.373 0.240
Batch: 180 | Loss: 2.577 | Acc: 56.509,73.532,80.827,% | Adaptive Acc: 74.845% | clf_exit: 0.388 0.374 0.238
Batch: 200 | Loss: 2.567 | Acc: 56.553,73.686,80.982,% | Adaptive Acc: 74.845% | clf_exit: 0.390 0.374 0.236
Batch: 220 | Loss: 2.568 | Acc: 56.434,73.681,81.102,% | Adaptive Acc: 74.844% | clf_exit: 0.392 0.374 0.234
Batch: 240 | Loss: 2.564 | Acc: 56.526,73.739,81.107,% | Adaptive Acc: 74.831% | clf_exit: 0.392 0.374 0.234
Batch: 260 | Loss: 2.562 | Acc: 56.588,73.788,81.103,% | Adaptive Acc: 74.823% | clf_exit: 0.394 0.373 0.233
Batch: 280 | Loss: 2.557 | Acc: 56.659,73.866,81.155,% | Adaptive Acc: 74.869% | clf_exit: 0.395 0.374 0.231
Batch: 300 | Loss: 2.555 | Acc: 56.777,73.868,81.115,% | Adaptive Acc: 74.836% | clf_exit: 0.396 0.374 0.230
Batch: 320 | Loss: 2.557 | Acc: 56.781,73.866,81.121,% | Adaptive Acc: 74.827% | clf_exit: 0.396 0.373 0.230
Batch: 340 | Loss: 2.553 | Acc: 56.809,73.903,81.156,% | Adaptive Acc: 74.874% | clf_exit: 0.397 0.374 0.229
Batch: 360 | Loss: 2.554 | Acc: 56.750,73.927,81.155,% | Adaptive Acc: 74.939% | clf_exit: 0.396 0.374 0.230
Batch: 380 | Loss: 2.554 | Acc: 56.771,73.921,81.150,% | Adaptive Acc: 74.945% | clf_exit: 0.397 0.374 0.229
Batch: 0 | Loss: 2.444 | Acc: 51.562,70.312,82.812,% | Adaptive Acc: 72.656% | clf_exit: 0.516 0.336 0.148
Batch: 20 | Loss: 2.696 | Acc: 53.051,72.284,80.841,% | Adaptive Acc: 70.647% | clf_exit: 0.493 0.359 0.148
Batch: 40 | Loss: 2.654 | Acc: 53.735,72.332,81.269,% | Adaptive Acc: 70.694% | clf_exit: 0.500 0.352 0.148
Batch: 60 | Loss: 2.652 | Acc: 53.983,72.259,80.994,% | Adaptive Acc: 70.530% | clf_exit: 0.494 0.356 0.149
Train all parameters

Epoch: 6
Batch: 0 | Loss: 2.387 | Acc: 56.250,73.438,84.375,% | Adaptive Acc: 77.344% | clf_exit: 0.352 0.430 0.219
Batch: 20 | Loss: 2.463 | Acc: 55.952,74.628,82.329,% | Adaptive Acc: 75.595% | clf_exit: 0.420 0.368 0.212
Batch: 40 | Loss: 2.402 | Acc: 57.908,75.724,82.812,% | Adaptive Acc: 76.010% | clf_exit: 0.421 0.375 0.205
Batch: 60 | Loss: 2.446 | Acc: 57.556,75.346,82.249,% | Adaptive Acc: 75.807% | clf_exit: 0.421 0.373 0.205
Batch: 80 | Loss: 2.455 | Acc: 57.716,75.164,81.887,% | Adaptive Acc: 75.675% | clf_exit: 0.415 0.375 0.210
Batch: 100 | Loss: 2.430 | Acc: 57.929,75.425,82.480,% | Adaptive Acc: 76.191% | clf_exit: 0.421 0.371 0.208
Batch: 120 | Loss: 2.435 | Acc: 58.148,75.400,82.515,% | Adaptive Acc: 76.143% | clf_exit: 0.423 0.369 0.208
Batch: 140 | Loss: 2.425 | Acc: 58.283,75.587,82.646,% | Adaptive Acc: 76.269% | clf_exit: 0.426 0.367 0.207
Batch: 160 | Loss: 2.417 | Acc: 58.264,75.718,82.856,% | Adaptive Acc: 76.412% | clf_exit: 0.426 0.367 0.207
Batch: 180 | Loss: 2.415 | Acc: 58.421,75.712,82.743,% | Adaptive Acc: 76.282% | clf_exit: 0.428 0.367 0.205
Batch: 200 | Loss: 2.414 | Acc: 58.271,75.711,82.735,% | Adaptive Acc: 76.139% | clf_exit: 0.430 0.365 0.205
Batch: 220 | Loss: 2.416 | Acc: 58.212,75.714,82.724,% | Adaptive Acc: 76.068% | clf_exit: 0.429 0.366 0.205
Batch: 240 | Loss: 2.417 | Acc: 58.234,75.703,82.764,% | Adaptive Acc: 76.092% | clf_exit: 0.428 0.366 0.205
Batch: 260 | Loss: 2.416 | Acc: 58.145,75.700,82.702,% | Adaptive Acc: 76.069% | clf_exit: 0.429 0.365 0.205
Batch: 280 | Loss: 2.412 | Acc: 58.319,75.740,82.735,% | Adaptive Acc: 76.120% | clf_exit: 0.431 0.365 0.204
Batch: 300 | Loss: 2.408 | Acc: 58.332,75.833,82.745,% | Adaptive Acc: 76.095% | clf_exit: 0.433 0.363 0.204
Batch: 320 | Loss: 2.408 | Acc: 58.311,75.918,82.771,% | Adaptive Acc: 76.110% | clf_exit: 0.434 0.363 0.203
Batch: 340 | Loss: 2.409 | Acc: 58.280,75.912,82.778,% | Adaptive Acc: 76.093% | clf_exit: 0.434 0.364 0.202
Batch: 360 | Loss: 2.413 | Acc: 58.286,75.870,82.726,% | Adaptive Acc: 76.017% | clf_exit: 0.435 0.363 0.202
Batch: 380 | Loss: 2.409 | Acc: 58.327,75.929,82.751,% | Adaptive Acc: 76.050% | clf_exit: 0.435 0.363 0.201
Batch: 0 | Loss: 2.501 | Acc: 57.812,71.094,78.906,% | Adaptive Acc: 68.750% | clf_exit: 0.555 0.266 0.180
Batch: 20 | Loss: 2.538 | Acc: 57.366,73.475,80.506,% | Adaptive Acc: 71.838% | clf_exit: 0.518 0.350 0.132
Batch: 40 | Loss: 2.502 | Acc: 57.793,74.066,81.174,% | Adaptive Acc: 72.180% | clf_exit: 0.531 0.341 0.128
Batch: 60 | Loss: 2.500 | Acc: 57.351,74.155,81.224,% | Adaptive Acc: 72.118% | clf_exit: 0.527 0.341 0.132
Train all parameters

Epoch: 7
Batch: 0 | Loss: 2.189 | Acc: 65.625,80.469,83.594,% | Adaptive Acc: 78.125% | clf_exit: 0.508 0.352 0.141
Batch: 20 | Loss: 2.339 | Acc: 57.515,76.190,84.301,% | Adaptive Acc: 75.818% | clf_exit: 0.462 0.344 0.193
Batch: 40 | Loss: 2.312 | Acc: 58.765,76.601,84.470,% | Adaptive Acc: 76.543% | clf_exit: 0.462 0.349 0.190
Batch: 60 | Loss: 2.310 | Acc: 58.837,76.614,84.516,% | Adaptive Acc: 76.358% | clf_exit: 0.461 0.347 0.193
Batch: 80 | Loss: 2.294 | Acc: 59.076,76.794,84.616,% | Adaptive Acc: 76.495% | clf_exit: 0.458 0.353 0.189
Batch: 100 | Loss: 2.311 | Acc: 58.857,76.740,84.213,% | Adaptive Acc: 76.207% | clf_exit: 0.460 0.351 0.189
Batch: 120 | Loss: 2.323 | Acc: 58.787,76.692,84.039,% | Adaptive Acc: 76.143% | clf_exit: 0.461 0.351 0.188
Batch: 140 | Loss: 2.314 | Acc: 58.954,76.867,84.203,% | Adaptive Acc: 76.457% | clf_exit: 0.461 0.351 0.188
Batch: 160 | Loss: 2.312 | Acc: 59.035,76.960,84.288,% | Adaptive Acc: 76.747% | clf_exit: 0.462 0.351 0.187
Batch: 180 | Loss: 2.294 | Acc: 59.250,77.175,84.448,% | Adaptive Acc: 76.817% | clf_exit: 0.466 0.349 0.185
Batch: 200 | Loss: 2.297 | Acc: 59.181,77.114,84.445,% | Adaptive Acc: 76.811% | clf_exit: 0.466 0.349 0.185
Batch: 220 | Loss: 2.308 | Acc: 58.990,77.004,84.265,% | Adaptive Acc: 76.651% | clf_exit: 0.465 0.349 0.185
Batch: 240 | Loss: 2.302 | Acc: 59.012,77.033,84.346,% | Adaptive Acc: 76.747% | clf_exit: 0.464 0.351 0.185
Batch: 260 | Loss: 2.299 | Acc: 59.100,77.101,84.300,% | Adaptive Acc: 76.751% | clf_exit: 0.465 0.351 0.185
Batch: 280 | Loss: 2.293 | Acc: 59.219,77.169,84.331,% | Adaptive Acc: 76.796% | clf_exit: 0.466 0.350 0.184
Batch: 300 | Loss: 2.286 | Acc: 59.300,77.271,84.440,% | Adaptive Acc: 76.869% | clf_exit: 0.468 0.350 0.183
Batch: 320 | Loss: 2.285 | Acc: 59.380,77.356,84.475,% | Adaptive Acc: 76.886% | clf_exit: 0.469 0.349 0.182
Batch: 340 | Loss: 2.288 | Acc: 59.373,77.330,84.402,% | Adaptive Acc: 76.805% | clf_exit: 0.471 0.348 0.181
Batch: 360 | Loss: 2.288 | Acc: 59.453,77.326,84.388,% | Adaptive Acc: 76.783% | clf_exit: 0.471 0.348 0.181
Batch: 380 | Loss: 2.286 | Acc: 59.463,77.352,84.414,% | Adaptive Acc: 76.860% | clf_exit: 0.471 0.348 0.181
Batch: 0 | Loss: 2.166 | Acc: 60.156,81.250,87.500,% | Adaptive Acc: 76.562% | clf_exit: 0.531 0.352 0.117
Batch: 20 | Loss: 2.389 | Acc: 58.073,75.223,83.073,% | Adaptive Acc: 73.512% | clf_exit: 0.554 0.332 0.114
Batch: 40 | Loss: 2.347 | Acc: 58.460,75.724,83.251,% | Adaptive Acc: 73.285% | clf_exit: 0.562 0.332 0.106
Batch: 60 | Loss: 2.336 | Acc: 58.440,76.101,83.530,% | Adaptive Acc: 73.578% | clf_exit: 0.560 0.332 0.108
Train all parameters

Epoch: 8
Batch: 0 | Loss: 1.902 | Acc: 65.625,84.375,87.500,% | Adaptive Acc: 78.906% | clf_exit: 0.516 0.344 0.141
Batch: 20 | Loss: 2.154 | Acc: 60.454,79.204,86.533,% | Adaptive Acc: 79.315% | clf_exit: 0.478 0.359 0.163
Batch: 40 | Loss: 2.201 | Acc: 60.023,78.639,85.309,% | Adaptive Acc: 78.258% | clf_exit: 0.477 0.360 0.163
Batch: 60 | Loss: 2.202 | Acc: 60.348,78.560,85.438,% | Adaptive Acc: 78.317% | clf_exit: 0.482 0.351 0.167
Batch: 80 | Loss: 2.196 | Acc: 60.484,78.800,85.446,% | Adaptive Acc: 78.096% | clf_exit: 0.486 0.350 0.164
Batch: 100 | Loss: 2.212 | Acc: 60.218,78.403,85.265,% | Adaptive Acc: 77.746% | clf_exit: 0.486 0.349 0.165
Batch: 120 | Loss: 2.204 | Acc: 60.266,78.738,85.544,% | Adaptive Acc: 78.131% | clf_exit: 0.482 0.352 0.167
Batch: 140 | Loss: 2.205 | Acc: 60.417,78.823,85.539,% | Adaptive Acc: 78.097% | clf_exit: 0.485 0.350 0.166
Batch: 160 | Loss: 2.202 | Acc: 60.569,78.770,85.612,% | Adaptive Acc: 78.193% | clf_exit: 0.484 0.350 0.165
Batch: 180 | Loss: 2.192 | Acc: 60.709,78.837,85.756,% | Adaptive Acc: 78.337% | clf_exit: 0.487 0.348 0.165
Batch: 200 | Loss: 2.195 | Acc: 60.584,78.809,85.658,% | Adaptive Acc: 78.094% | clf_exit: 0.487 0.349 0.165
Batch: 220 | Loss: 2.204 | Acc: 60.496,78.634,85.549,% | Adaptive Acc: 77.899% | clf_exit: 0.486 0.349 0.165
Batch: 240 | Loss: 2.199 | Acc: 60.581,78.650,85.516,% | Adaptive Acc: 77.908% | clf_exit: 0.487 0.348 0.164
Batch: 260 | Loss: 2.201 | Acc: 60.486,78.607,85.486,% | Adaptive Acc: 77.826% | clf_exit: 0.488 0.348 0.164
Batch: 280 | Loss: 2.200 | Acc: 60.415,78.667,85.509,% | Adaptive Acc: 77.844% | clf_exit: 0.488 0.349 0.164
Batch: 300 | Loss: 2.198 | Acc: 60.444,78.654,85.455,% | Adaptive Acc: 77.759% | clf_exit: 0.489 0.348 0.163
Batch: 320 | Loss: 2.199 | Acc: 60.439,78.707,85.380,% | Adaptive Acc: 77.731% | clf_exit: 0.489 0.348 0.163
Batch: 340 | Loss: 2.191 | Acc: 60.589,78.835,85.431,% | Adaptive Acc: 77.804% | clf_exit: 0.490 0.348 0.162
Batch: 360 | Loss: 2.187 | Acc: 60.691,78.885,85.435,% | Adaptive Acc: 77.878% | clf_exit: 0.491 0.348 0.162
Batch: 380 | Loss: 2.184 | Acc: 60.753,78.902,85.439,% | Adaptive Acc: 77.904% | clf_exit: 0.492 0.346 0.162
Batch: 0 | Loss: 2.087 | Acc: 59.375,77.344,84.375,% | Adaptive Acc: 72.656% | clf_exit: 0.586 0.312 0.102
Batch: 20 | Loss: 2.354 | Acc: 58.743,75.781,83.259,% | Adaptive Acc: 72.359% | clf_exit: 0.592 0.301 0.106
Batch: 40 | Loss: 2.311 | Acc: 58.689,76.925,83.937,% | Adaptive Acc: 72.694% | clf_exit: 0.599 0.291 0.109
Batch: 60 | Loss: 2.316 | Acc: 58.645,76.831,84.093,% | Adaptive Acc: 72.810% | clf_exit: 0.593 0.296 0.111
Train all parameters

Epoch: 9
Batch: 0 | Loss: 2.313 | Acc: 53.125,77.344,82.812,% | Adaptive Acc: 73.438% | clf_exit: 0.453 0.328 0.219
Batch: 20 | Loss: 2.124 | Acc: 60.379,78.869,86.496,% | Adaptive Acc: 77.530% | clf_exit: 0.495 0.343 0.162
Batch: 40 | Loss: 2.099 | Acc: 61.395,79.802,86.871,% | Adaptive Acc: 78.220% | clf_exit: 0.506 0.335 0.159
Batch: 60 | Loss: 2.077 | Acc: 61.898,80.174,86.911,% | Adaptive Acc: 78.599% | clf_exit: 0.508 0.343 0.150
Batch: 80 | Loss: 2.074 | Acc: 61.487,80.334,86.883,% | Adaptive Acc: 78.453% | clf_exit: 0.512 0.343 0.146
Batch: 100 | Loss: 2.084 | Acc: 61.293,80.384,86.881,% | Adaptive Acc: 78.759% | clf_exit: 0.508 0.344 0.148
Batch: 120 | Loss: 2.079 | Acc: 61.370,80.378,86.841,% | Adaptive Acc: 78.874% | clf_exit: 0.507 0.345 0.148
Batch: 140 | Loss: 2.077 | Acc: 61.215,80.402,86.813,% | Adaptive Acc: 78.851% | clf_exit: 0.507 0.347 0.145
Batch: 160 | Loss: 2.077 | Acc: 61.423,80.318,86.806,% | Adaptive Acc: 78.877% | clf_exit: 0.508 0.348 0.144
Batch: 180 | Loss: 2.077 | Acc: 61.447,80.395,86.891,% | Adaptive Acc: 78.958% | clf_exit: 0.509 0.346 0.145
Batch: 200 | Loss: 2.075 | Acc: 61.618,80.344,86.886,% | Adaptive Acc: 79.089% | clf_exit: 0.509 0.346 0.145
Batch: 220 | Loss: 2.077 | Acc: 61.475,80.366,86.786,% | Adaptive Acc: 78.949% | clf_exit: 0.510 0.345 0.146
Batch: 240 | Loss: 2.087 | Acc: 61.339,80.216,86.670,% | Adaptive Acc: 78.825% | clf_exit: 0.511 0.342 0.147
Batch: 260 | Loss: 2.083 | Acc: 61.384,80.265,86.647,% | Adaptive Acc: 78.867% | clf_exit: 0.512 0.342 0.146
Batch: 280 | Loss: 2.081 | Acc: 61.496,80.294,86.613,% | Adaptive Acc: 78.859% | clf_exit: 0.512 0.342 0.146
Batch: 300 | Loss: 2.081 | Acc: 61.579,80.266,86.605,% | Adaptive Acc: 78.867% | clf_exit: 0.514 0.341 0.145
Batch: 320 | Loss: 2.074 | Acc: 61.641,80.335,86.682,% | Adaptive Acc: 78.943% | clf_exit: 0.514 0.340 0.145
Batch: 340 | Loss: 2.078 | Acc: 61.611,80.306,86.650,% | Adaptive Acc: 78.890% | clf_exit: 0.514 0.341 0.145
Batch: 360 | Loss: 2.079 | Acc: 61.680,80.244,86.615,% | Adaptive Acc: 78.885% | clf_exit: 0.514 0.341 0.145
Batch: 380 | Loss: 2.077 | Acc: 61.758,80.274,86.618,% | Adaptive Acc: 78.871% | clf_exit: 0.515 0.340 0.146
Batch: 0 | Loss: 1.991 | Acc: 59.375,80.469,87.500,% | Adaptive Acc: 74.219% | clf_exit: 0.641 0.250 0.109
Batch: 20 | Loss: 2.299 | Acc: 60.640,76.414,84.635,% | Adaptive Acc: 74.405% | clf_exit: 0.600 0.303 0.097
Batch: 40 | Loss: 2.255 | Acc: 61.242,76.944,84.546,% | Adaptive Acc: 74.676% | clf_exit: 0.610 0.289 0.101
Batch: 60 | Loss: 2.259 | Acc: 61.027,76.691,84.657,% | Adaptive Acc: 74.577% | clf_exit: 0.601 0.298 0.101
Train all parameters

Epoch: 10
Batch: 0 | Loss: 1.948 | Acc: 67.188,84.375,85.938,% | Adaptive Acc: 79.688% | clf_exit: 0.570 0.344 0.086
Batch: 20 | Loss: 1.921 | Acc: 64.769,82.440,88.542,% | Adaptive Acc: 81.250% | clf_exit: 0.523 0.353 0.124
Batch: 40 | Loss: 1.953 | Acc: 63.834,81.688,88.205,% | Adaptive Acc: 80.488% | clf_exit: 0.528 0.344 0.128
Batch: 60 | Loss: 1.971 | Acc: 63.140,81.481,87.961,% | Adaptive Acc: 79.982% | clf_exit: 0.532 0.337 0.132
Batch: 80 | Loss: 1.992 | Acc: 62.770,81.366,87.664,% | Adaptive Acc: 79.417% | clf_exit: 0.534 0.334 0.131
Batch: 100 | Loss: 1.998 | Acc: 62.430,81.405,87.469,% | Adaptive Acc: 79.208% | clf_exit: 0.531 0.336 0.134
Batch: 120 | Loss: 1.991 | Acc: 62.616,81.386,87.526,% | Adaptive Acc: 79.287% | clf_exit: 0.534 0.333 0.133
Batch: 140 | Loss: 2.003 | Acc: 62.633,81.278,87.439,% | Adaptive Acc: 79.194% | clf_exit: 0.534 0.333 0.133
Batch: 160 | Loss: 1.997 | Acc: 62.660,81.342,87.515,% | Adaptive Acc: 79.294% | clf_exit: 0.533 0.334 0.133
Batch: 180 | Loss: 1.993 | Acc: 62.625,81.358,87.586,% | Adaptive Acc: 79.239% | clf_exit: 0.534 0.333 0.133
Batch: 200 | Loss: 1.990 | Acc: 62.803,81.472,87.589,% | Adaptive Acc: 79.396% | clf_exit: 0.535 0.333 0.132
Batch: 220 | Loss: 1.995 | Acc: 62.744,81.374,87.578,% | Adaptive Acc: 79.330% | clf_exit: 0.536 0.331 0.133
Batch: 240 | Loss: 1.988 | Acc: 62.908,81.490,87.662,% | Adaptive Acc: 79.451% | clf_exit: 0.536 0.331 0.133
Batch: 260 | Loss: 1.990 | Acc: 62.874,81.522,87.644,% | Adaptive Acc: 79.436% | clf_exit: 0.536 0.332 0.132
Batch: 280 | Loss: 1.990 | Acc: 62.778,81.428,87.589,% | Adaptive Acc: 79.373% | clf_exit: 0.536 0.331 0.133
Batch: 300 | Loss: 1.990 | Acc: 62.749,81.468,87.552,% | Adaptive Acc: 79.363% | clf_exit: 0.535 0.331 0.133
Batch: 320 | Loss: 1.991 | Acc: 62.785,81.447,87.505,% | Adaptive Acc: 79.378% | clf_exit: 0.535 0.331 0.134
Batch: 340 | Loss: 1.993 | Acc: 62.736,81.463,87.505,% | Adaptive Acc: 79.383% | clf_exit: 0.537 0.329 0.134
Batch: 360 | Loss: 1.999 | Acc: 62.690,81.378,87.435,% | Adaptive Acc: 79.372% | clf_exit: 0.536 0.330 0.134
Batch: 380 | Loss: 2.001 | Acc: 62.730,81.371,87.391,% | Adaptive Acc: 79.405% | clf_exit: 0.535 0.331 0.134
Batch: 0 | Loss: 1.935 | Acc: 57.812,84.375,89.062,% | Adaptive Acc: 72.656% | clf_exit: 0.656 0.242 0.102
Batch: 20 | Loss: 2.182 | Acc: 60.305,79.539,85.714,% | Adaptive Acc: 74.963% | clf_exit: 0.612 0.287 0.101
Batch: 40 | Loss: 2.144 | Acc: 60.690,79.821,86.014,% | Adaptive Acc: 74.905% | clf_exit: 0.619 0.282 0.099
Batch: 60 | Loss: 2.124 | Acc: 60.502,79.969,85.950,% | Adaptive Acc: 75.154% | clf_exit: 0.611 0.289 0.100
Train all parameters

Epoch: 11
Batch: 0 | Loss: 2.127 | Acc: 60.938,81.250,82.812,% | Adaptive Acc: 74.219% | clf_exit: 0.508 0.375 0.117
Batch: 20 | Loss: 1.915 | Acc: 63.207,83.185,88.207,% | Adaptive Acc: 79.911% | clf_exit: 0.550 0.317 0.133
Batch: 40 | Loss: 1.897 | Acc: 63.167,83.289,88.720,% | Adaptive Acc: 79.859% | clf_exit: 0.549 0.322 0.129
Batch: 60 | Loss: 1.906 | Acc: 63.537,83.389,88.576,% | Adaptive Acc: 80.161% | clf_exit: 0.546 0.327 0.127
Batch: 80 | Loss: 1.924 | Acc: 63.648,83.054,88.281,% | Adaptive Acc: 79.900% | clf_exit: 0.547 0.325 0.128
Batch: 100 | Loss: 1.924 | Acc: 63.475,82.905,88.374,% | Adaptive Acc: 79.788% | clf_exit: 0.547 0.326 0.127
Batch: 120 | Loss: 1.916 | Acc: 63.630,82.993,88.423,% | Adaptive Acc: 79.913% | clf_exit: 0.548 0.328 0.124
Batch: 140 | Loss: 1.915 | Acc: 63.664,82.962,88.370,% | Adaptive Acc: 79.904% | clf_exit: 0.549 0.328 0.123
Batch: 160 | Loss: 1.911 | Acc: 63.747,82.992,88.412,% | Adaptive Acc: 79.838% | clf_exit: 0.551 0.325 0.123
Batch: 180 | Loss: 1.912 | Acc: 63.799,82.920,88.445,% | Adaptive Acc: 79.895% | clf_exit: 0.552 0.325 0.123
Batch: 200 | Loss: 1.914 | Acc: 63.717,82.945,88.410,% | Adaptive Acc: 79.890% | clf_exit: 0.552 0.326 0.122
Batch: 220 | Loss: 1.916 | Acc: 63.698,82.890,88.377,% | Adaptive Acc: 79.946% | clf_exit: 0.552 0.326 0.122
Batch: 240 | Loss: 1.919 | Acc: 63.725,82.787,88.323,% | Adaptive Acc: 79.888% | clf_exit: 0.550 0.328 0.122
Batch: 260 | Loss: 1.926 | Acc: 63.673,82.663,88.221,% | Adaptive Acc: 79.816% | clf_exit: 0.549 0.328 0.123
Batch: 280 | Loss: 1.936 | Acc: 63.459,82.529,88.131,% | Adaptive Acc: 79.640% | clf_exit: 0.547 0.329 0.124
Batch: 300 | Loss: 1.937 | Acc: 63.530,82.493,88.053,% | Adaptive Acc: 79.633% | clf_exit: 0.548 0.328 0.124
Batch: 320 | Loss: 1.937 | Acc: 63.522,82.411,88.055,% | Adaptive Acc: 79.680% | clf_exit: 0.548 0.328 0.124
Batch: 340 | Loss: 1.938 | Acc: 63.556,82.377,87.974,% | Adaptive Acc: 79.653% | clf_exit: 0.548 0.327 0.125
Batch: 360 | Loss: 1.938 | Acc: 63.519,82.401,87.980,% | Adaptive Acc: 79.698% | clf_exit: 0.549 0.326 0.124
Batch: 380 | Loss: 1.938 | Acc: 63.499,82.388,87.978,% | Adaptive Acc: 79.731% | clf_exit: 0.549 0.326 0.124
Batch: 0 | Loss: 2.016 | Acc: 56.250,82.031,86.719,% | Adaptive Acc: 74.219% | clf_exit: 0.688 0.203 0.109
Batch: 20 | Loss: 2.162 | Acc: 59.821,78.906,85.938,% | Adaptive Acc: 73.661% | clf_exit: 0.652 0.263 0.085
Batch: 40 | Loss: 2.103 | Acc: 61.280,79.440,86.623,% | Adaptive Acc: 74.428% | clf_exit: 0.661 0.258 0.081
Batch: 60 | Loss: 2.101 | Acc: 60.899,79.162,86.399,% | Adaptive Acc: 74.155% | clf_exit: 0.655 0.262 0.083
Train all parameters

Epoch: 12
Batch: 0 | Loss: 1.599 | Acc: 64.844,87.500,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.508 0.359 0.133
Batch: 20 | Loss: 1.868 | Acc: 63.356,83.631,89.397,% | Adaptive Acc: 80.618% | clf_exit: 0.551 0.333 0.115
Batch: 40 | Loss: 1.877 | Acc: 63.720,83.270,89.101,% | Adaptive Acc: 80.393% | clf_exit: 0.549 0.336 0.116
Batch: 60 | Loss: 1.852 | Acc: 64.280,83.440,89.421,% | Adaptive Acc: 80.494% | clf_exit: 0.556 0.328 0.115
Batch: 80 | Loss: 1.843 | Acc: 64.169,83.729,89.313,% | Adaptive Acc: 80.768% | clf_exit: 0.560 0.326 0.114
Batch: 100 | Loss: 1.843 | Acc: 64.380,83.741,89.356,% | Adaptive Acc: 80.732% | clf_exit: 0.561 0.325 0.114
Batch: 120 | Loss: 1.856 | Acc: 64.243,83.510,89.127,% | Adaptive Acc: 80.604% | clf_exit: 0.559 0.326 0.116
Batch: 140 | Loss: 1.857 | Acc: 64.195,83.444,89.140,% | Adaptive Acc: 80.563% | clf_exit: 0.560 0.324 0.117
Batch: 160 | Loss: 1.863 | Acc: 64.179,83.371,89.067,% | Adaptive Acc: 80.551% | clf_exit: 0.559 0.325 0.116
Batch: 180 | Loss: 1.859 | Acc: 64.274,83.352,88.989,% | Adaptive Acc: 80.598% | clf_exit: 0.561 0.323 0.116
Batch: 200 | Loss: 1.856 | Acc: 64.381,83.353,89.074,% | Adaptive Acc: 80.671% | clf_exit: 0.563 0.320 0.117
Batch: 220 | Loss: 1.859 | Acc: 64.349,83.265,89.013,% | Adaptive Acc: 80.688% | clf_exit: 0.563 0.319 0.117
Batch: 240 | Loss: 1.860 | Acc: 64.370,83.253,88.968,% | Adaptive Acc: 80.731% | clf_exit: 0.564 0.318 0.118
Batch: 260 | Loss: 1.865 | Acc: 64.308,83.232,88.928,% | Adaptive Acc: 80.675% | clf_exit: 0.563 0.318 0.118
Batch: 280 | Loss: 1.868 | Acc: 64.299,83.257,88.871,% | Adaptive Acc: 80.680% | clf_exit: 0.564 0.318 0.119
Batch: 300 | Loss: 1.868 | Acc: 64.291,83.243,88.881,% | Adaptive Acc: 80.609% | clf_exit: 0.563 0.319 0.118
Batch: 320 | Loss: 1.863 | Acc: 64.435,83.304,88.885,% | Adaptive Acc: 80.629% | clf_exit: 0.564 0.319 0.117
Batch: 340 | Loss: 1.868 | Acc: 64.397,83.227,88.854,% | Adaptive Acc: 80.581% | clf_exit: 0.563 0.320 0.117
Batch: 360 | Loss: 1.869 | Acc: 64.335,83.235,88.822,% | Adaptive Acc: 80.527% | clf_exit: 0.563 0.320 0.117
Batch: 380 | Loss: 1.866 | Acc: 64.348,83.262,88.872,% | Adaptive Acc: 80.520% | clf_exit: 0.565 0.319 0.116
Batch: 0 | Loss: 2.001 | Acc: 60.938,82.812,86.719,% | Adaptive Acc: 80.469% | clf_exit: 0.625 0.258 0.117
Batch: 20 | Loss: 2.146 | Acc: 62.426,80.022,85.603,% | Adaptive Acc: 75.223% | clf_exit: 0.630 0.288 0.082
Batch: 40 | Loss: 2.098 | Acc: 62.995,80.240,85.804,% | Adaptive Acc: 75.553% | clf_exit: 0.637 0.282 0.081
Batch: 60 | Loss: 2.078 | Acc: 62.743,80.072,85.899,% | Adaptive Acc: 75.461% | clf_exit: 0.633 0.287 0.081
Train all parameters

Epoch: 13
Batch: 0 | Loss: 2.008 | Acc: 58.594,83.594,89.062,% | Adaptive Acc: 81.250% | clf_exit: 0.516 0.352 0.133
Batch: 20 | Loss: 1.778 | Acc: 65.625,84.375,90.737,% | Adaptive Acc: 81.696% | clf_exit: 0.569 0.316 0.115
Batch: 40 | Loss: 1.777 | Acc: 65.111,84.432,90.606,% | Adaptive Acc: 81.364% | clf_exit: 0.572 0.317 0.111
Batch: 60 | Loss: 1.791 | Acc: 65.074,84.004,90.100,% | Adaptive Acc: 81.237% | clf_exit: 0.571 0.319 0.110
Batch: 80 | Loss: 1.785 | Acc: 65.557,83.980,89.593,% | Adaptive Acc: 81.211% | clf_exit: 0.577 0.315 0.108
Batch: 100 | Loss: 1.789 | Acc: 65.664,84.058,89.573,% | Adaptive Acc: 81.490% | clf_exit: 0.577 0.314 0.108
Batch: 120 | Loss: 1.790 | Acc: 65.399,84.026,89.585,% | Adaptive Acc: 81.224% | clf_exit: 0.580 0.310 0.109
Batch: 140 | Loss: 1.788 | Acc: 65.503,84.159,89.473,% | Adaptive Acc: 81.178% | clf_exit: 0.581 0.311 0.108
Batch: 160 | Loss: 1.789 | Acc: 65.407,84.128,89.538,% | Adaptive Acc: 81.211% | clf_exit: 0.580 0.312 0.109
Batch: 180 | Loss: 1.794 | Acc: 65.275,84.081,89.524,% | Adaptive Acc: 81.203% | clf_exit: 0.580 0.311 0.110
Batch: 200 | Loss: 1.800 | Acc: 65.124,84.037,89.552,% | Adaptive Acc: 81.095% | clf_exit: 0.578 0.311 0.111
Batch: 220 | Loss: 1.796 | Acc: 65.127,84.050,89.625,% | Adaptive Acc: 81.126% | clf_exit: 0.579 0.310 0.111
Batch: 240 | Loss: 1.799 | Acc: 65.051,83.960,89.584,% | Adaptive Acc: 81.072% | clf_exit: 0.579 0.309 0.112
Batch: 260 | Loss: 1.802 | Acc: 65.041,83.929,89.500,% | Adaptive Acc: 81.076% | clf_exit: 0.580 0.309 0.112
Batch: 280 | Loss: 1.807 | Acc: 64.869,83.855,89.435,% | Adaptive Acc: 80.997% | clf_exit: 0.578 0.310 0.112
Batch: 300 | Loss: 1.807 | Acc: 64.839,83.908,89.457,% | Adaptive Acc: 81.019% | clf_exit: 0.577 0.311 0.112
Batch: 320 | Loss: 1.807 | Acc: 64.922,83.910,89.430,% | Adaptive Acc: 81.048% | clf_exit: 0.578 0.311 0.111
Batch: 340 | Loss: 1.811 | Acc: 64.892,83.887,89.379,% | Adaptive Acc: 81.023% | clf_exit: 0.578 0.311 0.111
Batch: 360 | Loss: 1.809 | Acc: 64.868,83.901,89.413,% | Adaptive Acc: 81.018% | clf_exit: 0.578 0.311 0.111
Batch: 380 | Loss: 1.805 | Acc: 64.883,83.922,89.458,% | Adaptive Acc: 81.063% | clf_exit: 0.577 0.312 0.111
Batch: 0 | Loss: 2.000 | Acc: 58.594,82.812,85.938,% | Adaptive Acc: 73.438% | clf_exit: 0.688 0.242 0.070
Batch: 20 | Loss: 2.117 | Acc: 61.235,81.399,86.012,% | Adaptive Acc: 73.251% | clf_exit: 0.673 0.256 0.071
Batch: 40 | Loss: 2.070 | Acc: 61.566,81.383,86.700,% | Adaptive Acc: 73.800% | clf_exit: 0.675 0.254 0.072
Batch: 60 | Loss: 2.069 | Acc: 61.104,81.224,86.732,% | Adaptive Acc: 73.732% | clf_exit: 0.665 0.266 0.069
Train all parameters

Epoch: 14
Batch: 0 | Loss: 1.597 | Acc: 71.875,89.844,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.570 0.336 0.094
Batch: 20 | Loss: 1.749 | Acc: 65.104,85.603,90.997,% | Adaptive Acc: 81.845% | clf_exit: 0.573 0.317 0.111
Batch: 40 | Loss: 1.752 | Acc: 65.263,85.652,90.777,% | Adaptive Acc: 81.974% | clf_exit: 0.570 0.322 0.108
Batch: 60 | Loss: 1.750 | Acc: 65.766,85.605,90.484,% | Adaptive Acc: 82.172% | clf_exit: 0.577 0.317 0.106
Batch: 80 | Loss: 1.751 | Acc: 65.625,85.475,90.548,% | Adaptive Acc: 81.993% | clf_exit: 0.579 0.317 0.104
Batch: 100 | Loss: 1.744 | Acc: 65.548,85.473,90.633,% | Adaptive Acc: 81.962% | clf_exit: 0.581 0.316 0.104
Batch: 120 | Loss: 1.743 | Acc: 65.580,85.447,90.567,% | Adaptive Acc: 81.999% | clf_exit: 0.580 0.316 0.104
Batch: 140 | Loss: 1.753 | Acc: 65.414,85.223,90.403,% | Adaptive Acc: 81.782% | clf_exit: 0.580 0.315 0.105
Batch: 160 | Loss: 1.761 | Acc: 65.242,85.069,90.280,% | Adaptive Acc: 81.570% | clf_exit: 0.580 0.315 0.106
Batch: 180 | Loss: 1.758 | Acc: 65.280,85.096,90.336,% | Adaptive Acc: 81.552% | clf_exit: 0.581 0.313 0.106
Batch: 200 | Loss: 1.756 | Acc: 65.450,85.016,90.291,% | Adaptive Acc: 81.561% | clf_exit: 0.584 0.311 0.105
Batch: 220 | Loss: 1.762 | Acc: 65.360,84.919,90.271,% | Adaptive Acc: 81.543% | clf_exit: 0.582 0.312 0.106
Batch: 240 | Loss: 1.764 | Acc: 65.362,84.780,90.155,% | Adaptive Acc: 81.506% | clf_exit: 0.580 0.313 0.107
Batch: 260 | Loss: 1.756 | Acc: 65.526,84.827,90.233,% | Adaptive Acc: 81.540% | clf_exit: 0.582 0.313 0.105
Batch: 280 | Loss: 1.757 | Acc: 65.472,84.842,90.144,% | Adaptive Acc: 81.481% | clf_exit: 0.582 0.313 0.105
Batch: 300 | Loss: 1.751 | Acc: 65.545,84.907,90.197,% | Adaptive Acc: 81.478% | clf_exit: 0.583 0.313 0.105
Batch: 320 | Loss: 1.757 | Acc: 65.460,84.818,90.099,% | Adaptive Acc: 81.360% | clf_exit: 0.583 0.313 0.105
Batch: 340 | Loss: 1.761 | Acc: 65.492,84.774,90.016,% | Adaptive Acc: 81.367% | clf_exit: 0.583 0.312 0.105
Batch: 360 | Loss: 1.759 | Acc: 65.558,84.823,90.021,% | Adaptive Acc: 81.434% | clf_exit: 0.583 0.313 0.104
Batch: 380 | Loss: 1.762 | Acc: 65.473,84.814,90.012,% | Adaptive Acc: 81.416% | clf_exit: 0.582 0.313 0.105
Batch: 0 | Loss: 1.905 | Acc: 64.062,79.688,86.719,% | Adaptive Acc: 75.000% | clf_exit: 0.641 0.320 0.039
Batch: 20 | Loss: 2.033 | Acc: 63.951,80.952,84.933,% | Adaptive Acc: 77.344% | clf_exit: 0.622 0.301 0.077
Batch: 40 | Loss: 2.007 | Acc: 64.386,81.098,85.633,% | Adaptive Acc: 76.963% | clf_exit: 0.633 0.288 0.079
Batch: 60 | Loss: 1.986 | Acc: 64.370,81.186,85.861,% | Adaptive Acc: 77.024% | clf_exit: 0.627 0.292 0.080
Train all parameters

Epoch: 15
Batch: 0 | Loss: 2.045 | Acc: 63.281,81.250,83.594,% | Adaptive Acc: 75.781% | clf_exit: 0.484 0.406 0.109
Batch: 20 | Loss: 1.758 | Acc: 64.993,84.784,89.807,% | Adaptive Acc: 81.882% | clf_exit: 0.560 0.332 0.109
Batch: 40 | Loss: 1.703 | Acc: 65.854,85.442,90.816,% | Adaptive Acc: 82.127% | clf_exit: 0.572 0.327 0.101
Batch: 60 | Loss: 1.717 | Acc: 65.779,85.284,90.587,% | Adaptive Acc: 81.698% | clf_exit: 0.582 0.319 0.100
Batch: 80 | Loss: 1.707 | Acc: 65.799,85.224,90.750,% | Adaptive Acc: 81.761% | clf_exit: 0.585 0.318 0.097
Batch: 100 | Loss: 1.710 | Acc: 65.764,85.164,90.811,% | Adaptive Acc: 81.915% | clf_exit: 0.585 0.317 0.098
Batch: 120 | Loss: 1.714 | Acc: 65.728,85.053,90.754,% | Adaptive Acc: 81.631% | clf_exit: 0.587 0.312 0.100
Batch: 140 | Loss: 1.712 | Acc: 65.769,85.095,90.703,% | Adaptive Acc: 81.605% | clf_exit: 0.590 0.310 0.100
Batch: 160 | Loss: 1.722 | Acc: 65.712,85.020,90.703,% | Adaptive Acc: 81.711% | clf_exit: 0.588 0.311 0.101
Batch: 180 | Loss: 1.722 | Acc: 65.716,85.018,90.603,% | Adaptive Acc: 81.634% | clf_exit: 0.589 0.310 0.102
Batch: 200 | Loss: 1.721 | Acc: 65.711,85.148,90.613,% | Adaptive Acc: 81.600% | clf_exit: 0.590 0.307 0.102
Batch: 220 | Loss: 1.720 | Acc: 65.773,85.185,90.618,% | Adaptive Acc: 81.600% | clf_exit: 0.592 0.307 0.101
Batch: 240 | Loss: 1.724 | Acc: 65.625,85.159,90.534,% | Adaptive Acc: 81.470% | clf_exit: 0.590 0.308 0.102
Batch: 260 | Loss: 1.725 | Acc: 65.658,85.114,90.520,% | Adaptive Acc: 81.513% | clf_exit: 0.590 0.308 0.102
Batch: 280 | Loss: 1.724 | Acc: 65.742,85.042,90.492,% | Adaptive Acc: 81.486% | clf_exit: 0.592 0.306 0.102
Batch: 300 | Loss: 1.725 | Acc: 65.757,85.003,90.443,% | Adaptive Acc: 81.442% | clf_exit: 0.594 0.304 0.102
Batch: 320 | Loss: 1.729 | Acc: 65.737,84.962,90.369,% | Adaptive Acc: 81.396% | clf_exit: 0.593 0.306 0.101
Batch: 340 | Loss: 1.726 | Acc: 65.836,84.984,90.359,% | Adaptive Acc: 81.447% | clf_exit: 0.594 0.305 0.101
Batch: 360 | Loss: 1.722 | Acc: 65.883,85.048,90.387,% | Adaptive Acc: 81.514% | clf_exit: 0.595 0.305 0.100
Batch: 380 | Loss: 1.721 | Acc: 65.914,85.054,90.354,% | Adaptive Acc: 81.515% | clf_exit: 0.595 0.304 0.100
Batch: 0 | Loss: 1.804 | Acc: 65.625,85.156,87.500,% | Adaptive Acc: 75.781% | clf_exit: 0.664 0.273 0.062
Batch: 20 | Loss: 1.911 | Acc: 63.244,82.738,87.537,% | Adaptive Acc: 76.711% | clf_exit: 0.657 0.272 0.071
Batch: 40 | Loss: 1.899 | Acc: 64.120,82.489,87.576,% | Adaptive Acc: 76.810% | clf_exit: 0.660 0.269 0.071
Batch: 60 | Loss: 1.896 | Acc: 63.909,82.211,87.564,% | Adaptive Acc: 76.537% | clf_exit: 0.658 0.270 0.072
Train all parameters

Epoch: 16
Batch: 0 | Loss: 1.645 | Acc: 71.094,82.812,93.750,% | Adaptive Acc: 82.812% | clf_exit: 0.680 0.203 0.117
Batch: 20 | Loss: 1.655 | Acc: 66.183,85.045,92.113,% | Adaptive Acc: 82.701% | clf_exit: 0.584 0.317 0.099
Batch: 40 | Loss: 1.650 | Acc: 66.159,85.423,91.387,% | Adaptive Acc: 81.688% | clf_exit: 0.604 0.303 0.093
Batch: 60 | Loss: 1.678 | Acc: 66.150,85.259,90.945,% | Adaptive Acc: 81.493% | clf_exit: 0.603 0.305 0.092
Batch: 80 | Loss: 1.680 | Acc: 66.426,85.407,91.030,% | Adaptive Acc: 81.655% | clf_exit: 0.603 0.304 0.093
Batch: 100 | Loss: 1.674 | Acc: 66.545,85.597,91.128,% | Adaptive Acc: 81.807% | clf_exit: 0.603 0.303 0.094
Batch: 120 | Loss: 1.664 | Acc: 66.781,85.653,91.135,% | Adaptive Acc: 81.954% | clf_exit: 0.605 0.301 0.094
Batch: 140 | Loss: 1.668 | Acc: 66.667,85.611,91.129,% | Adaptive Acc: 81.992% | clf_exit: 0.601 0.303 0.096
Batch: 160 | Loss: 1.663 | Acc: 66.731,85.680,91.105,% | Adaptive Acc: 82.080% | clf_exit: 0.603 0.302 0.095
Batch: 180 | Loss: 1.661 | Acc: 66.704,85.717,91.139,% | Adaptive Acc: 81.984% | clf_exit: 0.605 0.301 0.094
Batch: 200 | Loss: 1.667 | Acc: 66.748,85.681,90.994,% | Adaptive Acc: 81.911% | clf_exit: 0.605 0.300 0.095
Batch: 220 | Loss: 1.673 | Acc: 66.724,85.598,90.947,% | Adaptive Acc: 81.826% | clf_exit: 0.606 0.299 0.095
Batch: 240 | Loss: 1.675 | Acc: 66.653,85.630,90.897,% | Adaptive Acc: 81.879% | clf_exit: 0.605 0.300 0.095
Batch: 260 | Loss: 1.676 | Acc: 66.592,85.611,90.867,% | Adaptive Acc: 81.909% | clf_exit: 0.604 0.301 0.094
Batch: 280 | Loss: 1.675 | Acc: 66.623,85.565,90.878,% | Adaptive Acc: 81.851% | clf_exit: 0.605 0.300 0.095
Batch: 300 | Loss: 1.673 | Acc: 66.635,85.564,90.936,% | Adaptive Acc: 81.896% | clf_exit: 0.606 0.300 0.094
Batch: 320 | Loss: 1.678 | Acc: 66.489,85.521,90.917,% | Adaptive Acc: 81.822% | clf_exit: 0.606 0.300 0.094
Batch: 340 | Loss: 1.676 | Acc: 66.567,85.539,90.918,% | Adaptive Acc: 81.846% | clf_exit: 0.605 0.301 0.094
Batch: 360 | Loss: 1.676 | Acc: 66.608,85.550,90.911,% | Adaptive Acc: 81.891% | clf_exit: 0.605 0.301 0.094
Batch: 380 | Loss: 1.683 | Acc: 66.548,85.454,90.832,% | Adaptive Acc: 81.795% | clf_exit: 0.605 0.301 0.094
Batch: 0 | Loss: 1.767 | Acc: 58.594,86.719,88.281,% | Adaptive Acc: 69.531% | clf_exit: 0.742 0.172 0.086
Batch: 20 | Loss: 2.097 | Acc: 60.975,81.176,86.347,% | Adaptive Acc: 73.400% | clf_exit: 0.687 0.237 0.076
Batch: 40 | Loss: 2.054 | Acc: 62.062,81.726,86.795,% | Adaptive Acc: 73.876% | clf_exit: 0.693 0.230 0.077
Batch: 60 | Loss: 2.072 | Acc: 61.475,81.288,86.616,% | Adaptive Acc: 73.681% | clf_exit: 0.689 0.233 0.079
Train all parameters

Epoch: 17
Batch: 0 | Loss: 1.662 | Acc: 62.500,82.812,93.750,% | Adaptive Acc: 80.469% | clf_exit: 0.602 0.289 0.109
Batch: 20 | Loss: 1.598 | Acc: 67.225,86.793,92.634,% | Adaptive Acc: 83.147% | clf_exit: 0.608 0.298 0.094
Batch: 40 | Loss: 1.625 | Acc: 67.054,86.757,92.111,% | Adaptive Acc: 82.984% | clf_exit: 0.608 0.299 0.093
Batch: 60 | Loss: 1.643 | Acc: 66.931,86.347,91.598,% | Adaptive Acc: 82.877% | clf_exit: 0.603 0.303 0.094
Batch: 80 | Loss: 1.629 | Acc: 66.995,86.372,91.792,% | Adaptive Acc: 82.745% | clf_exit: 0.609 0.299 0.092
Batch: 100 | Loss: 1.630 | Acc: 66.631,86.309,91.739,% | Adaptive Acc: 82.441% | clf_exit: 0.608 0.300 0.092
Batch: 120 | Loss: 1.631 | Acc: 66.658,86.351,91.736,% | Adaptive Acc: 82.322% | clf_exit: 0.609 0.302 0.090
Batch: 140 | Loss: 1.635 | Acc: 66.700,86.375,91.656,% | Adaptive Acc: 82.258% | clf_exit: 0.611 0.300 0.089
Batch: 160 | Loss: 1.642 | Acc: 66.678,86.311,91.600,% | Adaptive Acc: 82.162% | clf_exit: 0.611 0.300 0.089
Batch: 180 | Loss: 1.642 | Acc: 66.704,86.304,91.635,% | Adaptive Acc: 82.243% | clf_exit: 0.611 0.300 0.089
Batch: 200 | Loss: 1.640 | Acc: 66.741,86.361,91.589,% | Adaptive Acc: 82.194% | clf_exit: 0.613 0.299 0.089
Batch: 220 | Loss: 1.644 | Acc: 66.643,86.287,91.544,% | Adaptive Acc: 82.180% | clf_exit: 0.612 0.299 0.089
Batch: 240 | Loss: 1.645 | Acc: 66.653,86.216,91.478,% | Adaptive Acc: 82.177% | clf_exit: 0.611 0.299 0.090
Batch: 260 | Loss: 1.645 | Acc: 66.649,86.279,91.439,% | Adaptive Acc: 82.193% | clf_exit: 0.611 0.299 0.090
Batch: 280 | Loss: 1.638 | Acc: 66.879,86.399,91.495,% | Adaptive Acc: 82.323% | clf_exit: 0.611 0.299 0.089
Batch: 300 | Loss: 1.639 | Acc: 66.944,86.392,91.424,% | Adaptive Acc: 82.244% | clf_exit: 0.612 0.299 0.089
Batch: 320 | Loss: 1.643 | Acc: 66.854,86.390,91.377,% | Adaptive Acc: 82.245% | clf_exit: 0.611 0.299 0.090
Batch: 340 | Loss: 1.645 | Acc: 66.846,86.377,91.365,% | Adaptive Acc: 82.242% | clf_exit: 0.610 0.300 0.090
Batch: 360 | Loss: 1.642 | Acc: 66.921,86.468,91.385,% | Adaptive Acc: 82.295% | clf_exit: 0.611 0.300 0.089
Batch: 380 | Loss: 1.640 | Acc: 66.976,86.481,91.357,% | Adaptive Acc: 82.324% | clf_exit: 0.612 0.300 0.089
Batch: 0 | Loss: 1.630 | Acc: 64.844,85.156,92.969,% | Adaptive Acc: 80.469% | clf_exit: 0.617 0.305 0.078
Batch: 20 | Loss: 1.997 | Acc: 61.272,82.254,88.058,% | Adaptive Acc: 76.637% | clf_exit: 0.601 0.313 0.086
Batch: 40 | Loss: 1.972 | Acc: 61.909,83.117,88.167,% | Adaptive Acc: 77.382% | clf_exit: 0.606 0.311 0.083
Batch: 60 | Loss: 1.968 | Acc: 62.026,83.017,87.974,% | Adaptive Acc: 77.331% | clf_exit: 0.604 0.316 0.080
Train all parameters

Epoch: 18
Batch: 0 | Loss: 1.530 | Acc: 68.750,88.281,89.844,% | Adaptive Acc: 86.719% | clf_exit: 0.586 0.273 0.141
Batch: 20 | Loss: 1.601 | Acc: 68.192,86.756,91.481,% | Adaptive Acc: 82.812% | clf_exit: 0.605 0.307 0.088
Batch: 40 | Loss: 1.574 | Acc: 67.340,86.852,92.321,% | Adaptive Acc: 82.622% | clf_exit: 0.611 0.305 0.084
Batch: 60 | Loss: 1.573 | Acc: 67.303,87.052,92.444,% | Adaptive Acc: 82.697% | clf_exit: 0.614 0.302 0.083
Batch: 80 | Loss: 1.583 | Acc: 67.332,86.825,92.294,% | Adaptive Acc: 82.610% | clf_exit: 0.615 0.301 0.084
Batch: 100 | Loss: 1.587 | Acc: 67.505,86.935,92.195,% | Adaptive Acc: 82.766% | clf_exit: 0.619 0.298 0.084
Batch: 120 | Loss: 1.596 | Acc: 67.517,86.757,92.052,% | Adaptive Acc: 82.606% | clf_exit: 0.618 0.297 0.085
Batch: 140 | Loss: 1.608 | Acc: 67.298,86.630,91.944,% | Adaptive Acc: 82.375% | clf_exit: 0.617 0.297 0.086
Batch: 160 | Loss: 1.609 | Acc: 67.294,86.651,91.969,% | Adaptive Acc: 82.575% | clf_exit: 0.616 0.297 0.087
Batch: 180 | Loss: 1.615 | Acc: 67.032,86.481,91.929,% | Adaptive Acc: 82.579% | clf_exit: 0.613 0.298 0.089
Batch: 200 | Loss: 1.623 | Acc: 66.978,86.326,91.853,% | Adaptive Acc: 82.521% | clf_exit: 0.612 0.299 0.089
Batch: 220 | Loss: 1.619 | Acc: 67.099,86.369,91.834,% | Adaptive Acc: 82.569% | clf_exit: 0.614 0.297 0.089
Batch: 240 | Loss: 1.615 | Acc: 67.243,86.424,91.802,% | Adaptive Acc: 82.589% | clf_exit: 0.615 0.297 0.088
Batch: 260 | Loss: 1.617 | Acc: 67.176,86.437,91.804,% | Adaptive Acc: 82.507% | clf_exit: 0.615 0.298 0.088
Batch: 280 | Loss: 1.616 | Acc: 67.204,86.388,91.754,% | Adaptive Acc: 82.498% | clf_exit: 0.615 0.297 0.087
Batch: 300 | Loss: 1.619 | Acc: 67.115,86.303,91.689,% | Adaptive Acc: 82.374% | clf_exit: 0.616 0.297 0.088
Batch: 320 | Loss: 1.616 | Acc: 67.068,86.327,91.703,% | Adaptive Acc: 82.365% | clf_exit: 0.616 0.296 0.088
Batch: 340 | Loss: 1.615 | Acc: 67.171,86.334,91.711,% | Adaptive Acc: 82.363% | clf_exit: 0.617 0.296 0.087
Batch: 360 | Loss: 1.615 | Acc: 67.166,86.344,91.655,% | Adaptive Acc: 82.367% | clf_exit: 0.617 0.296 0.087
Batch: 380 | Loss: 1.610 | Acc: 67.185,86.458,91.718,% | Adaptive Acc: 82.419% | clf_exit: 0.617 0.296 0.087
Batch: 0 | Loss: 1.867 | Acc: 64.062,81.250,85.156,% | Adaptive Acc: 74.219% | clf_exit: 0.680 0.289 0.031
Batch: 20 | Loss: 1.875 | Acc: 65.997,83.259,87.054,% | Adaptive Acc: 78.051% | clf_exit: 0.676 0.265 0.059
Batch: 40 | Loss: 1.866 | Acc: 66.330,82.946,87.309,% | Adaptive Acc: 77.725% | clf_exit: 0.687 0.258 0.055
Batch: 60 | Loss: 1.868 | Acc: 66.355,82.761,87.359,% | Adaptive Acc: 77.690% | clf_exit: 0.685 0.259 0.056
Train all parameters

Epoch: 19
Batch: 0 | Loss: 1.565 | Acc: 66.406,85.156,90.625,% | Adaptive Acc: 81.250% | clf_exit: 0.648 0.281 0.070
Batch: 20 | Loss: 1.560 | Acc: 66.778,86.384,92.150,% | Adaptive Acc: 81.920% | clf_exit: 0.640 0.284 0.076
Batch: 40 | Loss: 1.564 | Acc: 67.207,86.833,92.188,% | Adaptive Acc: 82.317% | clf_exit: 0.628 0.296 0.076
Batch: 60 | Loss: 1.563 | Acc: 67.713,87.167,92.277,% | Adaptive Acc: 82.556% | clf_exit: 0.625 0.297 0.079
Batch: 80 | Loss: 1.547 | Acc: 67.978,87.269,92.554,% | Adaptive Acc: 82.957% | clf_exit: 0.627 0.294 0.079
Batch: 100 | Loss: 1.571 | Acc: 67.706,86.951,92.203,% | Adaptive Acc: 82.735% | clf_exit: 0.624 0.295 0.081
Batch: 120 | Loss: 1.569 | Acc: 67.885,87.022,92.097,% | Adaptive Acc: 82.832% | clf_exit: 0.626 0.293 0.081
Batch: 140 | Loss: 1.568 | Acc: 67.880,87.007,92.027,% | Adaptive Acc: 82.663% | clf_exit: 0.628 0.290 0.082
Batch: 160 | Loss: 1.573 | Acc: 67.804,86.986,91.940,% | Adaptive Acc: 82.720% | clf_exit: 0.627 0.291 0.082
Batch: 180 | Loss: 1.573 | Acc: 67.818,87.012,92.041,% | Adaptive Acc: 82.648% | clf_exit: 0.627 0.291 0.081
Batch: 200 | Loss: 1.572 | Acc: 67.825,87.002,92.083,% | Adaptive Acc: 82.599% | clf_exit: 0.627 0.292 0.081
Batch: 220 | Loss: 1.570 | Acc: 67.870,87.044,92.060,% | Adaptive Acc: 82.622% | clf_exit: 0.627 0.292 0.081
Batch: 240 | Loss: 1.568 | Acc: 68.014,87.069,92.077,% | Adaptive Acc: 82.731% | clf_exit: 0.627 0.292 0.081
Batch: 260 | Loss: 1.574 | Acc: 67.846,86.988,92.038,% | Adaptive Acc: 82.663% | clf_exit: 0.626 0.293 0.081
Batch: 280 | Loss: 1.576 | Acc: 67.805,86.958,92.012,% | Adaptive Acc: 82.648% | clf_exit: 0.626 0.292 0.082
Batch: 300 | Loss: 1.574 | Acc: 67.865,87.020,92.032,% | Adaptive Acc: 82.678% | clf_exit: 0.627 0.292 0.082
Batch: 320 | Loss: 1.578 | Acc: 67.813,86.979,92.003,% | Adaptive Acc: 82.593% | clf_exit: 0.627 0.292 0.082
Batch: 340 | Loss: 1.572 | Acc: 67.944,87.090,92.013,% | Adaptive Acc: 82.698% | clf_exit: 0.628 0.291 0.081
Batch: 360 | Loss: 1.574 | Acc: 67.882,87.115,91.969,% | Adaptive Acc: 82.665% | clf_exit: 0.628 0.291 0.081
Batch: 380 | Loss: 1.576 | Acc: 67.860,87.018,91.948,% | Adaptive Acc: 82.630% | clf_exit: 0.629 0.290 0.081
Batch: 0 | Loss: 1.686 | Acc: 61.719,85.156,89.844,% | Adaptive Acc: 78.125% | clf_exit: 0.688 0.281 0.031
Batch: 20 | Loss: 1.875 | Acc: 65.513,83.036,87.686,% | Adaptive Acc: 77.344% | clf_exit: 0.685 0.249 0.066
Batch: 40 | Loss: 1.834 | Acc: 66.254,83.784,87.767,% | Adaptive Acc: 77.801% | clf_exit: 0.691 0.243 0.066
Batch: 60 | Loss: 1.831 | Acc: 65.932,83.632,87.513,% | Adaptive Acc: 77.446% | clf_exit: 0.686 0.249 0.064
Train all parameters

Epoch: 20
Batch: 0 | Loss: 1.519 | Acc: 74.219,88.281,91.406,% | Adaptive Acc: 83.594% | clf_exit: 0.695 0.242 0.062
Batch: 20 | Loss: 1.554 | Acc: 68.043,87.537,92.225,% | Adaptive Acc: 82.515% | clf_exit: 0.626 0.292 0.082
Batch: 40 | Loss: 1.543 | Acc: 68.236,87.138,92.340,% | Adaptive Acc: 82.832% | clf_exit: 0.632 0.289 0.079
Batch: 60 | Loss: 1.521 | Acc: 68.443,87.551,92.661,% | Adaptive Acc: 83.158% | clf_exit: 0.632 0.289 0.079
Batch: 80 | Loss: 1.517 | Acc: 68.605,87.616,92.602,% | Adaptive Acc: 83.285% | clf_exit: 0.632 0.290 0.078
Batch: 100 | Loss: 1.516 | Acc: 68.595,87.631,92.667,% | Adaptive Acc: 83.323% | clf_exit: 0.633 0.288 0.078
Batch: 120 | Loss: 1.526 | Acc: 68.614,87.403,92.652,% | Adaptive Acc: 83.400% | clf_exit: 0.629 0.291 0.079
Batch: 140 | Loss: 1.527 | Acc: 68.551,87.361,92.692,% | Adaptive Acc: 83.428% | clf_exit: 0.629 0.291 0.079
Batch: 160 | Loss: 1.521 | Acc: 68.663,87.418,92.716,% | Adaptive Acc: 83.443% | clf_exit: 0.630 0.291 0.079
Batch: 180 | Loss: 1.523 | Acc: 68.646,87.396,92.723,% | Adaptive Acc: 83.335% | clf_exit: 0.632 0.289 0.079
Batch: 200 | Loss: 1.520 | Acc: 68.692,87.442,92.666,% | Adaptive Acc: 83.298% | clf_exit: 0.634 0.288 0.078
Batch: 220 | Loss: 1.525 | Acc: 68.665,87.443,92.608,% | Adaptive Acc: 83.375% | clf_exit: 0.633 0.288 0.079
Batch: 240 | Loss: 1.526 | Acc: 68.630,87.438,92.538,% | Adaptive Acc: 83.305% | clf_exit: 0.634 0.288 0.078
Batch: 260 | Loss: 1.534 | Acc: 68.490,87.365,92.442,% | Adaptive Acc: 83.187% | clf_exit: 0.634 0.288 0.079
Batch: 280 | Loss: 1.543 | Acc: 68.461,87.300,92.332,% | Adaptive Acc: 83.127% | clf_exit: 0.633 0.288 0.080
Batch: 300 | Loss: 1.546 | Acc: 68.366,87.277,92.335,% | Adaptive Acc: 83.095% | clf_exit: 0.633 0.286 0.081
Batch: 320 | Loss: 1.546 | Acc: 68.295,87.288,92.355,% | Adaptive Acc: 83.143% | clf_exit: 0.632 0.287 0.081
Batch: 340 | Loss: 1.545 | Acc: 68.344,87.321,92.339,% | Adaptive Acc: 83.117% | clf_exit: 0.633 0.286 0.080
Batch: 360 | Loss: 1.547 | Acc: 68.311,87.286,92.320,% | Adaptive Acc: 83.087% | clf_exit: 0.633 0.287 0.081
Batch: 380 | Loss: 1.549 | Acc: 68.248,87.250,92.259,% | Adaptive Acc: 83.048% | clf_exit: 0.633 0.286 0.081
Batch: 0 | Loss: 1.596 | Acc: 64.844,85.938,92.188,% | Adaptive Acc: 77.344% | clf_exit: 0.773 0.172 0.055
Batch: 20 | Loss: 1.891 | Acc: 64.844,82.664,88.132,% | Adaptive Acc: 75.670% | clf_exit: 0.713 0.216 0.071
Batch: 40 | Loss: 1.849 | Acc: 65.282,82.889,88.053,% | Adaptive Acc: 76.429% | clf_exit: 0.708 0.224 0.068
Batch: 60 | Loss: 1.838 | Acc: 64.869,83.222,88.115,% | Adaptive Acc: 76.575% | clf_exit: 0.705 0.228 0.067
Train all parameters

Epoch: 21
Batch: 0 | Loss: 1.465 | Acc: 72.656,89.844,92.188,% | Adaptive Acc: 87.500% | clf_exit: 0.648 0.273 0.078
Batch: 20 | Loss: 1.505 | Acc: 69.048,87.612,92.932,% | Adaptive Acc: 83.668% | clf_exit: 0.642 0.282 0.076
Batch: 40 | Loss: 1.502 | Acc: 68.807,88.072,93.274,% | Adaptive Acc: 83.575% | clf_exit: 0.640 0.286 0.074
Batch: 60 | Loss: 1.492 | Acc: 68.993,88.115,93.302,% | Adaptive Acc: 83.709% | clf_exit: 0.642 0.284 0.074
Batch: 80 | Loss: 1.495 | Acc: 68.837,88.040,93.142,% | Adaptive Acc: 83.497% | clf_exit: 0.640 0.287 0.073
Batch: 100 | Loss: 1.503 | Acc: 68.603,87.902,93.062,% | Adaptive Acc: 83.571% | clf_exit: 0.635 0.290 0.075
Batch: 120 | Loss: 1.498 | Acc: 68.563,87.894,93.072,% | Adaptive Acc: 83.478% | clf_exit: 0.636 0.288 0.075
Batch: 140 | Loss: 1.497 | Acc: 68.562,87.899,92.985,% | Adaptive Acc: 83.588% | clf_exit: 0.635 0.289 0.076
Batch: 160 | Loss: 1.494 | Acc: 68.672,87.932,92.920,% | Adaptive Acc: 83.521% | clf_exit: 0.637 0.287 0.076
Batch: 180 | Loss: 1.497 | Acc: 68.659,87.936,92.904,% | Adaptive Acc: 83.542% | clf_exit: 0.637 0.286 0.077
Batch: 200 | Loss: 1.510 | Acc: 68.408,87.877,92.852,% | Adaptive Acc: 83.462% | clf_exit: 0.634 0.288 0.078
Batch: 220 | Loss: 1.513 | Acc: 68.361,87.892,92.799,% | Adaptive Acc: 83.502% | clf_exit: 0.634 0.288 0.078
Batch: 240 | Loss: 1.513 | Acc: 68.387,87.934,92.800,% | Adaptive Acc: 83.409% | clf_exit: 0.634 0.288 0.078
Batch: 260 | Loss: 1.512 | Acc: 68.385,87.883,92.771,% | Adaptive Acc: 83.399% | clf_exit: 0.635 0.287 0.078
Batch: 280 | Loss: 1.516 | Acc: 68.461,87.817,92.705,% | Adaptive Acc: 83.407% | clf_exit: 0.636 0.287 0.078
Batch: 300 | Loss: 1.518 | Acc: 68.394,87.752,92.720,% | Adaptive Acc: 83.347% | clf_exit: 0.635 0.287 0.078
Batch: 320 | Loss: 1.519 | Acc: 68.397,87.775,92.713,% | Adaptive Acc: 83.355% | clf_exit: 0.635 0.287 0.078
Batch: 340 | Loss: 1.518 | Acc: 68.487,87.789,92.675,% | Adaptive Acc: 83.397% | clf_exit: 0.636 0.286 0.078
Batch: 360 | Loss: 1.519 | Acc: 68.508,87.764,92.653,% | Adaptive Acc: 83.367% | clf_exit: 0.636 0.286 0.078
Batch: 380 | Loss: 1.522 | Acc: 68.547,87.728,92.587,% | Adaptive Acc: 83.294% | clf_exit: 0.636 0.286 0.078
Batch: 0 | Loss: 1.861 | Acc: 59.375,82.812,85.938,% | Adaptive Acc: 72.656% | clf_exit: 0.719 0.227 0.055
Batch: 20 | Loss: 1.916 | Acc: 64.360,82.850,87.277,% | Adaptive Acc: 76.004% | clf_exit: 0.699 0.243 0.058
Batch: 40 | Loss: 1.871 | Acc: 65.930,82.984,87.481,% | Adaptive Acc: 76.715% | clf_exit: 0.705 0.238 0.057
Batch: 60 | Loss: 1.891 | Acc: 65.638,82.646,87.205,% | Adaptive Acc: 76.678% | clf_exit: 0.699 0.241 0.059
Train all parameters

Epoch: 22
Batch: 0 | Loss: 1.613 | Acc: 70.312,86.719,91.406,% | Adaptive Acc: 82.812% | clf_exit: 0.602 0.328 0.070
Batch: 20 | Loss: 1.475 | Acc: 69.122,87.760,94.085,% | Adaptive Acc: 84.859% | clf_exit: 0.625 0.295 0.080
Batch: 40 | Loss: 1.497 | Acc: 68.559,87.843,93.655,% | Adaptive Acc: 83.880% | clf_exit: 0.631 0.289 0.080
Batch: 60 | Loss: 1.497 | Acc: 68.865,87.884,93.391,% | Adaptive Acc: 83.952% | clf_exit: 0.632 0.290 0.078
Batch: 80 | Loss: 1.471 | Acc: 69.203,88.243,93.625,% | Adaptive Acc: 84.298% | clf_exit: 0.636 0.288 0.076
Batch: 100 | Loss: 1.479 | Acc: 69.005,88.165,93.394,% | Adaptive Acc: 84.104% | clf_exit: 0.636 0.289 0.075
Batch: 120 | Loss: 1.476 | Acc: 68.957,88.320,93.434,% | Adaptive Acc: 84.039% | clf_exit: 0.638 0.287 0.075
Batch: 140 | Loss: 1.482 | Acc: 68.850,88.198,93.368,% | Adaptive Acc: 83.821% | clf_exit: 0.639 0.284 0.076
Batch: 160 | Loss: 1.481 | Acc: 68.881,88.160,93.236,% | Adaptive Acc: 83.895% | clf_exit: 0.638 0.287 0.075
Batch: 180 | Loss: 1.488 | Acc: 68.901,88.001,93.193,% | Adaptive Acc: 83.741% | clf_exit: 0.640 0.285 0.075
Batch: 200 | Loss: 1.487 | Acc: 68.894,88.040,93.155,% | Adaptive Acc: 83.800% | clf_exit: 0.639 0.286 0.075
Batch: 220 | Loss: 1.489 | Acc: 68.860,87.984,93.078,% | Adaptive Acc: 83.746% | clf_exit: 0.639 0.285 0.075
Batch: 240 | Loss: 1.491 | Acc: 68.860,87.970,93.043,% | Adaptive Acc: 83.662% | clf_exit: 0.640 0.284 0.075
Batch: 260 | Loss: 1.491 | Acc: 68.930,87.946,92.996,% | Adaptive Acc: 83.657% | clf_exit: 0.640 0.285 0.076
Batch: 280 | Loss: 1.492 | Acc: 68.908,87.917,93.019,% | Adaptive Acc: 83.574% | clf_exit: 0.641 0.284 0.075
Batch: 300 | Loss: 1.493 | Acc: 68.838,87.913,92.961,% | Adaptive Acc: 83.635% | clf_exit: 0.640 0.284 0.075
Batch: 320 | Loss: 1.493 | Acc: 68.928,87.885,92.957,% | Adaptive Acc: 83.650% | clf_exit: 0.641 0.284 0.075
Batch: 340 | Loss: 1.494 | Acc: 68.915,87.915,92.928,% | Adaptive Acc: 83.672% | clf_exit: 0.641 0.284 0.075
Batch: 360 | Loss: 1.497 | Acc: 68.910,87.894,92.889,% | Adaptive Acc: 83.676% | clf_exit: 0.641 0.284 0.076
Batch: 380 | Loss: 1.500 | Acc: 68.863,87.838,92.825,% | Adaptive Acc: 83.592% | clf_exit: 0.641 0.283 0.076
Batch: 0 | Loss: 2.074 | Acc: 63.281,76.562,89.062,% | Adaptive Acc: 70.312% | clf_exit: 0.703 0.234 0.062
Batch: 20 | Loss: 1.861 | Acc: 65.030,83.259,88.616,% | Adaptive Acc: 76.674% | clf_exit: 0.708 0.241 0.051
Batch: 40 | Loss: 1.812 | Acc: 65.949,84.146,88.967,% | Adaptive Acc: 76.353% | clf_exit: 0.726 0.229 0.045
Batch: 60 | Loss: 1.815 | Acc: 65.830,83.965,88.986,% | Adaptive Acc: 76.358% | clf_exit: 0.724 0.231 0.045
Train all parameters

Epoch: 23
Batch: 0 | Loss: 1.348 | Acc: 70.312,87.500,96.094,% | Adaptive Acc: 86.719% | clf_exit: 0.594 0.328 0.078
Batch: 20 | Loss: 1.493 | Acc: 68.713,88.430,93.862,% | Adaptive Acc: 83.482% | clf_exit: 0.644 0.288 0.068
Batch: 40 | Loss: 1.447 | Acc: 69.169,88.891,94.112,% | Adaptive Acc: 84.356% | clf_exit: 0.647 0.284 0.069
Batch: 60 | Loss: 1.447 | Acc: 69.339,88.461,93.904,% | Adaptive Acc: 83.952% | clf_exit: 0.645 0.281 0.074
Batch: 80 | Loss: 1.458 | Acc: 69.010,88.252,93.711,% | Adaptive Acc: 83.719% | clf_exit: 0.645 0.281 0.074
Batch: 100 | Loss: 1.445 | Acc: 69.361,88.482,93.781,% | Adaptive Acc: 83.911% | clf_exit: 0.648 0.279 0.073
Batch: 120 | Loss: 1.452 | Acc: 69.034,88.494,93.660,% | Adaptive Acc: 83.600% | clf_exit: 0.651 0.277 0.072
Batch: 140 | Loss: 1.462 | Acc: 68.844,88.276,93.578,% | Adaptive Acc: 83.494% | clf_exit: 0.650 0.277 0.073
Batch: 160 | Loss: 1.461 | Acc: 69.027,88.291,93.488,% | Adaptive Acc: 83.526% | clf_exit: 0.650 0.278 0.073
Batch: 180 | Loss: 1.458 | Acc: 69.251,88.281,93.444,% | Adaptive Acc: 83.667% | clf_exit: 0.650 0.277 0.072
Batch: 200 | Loss: 1.468 | Acc: 69.057,88.106,93.322,% | Adaptive Acc: 83.539% | clf_exit: 0.649 0.279 0.072
Batch: 220 | Loss: 1.464 | Acc: 69.135,88.182,93.351,% | Adaptive Acc: 83.611% | clf_exit: 0.648 0.280 0.072
Batch: 240 | Loss: 1.462 | Acc: 69.165,88.272,93.322,% | Adaptive Acc: 83.646% | clf_exit: 0.648 0.280 0.071
Batch: 260 | Loss: 1.466 | Acc: 69.151,88.221,93.262,% | Adaptive Acc: 83.636% | clf_exit: 0.647 0.280 0.072
Batch: 280 | Loss: 1.466 | Acc: 69.150,88.217,93.250,% | Adaptive Acc: 83.613% | clf_exit: 0.648 0.279 0.072
Batch: 300 | Loss: 1.471 | Acc: 69.157,88.154,93.189,% | Adaptive Acc: 83.565% | clf_exit: 0.648 0.280 0.072
Batch: 320 | Loss: 1.469 | Acc: 69.152,88.179,93.212,% | Adaptive Acc: 83.562% | clf_exit: 0.648 0.280 0.072
Batch: 340 | Loss: 1.470 | Acc: 69.181,88.226,93.207,% | Adaptive Acc: 83.626% | clf_exit: 0.647 0.280 0.073
Batch: 360 | Loss: 1.474 | Acc: 69.135,88.180,93.131,% | Adaptive Acc: 83.572% | clf_exit: 0.647 0.280 0.073
Batch: 380 | Loss: 1.475 | Acc: 69.084,88.220,93.137,% | Adaptive Acc: 83.633% | clf_exit: 0.647 0.280 0.073
Batch: 0 | Loss: 1.701 | Acc: 69.531,82.812,87.500,% | Adaptive Acc: 79.688% | clf_exit: 0.789 0.117 0.094
Batch: 20 | Loss: 1.852 | Acc: 64.397,83.222,88.690,% | Adaptive Acc: 76.674% | clf_exit: 0.718 0.219 0.063
Batch: 40 | Loss: 1.839 | Acc: 64.844,83.708,88.777,% | Adaptive Acc: 76.258% | clf_exit: 0.726 0.216 0.058
Batch: 60 | Loss: 1.836 | Acc: 64.895,83.683,88.704,% | Adaptive Acc: 76.037% | clf_exit: 0.724 0.218 0.057
Train all parameters

Epoch: 24
Batch: 0 | Loss: 1.427 | Acc: 68.750,88.281,94.531,% | Adaptive Acc: 84.375% | clf_exit: 0.641 0.258 0.102
Batch: 20 | Loss: 1.424 | Acc: 68.490,89.062,94.531,% | Adaptive Acc: 84.152% | clf_exit: 0.647 0.282 0.070
Batch: 40 | Loss: 1.457 | Acc: 68.483,88.891,93.941,% | Adaptive Acc: 84.089% | clf_exit: 0.638 0.289 0.073
Batch: 60 | Loss: 1.447 | Acc: 69.147,88.717,93.865,% | Adaptive Acc: 84.106% | clf_exit: 0.644 0.283 0.073
Batch: 80 | Loss: 1.437 | Acc: 69.570,88.764,93.856,% | Adaptive Acc: 84.066% | clf_exit: 0.651 0.277 0.071
Batch: 100 | Loss: 1.440 | Acc: 69.191,88.699,93.866,% | Adaptive Acc: 83.950% | clf_exit: 0.649 0.281 0.071
Batch: 120 | Loss: 1.437 | Acc: 69.325,88.623,93.653,% | Adaptive Acc: 84.013% | clf_exit: 0.648 0.281 0.071
Batch: 140 | Loss: 1.437 | Acc: 69.315,88.536,93.678,% | Adaptive Acc: 83.832% | clf_exit: 0.651 0.277 0.071
Batch: 160 | Loss: 1.441 | Acc: 69.332,88.500,93.634,% | Adaptive Acc: 83.909% | clf_exit: 0.651 0.278 0.071
Batch: 180 | Loss: 1.436 | Acc: 69.406,88.570,93.646,% | Adaptive Acc: 83.965% | clf_exit: 0.653 0.277 0.070
Batch: 200 | Loss: 1.438 | Acc: 69.438,88.491,93.633,% | Adaptive Acc: 83.947% | clf_exit: 0.654 0.275 0.071
Batch: 220 | Loss: 1.436 | Acc: 69.482,88.532,93.633,% | Adaptive Acc: 84.032% | clf_exit: 0.653 0.276 0.071
Batch: 240 | Loss: 1.435 | Acc: 69.453,88.524,93.630,% | Adaptive Acc: 84.022% | clf_exit: 0.653 0.277 0.070
Batch: 260 | Loss: 1.438 | Acc: 69.367,88.572,93.597,% | Adaptive Acc: 83.950% | clf_exit: 0.653 0.276 0.070
Batch: 280 | Loss: 1.440 | Acc: 69.342,88.506,93.555,% | Adaptive Acc: 83.850% | clf_exit: 0.654 0.276 0.070
Batch: 300 | Loss: 1.443 | Acc: 69.417,88.515,93.519,% | Adaptive Acc: 83.858% | clf_exit: 0.654 0.276 0.070
Batch: 320 | Loss: 1.442 | Acc: 69.531,88.529,93.490,% | Adaptive Acc: 83.847% | clf_exit: 0.656 0.274 0.070
Batch: 340 | Loss: 1.445 | Acc: 69.508,88.494,93.438,% | Adaptive Acc: 83.818% | clf_exit: 0.655 0.274 0.071
Batch: 360 | Loss: 1.447 | Acc: 69.512,88.476,93.376,% | Adaptive Acc: 83.776% | clf_exit: 0.655 0.274 0.071
Batch: 380 | Loss: 1.447 | Acc: 69.550,88.499,93.375,% | Adaptive Acc: 83.789% | clf_exit: 0.654 0.275 0.071
Batch: 0 | Loss: 1.948 | Acc: 70.312,82.031,84.375,% | Adaptive Acc: 74.219% | clf_exit: 0.734 0.211 0.055
Batch: 20 | Loss: 1.896 | Acc: 66.853,82.738,86.756,% | Adaptive Acc: 75.521% | clf_exit: 0.734 0.222 0.044
Batch: 40 | Loss: 1.859 | Acc: 67.321,82.774,86.681,% | Adaptive Acc: 75.572% | clf_exit: 0.749 0.207 0.044
Batch: 60 | Loss: 1.831 | Acc: 67.290,83.299,87.180,% | Adaptive Acc: 76.165% | clf_exit: 0.739 0.216 0.046
Train all parameters

Epoch: 25
Batch: 0 | Loss: 1.582 | Acc: 69.531,92.188,95.312,% | Adaptive Acc: 85.938% | clf_exit: 0.617 0.328 0.055
Batch: 20 | Loss: 1.371 | Acc: 70.908,90.030,94.717,% | Adaptive Acc: 84.561% | clf_exit: 0.654 0.285 0.061
Batch: 40 | Loss: 1.387 | Acc: 70.522,89.748,94.341,% | Adaptive Acc: 84.718% | clf_exit: 0.651 0.285 0.064
Batch: 60 | Loss: 1.406 | Acc: 69.992,89.037,94.301,% | Adaptive Acc: 84.080% | clf_exit: 0.654 0.281 0.065
Batch: 80 | Loss: 1.413 | Acc: 69.647,88.870,94.136,% | Adaptive Acc: 83.941% | clf_exit: 0.656 0.280 0.065
Batch: 100 | Loss: 1.405 | Acc: 69.887,89.024,94.183,% | Adaptive Acc: 84.274% | clf_exit: 0.656 0.279 0.065
Batch: 120 | Loss: 1.422 | Acc: 69.596,88.837,94.047,% | Adaptive Acc: 84.020% | clf_exit: 0.655 0.278 0.066
Batch: 140 | Loss: 1.419 | Acc: 69.747,88.896,94.099,% | Adaptive Acc: 84.159% | clf_exit: 0.657 0.278 0.066
Batch: 160 | Loss: 1.423 | Acc: 69.623,88.878,94.027,% | Adaptive Acc: 84.103% | clf_exit: 0.657 0.277 0.066
Batch: 180 | Loss: 1.421 | Acc: 69.734,88.898,93.910,% | Adaptive Acc: 84.133% | clf_exit: 0.657 0.277 0.065
Batch: 200 | Loss: 1.426 | Acc: 69.609,88.752,93.878,% | Adaptive Acc: 84.021% | clf_exit: 0.658 0.276 0.066
Batch: 220 | Loss: 1.427 | Acc: 69.697,88.744,93.845,% | Adaptive Acc: 84.128% | clf_exit: 0.657 0.276 0.067
Batch: 240 | Loss: 1.428 | Acc: 69.810,88.664,93.750,% | Adaptive Acc: 84.155% | clf_exit: 0.657 0.276 0.067
Batch: 260 | Loss: 1.430 | Acc: 69.672,88.634,93.699,% | Adaptive Acc: 83.968% | clf_exit: 0.657 0.276 0.067
Batch: 280 | Loss: 1.431 | Acc: 69.693,88.601,93.722,% | Adaptive Acc: 83.922% | clf_exit: 0.658 0.275 0.067
Batch: 300 | Loss: 1.434 | Acc: 69.619,88.637,93.683,% | Adaptive Acc: 83.908% | clf_exit: 0.658 0.275 0.068
Batch: 320 | Loss: 1.435 | Acc: 69.663,88.607,93.682,% | Adaptive Acc: 83.930% | clf_exit: 0.658 0.274 0.068
Batch: 340 | Loss: 1.435 | Acc: 69.641,88.643,93.729,% | Adaptive Acc: 83.942% | clf_exit: 0.657 0.275 0.068
Batch: 360 | Loss: 1.434 | Acc: 69.624,88.673,93.726,% | Adaptive Acc: 83.972% | clf_exit: 0.657 0.274 0.069
Batch: 380 | Loss: 1.437 | Acc: 69.634,88.618,93.658,% | Adaptive Acc: 83.955% | clf_exit: 0.656 0.275 0.069
Batch: 0 | Loss: 1.541 | Acc: 64.844,85.156,88.281,% | Adaptive Acc: 75.781% | clf_exit: 0.664 0.266 0.070
Batch: 20 | Loss: 1.786 | Acc: 66.667,83.929,88.393,% | Adaptive Acc: 77.046% | clf_exit: 0.718 0.233 0.049
Batch: 40 | Loss: 1.771 | Acc: 67.530,84.184,88.357,% | Adaptive Acc: 77.096% | clf_exit: 0.731 0.220 0.049
Batch: 60 | Loss: 1.776 | Acc: 67.456,84.247,88.179,% | Adaptive Acc: 77.113% | clf_exit: 0.726 0.225 0.048
Train all parameters

Epoch: 26
Batch: 0 | Loss: 1.382 | Acc: 74.219,89.844,92.969,% | Adaptive Acc: 86.719% | clf_exit: 0.656 0.297 0.047
Batch: 20 | Loss: 1.415 | Acc: 69.271,89.286,94.271,% | Adaptive Acc: 83.705% | clf_exit: 0.661 0.270 0.068
Batch: 40 | Loss: 1.381 | Acc: 69.855,89.653,94.512,% | Adaptive Acc: 84.851% | clf_exit: 0.660 0.272 0.068
Batch: 60 | Loss: 1.386 | Acc: 69.775,89.549,94.378,% | Adaptive Acc: 84.874% | clf_exit: 0.659 0.273 0.068
Batch: 80 | Loss: 1.380 | Acc: 69.927,89.583,94.444,% | Adaptive Acc: 84.954% | clf_exit: 0.661 0.272 0.067
Batch: 100 | Loss: 1.389 | Acc: 69.910,89.496,94.377,% | Adaptive Acc: 84.862% | clf_exit: 0.658 0.275 0.067
Batch: 120 | Loss: 1.391 | Acc: 70.035,89.263,94.383,% | Adaptive Acc: 84.898% | clf_exit: 0.659 0.274 0.067
Batch: 140 | Loss: 1.394 | Acc: 70.002,89.229,94.310,% | Adaptive Acc: 84.818% | clf_exit: 0.658 0.275 0.067
Batch: 160 | Loss: 1.396 | Acc: 69.944,89.237,94.245,% | Adaptive Acc: 84.778% | clf_exit: 0.657 0.276 0.067
Batch: 180 | Loss: 1.402 | Acc: 69.984,89.106,94.177,% | Adaptive Acc: 84.664% | clf_exit: 0.657 0.276 0.067
Batch: 200 | Loss: 1.401 | Acc: 69.974,89.031,94.271,% | Adaptive Acc: 84.639% | clf_exit: 0.658 0.274 0.068
Batch: 220 | Loss: 1.402 | Acc: 69.828,88.949,94.280,% | Adaptive Acc: 84.601% | clf_exit: 0.658 0.275 0.068
Batch: 240 | Loss: 1.404 | Acc: 69.817,88.946,94.230,% | Adaptive Acc: 84.612% | clf_exit: 0.656 0.276 0.068
Batch: 260 | Loss: 1.402 | Acc: 69.887,88.991,94.181,% | Adaptive Acc: 84.591% | clf_exit: 0.656 0.276 0.068
Batch: 280 | Loss: 1.401 | Acc: 69.873,89.057,94.214,% | Adaptive Acc: 84.617% | clf_exit: 0.656 0.277 0.067
Batch: 300 | Loss: 1.403 | Acc: 69.793,89.029,94.176,% | Adaptive Acc: 84.515% | clf_exit: 0.657 0.276 0.068
Batch: 320 | Loss: 1.410 | Acc: 69.745,88.938,94.081,% | Adaptive Acc: 84.385% | clf_exit: 0.657 0.275 0.068
Batch: 340 | Loss: 1.409 | Acc: 69.811,88.911,94.107,% | Adaptive Acc: 84.396% | clf_exit: 0.658 0.275 0.067
Batch: 360 | Loss: 1.411 | Acc: 69.784,88.885,94.064,% | Adaptive Acc: 84.358% | clf_exit: 0.657 0.275 0.068
Batch: 380 | Loss: 1.414 | Acc: 69.693,88.868,93.996,% | Adaptive Acc: 84.291% | clf_exit: 0.658 0.275 0.067
Batch: 0 | Loss: 1.703 | Acc: 61.719,88.281,87.500,% | Adaptive Acc: 75.000% | clf_exit: 0.766 0.188 0.047
Batch: 20 | Loss: 1.763 | Acc: 65.253,85.082,88.058,% | Adaptive Acc: 77.232% | clf_exit: 0.697 0.243 0.060
Batch: 40 | Loss: 1.755 | Acc: 66.139,85.042,88.129,% | Adaptive Acc: 77.668% | clf_exit: 0.700 0.240 0.059
Batch: 60 | Loss: 1.758 | Acc: 66.112,84.990,88.064,% | Adaptive Acc: 77.792% | clf_exit: 0.695 0.247 0.058
Train all parameters

Epoch: 27
Batch: 0 | Loss: 1.274 | Acc: 70.312,90.625,96.094,% | Adaptive Acc: 85.938% | clf_exit: 0.664 0.312 0.023
Batch: 20 | Loss: 1.378 | Acc: 69.940,89.249,94.903,% | Adaptive Acc: 84.263% | clf_exit: 0.672 0.265 0.064
Batch: 40 | Loss: 1.393 | Acc: 70.103,88.967,94.665,% | Adaptive Acc: 84.508% | clf_exit: 0.661 0.274 0.065
Batch: 60 | Loss: 1.395 | Acc: 69.941,89.229,94.621,% | Adaptive Acc: 84.631% | clf_exit: 0.659 0.274 0.067
Batch: 80 | Loss: 1.376 | Acc: 69.917,89.352,94.907,% | Adaptive Acc: 84.578% | clf_exit: 0.664 0.269 0.066
Batch: 100 | Loss: 1.377 | Acc: 69.848,89.233,94.848,% | Adaptive Acc: 84.406% | clf_exit: 0.667 0.268 0.066
Batch: 120 | Loss: 1.377 | Acc: 69.996,89.269,94.744,% | Adaptive Acc: 84.549% | clf_exit: 0.664 0.269 0.067
Batch: 140 | Loss: 1.379 | Acc: 69.914,89.323,94.692,% | Adaptive Acc: 84.597% | clf_exit: 0.663 0.270 0.067
Batch: 160 | Loss: 1.384 | Acc: 69.885,89.223,94.580,% | Adaptive Acc: 84.555% | clf_exit: 0.662 0.272 0.066
Batch: 180 | Loss: 1.391 | Acc: 69.915,89.093,94.423,% | Adaptive Acc: 84.543% | clf_exit: 0.661 0.271 0.067
Batch: 200 | Loss: 1.392 | Acc: 69.885,89.109,94.349,% | Adaptive Acc: 84.499% | clf_exit: 0.661 0.271 0.067
Batch: 220 | Loss: 1.398 | Acc: 69.881,89.006,94.227,% | Adaptive Acc: 84.329% | clf_exit: 0.662 0.270 0.067
Batch: 240 | Loss: 1.399 | Acc: 69.914,88.962,94.171,% | Adaptive Acc: 84.339% | clf_exit: 0.662 0.270 0.067
Batch: 260 | Loss: 1.402 | Acc: 69.861,88.946,94.118,% | Adaptive Acc: 84.300% | clf_exit: 0.663 0.270 0.067
Batch: 280 | Loss: 1.403 | Acc: 69.865,88.901,94.036,% | Adaptive Acc: 84.344% | clf_exit: 0.662 0.271 0.067
Batch: 300 | Loss: 1.403 | Acc: 69.952,88.865,93.986,% | Adaptive Acc: 84.346% | clf_exit: 0.661 0.272 0.067
Batch: 320 | Loss: 1.406 | Acc: 69.977,88.831,93.915,% | Adaptive Acc: 84.307% | clf_exit: 0.661 0.272 0.067
Batch: 340 | Loss: 1.406 | Acc: 69.983,88.788,93.892,% | Adaptive Acc: 84.311% | clf_exit: 0.662 0.271 0.067
Batch: 360 | Loss: 1.402 | Acc: 69.997,88.833,93.912,% | Adaptive Acc: 84.310% | clf_exit: 0.662 0.271 0.067
Batch: 380 | Loss: 1.406 | Acc: 69.972,88.794,93.906,% | Adaptive Acc: 84.279% | clf_exit: 0.662 0.270 0.067
Batch: 0 | Loss: 1.554 | Acc: 66.406,85.156,91.406,% | Adaptive Acc: 78.125% | clf_exit: 0.734 0.203 0.062
Batch: 20 | Loss: 1.744 | Acc: 66.555,83.631,88.579,% | Adaptive Acc: 77.641% | clf_exit: 0.713 0.235 0.051
Batch: 40 | Loss: 1.744 | Acc: 67.645,84.508,88.357,% | Adaptive Acc: 78.487% | clf_exit: 0.717 0.232 0.051
Batch: 60 | Loss: 1.753 | Acc: 67.572,84.554,88.179,% | Adaptive Acc: 78.317% | clf_exit: 0.713 0.237 0.050
Train all parameters

Epoch: 28
Batch: 0 | Loss: 1.064 | Acc: 75.000,93.750,96.875,% | Adaptive Acc: 93.750% | clf_exit: 0.609 0.336 0.055
Batch: 20 | Loss: 1.355 | Acc: 69.048,90.551,95.350,% | Adaptive Acc: 85.789% | clf_exit: 0.644 0.292 0.064
Batch: 40 | Loss: 1.328 | Acc: 69.722,90.072,95.103,% | Adaptive Acc: 85.252% | clf_exit: 0.660 0.276 0.064
Batch: 60 | Loss: 1.334 | Acc: 69.685,90.100,95.018,% | Adaptive Acc: 85.195% | clf_exit: 0.659 0.274 0.068
Batch: 80 | Loss: 1.338 | Acc: 69.878,89.853,94.811,% | Adaptive Acc: 85.041% | clf_exit: 0.664 0.270 0.066
Batch: 100 | Loss: 1.359 | Acc: 69.539,89.619,94.655,% | Adaptive Acc: 84.762% | clf_exit: 0.660 0.271 0.069
Batch: 120 | Loss: 1.367 | Acc: 69.344,89.372,94.641,% | Adaptive Acc: 84.478% | clf_exit: 0.661 0.270 0.069
Batch: 140 | Loss: 1.374 | Acc: 69.498,89.317,94.537,% | Adaptive Acc: 84.458% | clf_exit: 0.660 0.270 0.069
Batch: 160 | Loss: 1.372 | Acc: 69.560,89.349,94.468,% | Adaptive Acc: 84.341% | clf_exit: 0.662 0.270 0.068
Batch: 180 | Loss: 1.378 | Acc: 69.544,89.261,94.367,% | Adaptive Acc: 84.435% | clf_exit: 0.660 0.272 0.068
Batch: 200 | Loss: 1.374 | Acc: 69.737,89.276,94.321,% | Adaptive Acc: 84.391% | clf_exit: 0.662 0.271 0.067
Batch: 220 | Loss: 1.376 | Acc: 69.849,89.257,94.220,% | Adaptive Acc: 84.379% | clf_exit: 0.663 0.270 0.067
Batch: 240 | Loss: 1.381 | Acc: 69.742,89.202,94.155,% | Adaptive Acc: 84.362% | clf_exit: 0.661 0.272 0.067
Batch: 260 | Loss: 1.385 | Acc: 69.678,89.146,94.154,% | Adaptive Acc: 84.399% | clf_exit: 0.660 0.272 0.068
Batch: 280 | Loss: 1.381 | Acc: 69.837,89.163,94.181,% | Adaptive Acc: 84.408% | clf_exit: 0.662 0.271 0.067
Batch: 300 | Loss: 1.381 | Acc: 69.856,89.099,94.168,% | Adaptive Acc: 84.341% | clf_exit: 0.663 0.270 0.067
Batch: 320 | Loss: 1.382 | Acc: 69.935,89.087,94.161,% | Adaptive Acc: 84.346% | clf_exit: 0.663 0.270 0.067
Batch: 340 | Loss: 1.382 | Acc: 69.944,89.060,94.169,% | Adaptive Acc: 84.306% | clf_exit: 0.664 0.269 0.066
Batch: 360 | Loss: 1.382 | Acc: 70.003,89.073,94.189,% | Adaptive Acc: 84.317% | clf_exit: 0.665 0.269 0.066
Batch: 380 | Loss: 1.383 | Acc: 70.009,89.091,94.213,% | Adaptive Acc: 84.334% | clf_exit: 0.665 0.269 0.066
Batch: 0 | Loss: 1.552 | Acc: 65.625,85.156,92.188,% | Adaptive Acc: 75.000% | clf_exit: 0.766 0.172 0.062
Batch: 20 | Loss: 1.727 | Acc: 68.229,84.003,88.802,% | Adaptive Acc: 78.237% | clf_exit: 0.741 0.206 0.052
Batch: 40 | Loss: 1.695 | Acc: 68.693,84.832,89.005,% | Adaptive Acc: 78.277% | clf_exit: 0.752 0.201 0.048
Batch: 60 | Loss: 1.690 | Acc: 68.852,84.798,88.794,% | Adaptive Acc: 78.445% | clf_exit: 0.744 0.209 0.047
Train all parameters

Epoch: 29
Batch: 0 | Loss: 1.595 | Acc: 67.188,85.156,93.750,% | Adaptive Acc: 78.906% | clf_exit: 0.688 0.250 0.062
Batch: 20 | Loss: 1.347 | Acc: 70.424,89.509,95.126,% | Adaptive Acc: 84.338% | clf_exit: 0.671 0.268 0.061
Batch: 40 | Loss: 1.371 | Acc: 70.312,89.310,94.855,% | Adaptive Acc: 84.280% | clf_exit: 0.674 0.268 0.058
Batch: 60 | Loss: 1.370 | Acc: 70.325,89.319,94.659,% | Adaptive Acc: 84.349% | clf_exit: 0.670 0.270 0.060
Batch: 80 | Loss: 1.367 | Acc: 70.390,89.410,94.628,% | Adaptive Acc: 84.587% | clf_exit: 0.665 0.274 0.062
Batch: 100 | Loss: 1.348 | Acc: 70.390,89.488,94.848,% | Adaptive Acc: 84.568% | clf_exit: 0.670 0.269 0.061
Batch: 120 | Loss: 1.351 | Acc: 70.416,89.553,94.815,% | Adaptive Acc: 84.653% | clf_exit: 0.671 0.266 0.063
Batch: 140 | Loss: 1.349 | Acc: 70.429,89.428,94.797,% | Adaptive Acc: 84.641% | clf_exit: 0.670 0.268 0.062
Batch: 160 | Loss: 1.353 | Acc: 70.429,89.397,94.740,% | Adaptive Acc: 84.521% | clf_exit: 0.670 0.267 0.063
Batch: 180 | Loss: 1.354 | Acc: 70.429,89.369,94.786,% | Adaptive Acc: 84.638% | clf_exit: 0.670 0.267 0.063
Batch: 200 | Loss: 1.359 | Acc: 70.340,89.385,94.632,% | Adaptive Acc: 84.507% | clf_exit: 0.670 0.267 0.063
Batch: 220 | Loss: 1.359 | Acc: 70.380,89.370,94.630,% | Adaptive Acc: 84.513% | clf_exit: 0.670 0.266 0.063
Batch: 240 | Loss: 1.356 | Acc: 70.458,89.406,94.596,% | Adaptive Acc: 84.602% | clf_exit: 0.669 0.267 0.064
Batch: 260 | Loss: 1.358 | Acc: 70.522,89.350,94.486,% | Adaptive Acc: 84.555% | clf_exit: 0.671 0.265 0.064
Batch: 280 | Loss: 1.366 | Acc: 70.346,89.296,94.403,% | Adaptive Acc: 84.467% | clf_exit: 0.671 0.265 0.064
Batch: 300 | Loss: 1.371 | Acc: 70.346,89.244,94.311,% | Adaptive Acc: 84.481% | clf_exit: 0.669 0.266 0.064
Batch: 320 | Loss: 1.375 | Acc: 70.315,89.150,94.232,% | Adaptive Acc: 84.402% | clf_exit: 0.669 0.266 0.065
Batch: 340 | Loss: 1.380 | Acc: 70.241,89.140,94.178,% | Adaptive Acc: 84.325% | clf_exit: 0.669 0.266 0.065
Batch: 360 | Loss: 1.380 | Acc: 70.237,89.119,94.194,% | Adaptive Acc: 84.332% | clf_exit: 0.669 0.266 0.065
Batch: 380 | Loss: 1.381 | Acc: 70.265,89.093,94.162,% | Adaptive Acc: 84.307% | clf_exit: 0.670 0.265 0.065
Batch: 0 | Loss: 1.690 | Acc: 64.844,83.594,87.500,% | Adaptive Acc: 76.562% | clf_exit: 0.711 0.234 0.055
Batch: 20 | Loss: 1.797 | Acc: 67.374,84.598,88.393,% | Adaptive Acc: 77.046% | clf_exit: 0.756 0.195 0.049
Batch: 40 | Loss: 1.768 | Acc: 67.473,84.794,88.720,% | Adaptive Acc: 76.791% | clf_exit: 0.765 0.191 0.044
Batch: 60 | Loss: 1.776 | Acc: 67.572,84.644,88.512,% | Adaptive Acc: 76.537% | clf_exit: 0.765 0.193 0.043
Train all parameters

Epoch: 30
Batch: 0 | Loss: 1.353 | Acc: 67.969,88.281,93.750,% | Adaptive Acc: 85.938% | clf_exit: 0.672 0.258 0.070
Batch: 20 | Loss: 1.276 | Acc: 71.429,90.513,95.201,% | Adaptive Acc: 85.528% | clf_exit: 0.680 0.266 0.054
Batch: 40 | Loss: 1.307 | Acc: 70.903,90.301,94.970,% | Adaptive Acc: 85.328% | clf_exit: 0.678 0.262 0.060
Batch: 60 | Loss: 1.298 | Acc: 71.222,90.420,94.941,% | Adaptive Acc: 85.656% | clf_exit: 0.677 0.261 0.061
Batch: 80 | Loss: 1.310 | Acc: 71.055,90.201,94.975,% | Adaptive Acc: 85.475% | clf_exit: 0.673 0.265 0.062
Batch: 100 | Loss: 1.314 | Acc: 71.001,89.991,94.856,% | Adaptive Acc: 85.164% | clf_exit: 0.675 0.265 0.061
Batch: 120 | Loss: 1.319 | Acc: 70.894,89.954,94.718,% | Adaptive Acc: 85.079% | clf_exit: 0.676 0.263 0.061
Batch: 140 | Loss: 1.320 | Acc: 70.928,89.955,94.742,% | Adaptive Acc: 85.045% | clf_exit: 0.676 0.264 0.060
Batch: 160 | Loss: 1.330 | Acc: 70.807,89.907,94.638,% | Adaptive Acc: 84.889% | clf_exit: 0.676 0.264 0.060
Batch: 180 | Loss: 1.336 | Acc: 70.856,89.870,94.510,% | Adaptive Acc: 84.906% | clf_exit: 0.676 0.264 0.060
Batch: 200 | Loss: 1.340 | Acc: 70.678,89.820,94.516,% | Adaptive Acc: 84.849% | clf_exit: 0.675 0.263 0.061
Batch: 220 | Loss: 1.342 | Acc: 70.715,89.727,94.464,% | Adaptive Acc: 84.866% | clf_exit: 0.673 0.265 0.062
Batch: 240 | Loss: 1.346 | Acc: 70.705,89.682,94.337,% | Adaptive Acc: 84.822% | clf_exit: 0.673 0.265 0.062
Batch: 260 | Loss: 1.347 | Acc: 70.726,89.640,94.286,% | Adaptive Acc: 84.740% | clf_exit: 0.674 0.264 0.062
Batch: 280 | Loss: 1.349 | Acc: 70.691,89.613,94.275,% | Adaptive Acc: 84.764% | clf_exit: 0.674 0.264 0.062
Batch: 300 | Loss: 1.349 | Acc: 70.676,89.634,94.321,% | Adaptive Acc: 84.816% | clf_exit: 0.673 0.265 0.062
Batch: 320 | Loss: 1.351 | Acc: 70.670,89.639,94.307,% | Adaptive Acc: 84.781% | clf_exit: 0.673 0.265 0.063
Batch: 340 | Loss: 1.353 | Acc: 70.661,89.633,94.298,% | Adaptive Acc: 84.771% | clf_exit: 0.673 0.264 0.063
Batch: 360 | Loss: 1.354 | Acc: 70.635,89.627,94.304,% | Adaptive Acc: 84.795% | clf_exit: 0.673 0.265 0.063
Batch: 380 | Loss: 1.355 | Acc: 70.671,89.594,94.250,% | Adaptive Acc: 84.728% | clf_exit: 0.673 0.264 0.062
Batch: 0 | Loss: 1.705 | Acc: 60.938,86.719,89.062,% | Adaptive Acc: 74.219% | clf_exit: 0.758 0.188 0.055
Batch: 20 | Loss: 1.790 | Acc: 67.262,82.812,88.393,% | Adaptive Acc: 75.930% | clf_exit: 0.767 0.188 0.045
Batch: 40 | Loss: 1.751 | Acc: 67.988,83.689,88.681,% | Adaptive Acc: 76.715% | clf_exit: 0.772 0.185 0.043
Batch: 60 | Loss: 1.749 | Acc: 67.636,83.478,88.589,% | Adaptive Acc: 76.870% | clf_exit: 0.766 0.189 0.045
Train all parameters

Epoch: 31
Batch: 0 | Loss: 1.481 | Acc: 64.844,88.281,95.312,% | Adaptive Acc: 78.125% | clf_exit: 0.688 0.266 0.047
Batch: 20 | Loss: 1.348 | Acc: 70.052,89.397,94.978,% | Adaptive Acc: 84.673% | clf_exit: 0.669 0.265 0.066
Batch: 40 | Loss: 1.316 | Acc: 70.370,90.225,95.293,% | Adaptive Acc: 85.232% | clf_exit: 0.668 0.268 0.064
Batch: 60 | Loss: 1.330 | Acc: 70.197,90.010,95.044,% | Adaptive Acc: 85.105% | clf_exit: 0.668 0.270 0.062
Batch: 80 | Loss: 1.319 | Acc: 70.611,90.181,95.042,% | Adaptive Acc: 85.243% | clf_exit: 0.671 0.267 0.062
Batch: 100 | Loss: 1.319 | Acc: 70.630,90.246,94.972,% | Adaptive Acc: 85.295% | clf_exit: 0.672 0.267 0.061
Batch: 120 | Loss: 1.324 | Acc: 70.642,90.218,94.899,% | Adaptive Acc: 85.137% | clf_exit: 0.672 0.268 0.060
Batch: 140 | Loss: 1.330 | Acc: 70.562,90.010,94.825,% | Adaptive Acc: 85.062% | clf_exit: 0.673 0.265 0.062
Batch: 160 | Loss: 1.328 | Acc: 70.609,89.946,94.808,% | Adaptive Acc: 85.064% | clf_exit: 0.674 0.265 0.062
Batch: 180 | Loss: 1.333 | Acc: 70.520,89.896,94.777,% | Adaptive Acc: 84.988% | clf_exit: 0.674 0.265 0.062
Batch: 200 | Loss: 1.329 | Acc: 70.577,89.925,94.866,% | Adaptive Acc: 85.055% | clf_exit: 0.672 0.266 0.062
Batch: 220 | Loss: 1.329 | Acc: 70.631,89.858,94.860,% | Adaptive Acc: 85.050% | clf_exit: 0.672 0.266 0.062
Batch: 240 | Loss: 1.324 | Acc: 70.831,89.847,94.836,% | Adaptive Acc: 85.117% | clf_exit: 0.674 0.265 0.061
Batch: 260 | Loss: 1.323 | Acc: 70.935,89.814,94.819,% | Adaptive Acc: 85.144% | clf_exit: 0.675 0.264 0.061
Batch: 280 | Loss: 1.324 | Acc: 70.958,89.835,94.793,% | Adaptive Acc: 85.123% | clf_exit: 0.675 0.264 0.061
Batch: 300 | Loss: 1.330 | Acc: 70.884,89.724,94.687,% | Adaptive Acc: 84.988% | clf_exit: 0.675 0.263 0.061
Batch: 320 | Loss: 1.337 | Acc: 70.870,89.686,94.590,% | Adaptive Acc: 84.898% | clf_exit: 0.676 0.263 0.062
Batch: 340 | Loss: 1.341 | Acc: 70.851,89.599,94.536,% | Adaptive Acc: 84.861% | clf_exit: 0.675 0.263 0.062
Batch: 360 | Loss: 1.340 | Acc: 70.862,89.593,94.523,% | Adaptive Acc: 84.881% | clf_exit: 0.675 0.263 0.062
Batch: 380 | Loss: 1.346 | Acc: 70.731,89.493,94.453,% | Adaptive Acc: 84.777% | clf_exit: 0.675 0.263 0.062
Batch: 0 | Loss: 1.424 | Acc: 71.875,89.844,89.844,% | Adaptive Acc: 81.250% | clf_exit: 0.719 0.211 0.070
Batch: 20 | Loss: 1.693 | Acc: 67.374,85.007,89.211,% | Adaptive Acc: 78.051% | clf_exit: 0.726 0.223 0.051
Batch: 40 | Loss: 1.681 | Acc: 68.064,85.518,89.444,% | Adaptive Acc: 78.639% | clf_exit: 0.732 0.217 0.051
Batch: 60 | Loss: 1.676 | Acc: 68.084,85.195,89.421,% | Adaptive Acc: 78.637% | clf_exit: 0.727 0.223 0.049
Train all parameters

Epoch: 32
Batch: 0 | Loss: 1.306 | Acc: 71.875,89.844,96.094,% | Adaptive Acc: 87.500% | clf_exit: 0.625 0.320 0.055
Batch: 20 | Loss: 1.289 | Acc: 72.619,90.439,95.015,% | Adaptive Acc: 85.119% | clf_exit: 0.686 0.255 0.059
Batch: 40 | Loss: 1.293 | Acc: 72.066,90.377,95.198,% | Adaptive Acc: 85.232% | clf_exit: 0.682 0.260 0.058
Batch: 60 | Loss: 1.307 | Acc: 71.747,90.292,95.044,% | Adaptive Acc: 85.156% | clf_exit: 0.682 0.260 0.059
Batch: 80 | Loss: 1.316 | Acc: 71.190,90.181,95.187,% | Adaptive Acc: 84.848% | clf_exit: 0.680 0.261 0.059
Batch: 100 | Loss: 1.309 | Acc: 71.535,90.231,95.080,% | Adaptive Acc: 85.017% | clf_exit: 0.680 0.261 0.059
Batch: 120 | Loss: 1.313 | Acc: 71.294,90.121,95.170,% | Adaptive Acc: 84.866% | clf_exit: 0.680 0.260 0.060
Batch: 140 | Loss: 1.304 | Acc: 71.382,90.259,95.235,% | Adaptive Acc: 84.890% | clf_exit: 0.682 0.258 0.060
Batch: 160 | Loss: 1.310 | Acc: 71.118,90.149,95.148,% | Adaptive Acc: 84.724% | clf_exit: 0.682 0.258 0.060
Batch: 180 | Loss: 1.312 | Acc: 71.102,90.133,95.066,% | Adaptive Acc: 84.720% | clf_exit: 0.681 0.259 0.060
Batch: 200 | Loss: 1.317 | Acc: 71.008,90.054,95.037,% | Adaptive Acc: 84.600% | clf_exit: 0.680 0.260 0.060
Batch: 220 | Loss: 1.316 | Acc: 70.952,90.035,94.970,% | Adaptive Acc: 84.640% | clf_exit: 0.679 0.260 0.060
Batch: 240 | Loss: 1.318 | Acc: 70.941,89.973,94.911,% | Adaptive Acc: 84.625% | clf_exit: 0.679 0.261 0.060
Batch: 260 | Loss: 1.319 | Acc: 70.938,89.949,94.932,% | Adaptive Acc: 84.665% | clf_exit: 0.678 0.262 0.060
Batch: 280 | Loss: 1.317 | Acc: 71.027,89.986,94.929,% | Adaptive Acc: 84.723% | clf_exit: 0.679 0.261 0.060
Batch: 300 | Loss: 1.317 | Acc: 71.039,90.020,94.910,% | Adaptive Acc: 84.798% | clf_exit: 0.679 0.262 0.060
Batch: 320 | Loss: 1.316 | Acc: 71.048,89.990,94.896,% | Adaptive Acc: 84.747% | clf_exit: 0.680 0.261 0.059
Batch: 340 | Loss: 1.321 | Acc: 70.968,89.885,94.808,% | Adaptive Acc: 84.705% | clf_exit: 0.679 0.261 0.059
Batch: 360 | Loss: 1.324 | Acc: 70.923,89.816,94.761,% | Adaptive Acc: 84.620% | clf_exit: 0.680 0.261 0.060
Batch: 380 | Loss: 1.324 | Acc: 70.938,89.786,94.742,% | Adaptive Acc: 84.642% | clf_exit: 0.681 0.260 0.060
Batch: 0 | Loss: 1.759 | Acc: 70.312,85.938,86.719,% | Adaptive Acc: 73.438% | clf_exit: 0.750 0.195 0.055
Batch: 20 | Loss: 1.771 | Acc: 67.597,85.342,88.058,% | Adaptive Acc: 77.195% | clf_exit: 0.756 0.202 0.042
Batch: 40 | Loss: 1.745 | Acc: 67.816,85.328,88.700,% | Adaptive Acc: 77.229% | clf_exit: 0.757 0.205 0.037
Batch: 60 | Loss: 1.733 | Acc: 67.738,85.387,88.601,% | Adaptive Acc: 77.497% | clf_exit: 0.753 0.209 0.038
Train all parameters

Epoch: 33
Batch: 0 | Loss: 1.207 | Acc: 78.125,91.406,96.094,% | Adaptive Acc: 92.969% | clf_exit: 0.672 0.250 0.078
Batch: 20 | Loss: 1.293 | Acc: 71.354,90.551,95.201,% | Adaptive Acc: 84.970% | clf_exit: 0.692 0.249 0.060
Batch: 40 | Loss: 1.259 | Acc: 71.723,90.549,95.808,% | Adaptive Acc: 85.899% | clf_exit: 0.687 0.254 0.059
Batch: 60 | Loss: 1.256 | Acc: 71.952,90.497,95.505,% | Adaptive Acc: 86.066% | clf_exit: 0.687 0.256 0.057
Batch: 80 | Loss: 1.264 | Acc: 71.547,90.529,95.505,% | Adaptive Acc: 85.851% | clf_exit: 0.684 0.256 0.059
Batch: 100 | Loss: 1.271 | Acc: 71.728,90.362,95.483,% | Adaptive Acc: 85.651% | clf_exit: 0.686 0.254 0.059
Batch: 120 | Loss: 1.283 | Acc: 71.578,90.405,95.364,% | Adaptive Acc: 85.531% | clf_exit: 0.684 0.256 0.060
Batch: 140 | Loss: 1.286 | Acc: 71.465,90.398,95.307,% | Adaptive Acc: 85.372% | clf_exit: 0.686 0.255 0.059
Batch: 160 | Loss: 1.284 | Acc: 71.404,90.441,95.356,% | Adaptive Acc: 85.467% | clf_exit: 0.685 0.256 0.059
Batch: 180 | Loss: 1.294 | Acc: 71.379,90.426,95.196,% | Adaptive Acc: 85.342% | clf_exit: 0.684 0.257 0.059
Batch: 200 | Loss: 1.300 | Acc: 71.393,90.287,95.079,% | Adaptive Acc: 85.211% | clf_exit: 0.684 0.256 0.060
Batch: 220 | Loss: 1.309 | Acc: 71.253,90.190,95.040,% | Adaptive Acc: 85.107% | clf_exit: 0.684 0.256 0.060
Batch: 240 | Loss: 1.312 | Acc: 71.223,90.126,95.018,% | Adaptive Acc: 85.124% | clf_exit: 0.681 0.258 0.060
Batch: 260 | Loss: 1.312 | Acc: 71.187,90.086,94.965,% | Adaptive Acc: 85.037% | clf_exit: 0.682 0.257 0.061
Batch: 280 | Loss: 1.314 | Acc: 71.244,90.016,94.918,% | Adaptive Acc: 85.017% | clf_exit: 0.682 0.257 0.061
Batch: 300 | Loss: 1.319 | Acc: 71.229,89.979,94.822,% | Adaptive Acc: 84.899% | clf_exit: 0.682 0.257 0.061
Batch: 320 | Loss: 1.323 | Acc: 71.196,89.924,94.797,% | Adaptive Acc: 84.901% | clf_exit: 0.681 0.258 0.061
Batch: 340 | Loss: 1.325 | Acc: 71.107,89.917,94.795,% | Adaptive Acc: 84.863% | clf_exit: 0.681 0.258 0.061
Batch: 360 | Loss: 1.323 | Acc: 71.174,89.911,94.793,% | Adaptive Acc: 84.884% | clf_exit: 0.681 0.258 0.061
Batch: 380 | Loss: 1.324 | Acc: 71.151,89.883,94.749,% | Adaptive Acc: 84.816% | clf_exit: 0.682 0.257 0.061
Batch: 0 | Loss: 1.444 | Acc: 68.750,86.719,92.188,% | Adaptive Acc: 82.031% | clf_exit: 0.703 0.211 0.086
Batch: 20 | Loss: 1.755 | Acc: 66.332,85.156,89.397,% | Adaptive Acc: 78.162% | clf_exit: 0.706 0.243 0.051
Batch: 40 | Loss: 1.728 | Acc: 67.092,85.537,89.444,% | Adaptive Acc: 79.021% | clf_exit: 0.705 0.242 0.052
Batch: 60 | Loss: 1.745 | Acc: 66.739,85.233,89.127,% | Adaptive Acc: 78.676% | clf_exit: 0.701 0.248 0.051
Train all parameters

Epoch: 34
Batch: 0 | Loss: 1.128 | Acc: 71.094,92.188,97.656,% | Adaptive Acc: 87.500% | clf_exit: 0.672 0.273 0.055
Batch: 20 | Loss: 1.275 | Acc: 71.354,90.327,95.052,% | Adaptive Acc: 85.826% | clf_exit: 0.668 0.275 0.057
Batch: 40 | Loss: 1.273 | Acc: 71.227,90.301,95.408,% | Adaptive Acc: 85.404% | clf_exit: 0.679 0.264 0.056
Batch: 60 | Loss: 1.273 | Acc: 71.632,90.356,95.325,% | Adaptive Acc: 85.656% | clf_exit: 0.683 0.259 0.058
Batch: 80 | Loss: 1.268 | Acc: 71.431,90.567,95.496,% | Adaptive Acc: 85.706% | clf_exit: 0.684 0.259 0.058
Batch: 100 | Loss: 1.279 | Acc: 71.504,90.439,95.266,% | Adaptive Acc: 85.566% | clf_exit: 0.685 0.256 0.058
Batch: 120 | Loss: 1.288 | Acc: 71.391,90.386,95.177,% | Adaptive Acc: 85.453% | clf_exit: 0.683 0.258 0.059
Batch: 140 | Loss: 1.296 | Acc: 71.266,90.326,95.113,% | Adaptive Acc: 85.300% | clf_exit: 0.681 0.258 0.060
Batch: 160 | Loss: 1.298 | Acc: 71.176,90.329,95.177,% | Adaptive Acc: 85.253% | clf_exit: 0.681 0.259 0.060
Batch: 180 | Loss: 1.301 | Acc: 71.089,90.219,95.105,% | Adaptive Acc: 85.048% | clf_exit: 0.682 0.258 0.060
Batch: 200 | Loss: 1.304 | Acc: 71.191,90.209,95.087,% | Adaptive Acc: 85.110% | clf_exit: 0.681 0.259 0.060
Batch: 220 | Loss: 1.302 | Acc: 71.235,90.236,95.051,% | Adaptive Acc: 85.082% | clf_exit: 0.681 0.259 0.060
Batch: 240 | Loss: 1.303 | Acc: 71.246,90.139,95.018,% | Adaptive Acc: 84.981% | clf_exit: 0.681 0.259 0.060
Batch: 260 | Loss: 1.299 | Acc: 71.387,90.155,95.073,% | Adaptive Acc: 85.046% | clf_exit: 0.682 0.259 0.059
Batch: 280 | Loss: 1.296 | Acc: 71.380,90.202,95.090,% | Adaptive Acc: 85.081% | clf_exit: 0.682 0.259 0.059
Batch: 300 | Loss: 1.297 | Acc: 71.309,90.215,95.071,% | Adaptive Acc: 85.052% | clf_exit: 0.683 0.259 0.059
Batch: 320 | Loss: 1.295 | Acc: 71.366,90.206,95.069,% | Adaptive Acc: 85.052% | clf_exit: 0.683 0.258 0.059
Batch: 340 | Loss: 1.300 | Acc: 71.243,90.148,95.054,% | Adaptive Acc: 85.023% | clf_exit: 0.682 0.259 0.059
Batch: 360 | Loss: 1.300 | Acc: 71.271,90.103,95.023,% | Adaptive Acc: 85.007% | clf_exit: 0.683 0.258 0.059
Batch: 380 | Loss: 1.304 | Acc: 71.241,90.036,94.978,% | Adaptive Acc: 84.968% | clf_exit: 0.683 0.257 0.060
Batch: 0 | Loss: 1.687 | Acc: 63.281,85.938,88.281,% | Adaptive Acc: 74.219% | clf_exit: 0.766 0.195 0.039
Batch: 20 | Loss: 1.821 | Acc: 64.918,84.635,88.393,% | Adaptive Acc: 75.558% | clf_exit: 0.747 0.211 0.042
Batch: 40 | Loss: 1.814 | Acc: 64.977,84.585,88.510,% | Adaptive Acc: 75.381% | clf_exit: 0.751 0.206 0.044
Batch: 60 | Loss: 1.822 | Acc: 65.010,84.567,88.243,% | Adaptive Acc: 75.371% | clf_exit: 0.745 0.211 0.044
Train classifier parameters

Epoch: 35
Batch: 0 | Loss: 1.303 | Acc: 77.344,92.969,96.094,% | Adaptive Acc: 87.500% | clf_exit: 0.711 0.234 0.055
Batch: 20 | Loss: 1.931 | Acc: 61.012,85.007,88.244,% | Adaptive Acc: 76.376% | clf_exit: 0.622 0.300 0.078
Batch: 40 | Loss: 2.158 | Acc: 58.498,82.508,85.499,% | Adaptive Acc: 74.409% | clf_exit: 0.596 0.316 0.088
Batch: 60 | Loss: 2.254 | Acc: 57.902,81.340,84.375,% | Adaptive Acc: 73.847% | clf_exit: 0.588 0.315 0.098
Batch: 80 | Loss: 2.270 | Acc: 57.996,81.163,84.163,% | Adaptive Acc: 74.344% | clf_exit: 0.577 0.321 0.101
Batch: 100 | Loss: 2.298 | Acc: 57.944,80.616,83.640,% | Adaptive Acc: 74.188% | clf_exit: 0.573 0.323 0.104
Batch: 120 | Loss: 2.293 | Acc: 57.987,80.746,83.820,% | Adaptive Acc: 74.348% | clf_exit: 0.570 0.324 0.106
Batch: 140 | Loss: 2.294 | Acc: 58.006,80.602,83.832,% | Adaptive Acc: 74.512% | clf_exit: 0.565 0.327 0.108
Batch: 160 | Loss: 2.281 | Acc: 58.317,80.619,83.963,% | Adaptive Acc: 74.728% | clf_exit: 0.562 0.330 0.108
Batch: 180 | Loss: 2.268 | Acc: 58.473,80.685,84.120,% | Adaptive Acc: 74.957% | clf_exit: 0.560 0.333 0.107
Batch: 200 | Loss: 2.261 | Acc: 58.520,80.566,84.181,% | Adaptive Acc: 75.008% | clf_exit: 0.560 0.331 0.109
Batch: 220 | Loss: 2.246 | Acc: 58.728,80.755,84.262,% | Adaptive Acc: 75.212% | clf_exit: 0.557 0.332 0.110
Batch: 240 | Loss: 2.240 | Acc: 58.834,80.780,84.378,% | Adaptive Acc: 75.379% | clf_exit: 0.556 0.333 0.111
Batch: 260 | Loss: 2.226 | Acc: 59.052,80.828,84.516,% | Adaptive Acc: 75.566% | clf_exit: 0.555 0.333 0.112
Batch: 280 | Loss: 2.216 | Acc: 59.172,80.889,84.617,% | Adaptive Acc: 75.781% | clf_exit: 0.554 0.333 0.112
Batch: 300 | Loss: 2.208 | Acc: 59.349,80.905,84.689,% | Adaptive Acc: 75.945% | clf_exit: 0.552 0.335 0.113
Batch: 320 | Loss: 2.198 | Acc: 59.467,81.014,84.796,% | Adaptive Acc: 76.132% | clf_exit: 0.551 0.336 0.113
Batch: 340 | Loss: 2.190 | Acc: 59.675,81.115,84.838,% | Adaptive Acc: 76.301% | clf_exit: 0.549 0.338 0.114
Batch: 360 | Loss: 2.182 | Acc: 59.873,81.142,84.862,% | Adaptive Acc: 76.454% | clf_exit: 0.548 0.337 0.114
Batch: 380 | Loss: 2.177 | Acc: 59.980,81.150,84.873,% | Adaptive Acc: 76.554% | clf_exit: 0.547 0.338 0.115
Batch: 0 | Loss: 2.054 | Acc: 55.469,81.250,89.062,% | Adaptive Acc: 75.781% | clf_exit: 0.594 0.305 0.102
Batch: 20 | Loss: 2.306 | Acc: 61.272,79.948,81.510,% | Adaptive Acc: 74.665% | clf_exit: 0.595 0.312 0.093
Batch: 40 | Loss: 2.276 | Acc: 61.947,80.050,81.479,% | Adaptive Acc: 74.333% | clf_exit: 0.607 0.303 0.091
Batch: 60 | Loss: 2.272 | Acc: 62.116,80.110,81.468,% | Adaptive Acc: 74.501% | clf_exit: 0.607 0.299 0.094
Train classifier parameters

Epoch: 36
Batch: 0 | Loss: 2.361 | Acc: 58.594,79.688,82.031,% | Adaptive Acc: 74.219% | clf_exit: 0.508 0.375 0.117
Batch: 20 | Loss: 2.003 | Acc: 63.467,82.701,86.421,% | Adaptive Acc: 78.757% | clf_exit: 0.546 0.337 0.117
Batch: 40 | Loss: 2.024 | Acc: 63.072,82.393,86.604,% | Adaptive Acc: 79.402% | clf_exit: 0.535 0.340 0.125
Batch: 60 | Loss: 2.028 | Acc: 62.641,82.556,86.668,% | Adaptive Acc: 79.355% | clf_exit: 0.532 0.340 0.128
Batch: 80 | Loss: 2.013 | Acc: 62.905,82.726,86.796,% | Adaptive Acc: 79.659% | clf_exit: 0.535 0.340 0.125
Batch: 100 | Loss: 2.027 | Acc: 62.763,82.449,86.688,% | Adaptive Acc: 79.548% | clf_exit: 0.534 0.340 0.126
Batch: 120 | Loss: 2.025 | Acc: 62.855,82.503,86.667,% | Adaptive Acc: 79.655% | clf_exit: 0.534 0.341 0.125
Batch: 140 | Loss: 2.013 | Acc: 63.104,82.663,86.636,% | Adaptive Acc: 79.754% | clf_exit: 0.536 0.340 0.124
Batch: 160 | Loss: 2.009 | Acc: 63.238,82.715,86.631,% | Adaptive Acc: 79.872% | clf_exit: 0.537 0.339 0.124
Batch: 180 | Loss: 2.007 | Acc: 63.307,82.700,86.654,% | Adaptive Acc: 79.847% | clf_exit: 0.538 0.338 0.124
Batch: 200 | Loss: 2.003 | Acc: 63.406,82.665,86.684,% | Adaptive Acc: 79.839% | clf_exit: 0.538 0.338 0.124
Batch: 220 | Loss: 2.003 | Acc: 63.387,82.682,86.652,% | Adaptive Acc: 79.811% | clf_exit: 0.540 0.337 0.124
Batch: 240 | Loss: 2.006 | Acc: 63.317,82.667,86.677,% | Adaptive Acc: 79.811% | clf_exit: 0.538 0.338 0.124
Batch: 260 | Loss: 2.003 | Acc: 63.383,82.777,86.662,% | Adaptive Acc: 79.837% | clf_exit: 0.538 0.338 0.124
Batch: 280 | Loss: 2.006 | Acc: 63.398,82.746,86.630,% | Adaptive Acc: 79.829% | clf_exit: 0.537 0.339 0.123
Batch: 300 | Loss: 2.003 | Acc: 63.533,82.735,86.573,% | Adaptive Acc: 79.864% | clf_exit: 0.537 0.340 0.124
Batch: 320 | Loss: 2.001 | Acc: 63.605,82.749,86.490,% | Adaptive Acc: 79.865% | clf_exit: 0.538 0.339 0.123
Batch: 340 | Loss: 1.998 | Acc: 63.572,82.845,86.522,% | Adaptive Acc: 79.935% | clf_exit: 0.538 0.339 0.123
Batch: 360 | Loss: 1.998 | Acc: 63.602,82.841,86.481,% | Adaptive Acc: 79.932% | clf_exit: 0.540 0.338 0.122
Batch: 380 | Loss: 2.001 | Acc: 63.607,82.825,86.430,% | Adaptive Acc: 79.868% | clf_exit: 0.539 0.339 0.122
Batch: 0 | Loss: 1.955 | Acc: 56.250,82.812,89.062,% | Adaptive Acc: 75.000% | clf_exit: 0.617 0.305 0.078
Batch: 20 | Loss: 2.211 | Acc: 61.868,80.841,82.292,% | Adaptive Acc: 75.112% | clf_exit: 0.614 0.295 0.090
Batch: 40 | Loss: 2.184 | Acc: 62.576,81.040,82.450,% | Adaptive Acc: 75.248% | clf_exit: 0.622 0.288 0.090
Batch: 60 | Loss: 2.182 | Acc: 62.731,80.955,82.351,% | Adaptive Acc: 75.435% | clf_exit: 0.619 0.290 0.091
Train classifier parameters

Epoch: 37
Batch: 0 | Loss: 2.107 | Acc: 62.500,82.031,84.375,% | Adaptive Acc: 82.031% | clf_exit: 0.555 0.305 0.141
Batch: 20 | Loss: 2.001 | Acc: 63.393,82.999,86.161,% | Adaptive Acc: 80.097% | clf_exit: 0.544 0.328 0.129
Batch: 40 | Loss: 1.977 | Acc: 64.101,83.365,86.662,% | Adaptive Acc: 79.897% | clf_exit: 0.548 0.330 0.123
Batch: 60 | Loss: 1.993 | Acc: 63.256,83.030,86.360,% | Adaptive Acc: 79.611% | clf_exit: 0.545 0.329 0.126
Batch: 80 | Loss: 1.980 | Acc: 63.397,83.179,86.381,% | Adaptive Acc: 79.668% | clf_exit: 0.545 0.329 0.126
Batch: 100 | Loss: 1.988 | Acc: 63.281,83.083,86.193,% | Adaptive Acc: 79.432% | clf_exit: 0.543 0.330 0.127
Batch: 120 | Loss: 1.984 | Acc: 63.443,83.252,86.222,% | Adaptive Acc: 79.823% | clf_exit: 0.544 0.329 0.127
Batch: 140 | Loss: 1.972 | Acc: 63.819,83.400,86.408,% | Adaptive Acc: 80.125% | clf_exit: 0.543 0.332 0.125
Batch: 160 | Loss: 1.971 | Acc: 63.815,83.472,86.486,% | Adaptive Acc: 80.124% | clf_exit: 0.544 0.331 0.125
Batch: 180 | Loss: 1.973 | Acc: 63.730,83.356,86.430,% | Adaptive Acc: 80.011% | clf_exit: 0.546 0.331 0.124
Batch: 200 | Loss: 1.964 | Acc: 63.919,83.473,86.552,% | Adaptive Acc: 80.162% | clf_exit: 0.546 0.330 0.124
Batch: 220 | Loss: 1.963 | Acc: 64.084,83.459,86.510,% | Adaptive Acc: 80.161% | clf_exit: 0.545 0.332 0.123
Batch: 240 | Loss: 1.956 | Acc: 64.189,83.539,86.670,% | Adaptive Acc: 80.310% | clf_exit: 0.546 0.332 0.123
Batch: 260 | Loss: 1.955 | Acc: 64.242,83.483,86.656,% | Adaptive Acc: 80.331% | clf_exit: 0.545 0.333 0.122
Batch: 280 | Loss: 1.956 | Acc: 64.218,83.469,86.619,% | Adaptive Acc: 80.285% | clf_exit: 0.546 0.332 0.122
Batch: 300 | Loss: 1.956 | Acc: 64.229,83.474,86.659,% | Adaptive Acc: 80.316% | clf_exit: 0.545 0.333 0.122
Batch: 320 | Loss: 1.955 | Acc: 64.308,83.511,86.634,% | Adaptive Acc: 80.342% | clf_exit: 0.545 0.333 0.122
Batch: 340 | Loss: 1.950 | Acc: 64.399,83.603,86.643,% | Adaptive Acc: 80.400% | clf_exit: 0.545 0.334 0.121
Batch: 360 | Loss: 1.949 | Acc: 64.450,83.628,86.678,% | Adaptive Acc: 80.456% | clf_exit: 0.545 0.334 0.121
Batch: 380 | Loss: 1.945 | Acc: 64.466,83.647,86.696,% | Adaptive Acc: 80.500% | clf_exit: 0.546 0.333 0.121
Batch: 0 | Loss: 1.891 | Acc: 60.156,82.031,88.281,% | Adaptive Acc: 75.781% | clf_exit: 0.672 0.266 0.062
Batch: 20 | Loss: 2.162 | Acc: 63.728,81.399,82.403,% | Adaptive Acc: 76.525% | clf_exit: 0.615 0.295 0.090
Batch: 40 | Loss: 2.134 | Acc: 64.215,81.650,82.603,% | Adaptive Acc: 76.353% | clf_exit: 0.631 0.279 0.090
Batch: 60 | Loss: 2.132 | Acc: 64.165,81.532,82.608,% | Adaptive Acc: 76.486% | clf_exit: 0.628 0.282 0.089
Train classifier parameters

Epoch: 38
Batch: 0 | Loss: 1.669 | Acc: 68.750,87.500,90.625,% | Adaptive Acc: 81.250% | clf_exit: 0.531 0.398 0.070
Batch: 20 | Loss: 1.891 | Acc: 66.369,84.115,86.979,% | Adaptive Acc: 81.622% | clf_exit: 0.557 0.331 0.111
Batch: 40 | Loss: 1.896 | Acc: 64.996,84.165,87.214,% | Adaptive Acc: 81.098% | clf_exit: 0.554 0.333 0.113
Batch: 60 | Loss: 1.890 | Acc: 65.164,84.209,86.962,% | Adaptive Acc: 81.045% | clf_exit: 0.554 0.336 0.110
Batch: 80 | Loss: 1.912 | Acc: 65.394,83.931,86.458,% | Adaptive Acc: 80.883% | clf_exit: 0.554 0.332 0.114
Batch: 100 | Loss: 1.917 | Acc: 65.053,83.826,86.587,% | Adaptive Acc: 80.809% | clf_exit: 0.553 0.331 0.116
Batch: 120 | Loss: 1.908 | Acc: 65.154,84.072,86.654,% | Adaptive Acc: 80.992% | clf_exit: 0.555 0.330 0.115
Batch: 140 | Loss: 1.910 | Acc: 65.160,83.954,86.769,% | Adaptive Acc: 80.979% | clf_exit: 0.554 0.329 0.116
Batch: 160 | Loss: 1.911 | Acc: 65.174,83.885,86.728,% | Adaptive Acc: 80.944% | clf_exit: 0.554 0.329 0.117
Batch: 180 | Loss: 1.910 | Acc: 65.137,83.909,86.753,% | Adaptive Acc: 80.918% | clf_exit: 0.555 0.328 0.118
Batch: 200 | Loss: 1.908 | Acc: 65.108,83.990,86.773,% | Adaptive Acc: 80.811% | clf_exit: 0.554 0.329 0.117
Batch: 220 | Loss: 1.908 | Acc: 65.084,83.898,86.775,% | Adaptive Acc: 80.783% | clf_exit: 0.554 0.330 0.117
Batch: 240 | Loss: 1.912 | Acc: 65.129,83.847,86.677,% | Adaptive Acc: 80.731% | clf_exit: 0.555 0.328 0.117
Batch: 260 | Loss: 1.910 | Acc: 65.131,83.860,86.770,% | Adaptive Acc: 80.762% | clf_exit: 0.554 0.329 0.117
Batch: 280 | Loss: 1.911 | Acc: 65.230,83.877,86.819,% | Adaptive Acc: 80.805% | clf_exit: 0.556 0.328 0.116
Batch: 300 | Loss: 1.916 | Acc: 65.090,83.835,86.784,% | Adaptive Acc: 80.765% | clf_exit: 0.555 0.328 0.117
Batch: 320 | Loss: 1.915 | Acc: 65.065,83.883,86.787,% | Adaptive Acc: 80.773% | clf_exit: 0.555 0.328 0.117
Batch: 340 | Loss: 1.919 | Acc: 64.974,83.814,86.723,% | Adaptive Acc: 80.657% | clf_exit: 0.556 0.327 0.117
Batch: 360 | Loss: 1.916 | Acc: 64.984,83.873,86.743,% | Adaptive Acc: 80.707% | clf_exit: 0.556 0.327 0.117
Batch: 380 | Loss: 1.918 | Acc: 64.909,83.889,86.719,% | Adaptive Acc: 80.676% | clf_exit: 0.556 0.327 0.117
Batch: 0 | Loss: 1.896 | Acc: 57.031,82.812,89.062,% | Adaptive Acc: 76.562% | clf_exit: 0.617 0.328 0.055
Batch: 20 | Loss: 2.142 | Acc: 62.500,81.324,82.775,% | Adaptive Acc: 75.893% | clf_exit: 0.621 0.293 0.087
Batch: 40 | Loss: 2.116 | Acc: 63.262,81.879,82.946,% | Adaptive Acc: 76.048% | clf_exit: 0.633 0.281 0.086
Batch: 60 | Loss: 2.116 | Acc: 63.435,81.826,82.953,% | Adaptive Acc: 76.063% | clf_exit: 0.632 0.283 0.086
Train classifier parameters

Epoch: 39
Batch: 0 | Loss: 1.959 | Acc: 65.625,84.375,82.812,% | Adaptive Acc: 85.156% | clf_exit: 0.547 0.320 0.133
Batch: 20 | Loss: 1.937 | Acc: 66.555,83.891,86.124,% | Adaptive Acc: 81.510% | clf_exit: 0.550 0.324 0.125
Batch: 40 | Loss: 1.902 | Acc: 66.101,83.899,86.871,% | Adaptive Acc: 81.479% | clf_exit: 0.553 0.320 0.127
Batch: 60 | Loss: 1.917 | Acc: 65.689,83.658,86.719,% | Adaptive Acc: 80.930% | clf_exit: 0.558 0.318 0.124
Batch: 80 | Loss: 1.910 | Acc: 65.654,83.719,86.748,% | Adaptive Acc: 81.096% | clf_exit: 0.560 0.322 0.118
Batch: 100 | Loss: 1.919 | Acc: 65.223,83.625,86.711,% | Adaptive Acc: 80.786% | clf_exit: 0.559 0.324 0.118
Batch: 120 | Loss: 1.911 | Acc: 65.489,83.697,86.603,% | Adaptive Acc: 80.682% | clf_exit: 0.558 0.324 0.118
Batch: 140 | Loss: 1.904 | Acc: 65.536,83.721,86.602,% | Adaptive Acc: 80.740% | clf_exit: 0.559 0.325 0.117
Batch: 160 | Loss: 1.913 | Acc: 65.450,83.608,86.573,% | Adaptive Acc: 80.711% | clf_exit: 0.558 0.324 0.118
Batch: 180 | Loss: 1.909 | Acc: 65.474,83.676,86.654,% | Adaptive Acc: 80.844% | clf_exit: 0.558 0.324 0.119
Batch: 200 | Loss: 1.907 | Acc: 65.407,83.660,86.746,% | Adaptive Acc: 80.795% | clf_exit: 0.558 0.323 0.119
Batch: 220 | Loss: 1.902 | Acc: 65.544,83.700,86.758,% | Adaptive Acc: 80.783% | clf_exit: 0.558 0.322 0.119
Batch: 240 | Loss: 1.896 | Acc: 65.625,83.801,86.842,% | Adaptive Acc: 80.890% | clf_exit: 0.559 0.323 0.118
Batch: 260 | Loss: 1.896 | Acc: 65.673,83.851,86.859,% | Adaptive Acc: 80.897% | clf_exit: 0.558 0.324 0.119
Batch: 280 | Loss: 1.897 | Acc: 65.619,83.894,86.908,% | Adaptive Acc: 80.922% | clf_exit: 0.558 0.324 0.118
Batch: 300 | Loss: 1.893 | Acc: 65.620,83.952,86.960,% | Adaptive Acc: 81.035% | clf_exit: 0.558 0.324 0.117
Batch: 320 | Loss: 1.897 | Acc: 65.498,83.905,86.935,% | Adaptive Acc: 81.011% | clf_exit: 0.558 0.325 0.118
Batch: 340 | Loss: 1.898 | Acc: 65.538,83.912,86.886,% | Adaptive Acc: 80.989% | clf_exit: 0.559 0.323 0.118
Batch: 360 | Loss: 1.894 | Acc: 65.647,83.994,86.901,% | Adaptive Acc: 81.012% | clf_exit: 0.559 0.324 0.117
Batch: 380 | Loss: 1.892 | Acc: 65.598,84.121,86.922,% | Adaptive Acc: 80.957% | clf_exit: 0.560 0.324 0.116
Batch: 0 | Loss: 1.848 | Acc: 61.719,82.812,89.844,% | Adaptive Acc: 76.562% | clf_exit: 0.641 0.312 0.047
Batch: 20 | Loss: 2.101 | Acc: 63.728,81.845,83.147,% | Adaptive Acc: 76.228% | clf_exit: 0.630 0.287 0.083
Batch: 40 | Loss: 2.076 | Acc: 64.482,82.184,83.194,% | Adaptive Acc: 76.353% | clf_exit: 0.641 0.273 0.086
Batch: 60 | Loss: 2.076 | Acc: 64.562,81.903,83.133,% | Adaptive Acc: 76.678% | clf_exit: 0.636 0.278 0.086
Train classifier parameters

Epoch: 40
Batch: 0 | Loss: 1.789 | Acc: 62.500,85.938,90.625,% | Adaptive Acc: 81.250% | clf_exit: 0.641 0.289 0.070
Batch: 20 | Loss: 1.823 | Acc: 65.737,84.524,87.909,% | Adaptive Acc: 81.399% | clf_exit: 0.570 0.325 0.106
Batch: 40 | Loss: 1.844 | Acc: 65.911,84.699,87.710,% | Adaptive Acc: 81.212% | clf_exit: 0.564 0.327 0.109
Batch: 60 | Loss: 1.861 | Acc: 65.817,84.490,87.449,% | Adaptive Acc: 81.045% | clf_exit: 0.564 0.331 0.105
Batch: 80 | Loss: 1.867 | Acc: 65.808,84.452,87.654,% | Adaptive Acc: 81.057% | clf_exit: 0.565 0.328 0.107
Batch: 100 | Loss: 1.860 | Acc: 65.996,84.615,87.662,% | Adaptive Acc: 81.366% | clf_exit: 0.567 0.327 0.106
Batch: 120 | Loss: 1.868 | Acc: 65.774,84.452,87.610,% | Adaptive Acc: 81.179% | clf_exit: 0.567 0.326 0.108
Batch: 140 | Loss: 1.867 | Acc: 65.802,84.469,87.561,% | Adaptive Acc: 81.278% | clf_exit: 0.567 0.324 0.109
Batch: 160 | Loss: 1.868 | Acc: 65.717,84.555,87.539,% | Adaptive Acc: 81.265% | clf_exit: 0.568 0.324 0.108
Batch: 180 | Loss: 1.862 | Acc: 65.798,84.492,87.556,% | Adaptive Acc: 81.336% | clf_exit: 0.567 0.324 0.109
Batch: 200 | Loss: 1.861 | Acc: 65.882,84.492,87.504,% | Adaptive Acc: 81.324% | clf_exit: 0.567 0.323 0.110
Batch: 220 | Loss: 1.864 | Acc: 65.964,84.432,87.489,% | Adaptive Acc: 81.268% | clf_exit: 0.567 0.322 0.112
Batch: 240 | Loss: 1.863 | Acc: 66.024,84.411,87.425,% | Adaptive Acc: 81.260% | clf_exit: 0.568 0.321 0.112
Batch: 260 | Loss: 1.867 | Acc: 65.855,84.324,87.431,% | Adaptive Acc: 81.133% | clf_exit: 0.567 0.320 0.112
Batch: 280 | Loss: 1.866 | Acc: 65.797,84.328,87.422,% | Adaptive Acc: 81.133% | clf_exit: 0.567 0.321 0.112
Batch: 300 | Loss: 1.866 | Acc: 65.856,84.359,87.412,% | Adaptive Acc: 81.120% | clf_exit: 0.567 0.321 0.112
Batch: 320 | Loss: 1.862 | Acc: 65.876,84.399,87.420,% | Adaptive Acc: 81.106% | clf_exit: 0.568 0.319 0.113
Batch: 340 | Loss: 1.864 | Acc: 65.820,84.412,87.395,% | Adaptive Acc: 81.106% | clf_exit: 0.568 0.320 0.112
Batch: 360 | Loss: 1.865 | Acc: 65.841,84.423,87.385,% | Adaptive Acc: 81.081% | clf_exit: 0.568 0.319 0.112
Batch: 380 | Loss: 1.865 | Acc: 65.795,84.449,87.385,% | Adaptive Acc: 81.109% | clf_exit: 0.568 0.319 0.113
Batch: 0 | Loss: 1.810 | Acc: 62.500,82.812,89.844,% | Adaptive Acc: 78.125% | clf_exit: 0.641 0.289 0.070
Batch: 20 | Loss: 2.083 | Acc: 64.621,82.068,83.147,% | Adaptive Acc: 76.451% | clf_exit: 0.628 0.290 0.082
Batch: 40 | Loss: 2.057 | Acc: 65.320,82.431,83.194,% | Adaptive Acc: 76.963% | clf_exit: 0.639 0.273 0.087
Batch: 60 | Loss: 2.058 | Acc: 65.266,82.198,83.133,% | Adaptive Acc: 77.024% | clf_exit: 0.635 0.278 0.086
Train classifier parameters

Epoch: 41
Batch: 0 | Loss: 1.559 | Acc: 75.000,88.281,91.406,% | Adaptive Acc: 89.844% | clf_exit: 0.570 0.336 0.094
Batch: 20 | Loss: 1.837 | Acc: 66.481,84.896,87.798,% | Adaptive Acc: 81.808% | clf_exit: 0.575 0.320 0.105
Batch: 40 | Loss: 1.831 | Acc: 66.197,84.699,88.148,% | Adaptive Acc: 81.536% | clf_exit: 0.573 0.319 0.109
Batch: 60 | Loss: 1.837 | Acc: 65.881,84.670,87.743,% | Adaptive Acc: 81.481% | clf_exit: 0.573 0.321 0.106
Batch: 80 | Loss: 1.858 | Acc: 65.394,84.423,87.616,% | Adaptive Acc: 81.260% | clf_exit: 0.569 0.323 0.108
Batch: 100 | Loss: 1.854 | Acc: 65.834,84.421,87.430,% | Adaptive Acc: 81.242% | clf_exit: 0.571 0.321 0.108
Batch: 120 | Loss: 1.859 | Acc: 65.702,84.375,87.326,% | Adaptive Acc: 81.166% | clf_exit: 0.570 0.321 0.109
Batch: 140 | Loss: 1.857 | Acc: 65.957,84.425,87.361,% | Adaptive Acc: 81.256% | clf_exit: 0.572 0.320 0.109
Batch: 160 | Loss: 1.861 | Acc: 65.926,84.326,87.330,% | Adaptive Acc: 81.046% | clf_exit: 0.573 0.319 0.109
Batch: 180 | Loss: 1.860 | Acc: 65.979,84.397,87.263,% | Adaptive Acc: 81.099% | clf_exit: 0.574 0.317 0.109
Batch: 200 | Loss: 1.852 | Acc: 66.134,84.507,87.275,% | Adaptive Acc: 81.184% | clf_exit: 0.574 0.318 0.109
Batch: 220 | Loss: 1.849 | Acc: 66.244,84.470,87.330,% | Adaptive Acc: 81.162% | clf_exit: 0.576 0.317 0.107
Batch: 240 | Loss: 1.846 | Acc: 66.228,84.531,87.396,% | Adaptive Acc: 81.137% | clf_exit: 0.576 0.317 0.107
Batch: 260 | Loss: 1.844 | Acc: 66.206,84.567,87.404,% | Adaptive Acc: 81.142% | clf_exit: 0.575 0.318 0.107
Batch: 280 | Loss: 1.845 | Acc: 66.189,84.578,87.364,% | Adaptive Acc: 81.139% | clf_exit: 0.576 0.317 0.107
Batch: 300 | Loss: 1.844 | Acc: 66.196,84.593,87.401,% | Adaptive Acc: 81.164% | clf_exit: 0.576 0.317 0.107
Batch: 320 | Loss: 1.846 | Acc: 66.158,84.606,87.390,% | Adaptive Acc: 81.213% | clf_exit: 0.574 0.318 0.108
Batch: 340 | Loss: 1.846 | Acc: 66.138,84.606,87.445,% | Adaptive Acc: 81.234% | clf_exit: 0.573 0.318 0.108
Batch: 360 | Loss: 1.849 | Acc: 66.181,84.563,87.370,% | Adaptive Acc: 81.198% | clf_exit: 0.573 0.318 0.109
Batch: 380 | Loss: 1.853 | Acc: 66.088,84.531,87.344,% | Adaptive Acc: 81.145% | clf_exit: 0.573 0.318 0.109
Batch: 0 | Loss: 1.830 | Acc: 61.719,83.594,89.844,% | Adaptive Acc: 76.562% | clf_exit: 0.656 0.281 0.062
Batch: 20 | Loss: 2.076 | Acc: 64.397,82.143,82.775,% | Adaptive Acc: 76.079% | clf_exit: 0.636 0.282 0.083
Batch: 40 | Loss: 2.049 | Acc: 65.072,82.431,82.927,% | Adaptive Acc: 76.334% | clf_exit: 0.652 0.267 0.081
Batch: 60 | Loss: 2.049 | Acc: 65.151,82.147,82.992,% | Adaptive Acc: 76.691% | clf_exit: 0.646 0.272 0.082
Train classifier parameters

Epoch: 42
Batch: 0 | Loss: 1.954 | Acc: 61.719,85.938,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.547 0.344 0.109
Batch: 20 | Loss: 1.855 | Acc: 66.964,84.189,87.351,% | Adaptive Acc: 80.134% | clf_exit: 0.560 0.337 0.103
Batch: 40 | Loss: 1.839 | Acc: 66.044,84.718,87.576,% | Adaptive Acc: 80.469% | clf_exit: 0.571 0.325 0.105
Batch: 60 | Loss: 1.830 | Acc: 66.278,84.580,87.462,% | Adaptive Acc: 80.840% | clf_exit: 0.566 0.328 0.106
Batch: 80 | Loss: 1.829 | Acc: 66.416,84.520,87.519,% | Adaptive Acc: 80.999% | clf_exit: 0.572 0.321 0.108
Batch: 100 | Loss: 1.829 | Acc: 66.491,84.592,87.485,% | Adaptive Acc: 81.018% | clf_exit: 0.576 0.317 0.107
Batch: 120 | Loss: 1.830 | Acc: 66.316,84.543,87.616,% | Adaptive Acc: 81.095% | clf_exit: 0.577 0.317 0.107
Batch: 140 | Loss: 1.825 | Acc: 66.223,84.707,87.677,% | Adaptive Acc: 81.244% | clf_exit: 0.577 0.317 0.106
Batch: 160 | Loss: 1.834 | Acc: 65.936,84.734,87.587,% | Adaptive Acc: 81.148% | clf_exit: 0.574 0.317 0.109
Batch: 180 | Loss: 1.827 | Acc: 65.996,84.871,87.647,% | Adaptive Acc: 81.138% | clf_exit: 0.576 0.316 0.108
Batch: 200 | Loss: 1.833 | Acc: 65.990,84.838,87.570,% | Adaptive Acc: 81.149% | clf_exit: 0.576 0.316 0.108
Batch: 220 | Loss: 1.839 | Acc: 65.904,84.693,87.581,% | Adaptive Acc: 81.112% | clf_exit: 0.576 0.315 0.109
Batch: 240 | Loss: 1.842 | Acc: 65.709,84.728,87.575,% | Adaptive Acc: 81.065% | clf_exit: 0.574 0.317 0.109
Batch: 260 | Loss: 1.841 | Acc: 65.823,84.725,87.566,% | Adaptive Acc: 81.091% | clf_exit: 0.575 0.316 0.109
Batch: 280 | Loss: 1.840 | Acc: 65.847,84.803,87.519,% | Adaptive Acc: 81.103% | clf_exit: 0.575 0.316 0.109
Batch: 300 | Loss: 1.840 | Acc: 65.872,84.790,87.510,% | Adaptive Acc: 81.081% | clf_exit: 0.575 0.316 0.109
Batch: 320 | Loss: 1.837 | Acc: 65.949,84.847,87.585,% | Adaptive Acc: 81.165% | clf_exit: 0.576 0.316 0.108
Batch: 340 | Loss: 1.840 | Acc: 65.914,84.781,87.484,% | Adaptive Acc: 81.060% | clf_exit: 0.577 0.315 0.108
Batch: 360 | Loss: 1.839 | Acc: 65.930,84.797,87.515,% | Adaptive Acc: 81.146% | clf_exit: 0.577 0.315 0.108
Batch: 380 | Loss: 1.837 | Acc: 65.994,84.845,87.500,% | Adaptive Acc: 81.166% | clf_exit: 0.577 0.315 0.108
Batch: 0 | Loss: 1.801 | Acc: 62.500,83.594,90.625,% | Adaptive Acc: 77.344% | clf_exit: 0.672 0.258 0.070
Batch: 20 | Loss: 2.064 | Acc: 64.137,82.589,83.371,% | Adaptive Acc: 76.302% | clf_exit: 0.649 0.275 0.077
Batch: 40 | Loss: 2.037 | Acc: 65.244,82.565,83.308,% | Adaptive Acc: 76.429% | clf_exit: 0.661 0.260 0.079
Batch: 60 | Loss: 2.038 | Acc: 65.318,82.364,83.274,% | Adaptive Acc: 76.780% | clf_exit: 0.652 0.267 0.081
Train classifier parameters

Epoch: 43
Batch: 0 | Loss: 1.894 | Acc: 68.750,83.594,86.719,% | Adaptive Acc: 81.250% | clf_exit: 0.578 0.305 0.117
Batch: 20 | Loss: 1.808 | Acc: 67.634,84.524,87.240,% | Adaptive Acc: 81.882% | clf_exit: 0.574 0.320 0.106
Batch: 40 | Loss: 1.807 | Acc: 67.435,84.794,87.405,% | Adaptive Acc: 81.536% | clf_exit: 0.577 0.314 0.109
Batch: 60 | Loss: 1.821 | Acc: 66.099,84.618,87.731,% | Adaptive Acc: 80.994% | clf_exit: 0.578 0.317 0.105
Batch: 80 | Loss: 1.818 | Acc: 66.146,84.732,87.722,% | Adaptive Acc: 81.009% | clf_exit: 0.584 0.313 0.103
Batch: 100 | Loss: 1.808 | Acc: 66.422,84.824,87.856,% | Adaptive Acc: 81.072% | clf_exit: 0.587 0.312 0.101
Batch: 120 | Loss: 1.812 | Acc: 66.122,84.827,87.758,% | Adaptive Acc: 81.063% | clf_exit: 0.586 0.311 0.103
Batch: 140 | Loss: 1.812 | Acc: 66.201,84.846,87.788,% | Adaptive Acc: 81.012% | clf_exit: 0.586 0.310 0.104
Batch: 160 | Loss: 1.822 | Acc: 66.130,84.787,87.680,% | Adaptive Acc: 81.027% | clf_exit: 0.585 0.311 0.104
Batch: 180 | Loss: 1.825 | Acc: 66.083,84.720,87.625,% | Adaptive Acc: 80.995% | clf_exit: 0.584 0.311 0.105
Batch: 200 | Loss: 1.834 | Acc: 65.994,84.589,87.586,% | Adaptive Acc: 80.916% | clf_exit: 0.584 0.311 0.105
Batch: 220 | Loss: 1.834 | Acc: 66.205,84.527,87.595,% | Adaptive Acc: 80.964% | clf_exit: 0.584 0.311 0.105
Batch: 240 | Loss: 1.836 | Acc: 66.127,84.550,87.584,% | Adaptive Acc: 80.923% | clf_exit: 0.583 0.311 0.106
Batch: 260 | Loss: 1.834 | Acc: 66.263,84.680,87.569,% | Adaptive Acc: 81.008% | clf_exit: 0.583 0.310 0.107
Batch: 280 | Loss: 1.834 | Acc: 66.301,84.689,87.564,% | Adaptive Acc: 80.986% | clf_exit: 0.584 0.310 0.106
Batch: 300 | Loss: 1.830 | Acc: 66.328,84.749,87.575,% | Adaptive Acc: 81.066% | clf_exit: 0.584 0.310 0.106
Batch: 320 | Loss: 1.824 | Acc: 66.384,84.823,87.651,% | Adaptive Acc: 81.148% | clf_exit: 0.584 0.311 0.105
Batch: 340 | Loss: 1.828 | Acc: 66.393,84.794,87.624,% | Adaptive Acc: 81.149% | clf_exit: 0.583 0.311 0.106
Batch: 360 | Loss: 1.826 | Acc: 66.469,84.832,87.660,% | Adaptive Acc: 81.185% | clf_exit: 0.583 0.311 0.106
Batch: 380 | Loss: 1.829 | Acc: 66.343,84.802,87.609,% | Adaptive Acc: 81.131% | clf_exit: 0.583 0.311 0.107
Batch: 0 | Loss: 1.829 | Acc: 60.156,84.375,90.625,% | Adaptive Acc: 77.344% | clf_exit: 0.664 0.273 0.062
Batch: 20 | Loss: 2.070 | Acc: 64.174,82.254,83.296,% | Adaptive Acc: 76.265% | clf_exit: 0.637 0.283 0.080
Batch: 40 | Loss: 2.041 | Acc: 64.996,82.527,83.308,% | Adaptive Acc: 76.791% | clf_exit: 0.652 0.266 0.082
Batch: 60 | Loss: 2.040 | Acc: 65.074,82.287,83.222,% | Adaptive Acc: 77.088% | clf_exit: 0.648 0.269 0.083
Train classifier parameters

Epoch: 44
Batch: 0 | Loss: 1.801 | Acc: 65.625,89.844,90.625,% | Adaptive Acc: 82.812% | clf_exit: 0.531 0.344 0.125
Batch: 20 | Loss: 1.840 | Acc: 65.513,84.040,87.388,% | Adaptive Acc: 80.915% | clf_exit: 0.588 0.302 0.110
Batch: 40 | Loss: 1.843 | Acc: 65.930,84.165,87.405,% | Adaptive Acc: 81.536% | clf_exit: 0.581 0.306 0.112
Batch: 60 | Loss: 1.839 | Acc: 65.971,84.298,87.526,% | Adaptive Acc: 81.557% | clf_exit: 0.581 0.309 0.111
Batch: 80 | Loss: 1.829 | Acc: 66.194,84.626,87.818,% | Adaptive Acc: 81.790% | clf_exit: 0.581 0.310 0.109
Batch: 100 | Loss: 1.832 | Acc: 66.228,84.561,87.624,% | Adaptive Acc: 81.768% | clf_exit: 0.580 0.310 0.110
Batch: 120 | Loss: 1.831 | Acc: 66.167,84.472,87.707,% | Adaptive Acc: 81.702% | clf_exit: 0.579 0.310 0.111
Batch: 140 | Loss: 1.830 | Acc: 66.301,84.491,87.600,% | Adaptive Acc: 81.660% | clf_exit: 0.580 0.310 0.110
Batch: 160 | Loss: 1.840 | Acc: 66.159,84.424,87.432,% | Adaptive Acc: 81.449% | clf_exit: 0.580 0.309 0.111
Batch: 180 | Loss: 1.836 | Acc: 66.203,84.466,87.414,% | Adaptive Acc: 81.587% | clf_exit: 0.579 0.311 0.110
Batch: 200 | Loss: 1.838 | Acc: 66.181,84.418,87.391,% | Adaptive Acc: 81.503% | clf_exit: 0.579 0.311 0.110
Batch: 220 | Loss: 1.839 | Acc: 66.254,84.456,87.369,% | Adaptive Acc: 81.554% | clf_exit: 0.578 0.311 0.111
Batch: 240 | Loss: 1.832 | Acc: 66.322,84.531,87.481,% | Adaptive Acc: 81.668% | clf_exit: 0.578 0.312 0.110
Batch: 260 | Loss: 1.834 | Acc: 66.242,84.552,87.488,% | Adaptive Acc: 81.510% | clf_exit: 0.579 0.312 0.109
Batch: 280 | Loss: 1.830 | Acc: 66.284,84.586,87.503,% | Adaptive Acc: 81.550% | clf_exit: 0.580 0.311 0.109
Batch: 300 | Loss: 1.831 | Acc: 66.180,84.635,87.500,% | Adaptive Acc: 81.416% | clf_exit: 0.580 0.312 0.108
Batch: 320 | Loss: 1.831 | Acc: 66.190,84.628,87.454,% | Adaptive Acc: 81.352% | clf_exit: 0.580 0.312 0.108
Batch: 340 | Loss: 1.829 | Acc: 66.200,84.719,87.479,% | Adaptive Acc: 81.355% | clf_exit: 0.580 0.313 0.108
Batch: 360 | Loss: 1.828 | Acc: 66.240,84.713,87.483,% | Adaptive Acc: 81.324% | clf_exit: 0.581 0.312 0.108
Batch: 380 | Loss: 1.830 | Acc: 66.197,84.734,87.490,% | Adaptive Acc: 81.285% | clf_exit: 0.581 0.311 0.108
Batch: 0 | Loss: 1.818 | Acc: 58.594,83.594,89.844,% | Adaptive Acc: 75.781% | clf_exit: 0.688 0.258 0.055
Batch: 20 | Loss: 2.059 | Acc: 63.504,82.812,83.333,% | Adaptive Acc: 76.451% | clf_exit: 0.645 0.275 0.081
Batch: 40 | Loss: 2.034 | Acc: 64.729,82.870,83.498,% | Adaptive Acc: 76.772% | clf_exit: 0.657 0.262 0.081
Batch: 60 | Loss: 2.034 | Acc: 64.844,82.582,83.363,% | Adaptive Acc: 76.960% | clf_exit: 0.653 0.266 0.081
Train classifier parameters

Epoch: 45
Batch: 0 | Loss: 1.588 | Acc: 71.094,86.719,90.625,% | Adaptive Acc: 84.375% | clf_exit: 0.562 0.320 0.117
Batch: 20 | Loss: 1.890 | Acc: 65.402,84.226,86.235,% | Adaptive Acc: 80.060% | clf_exit: 0.579 0.317 0.104
Batch: 40 | Loss: 1.847 | Acc: 66.025,84.299,86.986,% | Adaptive Acc: 80.393% | clf_exit: 0.583 0.309 0.108
Batch: 60 | Loss: 1.856 | Acc: 66.137,84.144,86.924,% | Adaptive Acc: 80.341% | clf_exit: 0.581 0.310 0.109
Batch: 80 | Loss: 1.842 | Acc: 66.223,84.491,87.297,% | Adaptive Acc: 80.710% | clf_exit: 0.578 0.315 0.106
Batch: 100 | Loss: 1.831 | Acc: 66.468,84.638,87.500,% | Adaptive Acc: 80.956% | clf_exit: 0.580 0.313 0.107
Batch: 120 | Loss: 1.826 | Acc: 66.451,84.788,87.629,% | Adaptive Acc: 81.108% | clf_exit: 0.579 0.316 0.105
Batch: 140 | Loss: 1.828 | Acc: 66.445,84.702,87.633,% | Adaptive Acc: 81.106% | clf_exit: 0.579 0.314 0.106
Batch: 160 | Loss: 1.819 | Acc: 66.576,84.753,87.801,% | Adaptive Acc: 81.235% | clf_exit: 0.581 0.313 0.105
Batch: 180 | Loss: 1.823 | Acc: 66.493,84.612,87.716,% | Adaptive Acc: 81.220% | clf_exit: 0.582 0.312 0.106
Batch: 200 | Loss: 1.821 | Acc: 66.496,84.690,87.718,% | Adaptive Acc: 81.269% | clf_exit: 0.581 0.314 0.106
Batch: 220 | Loss: 1.818 | Acc: 66.488,84.820,87.705,% | Adaptive Acc: 81.285% | clf_exit: 0.581 0.313 0.106
Batch: 240 | Loss: 1.813 | Acc: 66.549,84.903,87.753,% | Adaptive Acc: 81.386% | clf_exit: 0.582 0.313 0.105
Batch: 260 | Loss: 1.818 | Acc: 66.436,84.848,87.683,% | Adaptive Acc: 81.304% | clf_exit: 0.583 0.311 0.105
Batch: 280 | Loss: 1.818 | Acc: 66.398,84.817,87.656,% | Adaptive Acc: 81.328% | clf_exit: 0.582 0.312 0.106
Batch: 300 | Loss: 1.811 | Acc: 66.583,84.873,87.710,% | Adaptive Acc: 81.382% | clf_exit: 0.584 0.311 0.106
Batch: 320 | Loss: 1.808 | Acc: 66.623,84.913,87.763,% | Adaptive Acc: 81.386% | clf_exit: 0.585 0.310 0.105
Batch: 340 | Loss: 1.809 | Acc: 66.553,84.907,87.777,% | Adaptive Acc: 81.335% | clf_exit: 0.585 0.310 0.105
Batch: 360 | Loss: 1.812 | Acc: 66.532,84.912,87.786,% | Adaptive Acc: 81.324% | clf_exit: 0.585 0.310 0.105
Batch: 380 | Loss: 1.814 | Acc: 66.544,84.898,87.754,% | Adaptive Acc: 81.307% | clf_exit: 0.585 0.310 0.105
Batch: 0 | Loss: 1.782 | Acc: 59.375,82.812,90.625,% | Adaptive Acc: 76.562% | clf_exit: 0.680 0.266 0.055
Batch: 20 | Loss: 2.042 | Acc: 64.360,82.701,83.445,% | Adaptive Acc: 76.228% | clf_exit: 0.648 0.276 0.076
Batch: 40 | Loss: 2.015 | Acc: 65.187,82.851,83.575,% | Adaptive Acc: 76.848% | clf_exit: 0.661 0.261 0.079
Batch: 60 | Loss: 2.016 | Acc: 65.433,82.569,83.466,% | Adaptive Acc: 77.100% | clf_exit: 0.655 0.265 0.080
Train classifier parameters

Epoch: 46
Batch: 0 | Loss: 1.966 | Acc: 66.406,82.031,82.812,% | Adaptive Acc: 78.906% | clf_exit: 0.609 0.281 0.109
Batch: 20 | Loss: 1.792 | Acc: 66.220,85.193,88.318,% | Adaptive Acc: 81.845% | clf_exit: 0.587 0.306 0.106
Batch: 40 | Loss: 1.794 | Acc: 66.178,85.252,87.824,% | Adaptive Acc: 81.345% | clf_exit: 0.595 0.303 0.103
Batch: 60 | Loss: 1.799 | Acc: 66.432,85.207,87.923,% | Adaptive Acc: 81.391% | clf_exit: 0.592 0.303 0.105
Batch: 80 | Loss: 1.793 | Acc: 66.734,85.436,87.915,% | Adaptive Acc: 81.356% | clf_exit: 0.593 0.306 0.101
Batch: 100 | Loss: 1.798 | Acc: 66.538,85.435,87.987,% | Adaptive Acc: 81.250% | clf_exit: 0.593 0.306 0.101
Batch: 120 | Loss: 1.792 | Acc: 66.522,85.473,87.991,% | Adaptive Acc: 81.360% | clf_exit: 0.593 0.304 0.102
Batch: 140 | Loss: 1.799 | Acc: 66.523,85.339,87.993,% | Adaptive Acc: 81.267% | clf_exit: 0.593 0.303 0.104
Batch: 160 | Loss: 1.810 | Acc: 66.266,85.137,87.927,% | Adaptive Acc: 81.158% | clf_exit: 0.590 0.305 0.104
Batch: 180 | Loss: 1.808 | Acc: 66.367,85.117,87.910,% | Adaptive Acc: 81.215% | clf_exit: 0.589 0.308 0.104
Batch: 200 | Loss: 1.814 | Acc: 66.313,85.005,87.838,% | Adaptive Acc: 81.273% | clf_exit: 0.588 0.308 0.105
Batch: 220 | Loss: 1.818 | Acc: 66.304,84.905,87.723,% | Adaptive Acc: 81.179% | clf_exit: 0.588 0.307 0.105
Batch: 240 | Loss: 1.817 | Acc: 66.358,84.962,87.753,% | Adaptive Acc: 81.208% | clf_exit: 0.588 0.307 0.105
Batch: 260 | Loss: 1.819 | Acc: 66.439,84.935,87.769,% | Adaptive Acc: 81.283% | clf_exit: 0.587 0.307 0.106
Batch: 280 | Loss: 1.819 | Acc: 66.470,84.956,87.784,% | Adaptive Acc: 81.297% | clf_exit: 0.587 0.307 0.106
Batch: 300 | Loss: 1.814 | Acc: 66.505,85.055,87.817,% | Adaptive Acc: 81.294% | clf_exit: 0.588 0.307 0.105
Batch: 320 | Loss: 1.812 | Acc: 66.650,85.052,87.816,% | Adaptive Acc: 81.347% | clf_exit: 0.589 0.306 0.105
Batch: 340 | Loss: 1.810 | Acc: 66.674,85.044,87.860,% | Adaptive Acc: 81.365% | clf_exit: 0.589 0.306 0.105
Batch: 360 | Loss: 1.810 | Acc: 66.636,85.005,87.831,% | Adaptive Acc: 81.311% | clf_exit: 0.590 0.306 0.105
Batch: 380 | Loss: 1.814 | Acc: 66.556,84.968,87.785,% | Adaptive Acc: 81.285% | clf_exit: 0.589 0.306 0.105
Batch: 0 | Loss: 1.789 | Acc: 60.156,83.594,91.406,% | Adaptive Acc: 76.562% | clf_exit: 0.672 0.273 0.055
Batch: 20 | Loss: 2.043 | Acc: 64.546,82.812,83.594,% | Adaptive Acc: 76.525% | clf_exit: 0.645 0.282 0.074
Batch: 40 | Loss: 2.016 | Acc: 65.396,82.832,83.670,% | Adaptive Acc: 76.658% | clf_exit: 0.661 0.263 0.076
Batch: 60 | Loss: 2.016 | Acc: 65.574,82.620,83.504,% | Adaptive Acc: 76.934% | clf_exit: 0.656 0.266 0.078
Train classifier parameters

Epoch: 47
Batch: 0 | Loss: 1.839 | Acc: 64.844,83.594,89.844,% | Adaptive Acc: 78.906% | clf_exit: 0.609 0.281 0.109
Batch: 20 | Loss: 1.805 | Acc: 66.109,85.640,88.504,% | Adaptive Acc: 81.213% | clf_exit: 0.573 0.325 0.103
Batch: 40 | Loss: 1.785 | Acc: 66.616,85.461,88.110,% | Adaptive Acc: 81.574% | clf_exit: 0.581 0.317 0.102
Batch: 60 | Loss: 1.790 | Acc: 66.445,85.528,87.948,% | Adaptive Acc: 81.404% | clf_exit: 0.582 0.314 0.104
Batch: 80 | Loss: 1.776 | Acc: 66.753,85.600,88.011,% | Adaptive Acc: 81.636% | clf_exit: 0.586 0.311 0.103
Batch: 100 | Loss: 1.780 | Acc: 66.832,85.249,88.134,% | Adaptive Acc: 81.459% | clf_exit: 0.587 0.311 0.101
Batch: 120 | Loss: 1.785 | Acc: 66.865,85.253,87.933,% | Adaptive Acc: 81.482% | clf_exit: 0.587 0.311 0.102
Batch: 140 | Loss: 1.787 | Acc: 66.905,85.234,87.916,% | Adaptive Acc: 81.510% | clf_exit: 0.588 0.309 0.103
Batch: 160 | Loss: 1.786 | Acc: 66.824,85.205,88.000,% | Adaptive Acc: 81.483% | clf_exit: 0.590 0.307 0.102
Batch: 180 | Loss: 1.796 | Acc: 66.626,85.117,87.945,% | Adaptive Acc: 81.457% | clf_exit: 0.589 0.308 0.104
Batch: 200 | Loss: 1.796 | Acc: 66.593,85.168,87.998,% | Adaptive Acc: 81.557% | clf_exit: 0.587 0.309 0.105
Batch: 220 | Loss: 1.795 | Acc: 66.696,85.188,87.967,% | Adaptive Acc: 81.596% | clf_exit: 0.587 0.308 0.105
Batch: 240 | Loss: 1.793 | Acc: 66.682,85.286,87.912,% | Adaptive Acc: 81.642% | clf_exit: 0.588 0.308 0.104
Batch: 260 | Loss: 1.795 | Acc: 66.646,85.327,87.892,% | Adaptive Acc: 81.606% | clf_exit: 0.587 0.308 0.105
Batch: 280 | Loss: 1.793 | Acc: 66.679,85.373,87.867,% | Adaptive Acc: 81.650% | clf_exit: 0.587 0.309 0.105
Batch: 300 | Loss: 1.796 | Acc: 66.598,85.330,87.848,% | Adaptive Acc: 81.582% | clf_exit: 0.587 0.308 0.105
Batch: 320 | Loss: 1.797 | Acc: 66.659,85.297,87.763,% | Adaptive Acc: 81.576% | clf_exit: 0.587 0.307 0.106
Batch: 340 | Loss: 1.796 | Acc: 66.642,85.310,87.773,% | Adaptive Acc: 81.543% | clf_exit: 0.587 0.307 0.106
Batch: 360 | Loss: 1.796 | Acc: 66.649,85.247,87.764,% | Adaptive Acc: 81.499% | clf_exit: 0.588 0.306 0.106
Batch: 380 | Loss: 1.796 | Acc: 66.671,85.283,87.781,% | Adaptive Acc: 81.490% | clf_exit: 0.588 0.306 0.106
Batch: 0 | Loss: 1.797 | Acc: 60.938,84.375,91.406,% | Adaptive Acc: 78.125% | clf_exit: 0.656 0.273 0.070
Batch: 20 | Loss: 2.042 | Acc: 64.025,82.440,83.333,% | Adaptive Acc: 76.339% | clf_exit: 0.645 0.277 0.077
Batch: 40 | Loss: 2.015 | Acc: 65.206,82.584,83.518,% | Adaptive Acc: 76.658% | clf_exit: 0.660 0.261 0.079
Batch: 60 | Loss: 2.015 | Acc: 65.254,82.428,83.466,% | Adaptive Acc: 77.049% | clf_exit: 0.657 0.262 0.081
Train classifier parameters

Epoch: 48
Batch: 0 | Loss: 1.555 | Acc: 66.406,88.281,90.625,% | Adaptive Acc: 83.594% | clf_exit: 0.562 0.336 0.102
Batch: 20 | Loss: 1.834 | Acc: 66.443,84.077,87.426,% | Adaptive Acc: 81.101% | clf_exit: 0.582 0.314 0.105
Batch: 40 | Loss: 1.807 | Acc: 65.968,84.737,87.652,% | Adaptive Acc: 80.926% | clf_exit: 0.587 0.308 0.105
Batch: 60 | Loss: 1.797 | Acc: 66.470,84.810,87.820,% | Adaptive Acc: 81.314% | clf_exit: 0.590 0.306 0.104
Batch: 80 | Loss: 1.792 | Acc: 66.426,85.012,88.108,% | Adaptive Acc: 81.404% | clf_exit: 0.587 0.310 0.103
Batch: 100 | Loss: 1.792 | Acc: 66.468,85.048,88.127,% | Adaptive Acc: 81.366% | clf_exit: 0.589 0.307 0.104
Batch: 120 | Loss: 1.791 | Acc: 66.555,85.027,88.178,% | Adaptive Acc: 81.321% | clf_exit: 0.591 0.306 0.102
Batch: 140 | Loss: 1.799 | Acc: 66.412,84.996,88.220,% | Adaptive Acc: 81.206% | clf_exit: 0.589 0.308 0.103
Batch: 160 | Loss: 1.787 | Acc: 66.605,85.132,88.296,% | Adaptive Acc: 81.488% | clf_exit: 0.588 0.308 0.104
Batch: 180 | Loss: 1.785 | Acc: 66.635,85.199,88.242,% | Adaptive Acc: 81.548% | clf_exit: 0.589 0.307 0.104
Batch: 200 | Loss: 1.788 | Acc: 66.686,85.164,88.207,% | Adaptive Acc: 81.444% | clf_exit: 0.590 0.307 0.104
Batch: 220 | Loss: 1.796 | Acc: 66.590,85.033,88.108,% | Adaptive Acc: 81.363% | clf_exit: 0.588 0.308 0.105
Batch: 240 | Loss: 1.802 | Acc: 66.510,84.926,88.096,% | Adaptive Acc: 81.325% | clf_exit: 0.587 0.308 0.105
Batch: 260 | Loss: 1.799 | Acc: 66.565,84.968,88.084,% | Adaptive Acc: 81.328% | clf_exit: 0.587 0.308 0.105
Batch: 280 | Loss: 1.798 | Acc: 66.612,84.989,88.045,% | Adaptive Acc: 81.397% | clf_exit: 0.587 0.307 0.105
Batch: 300 | Loss: 1.795 | Acc: 66.700,85.042,88.004,% | Adaptive Acc: 81.437% | clf_exit: 0.588 0.307 0.105
Batch: 320 | Loss: 1.791 | Acc: 66.754,85.110,88.011,% | Adaptive Acc: 81.493% | clf_exit: 0.589 0.306 0.104
Batch: 340 | Loss: 1.792 | Acc: 66.720,85.065,88.027,% | Adaptive Acc: 81.525% | clf_exit: 0.589 0.307 0.104
Batch: 360 | Loss: 1.790 | Acc: 66.683,85.113,88.102,% | Adaptive Acc: 81.553% | clf_exit: 0.589 0.306 0.104
Batch: 380 | Loss: 1.789 | Acc: 66.710,85.142,88.086,% | Adaptive Acc: 81.566% | clf_exit: 0.589 0.306 0.104
Batch: 0 | Loss: 1.778 | Acc: 60.938,83.594,91.406,% | Adaptive Acc: 76.562% | clf_exit: 0.664 0.281 0.055
Batch: 20 | Loss: 2.038 | Acc: 64.769,82.738,83.668,% | Adaptive Acc: 76.562% | clf_exit: 0.650 0.275 0.075
Batch: 40 | Loss: 2.012 | Acc: 65.549,82.812,83.708,% | Adaptive Acc: 76.944% | clf_exit: 0.667 0.257 0.076
Batch: 60 | Loss: 2.012 | Acc: 65.766,82.633,83.683,% | Adaptive Acc: 77.164% | clf_exit: 0.661 0.261 0.078
Train classifier parameters

Epoch: 49
Batch: 0 | Loss: 1.661 | Acc: 65.625,87.500,92.188,% | Adaptive Acc: 83.594% | clf_exit: 0.562 0.336 0.102
Batch: 20 | Loss: 1.738 | Acc: 66.592,86.086,88.281,% | Adaptive Acc: 82.106% | clf_exit: 0.592 0.305 0.103
Batch: 40 | Loss: 1.743 | Acc: 66.502,86.071,88.262,% | Adaptive Acc: 81.555% | clf_exit: 0.592 0.302 0.106
Batch: 60 | Loss: 1.756 | Acc: 66.957,85.822,88.397,% | Adaptive Acc: 81.621% | clf_exit: 0.593 0.303 0.105
Batch: 80 | Loss: 1.759 | Acc: 66.811,85.716,88.426,% | Adaptive Acc: 81.472% | clf_exit: 0.593 0.301 0.106
Batch: 100 | Loss: 1.770 | Acc: 66.592,85.566,88.227,% | Adaptive Acc: 81.343% | clf_exit: 0.593 0.302 0.106
Batch: 120 | Loss: 1.779 | Acc: 66.748,85.324,88.068,% | Adaptive Acc: 81.340% | clf_exit: 0.592 0.303 0.105
Batch: 140 | Loss: 1.785 | Acc: 66.545,85.267,87.777,% | Adaptive Acc: 81.139% | clf_exit: 0.593 0.302 0.105
Batch: 160 | Loss: 1.786 | Acc: 66.595,85.268,87.801,% | Adaptive Acc: 81.245% | clf_exit: 0.592 0.302 0.105
Batch: 180 | Loss: 1.789 | Acc: 66.497,85.225,87.781,% | Adaptive Acc: 81.151% | clf_exit: 0.593 0.302 0.105
Batch: 200 | Loss: 1.789 | Acc: 66.620,85.257,87.900,% | Adaptive Acc: 81.269% | clf_exit: 0.592 0.303 0.105
Batch: 220 | Loss: 1.787 | Acc: 66.534,85.160,87.949,% | Adaptive Acc: 81.204% | clf_exit: 0.592 0.304 0.105
Batch: 240 | Loss: 1.786 | Acc: 66.555,85.143,88.032,% | Adaptive Acc: 81.269% | clf_exit: 0.591 0.303 0.106
Batch: 260 | Loss: 1.784 | Acc: 66.625,85.219,87.991,% | Adaptive Acc: 81.319% | clf_exit: 0.592 0.303 0.105
Batch: 280 | Loss: 1.787 | Acc: 66.554,85.148,87.962,% | Adaptive Acc: 81.272% | clf_exit: 0.591 0.303 0.106
Batch: 300 | Loss: 1.784 | Acc: 66.640,85.239,87.967,% | Adaptive Acc: 81.294% | clf_exit: 0.593 0.303 0.105
Batch: 320 | Loss: 1.781 | Acc: 66.749,85.297,87.967,% | Adaptive Acc: 81.328% | clf_exit: 0.594 0.302 0.104
Batch: 340 | Loss: 1.779 | Acc: 66.766,85.376,87.997,% | Adaptive Acc: 81.420% | clf_exit: 0.593 0.303 0.104
Batch: 360 | Loss: 1.785 | Acc: 66.705,85.288,87.970,% | Adaptive Acc: 81.391% | clf_exit: 0.592 0.304 0.104
Batch: 380 | Loss: 1.787 | Acc: 66.661,85.265,87.896,% | Adaptive Acc: 81.344% | clf_exit: 0.592 0.304 0.103
Batch: 0 | Loss: 1.789 | Acc: 59.375,84.375,90.625,% | Adaptive Acc: 77.344% | clf_exit: 0.672 0.266 0.062
Batch: 20 | Loss: 2.039 | Acc: 63.988,82.701,83.817,% | Adaptive Acc: 76.525% | clf_exit: 0.654 0.272 0.074
Batch: 40 | Loss: 2.013 | Acc: 65.320,82.851,83.613,% | Adaptive Acc: 76.791% | clf_exit: 0.668 0.256 0.076
Batch: 60 | Loss: 2.012 | Acc: 65.382,82.633,83.555,% | Adaptive Acc: 76.947% | clf_exit: 0.666 0.257 0.077
Train all parameters

Epoch: 50
Batch: 0 | Loss: 1.553 | Acc: 75.000,85.938,90.625,% | Adaptive Acc: 84.375% | clf_exit: 0.625 0.297 0.078
Batch: 20 | Loss: 1.818 | Acc: 64.249,84.375,89.100,% | Adaptive Acc: 78.981% | clf_exit: 0.622 0.294 0.084
Batch: 40 | Loss: 2.054 | Acc: 62.290,81.631,85.785,% | Adaptive Acc: 76.791% | clf_exit: 0.588 0.316 0.097
Batch: 60 | Loss: 2.140 | Acc: 61.911,80.482,84.426,% | Adaptive Acc: 76.191% | clf_exit: 0.585 0.312 0.103
Batch: 80 | Loss: 2.138 | Acc: 62.625,80.430,84.250,% | Adaptive Acc: 76.427% | clf_exit: 0.590 0.304 0.105
Batch: 100 | Loss: 2.143 | Acc: 62.864,80.407,84.298,% | Adaptive Acc: 76.369% | clf_exit: 0.595 0.298 0.107
Batch: 120 | Loss: 2.123 | Acc: 63.113,80.566,84.472,% | Adaptive Acc: 76.653% | clf_exit: 0.597 0.295 0.108
Batch: 140 | Loss: 2.119 | Acc: 63.049,80.546,84.508,% | Adaptive Acc: 76.695% | clf_exit: 0.595 0.295 0.110
Batch: 160 | Loss: 2.099 | Acc: 63.165,80.799,84.724,% | Adaptive Acc: 76.936% | clf_exit: 0.597 0.292 0.111
Batch: 180 | Loss: 2.086 | Acc: 63.337,80.913,84.927,% | Adaptive Acc: 77.059% | clf_exit: 0.597 0.291 0.112
Batch: 200 | Loss: 2.061 | Acc: 63.790,81.203,85.199,% | Adaptive Acc: 77.398% | clf_exit: 0.601 0.290 0.109
Batch: 220 | Loss: 2.044 | Acc: 63.865,81.345,85.460,% | Adaptive Acc: 77.475% | clf_exit: 0.604 0.289 0.107
Batch: 240 | Loss: 2.032 | Acc: 63.858,81.461,85.613,% | Adaptive Acc: 77.532% | clf_exit: 0.605 0.289 0.107
Batch: 260 | Loss: 2.019 | Acc: 63.961,81.594,85.785,% | Adaptive Acc: 77.736% | clf_exit: 0.605 0.289 0.106
Batch: 280 | Loss: 2.001 | Acc: 64.202,81.767,85.960,% | Adaptive Acc: 77.908% | clf_exit: 0.607 0.287 0.106
Batch: 300 | Loss: 1.988 | Acc: 64.281,81.891,86.088,% | Adaptive Acc: 78.039% | clf_exit: 0.608 0.287 0.105
Batch: 320 | Loss: 1.974 | Acc: 64.479,82.060,86.237,% | Adaptive Acc: 78.269% | clf_exit: 0.610 0.286 0.104
Batch: 340 | Loss: 1.964 | Acc: 64.539,82.187,86.343,% | Adaptive Acc: 78.354% | clf_exit: 0.612 0.284 0.104
Batch: 360 | Loss: 1.954 | Acc: 64.658,82.302,86.483,% | Adaptive Acc: 78.415% | clf_exit: 0.614 0.283 0.103
Batch: 380 | Loss: 1.947 | Acc: 64.719,82.400,86.549,% | Adaptive Acc: 78.480% | clf_exit: 0.614 0.283 0.103
Batch: 0 | Loss: 1.658 | Acc: 67.969,88.281,87.500,% | Adaptive Acc: 79.688% | clf_exit: 0.688 0.219 0.094
Batch: 20 | Loss: 1.982 | Acc: 64.769,82.106,84.710,% | Adaptive Acc: 74.814% | clf_exit: 0.699 0.234 0.068
Batch: 40 | Loss: 1.953 | Acc: 65.549,82.793,84.832,% | Adaptive Acc: 75.305% | clf_exit: 0.706 0.229 0.065
Batch: 60 | Loss: 1.944 | Acc: 65.061,82.774,85.067,% | Adaptive Acc: 75.359% | clf_exit: 0.704 0.232 0.064
Train all parameters

Epoch: 51
Batch: 0 | Loss: 1.806 | Acc: 64.062,83.594,88.281,% | Adaptive Acc: 77.344% | clf_exit: 0.664 0.219 0.117
Batch: 20 | Loss: 1.656 | Acc: 68.080,85.714,90.216,% | Adaptive Acc: 82.180% | clf_exit: 0.633 0.278 0.089
Batch: 40 | Loss: 1.662 | Acc: 67.683,85.957,90.091,% | Adaptive Acc: 81.421% | clf_exit: 0.634 0.276 0.090
Batch: 60 | Loss: 1.673 | Acc: 67.495,85.515,89.754,% | Adaptive Acc: 81.148% | clf_exit: 0.636 0.279 0.085
Batch: 80 | Loss: 1.669 | Acc: 67.573,85.436,89.632,% | Adaptive Acc: 81.134% | clf_exit: 0.638 0.277 0.085
Batch: 100 | Loss: 1.675 | Acc: 67.350,85.257,89.596,% | Adaptive Acc: 81.064% | clf_exit: 0.639 0.275 0.086
Batch: 120 | Loss: 1.673 | Acc: 67.401,85.408,89.637,% | Adaptive Acc: 81.134% | clf_exit: 0.639 0.275 0.086
Batch: 140 | Loss: 1.672 | Acc: 67.453,85.533,89.750,% | Adaptive Acc: 81.123% | clf_exit: 0.640 0.275 0.085
Batch: 160 | Loss: 1.674 | Acc: 67.566,85.540,89.669,% | Adaptive Acc: 81.138% | clf_exit: 0.639 0.276 0.084
Batch: 180 | Loss: 1.680 | Acc: 67.654,85.424,89.580,% | Adaptive Acc: 81.082% | clf_exit: 0.640 0.276 0.084
Batch: 200 | Loss: 1.678 | Acc: 67.837,85.288,89.560,% | Adaptive Acc: 81.067% | clf_exit: 0.641 0.275 0.084
Batch: 220 | Loss: 1.677 | Acc: 67.820,85.312,89.649,% | Adaptive Acc: 81.056% | clf_exit: 0.642 0.274 0.084
Batch: 240 | Loss: 1.671 | Acc: 67.868,85.364,89.649,% | Adaptive Acc: 81.104% | clf_exit: 0.641 0.274 0.085
Batch: 260 | Loss: 1.677 | Acc: 67.840,85.348,89.535,% | Adaptive Acc: 81.139% | clf_exit: 0.641 0.274 0.085
Batch: 280 | Loss: 1.679 | Acc: 67.821,85.370,89.577,% | Adaptive Acc: 81.180% | clf_exit: 0.640 0.275 0.085
Batch: 300 | Loss: 1.676 | Acc: 67.800,85.512,89.667,% | Adaptive Acc: 81.253% | clf_exit: 0.640 0.275 0.085
Batch: 320 | Loss: 1.680 | Acc: 67.713,85.463,89.634,% | Adaptive Acc: 81.216% | clf_exit: 0.640 0.275 0.085
Batch: 340 | Loss: 1.674 | Acc: 67.905,85.557,89.660,% | Adaptive Acc: 81.332% | clf_exit: 0.641 0.274 0.085
Batch: 360 | Loss: 1.672 | Acc: 67.904,85.602,89.740,% | Adaptive Acc: 81.363% | clf_exit: 0.641 0.274 0.085
Batch: 380 | Loss: 1.670 | Acc: 67.932,85.648,89.749,% | Adaptive Acc: 81.404% | clf_exit: 0.641 0.274 0.085
Batch: 0 | Loss: 1.728 | Acc: 60.156,84.375,91.406,% | Adaptive Acc: 72.656% | clf_exit: 0.750 0.195 0.055
Batch: 20 | Loss: 1.997 | Acc: 64.286,81.287,86.682,% | Adaptive Acc: 74.963% | clf_exit: 0.719 0.228 0.053
Batch: 40 | Loss: 1.956 | Acc: 65.015,81.822,86.833,% | Adaptive Acc: 75.114% | clf_exit: 0.726 0.225 0.049
Batch: 60 | Loss: 1.939 | Acc: 64.908,82.249,86.668,% | Adaptive Acc: 75.231% | clf_exit: 0.720 0.231 0.049
Train all parameters

Epoch: 52
Batch: 0 | Loss: 1.672 | Acc: 67.188,85.938,89.062,% | Adaptive Acc: 82.812% | clf_exit: 0.617 0.312 0.070
Batch: 20 | Loss: 1.591 | Acc: 68.601,87.202,91.555,% | Adaptive Acc: 83.296% | clf_exit: 0.642 0.281 0.077
Batch: 40 | Loss: 1.622 | Acc: 67.626,86.528,91.254,% | Adaptive Acc: 82.641% | clf_exit: 0.642 0.279 0.079
Batch: 60 | Loss: 1.606 | Acc: 67.585,86.821,91.253,% | Adaptive Acc: 82.454% | clf_exit: 0.637 0.283 0.080
Batch: 80 | Loss: 1.595 | Acc: 67.824,86.979,91.184,% | Adaptive Acc: 82.533% | clf_exit: 0.638 0.283 0.079
Batch: 100 | Loss: 1.586 | Acc: 68.371,87.020,91.159,% | Adaptive Acc: 82.673% | clf_exit: 0.642 0.281 0.078
Batch: 120 | Loss: 1.586 | Acc: 68.337,87.061,91.051,% | Adaptive Acc: 82.593% | clf_exit: 0.644 0.279 0.077
Batch: 140 | Loss: 1.582 | Acc: 68.462,87.046,91.102,% | Adaptive Acc: 82.469% | clf_exit: 0.649 0.274 0.076
Batch: 160 | Loss: 1.573 | Acc: 68.342,87.141,91.246,% | Adaptive Acc: 82.497% | clf_exit: 0.648 0.275 0.077
Batch: 180 | Loss: 1.571 | Acc: 68.396,87.163,91.238,% | Adaptive Acc: 82.450% | clf_exit: 0.649 0.273 0.077
Batch: 200 | Loss: 1.572 | Acc: 68.606,87.170,91.262,% | Adaptive Acc: 82.525% | clf_exit: 0.650 0.272 0.077
Batch: 220 | Loss: 1.569 | Acc: 68.711,87.178,91.304,% | Adaptive Acc: 82.668% | clf_exit: 0.650 0.272 0.078
Batch: 240 | Loss: 1.564 | Acc: 68.744,87.202,91.354,% | Adaptive Acc: 82.673% | clf_exit: 0.651 0.272 0.078
Batch: 260 | Loss: 1.563 | Acc: 68.828,87.171,91.379,% | Adaptive Acc: 82.597% | clf_exit: 0.653 0.270 0.077
Batch: 280 | Loss: 1.560 | Acc: 68.883,87.208,91.376,% | Adaptive Acc: 82.618% | clf_exit: 0.654 0.269 0.077
Batch: 300 | Loss: 1.563 | Acc: 68.794,87.100,91.318,% | Adaptive Acc: 82.584% | clf_exit: 0.653 0.269 0.078
Batch: 320 | Loss: 1.560 | Acc: 68.860,87.164,91.372,% | Adaptive Acc: 82.659% | clf_exit: 0.652 0.270 0.078
Batch: 340 | Loss: 1.561 | Acc: 68.839,87.129,91.354,% | Adaptive Acc: 82.705% | clf_exit: 0.652 0.271 0.078
Batch: 360 | Loss: 1.559 | Acc: 68.837,87.191,91.372,% | Adaptive Acc: 82.722% | clf_exit: 0.652 0.271 0.078
Batch: 380 | Loss: 1.555 | Acc: 68.916,87.266,91.427,% | Adaptive Acc: 82.751% | clf_exit: 0.652 0.271 0.077
Batch: 0 | Loss: 2.009 | Acc: 56.250,83.594,89.062,% | Adaptive Acc: 69.531% | clf_exit: 0.688 0.234 0.078
Batch: 20 | Loss: 2.324 | Acc: 61.793,78.943,84.673,% | Adaptive Acc: 73.028% | clf_exit: 0.701 0.237 0.061
Batch: 40 | Loss: 2.289 | Acc: 61.757,79.097,84.737,% | Adaptive Acc: 72.752% | clf_exit: 0.702 0.241 0.057
Batch: 60 | Loss: 2.252 | Acc: 61.296,79.329,85.156,% | Adaptive Acc: 72.836% | clf_exit: 0.703 0.242 0.055
Train all parameters

Epoch: 53
Batch: 0 | Loss: 1.801 | Acc: 67.188,82.812,89.062,% | Adaptive Acc: 82.031% | clf_exit: 0.594 0.289 0.117
Batch: 20 | Loss: 1.550 | Acc: 68.266,87.128,91.629,% | Adaptive Acc: 82.292% | clf_exit: 0.657 0.268 0.075
Batch: 40 | Loss: 1.519 | Acc: 68.788,87.576,92.416,% | Adaptive Acc: 83.460% | clf_exit: 0.652 0.272 0.076
Batch: 60 | Loss: 1.520 | Acc: 69.057,87.551,92.277,% | Adaptive Acc: 83.133% | clf_exit: 0.659 0.267 0.074
Batch: 80 | Loss: 1.490 | Acc: 69.695,88.156,92.544,% | Adaptive Acc: 83.787% | clf_exit: 0.658 0.268 0.073
Batch: 100 | Loss: 1.471 | Acc: 69.825,88.366,92.582,% | Adaptive Acc: 83.818% | clf_exit: 0.661 0.267 0.072
Batch: 120 | Loss: 1.474 | Acc: 70.061,88.281,92.543,% | Adaptive Acc: 83.762% | clf_exit: 0.663 0.266 0.071
Batch: 140 | Loss: 1.480 | Acc: 70.002,88.087,92.393,% | Adaptive Acc: 83.627% | clf_exit: 0.663 0.266 0.071
Batch: 160 | Loss: 1.484 | Acc: 69.934,87.888,92.348,% | Adaptive Acc: 83.526% | clf_exit: 0.663 0.265 0.072
Batch: 180 | Loss: 1.486 | Acc: 69.790,87.910,92.321,% | Adaptive Acc: 83.499% | clf_exit: 0.662 0.265 0.073
Batch: 200 | Loss: 1.496 | Acc: 69.656,87.819,92.160,% | Adaptive Acc: 83.473% | clf_exit: 0.661 0.266 0.074
Batch: 220 | Loss: 1.489 | Acc: 69.789,87.914,92.237,% | Adaptive Acc: 83.569% | clf_exit: 0.663 0.265 0.073
Batch: 240 | Loss: 1.489 | Acc: 69.784,87.912,92.204,% | Adaptive Acc: 83.532% | clf_exit: 0.663 0.264 0.073
Batch: 260 | Loss: 1.492 | Acc: 69.750,87.892,92.170,% | Adaptive Acc: 83.543% | clf_exit: 0.661 0.265 0.073
Batch: 280 | Loss: 1.492 | Acc: 69.768,87.875,92.196,% | Adaptive Acc: 83.563% | clf_exit: 0.663 0.264 0.073
Batch: 300 | Loss: 1.488 | Acc: 69.832,87.907,92.232,% | Adaptive Acc: 83.555% | clf_exit: 0.663 0.264 0.073
Batch: 320 | Loss: 1.483 | Acc: 69.906,87.926,92.265,% | Adaptive Acc: 83.601% | clf_exit: 0.664 0.263 0.073
Batch: 340 | Loss: 1.481 | Acc: 69.941,87.954,92.288,% | Adaptive Acc: 83.624% | clf_exit: 0.665 0.263 0.072
Batch: 360 | Loss: 1.481 | Acc: 69.897,87.965,92.270,% | Adaptive Acc: 83.622% | clf_exit: 0.665 0.263 0.072
Batch: 380 | Loss: 1.482 | Acc: 69.792,87.974,92.241,% | Adaptive Acc: 83.577% | clf_exit: 0.664 0.263 0.073
Batch: 0 | Loss: 1.672 | Acc: 61.719,81.250,90.625,% | Adaptive Acc: 74.219% | clf_exit: 0.766 0.172 0.062
Batch: 20 | Loss: 2.058 | Acc: 61.644,79.613,86.756,% | Adaptive Acc: 72.507% | clf_exit: 0.737 0.209 0.054
Batch: 40 | Loss: 2.010 | Acc: 63.053,80.431,86.738,% | Adaptive Acc: 73.266% | clf_exit: 0.734 0.211 0.055
Batch: 60 | Loss: 1.991 | Acc: 63.128,80.648,87.295,% | Adaptive Acc: 73.438% | clf_exit: 0.733 0.213 0.055
Train all parameters

Epoch: 54
Batch: 0 | Loss: 1.315 | Acc: 75.781,91.406,90.625,% | Adaptive Acc: 86.719% | clf_exit: 0.664 0.273 0.062
Batch: 20 | Loss: 1.507 | Acc: 69.568,87.351,92.522,% | Adaptive Acc: 83.631% | clf_exit: 0.660 0.270 0.070
Batch: 40 | Loss: 1.475 | Acc: 70.103,87.729,92.607,% | Adaptive Acc: 83.975% | clf_exit: 0.666 0.263 0.071
Batch: 60 | Loss: 1.451 | Acc: 70.684,87.910,92.841,% | Adaptive Acc: 84.119% | clf_exit: 0.668 0.262 0.070
Batch: 80 | Loss: 1.445 | Acc: 70.467,87.992,92.978,% | Adaptive Acc: 83.671% | clf_exit: 0.671 0.261 0.068
Batch: 100 | Loss: 1.438 | Acc: 70.583,88.181,93.108,% | Adaptive Acc: 83.965% | clf_exit: 0.671 0.261 0.067
Batch: 120 | Loss: 1.422 | Acc: 70.816,88.456,93.156,% | Adaptive Acc: 84.233% | clf_exit: 0.671 0.262 0.067
Batch: 140 | Loss: 1.425 | Acc: 70.650,88.420,93.174,% | Adaptive Acc: 84.126% | clf_exit: 0.673 0.261 0.067
Batch: 160 | Loss: 1.422 | Acc: 70.681,88.534,93.109,% | Adaptive Acc: 84.249% | clf_exit: 0.672 0.261 0.067
Batch: 180 | Loss: 1.425 | Acc: 70.718,88.523,92.990,% | Adaptive Acc: 84.220% | clf_exit: 0.671 0.261 0.068
Batch: 200 | Loss: 1.427 | Acc: 70.682,88.584,92.980,% | Adaptive Acc: 84.247% | clf_exit: 0.671 0.261 0.067
Batch: 220 | Loss: 1.432 | Acc: 70.673,88.522,92.859,% | Adaptive Acc: 84.191% | clf_exit: 0.671 0.261 0.068
Batch: 240 | Loss: 1.435 | Acc: 70.578,88.515,92.797,% | Adaptive Acc: 84.125% | clf_exit: 0.671 0.261 0.068
Batch: 260 | Loss: 1.438 | Acc: 70.567,88.485,92.747,% | Adaptive Acc: 84.046% | clf_exit: 0.672 0.261 0.068
Batch: 280 | Loss: 1.435 | Acc: 70.593,88.579,92.771,% | Adaptive Acc: 84.036% | clf_exit: 0.672 0.260 0.068
Batch: 300 | Loss: 1.437 | Acc: 70.531,88.580,92.740,% | Adaptive Acc: 84.025% | clf_exit: 0.672 0.260 0.067
Batch: 320 | Loss: 1.440 | Acc: 70.493,88.522,92.696,% | Adaptive Acc: 83.952% | clf_exit: 0.672 0.261 0.067
Batch: 340 | Loss: 1.438 | Acc: 70.532,88.533,92.717,% | Adaptive Acc: 83.976% | clf_exit: 0.672 0.261 0.067
Batch: 360 | Loss: 1.440 | Acc: 70.561,88.470,92.692,% | Adaptive Acc: 83.899% | clf_exit: 0.673 0.260 0.067
Batch: 380 | Loss: 1.440 | Acc: 70.591,88.470,92.694,% | Adaptive Acc: 83.889% | clf_exit: 0.673 0.260 0.067
Batch: 0 | Loss: 1.591 | Acc: 61.719,86.719,92.188,% | Adaptive Acc: 77.344% | clf_exit: 0.711 0.219 0.070
Batch: 20 | Loss: 1.863 | Acc: 63.802,84.412,88.356,% | Adaptive Acc: 74.777% | clf_exit: 0.723 0.226 0.051
Batch: 40 | Loss: 1.827 | Acc: 64.882,84.642,88.681,% | Adaptive Acc: 75.819% | clf_exit: 0.719 0.229 0.052
Batch: 60 | Loss: 1.821 | Acc: 64.434,84.772,88.755,% | Adaptive Acc: 75.820% | clf_exit: 0.716 0.233 0.051
Train all parameters

Epoch: 55
Batch: 0 | Loss: 1.678 | Acc: 71.094,85.938,90.625,% | Adaptive Acc: 82.812% | clf_exit: 0.609 0.297 0.094
Batch: 20 | Loss: 1.450 | Acc: 70.350,88.244,93.080,% | Adaptive Acc: 83.333% | clf_exit: 0.666 0.263 0.072
Batch: 40 | Loss: 1.403 | Acc: 70.351,89.101,93.559,% | Adaptive Acc: 84.127% | clf_exit: 0.670 0.262 0.068
Batch: 60 | Loss: 1.361 | Acc: 70.889,89.818,93.968,% | Adaptive Acc: 84.862% | clf_exit: 0.672 0.263 0.065
Batch: 80 | Loss: 1.358 | Acc: 71.123,89.709,93.933,% | Adaptive Acc: 84.799% | clf_exit: 0.674 0.261 0.065
Batch: 100 | Loss: 1.363 | Acc: 70.916,89.627,93.866,% | Adaptive Acc: 84.568% | clf_exit: 0.678 0.258 0.064
Batch: 120 | Loss: 1.370 | Acc: 70.868,89.521,93.886,% | Adaptive Acc: 84.562% | clf_exit: 0.681 0.254 0.065
Batch: 140 | Loss: 1.367 | Acc: 71.000,89.522,93.794,% | Adaptive Acc: 84.619% | clf_exit: 0.680 0.256 0.064
Batch: 160 | Loss: 1.369 | Acc: 70.856,89.669,93.769,% | Adaptive Acc: 84.695% | clf_exit: 0.677 0.258 0.065
Batch: 180 | Loss: 1.368 | Acc: 70.934,89.637,93.737,% | Adaptive Acc: 84.617% | clf_exit: 0.677 0.258 0.065
Batch: 200 | Loss: 1.371 | Acc: 71.016,89.564,93.661,% | Adaptive Acc: 84.565% | clf_exit: 0.677 0.258 0.065
Batch: 220 | Loss: 1.369 | Acc: 71.048,89.589,93.662,% | Adaptive Acc: 84.605% | clf_exit: 0.678 0.257 0.065
Batch: 240 | Loss: 1.373 | Acc: 71.091,89.516,93.643,% | Adaptive Acc: 84.638% | clf_exit: 0.677 0.258 0.065
Batch: 260 | Loss: 1.371 | Acc: 71.169,89.473,93.603,% | Adaptive Acc: 84.668% | clf_exit: 0.677 0.258 0.065
Batch: 280 | Loss: 1.374 | Acc: 71.097,89.404,93.553,% | Adaptive Acc: 84.614% | clf_exit: 0.677 0.258 0.064
Batch: 300 | Loss: 1.377 | Acc: 71.083,89.361,93.519,% | Adaptive Acc: 84.564% | clf_exit: 0.677 0.258 0.065
Batch: 320 | Loss: 1.375 | Acc: 71.138,89.367,93.514,% | Adaptive Acc: 84.558% | clf_exit: 0.678 0.258 0.065
Batch: 340 | Loss: 1.379 | Acc: 71.043,89.292,93.450,% | Adaptive Acc: 84.508% | clf_exit: 0.677 0.258 0.066
Batch: 360 | Loss: 1.383 | Acc: 71.044,89.227,93.404,% | Adaptive Acc: 84.436% | clf_exit: 0.677 0.257 0.066
Batch: 380 | Loss: 1.389 | Acc: 70.901,89.173,93.373,% | Adaptive Acc: 84.375% | clf_exit: 0.676 0.257 0.066
Batch: 0 | Loss: 1.665 | Acc: 70.312,82.812,88.281,% | Adaptive Acc: 76.562% | clf_exit: 0.711 0.234 0.055
Batch: 20 | Loss: 1.754 | Acc: 70.089,82.775,88.356,% | Adaptive Acc: 78.274% | clf_exit: 0.738 0.211 0.051
Batch: 40 | Loss: 1.723 | Acc: 70.236,83.213,88.491,% | Adaptive Acc: 78.563% | clf_exit: 0.744 0.206 0.050
Batch: 60 | Loss: 1.706 | Acc: 69.800,83.414,88.640,% | Adaptive Acc: 78.612% | clf_exit: 0.742 0.211 0.047
Train all parameters

Epoch: 56
Batch: 0 | Loss: 1.171 | Acc: 70.312,91.406,99.219,% | Adaptive Acc: 90.625% | clf_exit: 0.711 0.219 0.070
Batch: 20 | Loss: 1.418 | Acc: 70.015,89.062,94.196,% | Adaptive Acc: 84.784% | clf_exit: 0.653 0.278 0.068
Batch: 40 | Loss: 1.396 | Acc: 70.065,89.310,93.902,% | Adaptive Acc: 84.375% | clf_exit: 0.656 0.275 0.069
Batch: 60 | Loss: 1.369 | Acc: 70.479,89.600,94.301,% | Adaptive Acc: 84.529% | clf_exit: 0.667 0.268 0.065
Batch: 80 | Loss: 1.369 | Acc: 70.611,89.410,94.126,% | Adaptive Acc: 84.298% | clf_exit: 0.676 0.261 0.063
Batch: 100 | Loss: 1.364 | Acc: 70.707,89.418,94.175,% | Adaptive Acc: 84.298% | clf_exit: 0.677 0.260 0.063
Batch: 120 | Loss: 1.359 | Acc: 70.732,89.579,94.299,% | Adaptive Acc: 84.343% | clf_exit: 0.677 0.261 0.062
Batch: 140 | Loss: 1.357 | Acc: 70.811,89.567,94.315,% | Adaptive Acc: 84.458% | clf_exit: 0.678 0.259 0.063
Batch: 160 | Loss: 1.357 | Acc: 70.943,89.523,94.211,% | Adaptive Acc: 84.511% | clf_exit: 0.679 0.258 0.064
Batch: 180 | Loss: 1.355 | Acc: 71.115,89.580,94.108,% | Adaptive Acc: 84.621% | clf_exit: 0.679 0.257 0.064
Batch: 200 | Loss: 1.355 | Acc: 71.148,89.517,94.026,% | Adaptive Acc: 84.565% | clf_exit: 0.679 0.257 0.064
Batch: 220 | Loss: 1.358 | Acc: 71.161,89.444,93.987,% | Adaptive Acc: 84.520% | clf_exit: 0.679 0.257 0.064
Batch: 240 | Loss: 1.352 | Acc: 71.217,89.481,94.055,% | Adaptive Acc: 84.498% | clf_exit: 0.681 0.256 0.064
Batch: 260 | Loss: 1.352 | Acc: 71.288,89.470,94.049,% | Adaptive Acc: 84.480% | clf_exit: 0.681 0.255 0.064
Batch: 280 | Loss: 1.356 | Acc: 71.227,89.463,93.970,% | Adaptive Acc: 84.417% | clf_exit: 0.681 0.255 0.064
Batch: 300 | Loss: 1.358 | Acc: 71.187,89.397,93.932,% | Adaptive Acc: 84.370% | clf_exit: 0.681 0.255 0.064
Batch: 320 | Loss: 1.360 | Acc: 71.157,89.393,93.940,% | Adaptive Acc: 84.409% | clf_exit: 0.681 0.256 0.064
Batch: 340 | Loss: 1.358 | Acc: 71.261,89.427,93.945,% | Adaptive Acc: 84.428% | clf_exit: 0.682 0.255 0.064
Batch: 360 | Loss: 1.355 | Acc: 71.314,89.469,93.999,% | Adaptive Acc: 84.494% | clf_exit: 0.681 0.255 0.064
Batch: 380 | Loss: 1.357 | Acc: 71.225,89.450,93.965,% | Adaptive Acc: 84.475% | clf_exit: 0.682 0.255 0.064
Batch: 0 | Loss: 1.372 | Acc: 72.656,91.406,93.750,% | Adaptive Acc: 82.812% | clf_exit: 0.781 0.172 0.047
Batch: 20 | Loss: 1.640 | Acc: 69.085,86.458,89.732,% | Adaptive Acc: 80.506% | clf_exit: 0.709 0.242 0.049
Batch: 40 | Loss: 1.628 | Acc: 69.779,86.242,89.787,% | Adaptive Acc: 80.583% | clf_exit: 0.711 0.242 0.047
Batch: 60 | Loss: 1.617 | Acc: 69.544,86.142,89.805,% | Adaptive Acc: 80.571% | clf_exit: 0.708 0.246 0.046
Train all parameters

Epoch: 57
Batch: 0 | Loss: 1.250 | Acc: 73.438,91.406,94.531,% | Adaptive Acc: 92.188% | clf_exit: 0.625 0.328 0.047
Batch: 20 | Loss: 1.410 | Acc: 70.238,89.360,93.899,% | Adaptive Acc: 84.598% | clf_exit: 0.674 0.253 0.073
Batch: 40 | Loss: 1.391 | Acc: 70.293,89.634,94.188,% | Adaptive Acc: 84.851% | clf_exit: 0.674 0.257 0.069
Batch: 60 | Loss: 1.349 | Acc: 70.876,89.921,94.557,% | Adaptive Acc: 84.887% | clf_exit: 0.683 0.252 0.065
Batch: 80 | Loss: 1.330 | Acc: 71.489,89.873,94.618,% | Adaptive Acc: 84.944% | clf_exit: 0.688 0.247 0.064
Batch: 100 | Loss: 1.327 | Acc: 71.481,89.875,94.508,% | Adaptive Acc: 84.955% | clf_exit: 0.687 0.250 0.064
Batch: 120 | Loss: 1.331 | Acc: 71.526,89.870,94.473,% | Adaptive Acc: 85.072% | clf_exit: 0.685 0.251 0.064
Batch: 140 | Loss: 1.340 | Acc: 71.415,89.794,94.365,% | Adaptive Acc: 84.879% | clf_exit: 0.683 0.253 0.064
Batch: 160 | Loss: 1.347 | Acc: 71.346,89.621,94.260,% | Adaptive Acc: 84.739% | clf_exit: 0.683 0.253 0.064
Batch: 180 | Loss: 1.342 | Acc: 71.413,89.706,94.311,% | Adaptive Acc: 84.811% | clf_exit: 0.682 0.254 0.064
Batch: 200 | Loss: 1.337 | Acc: 71.362,89.793,94.345,% | Adaptive Acc: 84.838% | clf_exit: 0.682 0.254 0.063
Batch: 220 | Loss: 1.334 | Acc: 71.504,89.837,94.330,% | Adaptive Acc: 84.965% | clf_exit: 0.682 0.255 0.063
Batch: 240 | Loss: 1.333 | Acc: 71.593,89.785,94.249,% | Adaptive Acc: 84.920% | clf_exit: 0.684 0.253 0.063
Batch: 260 | Loss: 1.339 | Acc: 71.573,89.664,94.145,% | Adaptive Acc: 84.842% | clf_exit: 0.685 0.252 0.063
Batch: 280 | Loss: 1.334 | Acc: 71.697,89.735,94.139,% | Adaptive Acc: 84.912% | clf_exit: 0.687 0.252 0.062
Batch: 300 | Loss: 1.337 | Acc: 71.579,89.722,94.124,% | Adaptive Acc: 84.762% | clf_exit: 0.687 0.251 0.062
Batch: 320 | Loss: 1.337 | Acc: 71.515,89.751,94.100,% | Adaptive Acc: 84.706% | clf_exit: 0.687 0.251 0.062
Batch: 340 | Loss: 1.342 | Acc: 71.483,89.681,94.032,% | Adaptive Acc: 84.677% | clf_exit: 0.686 0.251 0.062
Batch: 360 | Loss: 1.341 | Acc: 71.483,89.651,94.029,% | Adaptive Acc: 84.691% | clf_exit: 0.686 0.251 0.062
Batch: 380 | Loss: 1.342 | Acc: 71.549,89.598,94.006,% | Adaptive Acc: 84.685% | clf_exit: 0.688 0.250 0.062
Batch: 0 | Loss: 1.633 | Acc: 63.281,89.844,89.062,% | Adaptive Acc: 76.562% | clf_exit: 0.719 0.234 0.047
Batch: 20 | Loss: 1.742 | Acc: 66.704,85.603,89.100,% | Adaptive Acc: 78.683% | clf_exit: 0.705 0.241 0.054
Batch: 40 | Loss: 1.729 | Acc: 67.245,85.442,88.986,% | Adaptive Acc: 78.487% | clf_exit: 0.712 0.233 0.055
Batch: 60 | Loss: 1.723 | Acc: 67.252,85.784,88.730,% | Adaptive Acc: 78.612% | clf_exit: 0.711 0.234 0.055
Train all parameters

Epoch: 58
Batch: 0 | Loss: 1.369 | Acc: 71.875,89.844,93.750,% | Adaptive Acc: 85.938% | clf_exit: 0.641 0.266 0.094
Batch: 20 | Loss: 1.277 | Acc: 72.173,90.402,95.126,% | Adaptive Acc: 86.086% | clf_exit: 0.672 0.265 0.064
Batch: 40 | Loss: 1.269 | Acc: 72.466,90.892,94.912,% | Adaptive Acc: 85.861% | clf_exit: 0.692 0.249 0.059
Batch: 60 | Loss: 1.295 | Acc: 72.118,90.497,94.621,% | Adaptive Acc: 85.425% | clf_exit: 0.689 0.251 0.060
Batch: 80 | Loss: 1.316 | Acc: 71.595,90.210,94.338,% | Adaptive Acc: 85.021% | clf_exit: 0.687 0.254 0.059
Batch: 100 | Loss: 1.320 | Acc: 71.705,90.029,94.230,% | Adaptive Acc: 84.978% | clf_exit: 0.688 0.254 0.058
Batch: 120 | Loss: 1.312 | Acc: 71.933,90.141,94.325,% | Adaptive Acc: 84.930% | clf_exit: 0.690 0.253 0.057
Batch: 140 | Loss: 1.305 | Acc: 72.008,90.143,94.398,% | Adaptive Acc: 84.935% | clf_exit: 0.691 0.252 0.057
Batch: 160 | Loss: 1.305 | Acc: 71.967,90.145,94.415,% | Adaptive Acc: 84.986% | clf_exit: 0.691 0.252 0.057
Batch: 180 | Loss: 1.304 | Acc: 72.099,90.193,94.393,% | Adaptive Acc: 85.212% | clf_exit: 0.691 0.251 0.058
Batch: 200 | Loss: 1.310 | Acc: 72.042,90.073,94.352,% | Adaptive Acc: 85.051% | clf_exit: 0.691 0.251 0.058
Batch: 220 | Loss: 1.315 | Acc: 71.886,90.021,94.369,% | Adaptive Acc: 84.972% | clf_exit: 0.689 0.252 0.058
Batch: 240 | Loss: 1.319 | Acc: 71.849,89.902,94.317,% | Adaptive Acc: 84.858% | clf_exit: 0.690 0.251 0.059
Batch: 260 | Loss: 1.323 | Acc: 71.812,89.814,94.301,% | Adaptive Acc: 84.746% | clf_exit: 0.689 0.252 0.059
Batch: 280 | Loss: 1.324 | Acc: 71.842,89.777,94.248,% | Adaptive Acc: 84.745% | clf_exit: 0.690 0.252 0.059
Batch: 300 | Loss: 1.322 | Acc: 71.849,89.789,94.267,% | Adaptive Acc: 84.772% | clf_exit: 0.690 0.252 0.058
Batch: 320 | Loss: 1.321 | Acc: 71.897,89.817,94.234,% | Adaptive Acc: 84.867% | clf_exit: 0.690 0.252 0.059
Batch: 340 | Loss: 1.322 | Acc: 71.870,89.860,94.217,% | Adaptive Acc: 84.877% | clf_exit: 0.689 0.252 0.059
Batch: 360 | Loss: 1.321 | Acc: 71.949,89.839,94.183,% | Adaptive Acc: 84.886% | clf_exit: 0.690 0.251 0.059
Batch: 380 | Loss: 1.321 | Acc: 71.932,89.815,94.177,% | Adaptive Acc: 84.896% | clf_exit: 0.690 0.251 0.059
Batch: 0 | Loss: 1.464 | Acc: 67.969,83.594,88.281,% | Adaptive Acc: 76.562% | clf_exit: 0.750 0.172 0.078
Batch: 20 | Loss: 1.738 | Acc: 68.601,83.631,88.616,% | Adaptive Acc: 78.460% | clf_exit: 0.743 0.206 0.051
Batch: 40 | Loss: 1.727 | Acc: 69.798,83.441,88.796,% | Adaptive Acc: 78.906% | clf_exit: 0.745 0.208 0.046
Batch: 60 | Loss: 1.723 | Acc: 69.839,83.773,88.922,% | Adaptive Acc: 79.098% | clf_exit: 0.744 0.211 0.044
Train all parameters

Epoch: 59
Batch: 0 | Loss: 1.398 | Acc: 71.094,84.375,91.406,% | Adaptive Acc: 84.375% | clf_exit: 0.688 0.195 0.117
Batch: 20 | Loss: 1.279 | Acc: 72.582,90.067,94.717,% | Adaptive Acc: 85.640% | clf_exit: 0.711 0.226 0.063
Batch: 40 | Loss: 1.244 | Acc: 73.209,91.044,94.989,% | Adaptive Acc: 86.585% | clf_exit: 0.698 0.241 0.061
Batch: 60 | Loss: 1.241 | Acc: 73.066,91.086,95.120,% | Adaptive Acc: 86.181% | clf_exit: 0.699 0.244 0.057
Batch: 80 | Loss: 1.269 | Acc: 72.357,90.654,94.830,% | Adaptive Acc: 85.648% | clf_exit: 0.696 0.247 0.057
Batch: 100 | Loss: 1.259 | Acc: 72.455,90.679,94.872,% | Adaptive Acc: 85.690% | clf_exit: 0.697 0.246 0.056
Batch: 120 | Loss: 1.279 | Acc: 72.036,90.418,94.641,% | Adaptive Acc: 85.389% | clf_exit: 0.694 0.248 0.057
Batch: 140 | Loss: 1.283 | Acc: 71.875,90.354,94.697,% | Adaptive Acc: 85.284% | clf_exit: 0.693 0.250 0.057
Batch: 160 | Loss: 1.287 | Acc: 71.904,90.329,94.643,% | Adaptive Acc: 85.244% | clf_exit: 0.693 0.250 0.058
Batch: 180 | Loss: 1.284 | Acc: 72.043,90.275,94.717,% | Adaptive Acc: 85.307% | clf_exit: 0.693 0.249 0.058
Batch: 200 | Loss: 1.280 | Acc: 72.007,90.384,94.745,% | Adaptive Acc: 85.327% | clf_exit: 0.695 0.248 0.057
Batch: 220 | Loss: 1.281 | Acc: 72.084,90.420,94.708,% | Adaptive Acc: 85.375% | clf_exit: 0.693 0.249 0.057
Batch: 240 | Loss: 1.281 | Acc: 72.073,90.421,94.739,% | Adaptive Acc: 85.442% | clf_exit: 0.692 0.250 0.057
Batch: 260 | Loss: 1.279 | Acc: 72.201,90.409,94.711,% | Adaptive Acc: 85.471% | clf_exit: 0.693 0.249 0.057
Batch: 280 | Loss: 1.278 | Acc: 72.223,90.408,94.687,% | Adaptive Acc: 85.440% | clf_exit: 0.694 0.249 0.057
Batch: 300 | Loss: 1.283 | Acc: 72.223,90.334,94.658,% | Adaptive Acc: 85.372% | clf_exit: 0.694 0.249 0.057
Batch: 320 | Loss: 1.283 | Acc: 72.238,90.299,94.624,% | Adaptive Acc: 85.356% | clf_exit: 0.694 0.249 0.057
Batch: 340 | Loss: 1.288 | Acc: 72.159,90.231,94.602,% | Adaptive Acc: 85.271% | clf_exit: 0.693 0.249 0.058
Batch: 360 | Loss: 1.289 | Acc: 72.137,90.227,94.564,% | Adaptive Acc: 85.213% | clf_exit: 0.694 0.249 0.058
Batch: 380 | Loss: 1.289 | Acc: 72.191,90.207,94.564,% | Adaptive Acc: 85.191% | clf_exit: 0.694 0.248 0.057
Batch: 0 | Loss: 1.799 | Acc: 68.750,86.719,85.156,% | Adaptive Acc: 78.125% | clf_exit: 0.742 0.234 0.023
Batch: 20 | Loss: 1.848 | Acc: 66.369,82.887,87.946,% | Adaptive Acc: 75.558% | clf_exit: 0.774 0.186 0.040
Batch: 40 | Loss: 1.818 | Acc: 67.226,83.651,88.415,% | Adaptive Acc: 76.277% | clf_exit: 0.779 0.183 0.037
Batch: 60 | Loss: 1.794 | Acc: 67.149,83.940,88.730,% | Adaptive Acc: 75.986% | clf_exit: 0.777 0.187 0.036
Train all parameters

Epoch: 60
Batch: 0 | Loss: 1.612 | Acc: 67.188,87.500,94.531,% | Adaptive Acc: 85.156% | clf_exit: 0.594 0.281 0.125
Batch: 20 | Loss: 1.257 | Acc: 73.140,90.923,95.052,% | Adaptive Acc: 85.789% | clf_exit: 0.689 0.264 0.047
Batch: 40 | Loss: 1.236 | Acc: 73.361,90.930,95.122,% | Adaptive Acc: 86.147% | clf_exit: 0.699 0.252 0.049
Batch: 60 | Loss: 1.237 | Acc: 73.053,90.971,95.056,% | Adaptive Acc: 85.899% | clf_exit: 0.702 0.247 0.051
Batch: 80 | Loss: 1.245 | Acc: 72.936,90.847,94.946,% | Adaptive Acc: 85.639% | clf_exit: 0.700 0.249 0.051
Batch: 100 | Loss: 1.237 | Acc: 73.283,90.857,95.104,% | Adaptive Acc: 85.907% | clf_exit: 0.701 0.247 0.051
Batch: 120 | Loss: 1.238 | Acc: 73.199,90.806,95.170,% | Adaptive Acc: 85.757% | clf_exit: 0.704 0.245 0.051
Batch: 140 | Loss: 1.240 | Acc: 73.033,90.808,95.119,% | Adaptive Acc: 85.721% | clf_exit: 0.702 0.246 0.051
Batch: 160 | Loss: 1.247 | Acc: 72.923,90.727,95.094,% | Adaptive Acc: 85.540% | clf_exit: 0.703 0.246 0.050
Batch: 180 | Loss: 1.260 | Acc: 72.691,90.625,94.959,% | Adaptive Acc: 85.359% | clf_exit: 0.702 0.247 0.051
Batch: 200 | Loss: 1.266 | Acc: 72.512,90.574,94.900,% | Adaptive Acc: 85.308% | clf_exit: 0.700 0.249 0.052
Batch: 220 | Loss: 1.271 | Acc: 72.483,90.533,94.892,% | Adaptive Acc: 85.294% | clf_exit: 0.698 0.249 0.053
Batch: 240 | Loss: 1.271 | Acc: 72.517,90.537,94.904,% | Adaptive Acc: 85.399% | clf_exit: 0.697 0.250 0.053
Batch: 260 | Loss: 1.277 | Acc: 72.534,90.457,94.834,% | Adaptive Acc: 85.360% | clf_exit: 0.697 0.249 0.054
Batch: 280 | Loss: 1.279 | Acc: 72.556,90.419,94.759,% | Adaptive Acc: 85.334% | clf_exit: 0.698 0.249 0.054
Batch: 300 | Loss: 1.279 | Acc: 72.537,90.441,94.734,% | Adaptive Acc: 85.283% | clf_exit: 0.698 0.248 0.054
Batch: 320 | Loss: 1.282 | Acc: 72.530,90.343,94.675,% | Adaptive Acc: 85.254% | clf_exit: 0.698 0.249 0.054
Batch: 340 | Loss: 1.281 | Acc: 72.606,90.368,94.641,% | Adaptive Acc: 85.291% | clf_exit: 0.699 0.248 0.054
Batch: 360 | Loss: 1.279 | Acc: 72.695,90.357,94.672,% | Adaptive Acc: 85.358% | clf_exit: 0.698 0.247 0.054
Batch: 380 | Loss: 1.279 | Acc: 72.687,90.350,94.654,% | Adaptive Acc: 85.310% | clf_exit: 0.699 0.247 0.054
Batch: 0 | Loss: 1.483 | Acc: 63.281,89.062,91.406,% | Adaptive Acc: 77.344% | clf_exit: 0.766 0.195 0.039
Batch: 20 | Loss: 1.760 | Acc: 66.555,85.119,89.397,% | Adaptive Acc: 76.414% | clf_exit: 0.766 0.193 0.040
Batch: 40 | Loss: 1.742 | Acc: 67.607,85.137,89.329,% | Adaptive Acc: 76.982% | clf_exit: 0.770 0.189 0.041
Batch: 60 | Loss: 1.734 | Acc: 67.533,85.451,89.267,% | Adaptive Acc: 77.024% | clf_exit: 0.772 0.187 0.041
Train all parameters

Epoch: 61
Batch: 0 | Loss: 1.194 | Acc: 76.562,90.625,95.312,% | Adaptive Acc: 85.156% | clf_exit: 0.711 0.242 0.047
Batch: 20 | Loss: 1.275 | Acc: 72.024,90.588,95.312,% | Adaptive Acc: 85.417% | clf_exit: 0.682 0.254 0.064
Batch: 40 | Loss: 1.240 | Acc: 73.018,90.892,95.484,% | Adaptive Acc: 85.861% | clf_exit: 0.698 0.246 0.056
Batch: 60 | Loss: 1.247 | Acc: 72.874,90.702,95.607,% | Adaptive Acc: 85.925% | clf_exit: 0.698 0.247 0.056
Batch: 80 | Loss: 1.251 | Acc: 72.733,90.750,95.525,% | Adaptive Acc: 85.745% | clf_exit: 0.699 0.245 0.055
Batch: 100 | Loss: 1.244 | Acc: 72.594,90.896,95.637,% | Adaptive Acc: 85.821% | clf_exit: 0.699 0.246 0.054
Batch: 120 | Loss: 1.247 | Acc: 72.469,90.916,95.648,% | Adaptive Acc: 85.634% | clf_exit: 0.698 0.248 0.054
Batch: 140 | Loss: 1.249 | Acc: 72.734,90.725,95.612,% | Adaptive Acc: 85.677% | clf_exit: 0.700 0.246 0.054
Batch: 160 | Loss: 1.262 | Acc: 72.389,90.620,95.414,% | Adaptive Acc: 85.467% | clf_exit: 0.698 0.247 0.055
Batch: 180 | Loss: 1.266 | Acc: 72.406,90.547,95.291,% | Adaptive Acc: 85.441% | clf_exit: 0.698 0.246 0.056
Batch: 200 | Loss: 1.270 | Acc: 72.314,90.442,95.239,% | Adaptive Acc: 85.374% | clf_exit: 0.697 0.248 0.056
Batch: 220 | Loss: 1.265 | Acc: 72.419,90.431,95.217,% | Adaptive Acc: 85.432% | clf_exit: 0.697 0.248 0.055
Batch: 240 | Loss: 1.265 | Acc: 72.533,90.395,95.137,% | Adaptive Acc: 85.497% | clf_exit: 0.698 0.246 0.055
Batch: 260 | Loss: 1.262 | Acc: 72.548,90.445,95.163,% | Adaptive Acc: 85.497% | clf_exit: 0.698 0.246 0.055
Batch: 280 | Loss: 1.264 | Acc: 72.567,90.372,95.148,% | Adaptive Acc: 85.482% | clf_exit: 0.699 0.246 0.056
Batch: 300 | Loss: 1.264 | Acc: 72.571,90.384,95.115,% | Adaptive Acc: 85.465% | clf_exit: 0.699 0.245 0.056
Batch: 320 | Loss: 1.259 | Acc: 72.688,90.408,95.130,% | Adaptive Acc: 85.541% | clf_exit: 0.701 0.243 0.056
Batch: 340 | Loss: 1.262 | Acc: 72.688,90.400,95.088,% | Adaptive Acc: 85.493% | clf_exit: 0.701 0.243 0.056
Batch: 360 | Loss: 1.261 | Acc: 72.695,90.426,95.107,% | Adaptive Acc: 85.481% | clf_exit: 0.702 0.243 0.056
Batch: 380 | Loss: 1.261 | Acc: 72.646,90.453,95.085,% | Adaptive Acc: 85.454% | clf_exit: 0.702 0.243 0.056
Batch: 0 | Loss: 1.534 | Acc: 67.969,89.062,91.406,% | Adaptive Acc: 84.375% | clf_exit: 0.656 0.258 0.086
Batch: 20 | Loss: 1.695 | Acc: 68.452,84.747,89.286,% | Adaptive Acc: 79.055% | clf_exit: 0.709 0.233 0.058
Batch: 40 | Loss: 1.678 | Acc: 69.264,85.328,89.291,% | Adaptive Acc: 79.878% | clf_exit: 0.710 0.233 0.057
Batch: 60 | Loss: 1.680 | Acc: 69.480,84.849,89.037,% | Adaptive Acc: 79.777% | clf_exit: 0.708 0.236 0.056
Train all parameters

Epoch: 62
Batch: 0 | Loss: 1.116 | Acc: 72.656,93.750,96.875,% | Adaptive Acc: 89.844% | clf_exit: 0.672 0.273 0.055
Batch: 20 | Loss: 1.202 | Acc: 73.363,91.704,95.833,% | Adaptive Acc: 86.235% | clf_exit: 0.707 0.237 0.055
Batch: 40 | Loss: 1.222 | Acc: 72.389,91.368,95.617,% | Adaptive Acc: 85.671% | clf_exit: 0.705 0.242 0.053
Batch: 60 | Loss: 1.205 | Acc: 73.117,91.419,95.569,% | Adaptive Acc: 86.014% | clf_exit: 0.704 0.244 0.052
Batch: 80 | Loss: 1.215 | Acc: 73.139,91.184,95.428,% | Adaptive Acc: 85.889% | clf_exit: 0.707 0.240 0.052
Batch: 100 | Loss: 1.221 | Acc: 73.089,90.911,95.359,% | Adaptive Acc: 85.976% | clf_exit: 0.705 0.241 0.055
Batch: 120 | Loss: 1.225 | Acc: 73.102,90.870,95.358,% | Adaptive Acc: 85.912% | clf_exit: 0.705 0.241 0.054
Batch: 140 | Loss: 1.225 | Acc: 73.022,90.874,95.362,% | Adaptive Acc: 86.026% | clf_exit: 0.705 0.241 0.054
Batch: 160 | Loss: 1.224 | Acc: 73.156,90.887,95.429,% | Adaptive Acc: 86.054% | clf_exit: 0.705 0.240 0.055
Batch: 180 | Loss: 1.236 | Acc: 72.958,90.698,95.196,% | Adaptive Acc: 85.769% | clf_exit: 0.706 0.238 0.056
Batch: 200 | Loss: 1.245 | Acc: 72.757,90.590,95.052,% | Adaptive Acc: 85.615% | clf_exit: 0.704 0.240 0.056
Batch: 220 | Loss: 1.250 | Acc: 72.692,90.586,94.948,% | Adaptive Acc: 85.527% | clf_exit: 0.704 0.240 0.056
Batch: 240 | Loss: 1.246 | Acc: 72.653,90.677,94.982,% | Adaptive Acc: 85.591% | clf_exit: 0.703 0.242 0.055
Batch: 260 | Loss: 1.248 | Acc: 72.632,90.586,94.965,% | Adaptive Acc: 85.566% | clf_exit: 0.703 0.241 0.056
Batch: 280 | Loss: 1.253 | Acc: 72.612,90.572,94.982,% | Adaptive Acc: 85.545% | clf_exit: 0.703 0.242 0.055
Batch: 300 | Loss: 1.253 | Acc: 72.643,90.570,94.983,% | Adaptive Acc: 85.595% | clf_exit: 0.702 0.242 0.056
Batch: 320 | Loss: 1.250 | Acc: 72.741,90.615,95.035,% | Adaptive Acc: 85.667% | clf_exit: 0.702 0.242 0.056
Batch: 340 | Loss: 1.255 | Acc: 72.704,90.579,94.989,% | Adaptive Acc: 85.614% | clf_exit: 0.702 0.242 0.056
Batch: 360 | Loss: 1.256 | Acc: 72.654,90.590,94.968,% | Adaptive Acc: 85.611% | clf_exit: 0.702 0.243 0.056
Batch: 380 | Loss: 1.259 | Acc: 72.589,90.525,94.896,% | Adaptive Acc: 85.529% | clf_exit: 0.701 0.243 0.056
Batch: 0 | Loss: 1.633 | Acc: 70.312,83.594,89.844,% | Adaptive Acc: 75.000% | clf_exit: 0.766 0.180 0.055
Batch: 20 | Loss: 1.755 | Acc: 69.978,84.301,87.909,% | Adaptive Acc: 78.795% | clf_exit: 0.746 0.213 0.041
Batch: 40 | Loss: 1.717 | Acc: 70.408,84.261,88.662,% | Adaptive Acc: 78.792% | clf_exit: 0.752 0.210 0.038
Batch: 60 | Loss: 1.695 | Acc: 70.812,84.567,88.922,% | Adaptive Acc: 79.239% | clf_exit: 0.749 0.212 0.039
Train all parameters

Epoch: 63
Batch: 0 | Loss: 1.387 | Acc: 73.438,88.281,94.531,% | Adaptive Acc: 85.156% | clf_exit: 0.625 0.336 0.039
Batch: 20 | Loss: 1.241 | Acc: 72.396,90.290,95.350,% | Adaptive Acc: 85.417% | clf_exit: 0.695 0.248 0.057
Batch: 40 | Loss: 1.233 | Acc: 72.313,90.796,95.446,% | Adaptive Acc: 85.614% | clf_exit: 0.692 0.250 0.058
Batch: 60 | Loss: 1.197 | Acc: 73.143,91.291,95.581,% | Adaptive Acc: 86.142% | clf_exit: 0.700 0.247 0.053
Batch: 80 | Loss: 1.196 | Acc: 73.274,91.098,95.679,% | Adaptive Acc: 86.275% | clf_exit: 0.705 0.243 0.052
Batch: 100 | Loss: 1.209 | Acc: 73.058,90.996,95.684,% | Adaptive Acc: 86.177% | clf_exit: 0.705 0.241 0.054
Batch: 120 | Loss: 1.218 | Acc: 72.805,91.025,95.622,% | Adaptive Acc: 86.131% | clf_exit: 0.703 0.243 0.053
Batch: 140 | Loss: 1.217 | Acc: 72.989,90.957,95.645,% | Adaptive Acc: 86.187% | clf_exit: 0.701 0.245 0.054
Batch: 160 | Loss: 1.214 | Acc: 73.069,90.974,95.642,% | Adaptive Acc: 86.127% | clf_exit: 0.703 0.244 0.053
Batch: 180 | Loss: 1.216 | Acc: 73.118,90.966,95.576,% | Adaptive Acc: 86.114% | clf_exit: 0.703 0.244 0.053
Batch: 200 | Loss: 1.212 | Acc: 73.317,91.084,95.627,% | Adaptive Acc: 86.116% | clf_exit: 0.706 0.241 0.053
Batch: 220 | Loss: 1.210 | Acc: 73.339,91.109,95.620,% | Adaptive Acc: 86.100% | clf_exit: 0.706 0.241 0.053
Batch: 240 | Loss: 1.212 | Acc: 73.159,91.150,95.578,% | Adaptive Acc: 86.025% | clf_exit: 0.706 0.240 0.054
Batch: 260 | Loss: 1.218 | Acc: 73.108,91.074,95.477,% | Adaptive Acc: 85.917% | clf_exit: 0.707 0.240 0.054
Batch: 280 | Loss: 1.219 | Acc: 73.176,91.067,95.440,% | Adaptive Acc: 85.904% | clf_exit: 0.707 0.240 0.053
Batch: 300 | Loss: 1.220 | Acc: 73.225,91.061,95.447,% | Adaptive Acc: 85.971% | clf_exit: 0.707 0.240 0.053
Batch: 320 | Loss: 1.223 | Acc: 73.243,90.983,95.395,% | Adaptive Acc: 85.950% | clf_exit: 0.706 0.240 0.053
Batch: 340 | Loss: 1.227 | Acc: 73.211,90.909,95.333,% | Adaptive Acc: 85.928% | clf_exit: 0.706 0.241 0.054
Batch: 360 | Loss: 1.229 | Acc: 73.186,90.841,95.291,% | Adaptive Acc: 85.836% | clf_exit: 0.706 0.241 0.053
Batch: 380 | Loss: 1.233 | Acc: 73.111,90.783,95.263,% | Adaptive Acc: 85.771% | clf_exit: 0.706 0.240 0.054
Batch: 0 | Loss: 1.559 | Acc: 68.750,88.281,89.844,% | Adaptive Acc: 77.344% | clf_exit: 0.711 0.219 0.070
Batch: 20 | Loss: 1.706 | Acc: 70.312,86.124,88.728,% | Adaptive Acc: 78.906% | clf_exit: 0.765 0.190 0.045
Batch: 40 | Loss: 1.673 | Acc: 70.617,86.319,88.967,% | Adaptive Acc: 79.268% | clf_exit: 0.774 0.185 0.042
Batch: 60 | Loss: 1.652 | Acc: 70.530,86.270,89.152,% | Adaptive Acc: 79.239% | clf_exit: 0.774 0.186 0.040
Train all parameters

Epoch: 64
Batch: 0 | Loss: 1.222 | Acc: 74.219,90.625,96.094,% | Adaptive Acc: 86.719% | clf_exit: 0.703 0.266 0.031
Batch: 20 | Loss: 1.180 | Acc: 73.735,92.299,96.131,% | Adaptive Acc: 86.868% | clf_exit: 0.708 0.243 0.049
Batch: 40 | Loss: 1.187 | Acc: 73.876,91.711,95.922,% | Adaptive Acc: 86.204% | clf_exit: 0.708 0.240 0.051
Batch: 60 | Loss: 1.193 | Acc: 73.438,91.547,95.966,% | Adaptive Acc: 86.155% | clf_exit: 0.704 0.243 0.053
Batch: 80 | Loss: 1.215 | Acc: 72.984,91.291,95.795,% | Adaptive Acc: 85.938% | clf_exit: 0.702 0.245 0.054
Batch: 100 | Loss: 1.217 | Acc: 72.850,91.112,95.699,% | Adaptive Acc: 85.899% | clf_exit: 0.702 0.245 0.053
Batch: 120 | Loss: 1.232 | Acc: 72.643,90.877,95.538,% | Adaptive Acc: 85.550% | clf_exit: 0.704 0.242 0.054
Batch: 140 | Loss: 1.224 | Acc: 72.845,91.052,95.584,% | Adaptive Acc: 85.805% | clf_exit: 0.704 0.242 0.054
Batch: 160 | Loss: 1.225 | Acc: 72.909,90.974,95.511,% | Adaptive Acc: 85.748% | clf_exit: 0.706 0.240 0.054
Batch: 180 | Loss: 1.221 | Acc: 73.040,91.035,95.502,% | Adaptive Acc: 85.804% | clf_exit: 0.706 0.240 0.053
Batch: 200 | Loss: 1.222 | Acc: 73.072,91.045,95.491,% | Adaptive Acc: 85.906% | clf_exit: 0.705 0.241 0.053
Batch: 220 | Loss: 1.223 | Acc: 73.077,91.021,95.440,% | Adaptive Acc: 85.962% | clf_exit: 0.704 0.242 0.053
Batch: 240 | Loss: 1.221 | Acc: 73.146,90.988,95.432,% | Adaptive Acc: 85.950% | clf_exit: 0.704 0.242 0.053
Batch: 260 | Loss: 1.225 | Acc: 73.108,90.942,95.384,% | Adaptive Acc: 85.979% | clf_exit: 0.704 0.243 0.053
Batch: 280 | Loss: 1.225 | Acc: 73.176,90.934,95.343,% | Adaptive Acc: 86.001% | clf_exit: 0.705 0.242 0.053
Batch: 300 | Loss: 1.227 | Acc: 73.199,90.887,95.279,% | Adaptive Acc: 85.989% | clf_exit: 0.705 0.241 0.053
Batch: 320 | Loss: 1.226 | Acc: 73.253,90.885,95.281,% | Adaptive Acc: 85.935% | clf_exit: 0.706 0.241 0.053
Batch: 340 | Loss: 1.229 | Acc: 73.284,90.843,95.253,% | Adaptive Acc: 85.873% | clf_exit: 0.707 0.240 0.053
Batch: 360 | Loss: 1.232 | Acc: 73.295,90.755,95.230,% | Adaptive Acc: 85.836% | clf_exit: 0.707 0.239 0.054
Batch: 380 | Loss: 1.232 | Acc: 73.290,90.775,95.218,% | Adaptive Acc: 85.886% | clf_exit: 0.707 0.239 0.054
Batch: 0 | Loss: 1.574 | Acc: 69.531,85.938,90.625,% | Adaptive Acc: 75.781% | clf_exit: 0.758 0.203 0.039
Batch: 20 | Loss: 1.610 | Acc: 69.643,86.533,89.769,% | Adaptive Acc: 78.646% | clf_exit: 0.769 0.185 0.047
Batch: 40 | Loss: 1.579 | Acc: 70.122,87.138,89.748,% | Adaptive Acc: 78.849% | clf_exit: 0.777 0.178 0.045
Batch: 60 | Loss: 1.572 | Acc: 70.261,87.282,89.895,% | Adaptive Acc: 79.022% | clf_exit: 0.780 0.179 0.041
Train all parameters

Epoch: 65
Batch: 0 | Loss: 1.376 | Acc: 72.656,90.625,93.750,% | Adaptive Acc: 82.812% | clf_exit: 0.695 0.227 0.078
Batch: 20 | Loss: 1.231 | Acc: 71.949,91.109,95.945,% | Adaptive Acc: 85.863% | clf_exit: 0.717 0.233 0.050
Batch: 40 | Loss: 1.212 | Acc: 72.428,91.502,95.865,% | Adaptive Acc: 85.995% | clf_exit: 0.713 0.235 0.052
Batch: 60 | Loss: 1.209 | Acc: 72.900,91.611,95.684,% | Adaptive Acc: 86.040% | clf_exit: 0.713 0.234 0.053
Batch: 80 | Loss: 1.206 | Acc: 73.129,91.647,95.669,% | Adaptive Acc: 86.179% | clf_exit: 0.712 0.234 0.054
Batch: 100 | Loss: 1.204 | Acc: 73.337,91.654,95.792,% | Adaptive Acc: 86.324% | clf_exit: 0.709 0.238 0.053
Batch: 120 | Loss: 1.196 | Acc: 73.405,91.729,95.764,% | Adaptive Acc: 86.222% | clf_exit: 0.710 0.237 0.052
Batch: 140 | Loss: 1.191 | Acc: 73.449,91.656,95.689,% | Adaptive Acc: 86.215% | clf_exit: 0.711 0.237 0.052
Batch: 160 | Loss: 1.199 | Acc: 73.447,91.494,95.613,% | Adaptive Acc: 86.195% | clf_exit: 0.710 0.238 0.051
Batch: 180 | Loss: 1.205 | Acc: 73.351,91.415,95.610,% | Adaptive Acc: 86.106% | clf_exit: 0.710 0.239 0.052
Batch: 200 | Loss: 1.209 | Acc: 73.123,91.356,95.616,% | Adaptive Acc: 85.969% | clf_exit: 0.708 0.240 0.053
Batch: 220 | Loss: 1.209 | Acc: 73.123,91.328,95.652,% | Adaptive Acc: 85.934% | clf_exit: 0.708 0.240 0.052
Batch: 240 | Loss: 1.209 | Acc: 73.165,91.361,95.650,% | Adaptive Acc: 85.986% | clf_exit: 0.708 0.240 0.052
Batch: 260 | Loss: 1.213 | Acc: 73.072,91.272,95.597,% | Adaptive Acc: 85.881% | clf_exit: 0.708 0.239 0.052
Batch: 280 | Loss: 1.212 | Acc: 73.121,91.315,95.527,% | Adaptive Acc: 85.938% | clf_exit: 0.708 0.240 0.052
Batch: 300 | Loss: 1.215 | Acc: 73.061,91.238,95.525,% | Adaptive Acc: 85.899% | clf_exit: 0.707 0.241 0.052
Batch: 320 | Loss: 1.214 | Acc: 73.126,91.246,95.532,% | Adaptive Acc: 85.933% | clf_exit: 0.708 0.239 0.053
Batch: 340 | Loss: 1.217 | Acc: 73.092,91.122,95.471,% | Adaptive Acc: 85.830% | clf_exit: 0.708 0.240 0.053
Batch: 360 | Loss: 1.219 | Acc: 73.117,91.051,95.412,% | Adaptive Acc: 85.777% | clf_exit: 0.709 0.238 0.053
Batch: 380 | Loss: 1.218 | Acc: 73.167,91.068,95.384,% | Adaptive Acc: 85.765% | clf_exit: 0.709 0.238 0.053
Batch: 0 | Loss: 1.530 | Acc: 71.094,85.156,89.062,% | Adaptive Acc: 80.469% | clf_exit: 0.703 0.219 0.078
Batch: 20 | Loss: 1.619 | Acc: 71.205,84.524,88.914,% | Adaptive Acc: 80.283% | clf_exit: 0.734 0.211 0.055
Batch: 40 | Loss: 1.597 | Acc: 71.608,85.118,89.329,% | Adaptive Acc: 80.716% | clf_exit: 0.739 0.209 0.053
Batch: 60 | Loss: 1.580 | Acc: 71.670,85.720,89.191,% | Adaptive Acc: 81.084% | clf_exit: 0.737 0.211 0.052
Train all parameters

Epoch: 66
Batch: 0 | Loss: 1.008 | Acc: 75.781,92.969,96.094,% | Adaptive Acc: 85.938% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.143 | Acc: 73.810,92.262,96.540,% | Adaptive Acc: 86.942% | clf_exit: 0.721 0.230 0.049
Batch: 40 | Loss: 1.143 | Acc: 74.371,91.940,96.799,% | Adaptive Acc: 87.062% | clf_exit: 0.722 0.230 0.048
Batch: 60 | Loss: 1.153 | Acc: 74.321,91.688,96.414,% | Adaptive Acc: 86.655% | clf_exit: 0.720 0.231 0.049
Batch: 80 | Loss: 1.156 | Acc: 74.267,91.802,96.325,% | Adaptive Acc: 86.593% | clf_exit: 0.720 0.231 0.049
Batch: 100 | Loss: 1.161 | Acc: 74.149,91.716,96.287,% | Adaptive Acc: 86.324% | clf_exit: 0.720 0.231 0.048
Batch: 120 | Loss: 1.157 | Acc: 74.270,91.761,96.333,% | Adaptive Acc: 86.325% | clf_exit: 0.721 0.231 0.047
Batch: 140 | Loss: 1.159 | Acc: 74.280,91.722,96.282,% | Adaptive Acc: 86.403% | clf_exit: 0.722 0.230 0.048
Batch: 160 | Loss: 1.169 | Acc: 74.068,91.547,96.176,% | Adaptive Acc: 86.243% | clf_exit: 0.720 0.231 0.049
Batch: 180 | Loss: 1.166 | Acc: 74.132,91.579,96.146,% | Adaptive Acc: 86.343% | clf_exit: 0.718 0.233 0.049
Batch: 200 | Loss: 1.165 | Acc: 74.164,91.569,96.144,% | Adaptive Acc: 86.334% | clf_exit: 0.718 0.233 0.049
Batch: 220 | Loss: 1.164 | Acc: 74.148,91.537,96.115,% | Adaptive Acc: 86.280% | clf_exit: 0.720 0.232 0.049
Batch: 240 | Loss: 1.166 | Acc: 74.180,91.555,96.087,% | Adaptive Acc: 86.291% | clf_exit: 0.719 0.233 0.048
Batch: 260 | Loss: 1.170 | Acc: 74.075,91.517,96.055,% | Adaptive Acc: 86.276% | clf_exit: 0.718 0.234 0.048
Batch: 280 | Loss: 1.175 | Acc: 73.952,91.467,95.985,% | Adaptive Acc: 86.143% | clf_exit: 0.718 0.234 0.048
Batch: 300 | Loss: 1.177 | Acc: 73.915,91.406,95.951,% | Adaptive Acc: 86.057% | clf_exit: 0.718 0.234 0.048
Batch: 320 | Loss: 1.182 | Acc: 73.868,91.355,95.863,% | Adaptive Acc: 86.020% | clf_exit: 0.717 0.234 0.048
Batch: 340 | Loss: 1.188 | Acc: 73.813,91.287,95.817,% | Adaptive Acc: 86.018% | clf_exit: 0.717 0.234 0.049
Batch: 360 | Loss: 1.189 | Acc: 73.801,91.287,95.784,% | Adaptive Acc: 85.989% | clf_exit: 0.717 0.234 0.049
Batch: 380 | Loss: 1.192 | Acc: 73.805,91.255,95.749,% | Adaptive Acc: 85.987% | clf_exit: 0.717 0.234 0.049
Batch: 0 | Loss: 1.616 | Acc: 65.625,88.281,88.281,% | Adaptive Acc: 71.875% | clf_exit: 0.758 0.195 0.047
Batch: 20 | Loss: 1.732 | Acc: 66.369,86.496,89.658,% | Adaptive Acc: 77.344% | clf_exit: 0.731 0.225 0.044
Batch: 40 | Loss: 1.686 | Acc: 66.940,86.471,90.130,% | Adaptive Acc: 77.858% | clf_exit: 0.739 0.218 0.043
Batch: 60 | Loss: 1.679 | Acc: 66.688,86.488,90.228,% | Adaptive Acc: 77.907% | clf_exit: 0.733 0.226 0.041
Train all parameters

Epoch: 67
Batch: 0 | Loss: 1.073 | Acc: 75.781,94.531,96.094,% | Adaptive Acc: 84.375% | clf_exit: 0.781 0.180 0.039
Batch: 20 | Loss: 1.191 | Acc: 73.586,91.815,95.833,% | Adaptive Acc: 85.938% | clf_exit: 0.717 0.232 0.051
Batch: 40 | Loss: 1.181 | Acc: 73.723,91.635,96.170,% | Adaptive Acc: 86.071% | clf_exit: 0.719 0.233 0.049
Batch: 60 | Loss: 1.174 | Acc: 73.630,91.406,96.376,% | Adaptive Acc: 86.206% | clf_exit: 0.715 0.237 0.048
Batch: 80 | Loss: 1.173 | Acc: 73.534,91.348,96.383,% | Adaptive Acc: 86.265% | clf_exit: 0.716 0.235 0.049
Batch: 100 | Loss: 1.166 | Acc: 73.824,91.453,96.388,% | Adaptive Acc: 86.378% | clf_exit: 0.717 0.236 0.047
Batch: 120 | Loss: 1.170 | Acc: 73.709,91.516,96.371,% | Adaptive Acc: 86.441% | clf_exit: 0.714 0.239 0.048
Batch: 140 | Loss: 1.174 | Acc: 73.582,91.473,96.282,% | Adaptive Acc: 86.309% | clf_exit: 0.714 0.238 0.048
Batch: 160 | Loss: 1.170 | Acc: 73.748,91.498,96.191,% | Adaptive Acc: 86.282% | clf_exit: 0.716 0.237 0.047
Batch: 180 | Loss: 1.174 | Acc: 73.653,91.510,96.133,% | Adaptive Acc: 86.192% | clf_exit: 0.716 0.238 0.047
Batch: 200 | Loss: 1.177 | Acc: 73.624,91.519,96.051,% | Adaptive Acc: 86.190% | clf_exit: 0.715 0.238 0.047
Batch: 220 | Loss: 1.180 | Acc: 73.568,91.480,95.956,% | Adaptive Acc: 86.111% | clf_exit: 0.715 0.237 0.047
Batch: 240 | Loss: 1.183 | Acc: 73.629,91.380,95.883,% | Adaptive Acc: 86.083% | clf_exit: 0.716 0.237 0.047
Batch: 260 | Loss: 1.186 | Acc: 73.632,91.343,95.875,% | Adaptive Acc: 86.066% | clf_exit: 0.717 0.235 0.048
Batch: 280 | Loss: 1.186 | Acc: 73.691,91.295,95.838,% | Adaptive Acc: 86.051% | clf_exit: 0.718 0.234 0.048
Batch: 300 | Loss: 1.186 | Acc: 73.767,91.310,95.845,% | Adaptive Acc: 86.109% | clf_exit: 0.718 0.234 0.048
Batch: 320 | Loss: 1.183 | Acc: 73.773,91.370,95.833,% | Adaptive Acc: 86.108% | clf_exit: 0.719 0.233 0.048
Batch: 340 | Loss: 1.184 | Acc: 73.804,91.356,95.837,% | Adaptive Acc: 86.153% | clf_exit: 0.717 0.234 0.049
Batch: 360 | Loss: 1.187 | Acc: 73.756,91.300,95.838,% | Adaptive Acc: 86.080% | clf_exit: 0.717 0.234 0.049
Batch: 380 | Loss: 1.190 | Acc: 73.667,91.273,95.786,% | Adaptive Acc: 86.077% | clf_exit: 0.716 0.234 0.050
Batch: 0 | Loss: 1.489 | Acc: 71.875,86.719,88.281,% | Adaptive Acc: 78.125% | clf_exit: 0.781 0.164 0.055
Batch: 20 | Loss: 1.589 | Acc: 72.433,86.049,90.216,% | Adaptive Acc: 80.506% | clf_exit: 0.763 0.194 0.044
Batch: 40 | Loss: 1.588 | Acc: 72.809,85.499,89.939,% | Adaptive Acc: 80.431% | clf_exit: 0.770 0.185 0.045
Batch: 60 | Loss: 1.589 | Acc: 72.861,85.169,89.690,% | Adaptive Acc: 80.533% | clf_exit: 0.768 0.187 0.045
Train all parameters

Epoch: 68
Batch: 0 | Loss: 1.086 | Acc: 71.875,92.969,95.312,% | Adaptive Acc: 85.156% | clf_exit: 0.711 0.266 0.023
Batch: 20 | Loss: 1.187 | Acc: 73.140,91.667,96.168,% | Adaptive Acc: 86.161% | clf_exit: 0.709 0.239 0.052
Batch: 40 | Loss: 1.158 | Acc: 73.704,91.825,96.380,% | Adaptive Acc: 86.528% | clf_exit: 0.712 0.238 0.050
Batch: 60 | Loss: 1.158 | Acc: 73.591,91.829,96.196,% | Adaptive Acc: 86.142% | clf_exit: 0.712 0.238 0.050
Batch: 80 | Loss: 1.156 | Acc: 73.611,91.599,96.306,% | Adaptive Acc: 86.101% | clf_exit: 0.713 0.237 0.049
Batch: 100 | Loss: 1.155 | Acc: 73.708,91.700,96.295,% | Adaptive Acc: 86.262% | clf_exit: 0.713 0.237 0.050
Batch: 120 | Loss: 1.153 | Acc: 73.786,91.742,96.391,% | Adaptive Acc: 86.325% | clf_exit: 0.713 0.239 0.048
Batch: 140 | Loss: 1.141 | Acc: 74.053,91.922,96.454,% | Adaptive Acc: 86.480% | clf_exit: 0.715 0.238 0.047
Batch: 160 | Loss: 1.152 | Acc: 74.039,91.688,96.385,% | Adaptive Acc: 86.452% | clf_exit: 0.713 0.239 0.048
Batch: 180 | Loss: 1.159 | Acc: 74.020,91.644,96.310,% | Adaptive Acc: 86.365% | clf_exit: 0.714 0.239 0.048
Batch: 200 | Loss: 1.169 | Acc: 73.838,91.562,96.249,% | Adaptive Acc: 86.248% | clf_exit: 0.713 0.239 0.048
Batch: 220 | Loss: 1.170 | Acc: 73.734,91.541,96.214,% | Adaptive Acc: 86.185% | clf_exit: 0.713 0.239 0.048
Batch: 240 | Loss: 1.173 | Acc: 73.736,91.510,96.165,% | Adaptive Acc: 86.255% | clf_exit: 0.713 0.239 0.048
Batch: 260 | Loss: 1.177 | Acc: 73.638,91.445,96.088,% | Adaptive Acc: 86.165% | clf_exit: 0.713 0.239 0.049
Batch: 280 | Loss: 1.177 | Acc: 73.721,91.431,96.097,% | Adaptive Acc: 86.174% | clf_exit: 0.713 0.238 0.049
Batch: 300 | Loss: 1.176 | Acc: 73.765,91.419,96.089,% | Adaptive Acc: 86.205% | clf_exit: 0.713 0.238 0.049
Batch: 320 | Loss: 1.180 | Acc: 73.768,91.399,95.999,% | Adaptive Acc: 86.234% | clf_exit: 0.713 0.238 0.049
Batch: 340 | Loss: 1.183 | Acc: 73.728,91.335,95.894,% | Adaptive Acc: 86.190% | clf_exit: 0.713 0.238 0.049
Batch: 360 | Loss: 1.186 | Acc: 73.632,91.263,95.854,% | Adaptive Acc: 86.139% | clf_exit: 0.713 0.238 0.049
Batch: 380 | Loss: 1.187 | Acc: 73.604,91.298,95.839,% | Adaptive Acc: 86.192% | clf_exit: 0.713 0.238 0.049
Batch: 0 | Loss: 1.437 | Acc: 70.312,86.719,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.734 0.227 0.039
Batch: 20 | Loss: 1.629 | Acc: 69.420,86.644,89.509,% | Adaptive Acc: 78.385% | clf_exit: 0.757 0.205 0.038
Batch: 40 | Loss: 1.622 | Acc: 69.836,86.757,89.691,% | Adaptive Acc: 78.830% | clf_exit: 0.761 0.203 0.037
Batch: 60 | Loss: 1.610 | Acc: 69.185,86.860,89.908,% | Adaptive Acc: 78.573% | clf_exit: 0.760 0.203 0.037
Train all parameters

Epoch: 69
Batch: 0 | Loss: 1.038 | Acc: 76.562,95.312,96.875,% | Adaptive Acc: 90.625% | clf_exit: 0.711 0.258 0.031
Batch: 20 | Loss: 1.100 | Acc: 75.074,92.597,96.801,% | Adaptive Acc: 86.756% | clf_exit: 0.724 0.231 0.045
Batch: 40 | Loss: 1.103 | Acc: 75.191,92.683,96.818,% | Adaptive Acc: 86.871% | clf_exit: 0.731 0.224 0.045
Batch: 60 | Loss: 1.111 | Acc: 74.846,92.572,96.785,% | Adaptive Acc: 86.642% | clf_exit: 0.730 0.226 0.044
Batch: 80 | Loss: 1.127 | Acc: 74.296,92.245,96.624,% | Adaptive Acc: 86.208% | clf_exit: 0.728 0.228 0.044
Batch: 100 | Loss: 1.135 | Acc: 74.250,92.079,96.426,% | Adaptive Acc: 86.440% | clf_exit: 0.726 0.228 0.046
Batch: 120 | Loss: 1.140 | Acc: 74.057,92.071,96.404,% | Adaptive Acc: 86.570% | clf_exit: 0.722 0.230 0.048
Batch: 140 | Loss: 1.139 | Acc: 74.230,92.032,96.293,% | Adaptive Acc: 86.442% | clf_exit: 0.724 0.230 0.046
Batch: 160 | Loss: 1.141 | Acc: 74.194,92.018,96.210,% | Adaptive Acc: 86.350% | clf_exit: 0.725 0.229 0.046
Batch: 180 | Loss: 1.150 | Acc: 74.055,91.877,96.128,% | Adaptive Acc: 86.253% | clf_exit: 0.724 0.229 0.047
Batch: 200 | Loss: 1.152 | Acc: 74.036,91.822,96.067,% | Adaptive Acc: 86.264% | clf_exit: 0.725 0.229 0.047
Batch: 220 | Loss: 1.154 | Acc: 73.989,91.781,96.023,% | Adaptive Acc: 86.227% | clf_exit: 0.725 0.228 0.047
Batch: 240 | Loss: 1.156 | Acc: 73.940,91.779,95.990,% | Adaptive Acc: 86.190% | clf_exit: 0.724 0.229 0.047
Batch: 260 | Loss: 1.160 | Acc: 73.940,91.736,95.941,% | Adaptive Acc: 86.156% | clf_exit: 0.724 0.228 0.048
Batch: 280 | Loss: 1.161 | Acc: 73.910,91.757,95.899,% | Adaptive Acc: 86.218% | clf_exit: 0.723 0.229 0.048
Batch: 300 | Loss: 1.161 | Acc: 73.863,91.770,95.920,% | Adaptive Acc: 86.200% | clf_exit: 0.723 0.229 0.048
Batch: 320 | Loss: 1.164 | Acc: 73.795,91.735,95.889,% | Adaptive Acc: 86.137% | clf_exit: 0.722 0.230 0.048
Batch: 340 | Loss: 1.167 | Acc: 73.767,91.690,95.885,% | Adaptive Acc: 86.148% | clf_exit: 0.722 0.230 0.048
Batch: 360 | Loss: 1.167 | Acc: 73.782,91.675,95.884,% | Adaptive Acc: 86.113% | clf_exit: 0.722 0.230 0.048
Batch: 380 | Loss: 1.171 | Acc: 73.716,91.638,95.846,% | Adaptive Acc: 86.044% | clf_exit: 0.722 0.230 0.048
Batch: 0 | Loss: 1.600 | Acc: 75.781,85.938,89.844,% | Adaptive Acc: 78.125% | clf_exit: 0.773 0.172 0.055
Batch: 20 | Loss: 1.750 | Acc: 68.713,85.603,88.281,% | Adaptive Acc: 77.939% | clf_exit: 0.743 0.208 0.049
Batch: 40 | Loss: 1.714 | Acc: 69.226,85.918,88.681,% | Adaptive Acc: 78.506% | clf_exit: 0.755 0.197 0.048
Batch: 60 | Loss: 1.697 | Acc: 69.262,85.963,88.755,% | Adaptive Acc: 78.560% | clf_exit: 0.753 0.201 0.047
Train all parameters

Epoch: 70
Batch: 0 | Loss: 1.190 | Acc: 75.781,90.625,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.656 0.266 0.078
Batch: 20 | Loss: 1.209 | Acc: 73.586,91.183,95.312,% | Adaptive Acc: 85.938% | clf_exit: 0.706 0.244 0.049
Batch: 40 | Loss: 1.157 | Acc: 73.742,91.787,95.865,% | Adaptive Acc: 86.223% | clf_exit: 0.720 0.230 0.050
Batch: 60 | Loss: 1.146 | Acc: 73.924,92.047,96.004,% | Adaptive Acc: 86.501% | clf_exit: 0.715 0.236 0.049
Batch: 80 | Loss: 1.154 | Acc: 73.978,91.860,95.978,% | Adaptive Acc: 86.535% | clf_exit: 0.715 0.236 0.049
Batch: 100 | Loss: 1.149 | Acc: 74.180,91.747,96.086,% | Adaptive Acc: 86.440% | clf_exit: 0.720 0.231 0.049
Batch: 120 | Loss: 1.153 | Acc: 73.999,91.826,96.132,% | Adaptive Acc: 86.254% | clf_exit: 0.720 0.231 0.049
Batch: 140 | Loss: 1.149 | Acc: 74.219,91.777,96.133,% | Adaptive Acc: 86.298% | clf_exit: 0.723 0.229 0.048
Batch: 160 | Loss: 1.152 | Acc: 74.136,91.741,96.128,% | Adaptive Acc: 86.282% | clf_exit: 0.723 0.229 0.048
Batch: 180 | Loss: 1.154 | Acc: 74.197,91.674,96.033,% | Adaptive Acc: 86.209% | clf_exit: 0.722 0.229 0.048
Batch: 200 | Loss: 1.162 | Acc: 73.986,91.628,95.954,% | Adaptive Acc: 86.070% | clf_exit: 0.722 0.230 0.048
Batch: 220 | Loss: 1.164 | Acc: 74.070,91.608,95.931,% | Adaptive Acc: 86.114% | clf_exit: 0.722 0.230 0.048
Batch: 240 | Loss: 1.162 | Acc: 74.073,91.604,95.954,% | Adaptive Acc: 86.135% | clf_exit: 0.722 0.230 0.048
Batch: 260 | Loss: 1.160 | Acc: 74.099,91.616,95.983,% | Adaptive Acc: 86.189% | clf_exit: 0.722 0.230 0.048
Batch: 280 | Loss: 1.167 | Acc: 73.980,91.506,95.896,% | Adaptive Acc: 86.174% | clf_exit: 0.721 0.230 0.048
Batch: 300 | Loss: 1.167 | Acc: 74.040,91.456,95.881,% | Adaptive Acc: 86.179% | clf_exit: 0.722 0.230 0.048
Batch: 320 | Loss: 1.169 | Acc: 73.975,91.467,95.860,% | Adaptive Acc: 86.101% | clf_exit: 0.722 0.230 0.048
Batch: 340 | Loss: 1.170 | Acc: 73.974,91.470,95.842,% | Adaptive Acc: 86.091% | clf_exit: 0.722 0.230 0.048
Batch: 360 | Loss: 1.171 | Acc: 73.940,91.491,95.828,% | Adaptive Acc: 86.035% | clf_exit: 0.722 0.230 0.048
Batch: 380 | Loss: 1.174 | Acc: 73.831,91.425,95.827,% | Adaptive Acc: 86.022% | clf_exit: 0.721 0.230 0.049
Batch: 0 | Loss: 1.536 | Acc: 73.438,90.625,87.500,% | Adaptive Acc: 80.469% | clf_exit: 0.773 0.172 0.055
Batch: 20 | Loss: 1.672 | Acc: 71.205,86.086,88.393,% | Adaptive Acc: 79.725% | clf_exit: 0.779 0.178 0.043
Batch: 40 | Loss: 1.618 | Acc: 71.989,86.185,88.834,% | Adaptive Acc: 79.821% | clf_exit: 0.783 0.175 0.042
Batch: 60 | Loss: 1.595 | Acc: 71.811,86.411,89.139,% | Adaptive Acc: 79.854% | clf_exit: 0.783 0.176 0.041
Train all parameters

Epoch: 71
Batch: 0 | Loss: 1.224 | Acc: 75.000,91.406,95.312,% | Adaptive Acc: 85.938% | clf_exit: 0.758 0.203 0.039
Batch: 20 | Loss: 1.071 | Acc: 75.000,93.638,96.875,% | Adaptive Acc: 87.872% | clf_exit: 0.720 0.232 0.048
Batch: 40 | Loss: 1.105 | Acc: 74.352,93.007,96.494,% | Adaptive Acc: 87.176% | clf_exit: 0.724 0.229 0.047
Batch: 60 | Loss: 1.124 | Acc: 73.963,92.469,96.350,% | Adaptive Acc: 86.744% | clf_exit: 0.724 0.230 0.046
Batch: 80 | Loss: 1.123 | Acc: 74.286,92.313,96.402,% | Adaptive Acc: 86.902% | clf_exit: 0.725 0.228 0.047
Batch: 100 | Loss: 1.139 | Acc: 73.933,92.188,96.272,% | Adaptive Acc: 86.703% | clf_exit: 0.723 0.230 0.047
Batch: 120 | Loss: 1.150 | Acc: 73.902,92.071,96.165,% | Adaptive Acc: 86.648% | clf_exit: 0.719 0.233 0.048
Batch: 140 | Loss: 1.156 | Acc: 73.798,91.977,96.127,% | Adaptive Acc: 86.508% | clf_exit: 0.720 0.232 0.049
Batch: 160 | Loss: 1.155 | Acc: 73.860,91.940,96.123,% | Adaptive Acc: 86.525% | clf_exit: 0.720 0.232 0.048
Batch: 180 | Loss: 1.159 | Acc: 73.873,91.907,96.055,% | Adaptive Acc: 86.512% | clf_exit: 0.721 0.232 0.047
Batch: 200 | Loss: 1.159 | Acc: 73.931,91.853,96.012,% | Adaptive Acc: 86.540% | clf_exit: 0.721 0.231 0.048
Batch: 220 | Loss: 1.159 | Acc: 73.947,91.795,95.938,% | Adaptive Acc: 86.510% | clf_exit: 0.721 0.231 0.048
Batch: 240 | Loss: 1.160 | Acc: 73.992,91.721,95.971,% | Adaptive Acc: 86.476% | clf_exit: 0.722 0.230 0.048
Batch: 260 | Loss: 1.162 | Acc: 73.964,91.715,95.959,% | Adaptive Acc: 86.443% | clf_exit: 0.721 0.230 0.048
Batch: 280 | Loss: 1.160 | Acc: 73.982,91.743,95.930,% | Adaptive Acc: 86.407% | clf_exit: 0.723 0.229 0.048
Batch: 300 | Loss: 1.158 | Acc: 73.996,91.744,95.954,% | Adaptive Acc: 86.412% | clf_exit: 0.724 0.229 0.048
Batch: 320 | Loss: 1.158 | Acc: 74.078,91.737,95.931,% | Adaptive Acc: 86.483% | clf_exit: 0.724 0.229 0.048
Batch: 340 | Loss: 1.158 | Acc: 74.077,91.693,95.940,% | Adaptive Acc: 86.448% | clf_exit: 0.724 0.228 0.048
Batch: 360 | Loss: 1.160 | Acc: 74.080,91.666,95.931,% | Adaptive Acc: 86.437% | clf_exit: 0.724 0.229 0.048
Batch: 380 | Loss: 1.161 | Acc: 74.071,91.658,95.909,% | Adaptive Acc: 86.430% | clf_exit: 0.723 0.229 0.048
Batch: 0 | Loss: 1.552 | Acc: 67.188,89.844,87.500,% | Adaptive Acc: 72.656% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 1.670 | Acc: 69.048,85.789,89.844,% | Adaptive Acc: 77.902% | clf_exit: 0.792 0.171 0.037
Batch: 40 | Loss: 1.656 | Acc: 69.169,86.414,90.111,% | Adaptive Acc: 78.144% | clf_exit: 0.794 0.169 0.038
Batch: 60 | Loss: 1.655 | Acc: 68.916,86.206,90.010,% | Adaptive Acc: 78.087% | clf_exit: 0.788 0.175 0.037
Train all parameters

Epoch: 72
Batch: 0 | Loss: 0.999 | Acc: 79.688,94.531,96.875,% | Adaptive Acc: 91.406% | clf_exit: 0.758 0.211 0.031
Batch: 20 | Loss: 1.133 | Acc: 75.409,92.001,95.722,% | Adaptive Acc: 88.244% | clf_exit: 0.716 0.235 0.049
Batch: 40 | Loss: 1.157 | Acc: 74.562,91.997,95.808,% | Adaptive Acc: 87.176% | clf_exit: 0.717 0.234 0.049
Batch: 60 | Loss: 1.167 | Acc: 74.449,92.021,95.876,% | Adaptive Acc: 86.591% | clf_exit: 0.721 0.230 0.049
Batch: 80 | Loss: 1.173 | Acc: 73.698,91.927,95.891,% | Adaptive Acc: 86.507% | clf_exit: 0.715 0.236 0.049
Batch: 100 | Loss: 1.165 | Acc: 73.894,92.048,96.016,% | Adaptive Acc: 86.541% | clf_exit: 0.714 0.237 0.049
Batch: 120 | Loss: 1.157 | Acc: 74.122,92.033,96.068,% | Adaptive Acc: 86.635% | clf_exit: 0.718 0.234 0.048
Batch: 140 | Loss: 1.159 | Acc: 74.047,92.005,96.110,% | Adaptive Acc: 86.525% | clf_exit: 0.718 0.233 0.049
Batch: 160 | Loss: 1.159 | Acc: 74.054,91.969,96.152,% | Adaptive Acc: 86.491% | clf_exit: 0.720 0.232 0.048
Batch: 180 | Loss: 1.157 | Acc: 74.150,91.881,96.120,% | Adaptive Acc: 86.494% | clf_exit: 0.720 0.232 0.048
Batch: 200 | Loss: 1.154 | Acc: 74.296,91.818,96.152,% | Adaptive Acc: 86.594% | clf_exit: 0.721 0.231 0.048
Batch: 220 | Loss: 1.149 | Acc: 74.321,91.841,96.235,% | Adaptive Acc: 86.581% | clf_exit: 0.722 0.230 0.048
Batch: 240 | Loss: 1.153 | Acc: 74.209,91.828,96.136,% | Adaptive Acc: 86.553% | clf_exit: 0.721 0.231 0.048
Batch: 260 | Loss: 1.152 | Acc: 74.258,91.867,96.133,% | Adaptive Acc: 86.557% | clf_exit: 0.722 0.230 0.048
Batch: 280 | Loss: 1.155 | Acc: 74.208,91.848,96.141,% | Adaptive Acc: 86.477% | clf_exit: 0.723 0.230 0.047
Batch: 300 | Loss: 1.156 | Acc: 74.203,91.814,96.135,% | Adaptive Acc: 86.412% | clf_exit: 0.723 0.229 0.047
Batch: 320 | Loss: 1.159 | Acc: 74.119,91.742,96.096,% | Adaptive Acc: 86.373% | clf_exit: 0.723 0.229 0.048
Batch: 340 | Loss: 1.159 | Acc: 74.061,91.766,96.071,% | Adaptive Acc: 86.400% | clf_exit: 0.723 0.230 0.048
Batch: 360 | Loss: 1.159 | Acc: 74.007,91.750,96.061,% | Adaptive Acc: 86.351% | clf_exit: 0.723 0.229 0.048
Batch: 380 | Loss: 1.158 | Acc: 73.993,91.769,96.053,% | Adaptive Acc: 86.341% | clf_exit: 0.723 0.229 0.048
Batch: 0 | Loss: 1.437 | Acc: 69.531,85.938,89.844,% | Adaptive Acc: 78.906% | clf_exit: 0.742 0.195 0.062
Batch: 20 | Loss: 1.598 | Acc: 70.015,86.682,90.327,% | Adaptive Acc: 79.874% | clf_exit: 0.765 0.191 0.045
Batch: 40 | Loss: 1.579 | Acc: 71.265,86.623,90.225,% | Adaptive Acc: 80.202% | clf_exit: 0.774 0.183 0.043
Batch: 60 | Loss: 1.592 | Acc: 71.068,86.142,90.100,% | Adaptive Acc: 80.097% | clf_exit: 0.772 0.185 0.043
Train all parameters

Epoch: 73
Batch: 0 | Loss: 1.114 | Acc: 72.656,93.750,97.656,% | Adaptive Acc: 87.500% | clf_exit: 0.688 0.266 0.047
Batch: 20 | Loss: 1.116 | Acc: 76.004,92.374,95.796,% | Adaptive Acc: 87.054% | clf_exit: 0.730 0.223 0.048
Batch: 40 | Loss: 1.124 | Acc: 74.886,92.092,96.075,% | Adaptive Acc: 86.814% | clf_exit: 0.729 0.220 0.050
Batch: 60 | Loss: 1.120 | Acc: 74.872,92.252,96.311,% | Adaptive Acc: 86.860% | clf_exit: 0.730 0.222 0.048
Batch: 80 | Loss: 1.127 | Acc: 74.711,92.284,96.277,% | Adaptive Acc: 86.738% | clf_exit: 0.728 0.224 0.048
Batch: 100 | Loss: 1.145 | Acc: 74.288,91.979,96.132,% | Adaptive Acc: 86.363% | clf_exit: 0.727 0.225 0.047
Batch: 120 | Loss: 1.146 | Acc: 74.264,92.026,96.094,% | Adaptive Acc: 86.351% | clf_exit: 0.725 0.228 0.047
Batch: 140 | Loss: 1.149 | Acc: 74.186,91.988,96.038,% | Adaptive Acc: 86.287% | clf_exit: 0.725 0.228 0.047
Batch: 160 | Loss: 1.148 | Acc: 74.170,91.989,96.040,% | Adaptive Acc: 86.340% | clf_exit: 0.723 0.230 0.046
Batch: 180 | Loss: 1.151 | Acc: 74.042,91.954,96.115,% | Adaptive Acc: 86.283% | clf_exit: 0.722 0.231 0.047
Batch: 200 | Loss: 1.145 | Acc: 74.176,92.032,96.241,% | Adaptive Acc: 86.454% | clf_exit: 0.722 0.232 0.046
Batch: 220 | Loss: 1.146 | Acc: 74.109,91.997,96.260,% | Adaptive Acc: 86.439% | clf_exit: 0.722 0.231 0.046
Batch: 240 | Loss: 1.140 | Acc: 74.251,92.016,96.304,% | Adaptive Acc: 86.511% | clf_exit: 0.723 0.231 0.046
Batch: 260 | Loss: 1.142 | Acc: 74.246,91.993,96.303,% | Adaptive Acc: 86.521% | clf_exit: 0.723 0.231 0.046
Batch: 280 | Loss: 1.145 | Acc: 74.244,91.962,96.238,% | Adaptive Acc: 86.541% | clf_exit: 0.722 0.232 0.046
Batch: 300 | Loss: 1.146 | Acc: 74.214,91.956,96.203,% | Adaptive Acc: 86.490% | clf_exit: 0.723 0.232 0.046
Batch: 320 | Loss: 1.146 | Acc: 74.250,91.944,96.150,% | Adaptive Acc: 86.495% | clf_exit: 0.723 0.232 0.045
Batch: 340 | Loss: 1.150 | Acc: 74.173,91.880,96.101,% | Adaptive Acc: 86.361% | clf_exit: 0.723 0.231 0.046
Batch: 360 | Loss: 1.150 | Acc: 74.286,91.876,96.055,% | Adaptive Acc: 86.403% | clf_exit: 0.724 0.230 0.046
Batch: 380 | Loss: 1.151 | Acc: 74.323,91.882,96.018,% | Adaptive Acc: 86.401% | clf_exit: 0.724 0.230 0.046
Batch: 0 | Loss: 1.523 | Acc: 71.094,89.844,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.727 0.242 0.031
Batch: 20 | Loss: 1.638 | Acc: 71.652,85.640,88.170,% | Adaptive Acc: 79.241% | clf_exit: 0.759 0.207 0.034
Batch: 40 | Loss: 1.625 | Acc: 72.046,85.747,88.700,% | Adaptive Acc: 79.668% | clf_exit: 0.761 0.203 0.036
Batch: 60 | Loss: 1.610 | Acc: 71.990,85.835,88.960,% | Adaptive Acc: 80.085% | clf_exit: 0.757 0.205 0.038
Train all parameters

Epoch: 74
Batch: 0 | Loss: 1.086 | Acc: 70.312,96.875,97.656,% | Adaptive Acc: 84.375% | clf_exit: 0.703 0.273 0.023
Batch: 20 | Loss: 1.087 | Acc: 75.260,93.229,96.875,% | Adaptive Acc: 87.872% | clf_exit: 0.717 0.236 0.047
Batch: 40 | Loss: 1.122 | Acc: 74.447,92.359,96.551,% | Adaptive Acc: 86.852% | clf_exit: 0.720 0.227 0.053
Batch: 60 | Loss: 1.148 | Acc: 74.103,92.149,96.414,% | Adaptive Acc: 86.514% | clf_exit: 0.715 0.235 0.050
Batch: 80 | Loss: 1.138 | Acc: 73.958,92.303,96.595,% | Adaptive Acc: 86.507% | clf_exit: 0.717 0.236 0.048
Batch: 100 | Loss: 1.135 | Acc: 73.948,92.296,96.550,% | Adaptive Acc: 86.556% | clf_exit: 0.716 0.236 0.047
Batch: 120 | Loss: 1.140 | Acc: 73.728,92.200,96.494,% | Adaptive Acc: 86.454% | clf_exit: 0.716 0.237 0.048
Batch: 140 | Loss: 1.136 | Acc: 73.881,92.298,96.498,% | Adaptive Acc: 86.625% | clf_exit: 0.716 0.237 0.046
Batch: 160 | Loss: 1.136 | Acc: 73.986,92.265,96.530,% | Adaptive Acc: 86.612% | clf_exit: 0.717 0.237 0.046
Batch: 180 | Loss: 1.136 | Acc: 74.150,92.153,96.495,% | Adaptive Acc: 86.637% | clf_exit: 0.717 0.237 0.046
Batch: 200 | Loss: 1.137 | Acc: 74.149,92.106,96.459,% | Adaptive Acc: 86.703% | clf_exit: 0.717 0.236 0.046
Batch: 220 | Loss: 1.137 | Acc: 74.247,92.124,96.430,% | Adaptive Acc: 86.715% | clf_exit: 0.717 0.236 0.046
Batch: 240 | Loss: 1.139 | Acc: 74.271,91.993,96.392,% | Adaptive Acc: 86.664% | clf_exit: 0.718 0.235 0.047
Batch: 260 | Loss: 1.138 | Acc: 74.255,92.032,96.378,% | Adaptive Acc: 86.632% | clf_exit: 0.719 0.234 0.047
Batch: 280 | Loss: 1.140 | Acc: 74.291,92.035,96.333,% | Adaptive Acc: 86.616% | clf_exit: 0.719 0.234 0.048
Batch: 300 | Loss: 1.141 | Acc: 74.208,92.045,96.346,% | Adaptive Acc: 86.555% | clf_exit: 0.719 0.233 0.047
Batch: 320 | Loss: 1.144 | Acc: 74.194,91.993,96.320,% | Adaptive Acc: 86.466% | clf_exit: 0.721 0.232 0.047
Batch: 340 | Loss: 1.147 | Acc: 74.235,91.942,96.284,% | Adaptive Acc: 86.483% | clf_exit: 0.721 0.231 0.048
Batch: 360 | Loss: 1.151 | Acc: 74.195,91.913,96.237,% | Adaptive Acc: 86.470% | clf_exit: 0.720 0.232 0.048
Batch: 380 | Loss: 1.151 | Acc: 74.272,91.864,96.188,% | Adaptive Acc: 86.454% | clf_exit: 0.721 0.232 0.047
Batch: 0 | Loss: 1.585 | Acc: 70.312,84.375,85.156,% | Adaptive Acc: 75.000% | clf_exit: 0.797 0.156 0.047
Batch: 20 | Loss: 1.636 | Acc: 70.015,85.863,89.249,% | Adaptive Acc: 78.199% | clf_exit: 0.813 0.155 0.032
Batch: 40 | Loss: 1.617 | Acc: 70.941,86.071,89.844,% | Adaptive Acc: 78.601% | clf_exit: 0.817 0.152 0.030
Batch: 60 | Loss: 1.608 | Acc: 70.889,86.181,89.869,% | Adaptive Acc: 78.394% | clf_exit: 0.818 0.151 0.032
Train all parameters

Epoch: 75
Batch: 0 | Loss: 1.106 | Acc: 75.781,92.188,96.875,% | Adaptive Acc: 88.281% | clf_exit: 0.719 0.242 0.039
Batch: 20 | Loss: 1.174 | Acc: 74.182,92.039,95.871,% | Adaptive Acc: 86.644% | clf_exit: 0.725 0.229 0.046
Batch: 40 | Loss: 1.170 | Acc: 73.704,91.883,96.113,% | Adaptive Acc: 86.623% | clf_exit: 0.721 0.232 0.046
Batch: 60 | Loss: 1.151 | Acc: 73.796,92.418,96.401,% | Adaptive Acc: 86.885% | clf_exit: 0.727 0.226 0.047
Batch: 80 | Loss: 1.134 | Acc: 74.035,92.583,96.557,% | Adaptive Acc: 87.095% | clf_exit: 0.727 0.227 0.046
Batch: 100 | Loss: 1.141 | Acc: 74.103,92.435,96.388,% | Adaptive Acc: 86.866% | clf_exit: 0.728 0.225 0.046
Batch: 120 | Loss: 1.138 | Acc: 74.186,92.284,96.307,% | Adaptive Acc: 86.848% | clf_exit: 0.728 0.225 0.046
Batch: 140 | Loss: 1.134 | Acc: 74.307,92.282,96.299,% | Adaptive Acc: 86.907% | clf_exit: 0.727 0.227 0.046
Batch: 160 | Loss: 1.135 | Acc: 74.262,92.241,96.249,% | Adaptive Acc: 86.792% | clf_exit: 0.730 0.225 0.046
Batch: 180 | Loss: 1.135 | Acc: 74.158,92.209,96.219,% | Adaptive Acc: 86.758% | clf_exit: 0.728 0.226 0.045
Batch: 200 | Loss: 1.141 | Acc: 73.997,92.145,96.206,% | Adaptive Acc: 86.625% | clf_exit: 0.728 0.226 0.046
Batch: 220 | Loss: 1.143 | Acc: 74.021,92.113,96.133,% | Adaptive Acc: 86.560% | clf_exit: 0.729 0.225 0.046
Batch: 240 | Loss: 1.143 | Acc: 74.002,92.071,96.159,% | Adaptive Acc: 86.570% | clf_exit: 0.728 0.226 0.047
Batch: 260 | Loss: 1.141 | Acc: 74.093,92.038,96.160,% | Adaptive Acc: 86.620% | clf_exit: 0.728 0.226 0.047
Batch: 280 | Loss: 1.139 | Acc: 74.202,92.037,96.152,% | Adaptive Acc: 86.555% | clf_exit: 0.729 0.224 0.046
Batch: 300 | Loss: 1.139 | Acc: 74.250,92.014,96.140,% | Adaptive Acc: 86.514% | clf_exit: 0.729 0.225 0.046
Batch: 320 | Loss: 1.144 | Acc: 74.204,91.971,96.072,% | Adaptive Acc: 86.541% | clf_exit: 0.729 0.225 0.046
Batch: 340 | Loss: 1.145 | Acc: 74.244,91.940,96.011,% | Adaptive Acc: 86.554% | clf_exit: 0.729 0.225 0.046
Batch: 360 | Loss: 1.148 | Acc: 74.214,91.848,95.947,% | Adaptive Acc: 86.481% | clf_exit: 0.728 0.225 0.047
Batch: 380 | Loss: 1.151 | Acc: 74.206,91.794,95.973,% | Adaptive Acc: 86.438% | clf_exit: 0.728 0.226 0.047
Batch: 0 | Loss: 1.316 | Acc: 73.438,93.750,91.406,% | Adaptive Acc: 85.156% | clf_exit: 0.695 0.266 0.039
Batch: 20 | Loss: 1.522 | Acc: 70.647,87.835,90.699,% | Adaptive Acc: 80.357% | clf_exit: 0.756 0.194 0.050
Batch: 40 | Loss: 1.514 | Acc: 71.570,87.557,90.663,% | Adaptive Acc: 80.774% | clf_exit: 0.768 0.184 0.048
Batch: 60 | Loss: 1.491 | Acc: 71.747,87.526,90.868,% | Adaptive Acc: 80.943% | clf_exit: 0.770 0.185 0.045
Train all parameters

Epoch: 76
Batch: 0 | Loss: 1.113 | Acc: 75.000,92.188,96.094,% | Adaptive Acc: 88.281% | clf_exit: 0.719 0.234 0.047
Batch: 20 | Loss: 1.087 | Acc: 75.335,92.374,96.652,% | Adaptive Acc: 87.165% | clf_exit: 0.731 0.222 0.047
Batch: 40 | Loss: 1.107 | Acc: 74.371,92.569,96.684,% | Adaptive Acc: 86.833% | clf_exit: 0.726 0.230 0.044
Batch: 60 | Loss: 1.121 | Acc: 74.321,92.188,96.555,% | Adaptive Acc: 86.527% | clf_exit: 0.724 0.231 0.045
Batch: 80 | Loss: 1.116 | Acc: 74.527,92.110,96.528,% | Adaptive Acc: 86.757% | clf_exit: 0.727 0.227 0.046
Batch: 100 | Loss: 1.128 | Acc: 74.288,91.986,96.303,% | Adaptive Acc: 86.757% | clf_exit: 0.725 0.229 0.046
Batch: 120 | Loss: 1.129 | Acc: 74.374,91.955,96.320,% | Adaptive Acc: 86.829% | clf_exit: 0.725 0.230 0.045
Batch: 140 | Loss: 1.135 | Acc: 74.269,91.866,96.282,% | Adaptive Acc: 86.719% | clf_exit: 0.725 0.229 0.046
Batch: 160 | Loss: 1.129 | Acc: 74.384,92.013,96.332,% | Adaptive Acc: 86.767% | clf_exit: 0.726 0.229 0.045
Batch: 180 | Loss: 1.126 | Acc: 74.456,92.067,96.357,% | Adaptive Acc: 86.706% | clf_exit: 0.727 0.228 0.045
Batch: 200 | Loss: 1.126 | Acc: 74.460,92.048,96.362,% | Adaptive Acc: 86.727% | clf_exit: 0.728 0.227 0.045
Batch: 220 | Loss: 1.130 | Acc: 74.456,92.014,96.292,% | Adaptive Acc: 86.754% | clf_exit: 0.727 0.227 0.046
Batch: 240 | Loss: 1.131 | Acc: 74.407,91.961,96.288,% | Adaptive Acc: 86.612% | clf_exit: 0.728 0.226 0.045
Batch: 260 | Loss: 1.137 | Acc: 74.374,91.786,96.234,% | Adaptive Acc: 86.476% | clf_exit: 0.728 0.227 0.046
Batch: 280 | Loss: 1.139 | Acc: 74.369,91.821,96.249,% | Adaptive Acc: 86.455% | clf_exit: 0.728 0.227 0.045
Batch: 300 | Loss: 1.140 | Acc: 74.372,91.811,96.239,% | Adaptive Acc: 86.462% | clf_exit: 0.728 0.226 0.046
Batch: 320 | Loss: 1.143 | Acc: 74.406,91.764,96.189,% | Adaptive Acc: 86.427% | clf_exit: 0.729 0.226 0.045
Batch: 340 | Loss: 1.143 | Acc: 74.393,91.727,96.167,% | Adaptive Acc: 86.448% | clf_exit: 0.728 0.226 0.046
Batch: 360 | Loss: 1.143 | Acc: 74.414,91.709,96.172,% | Adaptive Acc: 86.481% | clf_exit: 0.728 0.226 0.046
Batch: 380 | Loss: 1.145 | Acc: 74.409,91.689,96.153,% | Adaptive Acc: 86.450% | clf_exit: 0.728 0.226 0.046
Batch: 0 | Loss: 1.404 | Acc: 67.969,89.062,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.773 0.203 0.023
Batch: 20 | Loss: 1.613 | Acc: 70.424,86.161,89.062,% | Adaptive Acc: 79.576% | clf_exit: 0.753 0.207 0.039
Batch: 40 | Loss: 1.588 | Acc: 71.322,86.109,89.272,% | Adaptive Acc: 79.897% | clf_exit: 0.763 0.197 0.040
Batch: 60 | Loss: 1.584 | Acc: 71.350,86.258,89.024,% | Adaptive Acc: 80.097% | clf_exit: 0.763 0.198 0.039
Train all parameters

Epoch: 77
Batch: 0 | Loss: 1.176 | Acc: 69.531,91.406,97.656,% | Adaptive Acc: 83.594% | clf_exit: 0.672 0.312 0.016
Batch: 20 | Loss: 1.098 | Acc: 75.484,92.188,96.615,% | Adaptive Acc: 86.905% | clf_exit: 0.724 0.233 0.043
Batch: 40 | Loss: 1.120 | Acc: 74.905,92.016,96.704,% | Adaptive Acc: 86.376% | clf_exit: 0.732 0.224 0.044
Batch: 60 | Loss: 1.128 | Acc: 74.718,92.149,96.670,% | Adaptive Acc: 86.360% | clf_exit: 0.728 0.225 0.046
Batch: 80 | Loss: 1.114 | Acc: 74.894,92.284,96.740,% | Adaptive Acc: 86.516% | clf_exit: 0.728 0.226 0.046
Batch: 100 | Loss: 1.104 | Acc: 75.085,92.311,96.860,% | Adaptive Acc: 86.812% | clf_exit: 0.729 0.225 0.046
Batch: 120 | Loss: 1.101 | Acc: 75.026,92.420,96.856,% | Adaptive Acc: 86.977% | clf_exit: 0.729 0.225 0.046
Batch: 140 | Loss: 1.107 | Acc: 74.878,92.442,96.809,% | Adaptive Acc: 86.918% | clf_exit: 0.728 0.226 0.046
Batch: 160 | Loss: 1.108 | Acc: 74.748,92.445,96.836,% | Adaptive Acc: 86.855% | clf_exit: 0.729 0.225 0.046
Batch: 180 | Loss: 1.104 | Acc: 74.853,92.554,96.888,% | Adaptive Acc: 86.853% | clf_exit: 0.730 0.224 0.045
Batch: 200 | Loss: 1.106 | Acc: 74.817,92.448,96.891,% | Adaptive Acc: 86.847% | clf_exit: 0.731 0.224 0.046
Batch: 220 | Loss: 1.105 | Acc: 74.756,92.477,96.875,% | Adaptive Acc: 86.842% | clf_exit: 0.729 0.226 0.045
Batch: 240 | Loss: 1.105 | Acc: 74.822,92.482,96.820,% | Adaptive Acc: 86.894% | clf_exit: 0.730 0.225 0.045
Batch: 260 | Loss: 1.107 | Acc: 74.758,92.433,96.758,% | Adaptive Acc: 86.824% | clf_exit: 0.730 0.224 0.046
Batch: 280 | Loss: 1.107 | Acc: 74.853,92.421,96.714,% | Adaptive Acc: 86.841% | clf_exit: 0.730 0.225 0.045
Batch: 300 | Loss: 1.105 | Acc: 74.826,92.437,96.748,% | Adaptive Acc: 86.862% | clf_exit: 0.731 0.224 0.045
Batch: 320 | Loss: 1.105 | Acc: 74.798,92.404,96.710,% | Adaptive Acc: 86.833% | clf_exit: 0.732 0.224 0.045
Batch: 340 | Loss: 1.108 | Acc: 74.782,92.320,96.646,% | Adaptive Acc: 86.801% | clf_exit: 0.731 0.224 0.045
Batch: 360 | Loss: 1.110 | Acc: 74.729,92.298,96.624,% | Adaptive Acc: 86.708% | clf_exit: 0.732 0.224 0.045
Batch: 380 | Loss: 1.114 | Acc: 74.686,92.235,96.524,% | Adaptive Acc: 86.635% | clf_exit: 0.732 0.223 0.045
Batch: 0 | Loss: 1.436 | Acc: 71.875,89.062,92.188,% | Adaptive Acc: 80.469% | clf_exit: 0.773 0.180 0.047
Batch: 20 | Loss: 1.589 | Acc: 71.503,86.905,89.323,% | Adaptive Acc: 79.129% | clf_exit: 0.793 0.171 0.036
Batch: 40 | Loss: 1.570 | Acc: 72.370,86.662,89.234,% | Adaptive Acc: 79.707% | clf_exit: 0.794 0.168 0.038
Batch: 60 | Loss: 1.555 | Acc: 72.387,86.796,89.562,% | Adaptive Acc: 80.110% | clf_exit: 0.792 0.171 0.037
Train all parameters

Epoch: 78
Batch: 0 | Loss: 0.991 | Acc: 80.469,92.188,96.875,% | Adaptive Acc: 89.062% | clf_exit: 0.727 0.219 0.055
Batch: 20 | Loss: 1.092 | Acc: 75.112,92.485,96.466,% | Adaptive Acc: 87.091% | clf_exit: 0.733 0.227 0.041
Batch: 40 | Loss: 1.096 | Acc: 75.210,92.721,96.627,% | Adaptive Acc: 86.966% | clf_exit: 0.733 0.226 0.040
Batch: 60 | Loss: 1.091 | Acc: 75.192,92.879,96.773,% | Adaptive Acc: 86.936% | clf_exit: 0.737 0.223 0.040
Batch: 80 | Loss: 1.095 | Acc: 75.048,92.834,96.701,% | Adaptive Acc: 86.873% | clf_exit: 0.736 0.222 0.041
Batch: 100 | Loss: 1.089 | Acc: 74.892,92.860,96.736,% | Adaptive Acc: 86.765% | clf_exit: 0.736 0.222 0.042
Batch: 120 | Loss: 1.092 | Acc: 74.871,92.698,96.707,% | Adaptive Acc: 86.706% | clf_exit: 0.736 0.222 0.043
Batch: 140 | Loss: 1.095 | Acc: 74.839,92.603,96.725,% | Adaptive Acc: 86.630% | clf_exit: 0.736 0.221 0.043
Batch: 160 | Loss: 1.097 | Acc: 74.932,92.440,96.705,% | Adaptive Acc: 86.496% | clf_exit: 0.737 0.220 0.043
Batch: 180 | Loss: 1.097 | Acc: 75.004,92.403,96.737,% | Adaptive Acc: 86.568% | clf_exit: 0.737 0.221 0.043
Batch: 200 | Loss: 1.099 | Acc: 74.961,92.362,96.688,% | Adaptive Acc: 86.571% | clf_exit: 0.737 0.221 0.043
Batch: 220 | Loss: 1.096 | Acc: 75.028,92.347,96.748,% | Adaptive Acc: 86.574% | clf_exit: 0.738 0.220 0.042
Batch: 240 | Loss: 1.099 | Acc: 75.029,92.288,96.719,% | Adaptive Acc: 86.560% | clf_exit: 0.737 0.220 0.043
Batch: 260 | Loss: 1.104 | Acc: 74.970,92.211,96.630,% | Adaptive Acc: 86.476% | clf_exit: 0.737 0.220 0.043
Batch: 280 | Loss: 1.114 | Acc: 74.755,92.082,96.536,% | Adaptive Acc: 86.418% | clf_exit: 0.735 0.221 0.044
Batch: 300 | Loss: 1.117 | Acc: 74.722,92.042,96.470,% | Adaptive Acc: 86.348% | clf_exit: 0.735 0.221 0.044
Batch: 320 | Loss: 1.118 | Acc: 74.747,92.066,96.430,% | Adaptive Acc: 86.334% | clf_exit: 0.736 0.221 0.043
Batch: 340 | Loss: 1.119 | Acc: 74.711,92.032,96.412,% | Adaptive Acc: 86.290% | clf_exit: 0.736 0.221 0.043
Batch: 360 | Loss: 1.121 | Acc: 74.691,92.027,96.360,% | Adaptive Acc: 86.301% | clf_exit: 0.736 0.221 0.043
Batch: 380 | Loss: 1.126 | Acc: 74.594,91.974,96.342,% | Adaptive Acc: 86.261% | clf_exit: 0.735 0.222 0.043
Batch: 0 | Loss: 1.514 | Acc: 65.625,87.500,90.625,% | Adaptive Acc: 78.906% | clf_exit: 0.766 0.203 0.031
Batch: 20 | Loss: 1.651 | Acc: 70.387,85.751,88.728,% | Adaptive Acc: 78.646% | clf_exit: 0.780 0.177 0.043
Batch: 40 | Loss: 1.602 | Acc: 70.922,86.166,89.425,% | Adaptive Acc: 78.697% | clf_exit: 0.789 0.168 0.043
Batch: 60 | Loss: 1.589 | Acc: 71.311,86.155,89.344,% | Adaptive Acc: 79.303% | clf_exit: 0.787 0.171 0.042
Train all parameters

Epoch: 79
Batch: 0 | Loss: 1.235 | Acc: 73.438,89.844,94.531,% | Adaptive Acc: 82.812% | clf_exit: 0.758 0.188 0.055
Batch: 20 | Loss: 1.114 | Acc: 75.372,92.262,95.685,% | Adaptive Acc: 87.202% | clf_exit: 0.730 0.227 0.043
Batch: 40 | Loss: 1.096 | Acc: 75.457,92.645,96.037,% | Adaptive Acc: 87.386% | clf_exit: 0.734 0.224 0.041
Batch: 60 | Loss: 1.105 | Acc: 75.359,92.725,96.119,% | Adaptive Acc: 87.436% | clf_exit: 0.734 0.225 0.041
Batch: 80 | Loss: 1.100 | Acc: 75.222,92.670,96.325,% | Adaptive Acc: 87.336% | clf_exit: 0.734 0.225 0.041
Batch: 100 | Loss: 1.085 | Acc: 75.549,92.822,96.442,% | Adaptive Acc: 87.384% | clf_exit: 0.736 0.223 0.041
Batch: 120 | Loss: 1.079 | Acc: 75.517,92.891,96.636,% | Adaptive Acc: 87.442% | clf_exit: 0.738 0.220 0.042
Batch: 140 | Loss: 1.080 | Acc: 75.543,92.786,96.615,% | Adaptive Acc: 87.289% | clf_exit: 0.739 0.219 0.042
Batch: 160 | Loss: 1.080 | Acc: 75.510,92.668,96.618,% | Adaptive Acc: 87.136% | clf_exit: 0.740 0.218 0.042
Batch: 180 | Loss: 1.090 | Acc: 75.272,92.550,96.607,% | Adaptive Acc: 87.099% | clf_exit: 0.737 0.219 0.043
Batch: 200 | Loss: 1.093 | Acc: 75.062,92.537,96.615,% | Adaptive Acc: 86.960% | clf_exit: 0.735 0.222 0.043
Batch: 220 | Loss: 1.097 | Acc: 74.958,92.492,96.514,% | Adaptive Acc: 86.906% | clf_exit: 0.735 0.221 0.044
Batch: 240 | Loss: 1.104 | Acc: 74.825,92.395,96.450,% | Adaptive Acc: 86.845% | clf_exit: 0.734 0.223 0.044
Batch: 260 | Loss: 1.107 | Acc: 74.820,92.352,96.408,% | Adaptive Acc: 86.835% | clf_exit: 0.734 0.222 0.044
Batch: 280 | Loss: 1.107 | Acc: 74.897,92.321,96.397,% | Adaptive Acc: 86.880% | clf_exit: 0.733 0.223 0.044
Batch: 300 | Loss: 1.111 | Acc: 74.886,92.299,96.314,% | Adaptive Acc: 86.851% | clf_exit: 0.733 0.224 0.044
Batch: 320 | Loss: 1.119 | Acc: 74.749,92.183,96.262,% | Adaptive Acc: 86.728% | clf_exit: 0.731 0.224 0.045
Batch: 340 | Loss: 1.119 | Acc: 74.810,92.153,96.275,% | Adaptive Acc: 86.790% | clf_exit: 0.731 0.224 0.045
Batch: 360 | Loss: 1.120 | Acc: 74.825,92.131,96.260,% | Adaptive Acc: 86.762% | clf_exit: 0.731 0.224 0.045
Batch: 380 | Loss: 1.126 | Acc: 74.733,92.032,96.190,% | Adaptive Acc: 86.665% | clf_exit: 0.731 0.224 0.045
Batch: 0 | Loss: 1.316 | Acc: 71.875,85.938,91.406,% | Adaptive Acc: 81.250% | clf_exit: 0.766 0.188 0.047
Batch: 20 | Loss: 1.543 | Acc: 70.685,86.607,90.848,% | Adaptive Acc: 79.278% | clf_exit: 0.788 0.180 0.032
Batch: 40 | Loss: 1.528 | Acc: 71.646,86.947,90.873,% | Adaptive Acc: 79.592% | clf_exit: 0.793 0.172 0.035
Batch: 60 | Loss: 1.532 | Acc: 71.491,86.770,90.740,% | Adaptive Acc: 79.534% | clf_exit: 0.793 0.173 0.034
Train all parameters

Epoch: 80
Batch: 0 | Loss: 0.929 | Acc: 78.125,96.094,98.438,% | Adaptive Acc: 89.062% | clf_exit: 0.766 0.180 0.055
Batch: 20 | Loss: 1.072 | Acc: 75.223,93.192,96.540,% | Adaptive Acc: 87.500% | clf_exit: 0.750 0.204 0.046
Batch: 40 | Loss: 1.082 | Acc: 75.133,92.912,96.627,% | Adaptive Acc: 86.966% | clf_exit: 0.742 0.213 0.046
Batch: 60 | Loss: 1.088 | Acc: 74.898,92.738,96.619,% | Adaptive Acc: 87.129% | clf_exit: 0.739 0.215 0.046
Batch: 80 | Loss: 1.098 | Acc: 74.971,92.564,96.518,% | Adaptive Acc: 87.105% | clf_exit: 0.738 0.216 0.046
Batch: 100 | Loss: 1.097 | Acc: 74.954,92.613,96.573,% | Adaptive Acc: 87.059% | clf_exit: 0.737 0.217 0.046
Batch: 120 | Loss: 1.093 | Acc: 75.142,92.465,96.513,% | Adaptive Acc: 87.061% | clf_exit: 0.739 0.215 0.046
Batch: 140 | Loss: 1.083 | Acc: 75.255,92.620,96.631,% | Adaptive Acc: 87.118% | clf_exit: 0.741 0.214 0.045
Batch: 160 | Loss: 1.091 | Acc: 74.918,92.537,96.661,% | Adaptive Acc: 86.942% | clf_exit: 0.737 0.217 0.045
Batch: 180 | Loss: 1.090 | Acc: 75.000,92.580,96.707,% | Adaptive Acc: 87.017% | clf_exit: 0.736 0.220 0.044
Batch: 200 | Loss: 1.098 | Acc: 74.860,92.448,96.657,% | Adaptive Acc: 86.866% | clf_exit: 0.736 0.220 0.044
Batch: 220 | Loss: 1.099 | Acc: 74.855,92.410,96.645,% | Adaptive Acc: 86.839% | clf_exit: 0.735 0.221 0.045
Batch: 240 | Loss: 1.098 | Acc: 74.900,92.421,96.651,% | Adaptive Acc: 86.845% | clf_exit: 0.735 0.221 0.045
Batch: 260 | Loss: 1.096 | Acc: 74.943,92.397,96.621,% | Adaptive Acc: 86.815% | clf_exit: 0.735 0.220 0.045
Batch: 280 | Loss: 1.100 | Acc: 74.869,92.321,96.558,% | Adaptive Acc: 86.783% | clf_exit: 0.734 0.221 0.046
Batch: 300 | Loss: 1.102 | Acc: 74.800,92.299,96.532,% | Adaptive Acc: 86.786% | clf_exit: 0.733 0.221 0.046
Batch: 320 | Loss: 1.102 | Acc: 74.844,92.280,96.534,% | Adaptive Acc: 86.804% | clf_exit: 0.733 0.221 0.046
Batch: 340 | Loss: 1.105 | Acc: 74.837,92.233,96.499,% | Adaptive Acc: 86.739% | clf_exit: 0.734 0.221 0.045
Batch: 360 | Loss: 1.107 | Acc: 74.803,92.177,96.457,% | Adaptive Acc: 86.706% | clf_exit: 0.733 0.221 0.045
Batch: 380 | Loss: 1.108 | Acc: 74.826,92.157,96.422,% | Adaptive Acc: 86.711% | clf_exit: 0.734 0.221 0.045
Batch: 0 | Loss: 1.284 | Acc: 72.656,89.844,92.969,% | Adaptive Acc: 82.031% | clf_exit: 0.742 0.227 0.031
Batch: 20 | Loss: 1.530 | Acc: 72.693,87.872,90.402,% | Adaptive Acc: 81.287% | clf_exit: 0.767 0.192 0.040
Batch: 40 | Loss: 1.527 | Acc: 72.980,87.652,90.339,% | Adaptive Acc: 81.002% | clf_exit: 0.772 0.187 0.040
Batch: 60 | Loss: 1.520 | Acc: 73.066,87.410,90.356,% | Adaptive Acc: 81.314% | clf_exit: 0.771 0.188 0.041
Train all parameters

Epoch: 81
Batch: 0 | Loss: 1.025 | Acc: 81.250,90.625,96.094,% | Adaptive Acc: 88.281% | clf_exit: 0.773 0.188 0.039
Batch: 20 | Loss: 1.053 | Acc: 75.930,92.820,96.801,% | Adaptive Acc: 87.202% | clf_exit: 0.751 0.209 0.039
Batch: 40 | Loss: 1.050 | Acc: 76.391,92.988,96.780,% | Adaptive Acc: 87.671% | clf_exit: 0.747 0.213 0.040
Batch: 60 | Loss: 1.054 | Acc: 76.434,92.789,96.696,% | Adaptive Acc: 87.423% | clf_exit: 0.747 0.213 0.040
Batch: 80 | Loss: 1.052 | Acc: 76.235,92.882,96.865,% | Adaptive Acc: 87.654% | clf_exit: 0.743 0.215 0.041
Batch: 100 | Loss: 1.067 | Acc: 75.851,92.744,96.751,% | Adaptive Acc: 87.554% | clf_exit: 0.740 0.218 0.042
Batch: 120 | Loss: 1.061 | Acc: 75.994,92.807,96.739,% | Adaptive Acc: 87.565% | clf_exit: 0.739 0.219 0.042
Batch: 140 | Loss: 1.071 | Acc: 75.665,92.675,96.648,% | Adaptive Acc: 87.439% | clf_exit: 0.738 0.219 0.043
Batch: 160 | Loss: 1.077 | Acc: 75.471,92.571,96.647,% | Adaptive Acc: 87.253% | clf_exit: 0.737 0.220 0.043
Batch: 180 | Loss: 1.084 | Acc: 75.337,92.528,96.525,% | Adaptive Acc: 87.073% | clf_exit: 0.735 0.221 0.043
Batch: 200 | Loss: 1.083 | Acc: 75.412,92.522,96.525,% | Adaptive Acc: 87.092% | clf_exit: 0.737 0.219 0.043
Batch: 220 | Loss: 1.086 | Acc: 75.410,92.488,96.497,% | Adaptive Acc: 87.062% | clf_exit: 0.737 0.219 0.044
Batch: 240 | Loss: 1.090 | Acc: 75.298,92.463,96.476,% | Adaptive Acc: 87.017% | clf_exit: 0.735 0.221 0.044
Batch: 260 | Loss: 1.097 | Acc: 75.233,92.316,96.417,% | Adaptive Acc: 86.931% | clf_exit: 0.735 0.220 0.045
Batch: 280 | Loss: 1.101 | Acc: 75.239,92.279,96.350,% | Adaptive Acc: 86.880% | clf_exit: 0.735 0.220 0.044
Batch: 300 | Loss: 1.106 | Acc: 75.145,92.198,96.325,% | Adaptive Acc: 86.812% | clf_exit: 0.735 0.221 0.044
Batch: 320 | Loss: 1.111 | Acc: 75.083,92.097,96.308,% | Adaptive Acc: 86.704% | clf_exit: 0.734 0.221 0.044
Batch: 340 | Loss: 1.116 | Acc: 75.092,91.979,96.245,% | Adaptive Acc: 86.671% | clf_exit: 0.734 0.221 0.045
Batch: 360 | Loss: 1.116 | Acc: 75.043,92.006,96.224,% | Adaptive Acc: 86.634% | clf_exit: 0.734 0.221 0.045
Batch: 380 | Loss: 1.118 | Acc: 74.951,92.019,96.213,% | Adaptive Acc: 86.610% | clf_exit: 0.734 0.221 0.045
Batch: 0 | Loss: 1.620 | Acc: 67.969,85.938,88.281,% | Adaptive Acc: 77.344% | clf_exit: 0.742 0.180 0.078
Batch: 20 | Loss: 1.602 | Acc: 69.531,86.533,90.253,% | Adaptive Acc: 78.348% | clf_exit: 0.771 0.191 0.038
Batch: 40 | Loss: 1.574 | Acc: 70.351,86.395,90.682,% | Adaptive Acc: 78.773% | clf_exit: 0.780 0.181 0.039
Batch: 60 | Loss: 1.556 | Acc: 70.453,86.783,91.022,% | Adaptive Acc: 79.073% | clf_exit: 0.779 0.184 0.037
Train all parameters

Epoch: 82
Batch: 0 | Loss: 1.217 | Acc: 71.094,89.844,96.875,% | Adaptive Acc: 85.156% | clf_exit: 0.766 0.188 0.047
Batch: 20 | Loss: 1.077 | Acc: 75.744,92.150,97.024,% | Adaptive Acc: 87.054% | clf_exit: 0.742 0.215 0.044
Batch: 40 | Loss: 1.066 | Acc: 75.800,92.797,96.970,% | Adaptive Acc: 87.538% | clf_exit: 0.736 0.222 0.042
Batch: 60 | Loss: 1.080 | Acc: 75.589,92.636,97.029,% | Adaptive Acc: 87.231% | clf_exit: 0.734 0.223 0.043
Batch: 80 | Loss: 1.098 | Acc: 75.135,92.458,96.875,% | Adaptive Acc: 87.269% | clf_exit: 0.731 0.226 0.043
Batch: 100 | Loss: 1.104 | Acc: 74.899,92.404,96.836,% | Adaptive Acc: 86.943% | clf_exit: 0.729 0.227 0.043
Batch: 120 | Loss: 1.106 | Acc: 74.929,92.342,96.804,% | Adaptive Acc: 86.983% | clf_exit: 0.728 0.226 0.045
Batch: 140 | Loss: 1.111 | Acc: 74.884,92.210,96.725,% | Adaptive Acc: 86.713% | clf_exit: 0.730 0.226 0.044
Batch: 160 | Loss: 1.107 | Acc: 74.922,92.314,96.676,% | Adaptive Acc: 86.772% | clf_exit: 0.730 0.226 0.044
Batch: 180 | Loss: 1.107 | Acc: 74.948,92.274,96.607,% | Adaptive Acc: 86.783% | clf_exit: 0.731 0.225 0.044
Batch: 200 | Loss: 1.109 | Acc: 74.914,92.296,96.552,% | Adaptive Acc: 86.855% | clf_exit: 0.730 0.226 0.044
Batch: 220 | Loss: 1.111 | Acc: 74.894,92.276,96.560,% | Adaptive Acc: 86.910% | clf_exit: 0.729 0.226 0.045
Batch: 240 | Loss: 1.116 | Acc: 74.831,92.220,96.531,% | Adaptive Acc: 86.891% | clf_exit: 0.729 0.226 0.045
Batch: 260 | Loss: 1.117 | Acc: 74.820,92.199,96.516,% | Adaptive Acc: 86.940% | clf_exit: 0.730 0.226 0.045
Batch: 280 | Loss: 1.120 | Acc: 74.689,92.096,96.469,% | Adaptive Acc: 86.855% | clf_exit: 0.729 0.226 0.045
Batch: 300 | Loss: 1.121 | Acc: 74.670,92.042,96.465,% | Adaptive Acc: 86.833% | clf_exit: 0.729 0.225 0.045
Batch: 320 | Loss: 1.121 | Acc: 74.693,92.059,96.459,% | Adaptive Acc: 86.823% | clf_exit: 0.729 0.225 0.045
Batch: 340 | Loss: 1.120 | Acc: 74.746,92.071,96.433,% | Adaptive Acc: 86.838% | clf_exit: 0.730 0.224 0.046
Batch: 360 | Loss: 1.120 | Acc: 74.768,92.053,96.392,% | Adaptive Acc: 86.827% | clf_exit: 0.730 0.225 0.045
Batch: 380 | Loss: 1.121 | Acc: 74.750,92.005,96.356,% | Adaptive Acc: 86.791% | clf_exit: 0.730 0.224 0.046
Batch: 0 | Loss: 1.720 | Acc: 66.406,83.594,85.156,% | Adaptive Acc: 78.125% | clf_exit: 0.750 0.180 0.070
Batch: 20 | Loss: 1.808 | Acc: 69.568,83.929,87.835,% | Adaptive Acc: 77.232% | clf_exit: 0.773 0.189 0.038
Batch: 40 | Loss: 1.759 | Acc: 70.293,84.318,88.434,% | Adaptive Acc: 78.220% | clf_exit: 0.776 0.186 0.038
Batch: 60 | Loss: 1.744 | Acc: 70.428,84.413,88.589,% | Adaptive Acc: 77.984% | clf_exit: 0.774 0.189 0.037
Train all parameters

Epoch: 83
Batch: 0 | Loss: 1.429 | Acc: 71.094,87.500,94.531,% | Adaptive Acc: 85.938% | clf_exit: 0.641 0.289 0.070
Batch: 20 | Loss: 1.129 | Acc: 74.926,92.448,95.945,% | Adaptive Acc: 86.905% | clf_exit: 0.739 0.214 0.047
Batch: 40 | Loss: 1.105 | Acc: 74.695,92.816,96.227,% | Adaptive Acc: 87.214% | clf_exit: 0.730 0.223 0.048
Batch: 60 | Loss: 1.097 | Acc: 74.885,92.610,96.542,% | Adaptive Acc: 87.013% | clf_exit: 0.729 0.224 0.046
Batch: 80 | Loss: 1.089 | Acc: 74.778,92.863,96.557,% | Adaptive Acc: 87.143% | clf_exit: 0.730 0.224 0.045
Batch: 100 | Loss: 1.085 | Acc: 74.807,92.783,96.612,% | Adaptive Acc: 87.206% | clf_exit: 0.730 0.224 0.046
Batch: 120 | Loss: 1.085 | Acc: 74.923,92.769,96.668,% | Adaptive Acc: 87.093% | clf_exit: 0.733 0.223 0.045
Batch: 140 | Loss: 1.079 | Acc: 75.050,92.791,96.709,% | Adaptive Acc: 87.073% | clf_exit: 0.735 0.221 0.044
Batch: 160 | Loss: 1.081 | Acc: 74.976,92.697,96.749,% | Adaptive Acc: 86.981% | clf_exit: 0.736 0.221 0.044
Batch: 180 | Loss: 1.085 | Acc: 74.849,92.658,96.758,% | Adaptive Acc: 86.913% | clf_exit: 0.735 0.221 0.044
Batch: 200 | Loss: 1.087 | Acc: 74.891,92.607,96.747,% | Adaptive Acc: 86.913% | clf_exit: 0.735 0.221 0.044
Batch: 220 | Loss: 1.092 | Acc: 74.954,92.474,96.688,% | Adaptive Acc: 86.949% | clf_exit: 0.735 0.220 0.045
Batch: 240 | Loss: 1.094 | Acc: 74.893,92.437,96.687,% | Adaptive Acc: 86.897% | clf_exit: 0.736 0.220 0.045
Batch: 260 | Loss: 1.099 | Acc: 74.889,92.367,96.674,% | Adaptive Acc: 86.850% | clf_exit: 0.734 0.221 0.045
Batch: 280 | Loss: 1.103 | Acc: 74.894,92.301,96.603,% | Adaptive Acc: 86.763% | clf_exit: 0.735 0.220 0.045
Batch: 300 | Loss: 1.104 | Acc: 74.849,92.263,96.634,% | Adaptive Acc: 86.784% | clf_exit: 0.733 0.222 0.045
Batch: 320 | Loss: 1.104 | Acc: 74.869,92.258,96.600,% | Adaptive Acc: 86.775% | clf_exit: 0.734 0.221 0.045
Batch: 340 | Loss: 1.105 | Acc: 74.947,92.222,96.547,% | Adaptive Acc: 86.806% | clf_exit: 0.735 0.220 0.045
Batch: 360 | Loss: 1.108 | Acc: 74.963,92.188,96.481,% | Adaptive Acc: 86.760% | clf_exit: 0.735 0.220 0.045
Batch: 380 | Loss: 1.111 | Acc: 74.871,92.157,96.440,% | Adaptive Acc: 86.750% | clf_exit: 0.733 0.221 0.045
Batch: 0 | Loss: 1.470 | Acc: 72.656,89.062,89.844,% | Adaptive Acc: 78.906% | clf_exit: 0.789 0.141 0.070
Batch: 20 | Loss: 1.579 | Acc: 72.954,85.640,89.360,% | Adaptive Acc: 80.804% | clf_exit: 0.777 0.182 0.041
Batch: 40 | Loss: 1.554 | Acc: 72.961,86.033,89.501,% | Adaptive Acc: 81.079% | clf_exit: 0.779 0.180 0.041
Batch: 60 | Loss: 1.540 | Acc: 73.348,86.130,89.421,% | Adaptive Acc: 81.481% | clf_exit: 0.777 0.180 0.044
Train all parameters

Epoch: 84
Batch: 0 | Loss: 1.103 | Acc: 77.344,92.188,95.312,% | Adaptive Acc: 88.281% | clf_exit: 0.742 0.219 0.039
Batch: 20 | Loss: 1.089 | Acc: 75.372,92.485,96.726,% | Adaptive Acc: 87.351% | clf_exit: 0.736 0.219 0.044
Batch: 40 | Loss: 1.094 | Acc: 75.191,92.359,96.704,% | Adaptive Acc: 87.252% | clf_exit: 0.732 0.221 0.047
Batch: 60 | Loss: 1.080 | Acc: 75.269,92.559,96.862,% | Adaptive Acc: 87.154% | clf_exit: 0.735 0.219 0.046
Batch: 80 | Loss: 1.093 | Acc: 75.299,92.515,96.779,% | Adaptive Acc: 87.056% | clf_exit: 0.736 0.220 0.045
Batch: 100 | Loss: 1.091 | Acc: 75.201,92.559,96.767,% | Adaptive Acc: 87.090% | clf_exit: 0.737 0.218 0.045
Batch: 120 | Loss: 1.094 | Acc: 75.110,92.523,96.714,% | Adaptive Acc: 87.113% | clf_exit: 0.734 0.220 0.045
Batch: 140 | Loss: 1.090 | Acc: 75.233,92.520,96.725,% | Adaptive Acc: 87.168% | clf_exit: 0.736 0.219 0.045
Batch: 160 | Loss: 1.086 | Acc: 75.257,92.522,96.773,% | Adaptive Acc: 87.126% | clf_exit: 0.735 0.221 0.044
Batch: 180 | Loss: 1.087 | Acc: 75.151,92.537,96.754,% | Adaptive Acc: 87.099% | clf_exit: 0.735 0.221 0.044
Batch: 200 | Loss: 1.090 | Acc: 75.082,92.502,96.708,% | Adaptive Acc: 87.010% | clf_exit: 0.735 0.222 0.044
Batch: 220 | Loss: 1.095 | Acc: 75.028,92.417,96.617,% | Adaptive Acc: 86.924% | clf_exit: 0.735 0.221 0.044
Batch: 240 | Loss: 1.095 | Acc: 75.026,92.444,96.609,% | Adaptive Acc: 86.936% | clf_exit: 0.734 0.223 0.044
Batch: 260 | Loss: 1.097 | Acc: 75.069,92.343,96.552,% | Adaptive Acc: 86.913% | clf_exit: 0.734 0.222 0.044
Batch: 280 | Loss: 1.099 | Acc: 75.058,92.299,96.500,% | Adaptive Acc: 86.836% | clf_exit: 0.734 0.222 0.044
Batch: 300 | Loss: 1.100 | Acc: 75.135,92.263,96.465,% | Adaptive Acc: 86.838% | clf_exit: 0.735 0.221 0.044
Batch: 320 | Loss: 1.102 | Acc: 75.122,92.219,96.459,% | Adaptive Acc: 86.857% | clf_exit: 0.735 0.221 0.044
Batch: 340 | Loss: 1.101 | Acc: 75.160,92.233,96.435,% | Adaptive Acc: 86.861% | clf_exit: 0.736 0.220 0.044
Batch: 360 | Loss: 1.102 | Acc: 75.156,92.224,96.427,% | Adaptive Acc: 86.864% | clf_exit: 0.735 0.220 0.044
Batch: 380 | Loss: 1.102 | Acc: 75.133,92.229,96.453,% | Adaptive Acc: 86.905% | clf_exit: 0.735 0.220 0.044
Batch: 0 | Loss: 1.613 | Acc: 70.312,85.938,86.719,% | Adaptive Acc: 77.344% | clf_exit: 0.773 0.219 0.008
Batch: 20 | Loss: 1.668 | Acc: 69.978,85.900,89.397,% | Adaptive Acc: 78.385% | clf_exit: 0.778 0.184 0.038
Batch: 40 | Loss: 1.668 | Acc: 70.312,85.442,89.177,% | Adaptive Acc: 78.106% | clf_exit: 0.786 0.177 0.037
Batch: 60 | Loss: 1.643 | Acc: 70.876,85.809,89.242,% | Adaptive Acc: 78.932% | clf_exit: 0.781 0.181 0.038
Train classifier parameters

Epoch: 85
Batch: 0 | Loss: 1.075 | Acc: 79.688,92.188,96.094,% | Adaptive Acc: 89.062% | clf_exit: 0.680 0.289 0.031
Batch: 20 | Loss: 1.356 | Acc: 71.875,88.765,93.415,% | Adaptive Acc: 83.408% | clf_exit: 0.717 0.224 0.059
Batch: 40 | Loss: 1.514 | Acc: 69.188,87.214,92.207,% | Adaptive Acc: 82.031% | clf_exit: 0.699 0.229 0.072
Batch: 60 | Loss: 1.557 | Acc: 68.558,87.026,91.534,% | Adaptive Acc: 81.826% | clf_exit: 0.687 0.239 0.074
Batch: 80 | Loss: 1.574 | Acc: 68.528,86.998,91.339,% | Adaptive Acc: 81.983% | clf_exit: 0.680 0.246 0.074
Batch: 100 | Loss: 1.591 | Acc: 68.278,86.719,91.228,% | Adaptive Acc: 81.590% | clf_exit: 0.676 0.250 0.074
Batch: 120 | Loss: 1.587 | Acc: 68.311,86.816,91.264,% | Adaptive Acc: 81.528% | clf_exit: 0.676 0.251 0.073
Batch: 140 | Loss: 1.581 | Acc: 68.562,86.946,91.262,% | Adaptive Acc: 81.793% | clf_exit: 0.674 0.253 0.073
Batch: 160 | Loss: 1.580 | Acc: 68.483,86.927,91.314,% | Adaptive Acc: 81.852% | clf_exit: 0.673 0.253 0.074
Batch: 180 | Loss: 1.584 | Acc: 68.413,86.814,91.169,% | Adaptive Acc: 81.815% | clf_exit: 0.671 0.254 0.075
Batch: 200 | Loss: 1.583 | Acc: 68.385,86.785,91.150,% | Adaptive Acc: 81.845% | clf_exit: 0.669 0.255 0.076
Batch: 220 | Loss: 1.584 | Acc: 68.503,86.772,91.092,% | Adaptive Acc: 81.968% | clf_exit: 0.668 0.255 0.077
Batch: 240 | Loss: 1.579 | Acc: 68.617,86.797,91.098,% | Adaptive Acc: 82.044% | clf_exit: 0.667 0.256 0.077
Batch: 260 | Loss: 1.577 | Acc: 68.633,86.794,91.113,% | Adaptive Acc: 82.124% | clf_exit: 0.665 0.258 0.077
Batch: 280 | Loss: 1.575 | Acc: 68.708,86.861,91.114,% | Adaptive Acc: 82.190% | clf_exit: 0.664 0.258 0.077
Batch: 300 | Loss: 1.570 | Acc: 68.784,86.963,91.201,% | Adaptive Acc: 82.296% | clf_exit: 0.664 0.259 0.077
Batch: 320 | Loss: 1.567 | Acc: 68.889,86.952,91.253,% | Adaptive Acc: 82.316% | clf_exit: 0.665 0.258 0.078
Batch: 340 | Loss: 1.568 | Acc: 68.931,86.900,91.255,% | Adaptive Acc: 82.329% | clf_exit: 0.664 0.259 0.078
Batch: 360 | Loss: 1.569 | Acc: 68.982,86.907,91.248,% | Adaptive Acc: 82.373% | clf_exit: 0.663 0.259 0.078
Batch: 380 | Loss: 1.563 | Acc: 69.135,86.979,91.287,% | Adaptive Acc: 82.456% | clf_exit: 0.664 0.259 0.077
Batch: 0 | Loss: 1.684 | Acc: 67.969,84.375,88.281,% | Adaptive Acc: 82.031% | clf_exit: 0.719 0.227 0.055
Batch: 20 | Loss: 1.875 | Acc: 68.006,83.966,86.384,% | Adaptive Acc: 78.646% | clf_exit: 0.715 0.222 0.064
Batch: 40 | Loss: 1.873 | Acc: 68.216,84.032,86.242,% | Adaptive Acc: 78.430% | clf_exit: 0.718 0.220 0.062
Batch: 60 | Loss: 1.863 | Acc: 68.302,83.863,86.437,% | Adaptive Acc: 78.522% | clf_exit: 0.717 0.218 0.066
Train classifier parameters

Epoch: 86
Batch: 0 | Loss: 1.732 | Acc: 65.625,82.812,92.969,% | Adaptive Acc: 82.812% | clf_exit: 0.664 0.258 0.078
Batch: 20 | Loss: 1.594 | Acc: 69.643,86.942,90.737,% | Adaptive Acc: 83.557% | clf_exit: 0.652 0.270 0.078
Batch: 40 | Loss: 1.559 | Acc: 70.160,86.852,91.368,% | Adaptive Acc: 83.460% | clf_exit: 0.649 0.272 0.079
Batch: 60 | Loss: 1.537 | Acc: 69.980,86.975,91.534,% | Adaptive Acc: 83.427% | clf_exit: 0.650 0.270 0.080
Batch: 80 | Loss: 1.545 | Acc: 70.042,87.018,91.223,% | Adaptive Acc: 83.382% | clf_exit: 0.648 0.269 0.082
Batch: 100 | Loss: 1.539 | Acc: 69.879,87.020,91.252,% | Adaptive Acc: 83.470% | clf_exit: 0.647 0.270 0.082
Batch: 120 | Loss: 1.529 | Acc: 70.093,87.216,91.445,% | Adaptive Acc: 83.574% | clf_exit: 0.649 0.267 0.084
Batch: 140 | Loss: 1.518 | Acc: 70.268,87.345,91.556,% | Adaptive Acc: 83.616% | clf_exit: 0.651 0.266 0.082
Batch: 160 | Loss: 1.524 | Acc: 70.293,87.379,91.494,% | Adaptive Acc: 83.589% | clf_exit: 0.651 0.267 0.083
Batch: 180 | Loss: 1.521 | Acc: 70.407,87.522,91.497,% | Adaptive Acc: 83.676% | clf_exit: 0.651 0.266 0.083
Batch: 200 | Loss: 1.516 | Acc: 70.530,87.535,91.515,% | Adaptive Acc: 83.811% | clf_exit: 0.652 0.265 0.083
Batch: 220 | Loss: 1.514 | Acc: 70.546,87.542,91.505,% | Adaptive Acc: 83.827% | clf_exit: 0.653 0.265 0.082
Batch: 240 | Loss: 1.509 | Acc: 70.620,87.643,91.536,% | Adaptive Acc: 83.798% | clf_exit: 0.655 0.264 0.081
Batch: 260 | Loss: 1.502 | Acc: 70.773,87.745,91.538,% | Adaptive Acc: 83.920% | clf_exit: 0.655 0.264 0.081
Batch: 280 | Loss: 1.502 | Acc: 70.718,87.747,91.604,% | Adaptive Acc: 83.975% | clf_exit: 0.654 0.265 0.081
Batch: 300 | Loss: 1.501 | Acc: 70.780,87.705,91.681,% | Adaptive Acc: 83.936% | clf_exit: 0.655 0.265 0.081
Batch: 320 | Loss: 1.500 | Acc: 70.768,87.697,91.681,% | Adaptive Acc: 83.910% | clf_exit: 0.654 0.265 0.081
Batch: 340 | Loss: 1.500 | Acc: 70.768,87.722,91.674,% | Adaptive Acc: 83.917% | clf_exit: 0.655 0.265 0.081
Batch: 360 | Loss: 1.502 | Acc: 70.735,87.706,91.646,% | Adaptive Acc: 83.918% | clf_exit: 0.655 0.264 0.081
Batch: 380 | Loss: 1.499 | Acc: 70.839,87.765,91.706,% | Adaptive Acc: 83.983% | clf_exit: 0.654 0.264 0.081
Batch: 0 | Loss: 1.650 | Acc: 69.531,86.719,87.500,% | Adaptive Acc: 79.688% | clf_exit: 0.734 0.227 0.039
Batch: 20 | Loss: 1.819 | Acc: 68.415,84.226,86.570,% | Adaptive Acc: 79.129% | clf_exit: 0.714 0.218 0.068
Batch: 40 | Loss: 1.819 | Acc: 69.074,84.051,86.547,% | Adaptive Acc: 79.383% | clf_exit: 0.717 0.219 0.064
Batch: 60 | Loss: 1.805 | Acc: 69.224,84.080,86.808,% | Adaptive Acc: 79.380% | clf_exit: 0.714 0.220 0.066
Train classifier parameters

Epoch: 87
Batch: 0 | Loss: 1.692 | Acc: 67.188,85.938,89.062,% | Adaptive Acc: 82.812% | clf_exit: 0.594 0.312 0.094
Batch: 20 | Loss: 1.482 | Acc: 71.391,88.504,90.848,% | Adaptive Acc: 84.152% | clf_exit: 0.656 0.271 0.073
Batch: 40 | Loss: 1.506 | Acc: 70.808,87.633,91.082,% | Adaptive Acc: 83.270% | clf_exit: 0.661 0.264 0.075
Batch: 60 | Loss: 1.500 | Acc: 70.799,87.731,91.214,% | Adaptive Acc: 83.350% | clf_exit: 0.655 0.265 0.079
Batch: 80 | Loss: 1.481 | Acc: 70.910,87.867,91.532,% | Adaptive Acc: 83.767% | clf_exit: 0.658 0.262 0.080
Batch: 100 | Loss: 1.477 | Acc: 71.101,87.894,91.530,% | Adaptive Acc: 83.787% | clf_exit: 0.660 0.261 0.080
Batch: 120 | Loss: 1.471 | Acc: 71.139,88.010,91.645,% | Adaptive Acc: 84.007% | clf_exit: 0.659 0.262 0.079
Batch: 140 | Loss: 1.474 | Acc: 71.144,88.010,91.628,% | Adaptive Acc: 84.065% | clf_exit: 0.656 0.263 0.081
Batch: 160 | Loss: 1.473 | Acc: 71.254,87.917,91.552,% | Adaptive Acc: 84.060% | clf_exit: 0.658 0.261 0.082
Batch: 180 | Loss: 1.469 | Acc: 71.249,87.988,91.687,% | Adaptive Acc: 84.142% | clf_exit: 0.657 0.262 0.081
Batch: 200 | Loss: 1.469 | Acc: 71.206,88.075,91.737,% | Adaptive Acc: 84.153% | clf_exit: 0.657 0.262 0.081
Batch: 220 | Loss: 1.469 | Acc: 71.172,88.051,91.739,% | Adaptive Acc: 84.152% | clf_exit: 0.657 0.262 0.081
Batch: 240 | Loss: 1.468 | Acc: 71.175,88.058,91.808,% | Adaptive Acc: 84.184% | clf_exit: 0.658 0.261 0.081
Batch: 260 | Loss: 1.472 | Acc: 71.124,88.078,91.756,% | Adaptive Acc: 84.124% | clf_exit: 0.658 0.261 0.081
Batch: 280 | Loss: 1.467 | Acc: 71.185,88.128,91.840,% | Adaptive Acc: 84.111% | clf_exit: 0.659 0.260 0.081
Batch: 300 | Loss: 1.465 | Acc: 71.281,88.190,91.842,% | Adaptive Acc: 84.147% | clf_exit: 0.659 0.260 0.081
Batch: 320 | Loss: 1.461 | Acc: 71.388,88.237,91.893,% | Adaptive Acc: 84.229% | clf_exit: 0.660 0.259 0.081
Batch: 340 | Loss: 1.464 | Acc: 71.431,88.187,91.828,% | Adaptive Acc: 84.194% | clf_exit: 0.660 0.258 0.081
Batch: 360 | Loss: 1.464 | Acc: 71.362,88.218,91.852,% | Adaptive Acc: 84.265% | clf_exit: 0.659 0.259 0.081
Batch: 380 | Loss: 1.463 | Acc: 71.309,88.238,91.898,% | Adaptive Acc: 84.254% | clf_exit: 0.659 0.260 0.081
Batch: 0 | Loss: 1.609 | Acc: 67.188,89.844,87.500,% | Adaptive Acc: 78.906% | clf_exit: 0.703 0.234 0.062
Batch: 20 | Loss: 1.789 | Acc: 69.122,84.896,86.570,% | Adaptive Acc: 79.278% | clf_exit: 0.724 0.210 0.066
Batch: 40 | Loss: 1.793 | Acc: 69.741,84.661,86.585,% | Adaptive Acc: 79.135% | clf_exit: 0.725 0.210 0.065
Batch: 60 | Loss: 1.780 | Acc: 69.877,84.695,86.821,% | Adaptive Acc: 79.355% | clf_exit: 0.723 0.211 0.066
Train classifier parameters

Epoch: 88
Batch: 0 | Loss: 1.409 | Acc: 77.344,89.062,88.281,% | Adaptive Acc: 85.156% | clf_exit: 0.656 0.273 0.070
Batch: 20 | Loss: 1.452 | Acc: 70.610,88.318,91.890,% | Adaptive Acc: 84.263% | clf_exit: 0.663 0.260 0.077
Batch: 40 | Loss: 1.439 | Acc: 71.380,88.472,92.016,% | Adaptive Acc: 84.470% | clf_exit: 0.666 0.254 0.080
Batch: 60 | Loss: 1.451 | Acc: 71.158,88.409,92.264,% | Adaptive Acc: 84.452% | clf_exit: 0.661 0.257 0.082
Batch: 80 | Loss: 1.457 | Acc: 71.402,88.002,92.188,% | Adaptive Acc: 84.443% | clf_exit: 0.662 0.255 0.083
Batch: 100 | Loss: 1.456 | Acc: 71.527,88.243,92.102,% | Adaptive Acc: 84.561% | clf_exit: 0.662 0.256 0.083
Batch: 120 | Loss: 1.454 | Acc: 71.597,88.255,92.181,% | Adaptive Acc: 84.569% | clf_exit: 0.662 0.257 0.082
Batch: 140 | Loss: 1.451 | Acc: 71.615,88.320,92.215,% | Adaptive Acc: 84.536% | clf_exit: 0.663 0.256 0.081
Batch: 160 | Loss: 1.451 | Acc: 71.530,88.291,92.188,% | Adaptive Acc: 84.496% | clf_exit: 0.662 0.258 0.080
Batch: 180 | Loss: 1.452 | Acc: 71.495,88.277,92.157,% | Adaptive Acc: 84.504% | clf_exit: 0.662 0.259 0.080
Batch: 200 | Loss: 1.449 | Acc: 71.389,88.343,92.277,% | Adaptive Acc: 84.406% | clf_exit: 0.664 0.258 0.079
Batch: 220 | Loss: 1.447 | Acc: 71.355,88.363,92.251,% | Adaptive Acc: 84.371% | clf_exit: 0.664 0.259 0.078
Batch: 240 | Loss: 1.444 | Acc: 71.398,88.372,92.226,% | Adaptive Acc: 84.469% | clf_exit: 0.662 0.260 0.078
Batch: 260 | Loss: 1.440 | Acc: 71.492,88.413,92.250,% | Adaptive Acc: 84.522% | clf_exit: 0.663 0.260 0.078
Batch: 280 | Loss: 1.442 | Acc: 71.514,88.392,92.288,% | Adaptive Acc: 84.522% | clf_exit: 0.663 0.260 0.078
Batch: 300 | Loss: 1.441 | Acc: 71.543,88.432,92.260,% | Adaptive Acc: 84.497% | clf_exit: 0.664 0.259 0.077
Batch: 320 | Loss: 1.439 | Acc: 71.590,88.456,92.205,% | Adaptive Acc: 84.502% | clf_exit: 0.664 0.258 0.078
Batch: 340 | Loss: 1.435 | Acc: 71.648,88.499,92.217,% | Adaptive Acc: 84.517% | clf_exit: 0.665 0.257 0.078
Batch: 360 | Loss: 1.433 | Acc: 71.669,88.517,92.261,% | Adaptive Acc: 84.522% | clf_exit: 0.666 0.256 0.078
Batch: 380 | Loss: 1.435 | Acc: 71.586,88.536,92.233,% | Adaptive Acc: 84.480% | clf_exit: 0.667 0.256 0.078
Batch: 0 | Loss: 1.591 | Acc: 71.094,89.844,87.500,% | Adaptive Acc: 79.688% | clf_exit: 0.727 0.227 0.047
Batch: 20 | Loss: 1.769 | Acc: 69.643,84.747,86.756,% | Adaptive Acc: 79.501% | clf_exit: 0.724 0.209 0.067
Batch: 40 | Loss: 1.773 | Acc: 70.122,84.642,86.890,% | Adaptive Acc: 79.649% | clf_exit: 0.726 0.210 0.064
Batch: 60 | Loss: 1.762 | Acc: 70.364,84.734,87.154,% | Adaptive Acc: 79.649% | clf_exit: 0.723 0.213 0.064
Train classifier parameters

Epoch: 89
Batch: 0 | Loss: 1.332 | Acc: 78.906,91.406,89.844,% | Adaptive Acc: 87.500% | clf_exit: 0.711 0.211 0.078
Batch: 20 | Loss: 1.351 | Acc: 73.810,89.211,92.262,% | Adaptive Acc: 85.528% | clf_exit: 0.698 0.230 0.073
Batch: 40 | Loss: 1.375 | Acc: 73.361,89.120,92.530,% | Adaptive Acc: 85.252% | clf_exit: 0.687 0.237 0.077
Batch: 60 | Loss: 1.401 | Acc: 72.387,88.819,92.495,% | Adaptive Acc: 84.964% | clf_exit: 0.679 0.244 0.077
Batch: 80 | Loss: 1.389 | Acc: 72.512,89.062,92.496,% | Adaptive Acc: 85.089% | clf_exit: 0.679 0.247 0.075
Batch: 100 | Loss: 1.385 | Acc: 72.486,89.171,92.474,% | Adaptive Acc: 85.017% | clf_exit: 0.678 0.248 0.074
Batch: 120 | Loss: 1.399 | Acc: 72.308,88.985,92.336,% | Adaptive Acc: 84.756% | clf_exit: 0.677 0.248 0.075
Batch: 140 | Loss: 1.414 | Acc: 72.080,88.763,92.232,% | Adaptive Acc: 84.608% | clf_exit: 0.674 0.250 0.076
Batch: 160 | Loss: 1.417 | Acc: 72.166,88.728,92.236,% | Adaptive Acc: 84.676% | clf_exit: 0.672 0.252 0.077
Batch: 180 | Loss: 1.415 | Acc: 72.268,88.829,92.274,% | Adaptive Acc: 84.716% | clf_exit: 0.672 0.251 0.076
Batch: 200 | Loss: 1.419 | Acc: 72.135,88.825,92.254,% | Adaptive Acc: 84.643% | clf_exit: 0.672 0.252 0.076
Batch: 220 | Loss: 1.422 | Acc: 72.069,88.758,92.241,% | Adaptive Acc: 84.555% | clf_exit: 0.672 0.252 0.076
Batch: 240 | Loss: 1.417 | Acc: 72.099,88.803,92.282,% | Adaptive Acc: 84.647% | clf_exit: 0.673 0.251 0.076
Batch: 260 | Loss: 1.420 | Acc: 72.031,88.748,92.277,% | Adaptive Acc: 84.650% | clf_exit: 0.672 0.252 0.076
Batch: 280 | Loss: 1.424 | Acc: 71.992,88.732,92.249,% | Adaptive Acc: 84.597% | clf_exit: 0.672 0.252 0.076
Batch: 300 | Loss: 1.425 | Acc: 71.987,88.741,92.239,% | Adaptive Acc: 84.637% | clf_exit: 0.671 0.253 0.076
Batch: 320 | Loss: 1.426 | Acc: 71.970,88.707,92.263,% | Adaptive Acc: 84.648% | clf_exit: 0.671 0.253 0.076
Batch: 340 | Loss: 1.430 | Acc: 71.834,88.648,92.240,% | Adaptive Acc: 84.574% | clf_exit: 0.671 0.254 0.076
Batch: 360 | Loss: 1.430 | Acc: 71.801,88.623,92.231,% | Adaptive Acc: 84.563% | clf_exit: 0.670 0.254 0.076
Batch: 380 | Loss: 1.429 | Acc: 71.848,88.648,92.214,% | Adaptive Acc: 84.596% | clf_exit: 0.670 0.254 0.076
Batch: 0 | Loss: 1.575 | Acc: 69.531,89.062,87.500,% | Adaptive Acc: 78.906% | clf_exit: 0.750 0.211 0.039
Batch: 20 | Loss: 1.749 | Acc: 69.568,85.045,86.979,% | Adaptive Acc: 79.874% | clf_exit: 0.729 0.205 0.065
Batch: 40 | Loss: 1.753 | Acc: 70.312,84.966,87.176,% | Adaptive Acc: 79.783% | clf_exit: 0.733 0.203 0.063
Batch: 60 | Loss: 1.743 | Acc: 70.543,84.849,87.410,% | Adaptive Acc: 79.790% | clf_exit: 0.730 0.205 0.065
Train classifier parameters

Epoch: 90
Batch: 0 | Loss: 1.635 | Acc: 70.312,85.938,90.625,% | Adaptive Acc: 80.469% | clf_exit: 0.727 0.195 0.078
Batch: 20 | Loss: 1.453 | Acc: 72.210,88.132,92.001,% | Adaptive Acc: 85.305% | clf_exit: 0.655 0.263 0.082
Batch: 40 | Loss: 1.435 | Acc: 72.142,88.053,91.940,% | Adaptive Acc: 85.080% | clf_exit: 0.663 0.258 0.079
Batch: 60 | Loss: 1.419 | Acc: 72.413,88.461,92.008,% | Adaptive Acc: 85.233% | clf_exit: 0.668 0.254 0.078
Batch: 80 | Loss: 1.424 | Acc: 72.357,88.474,91.975,% | Adaptive Acc: 85.320% | clf_exit: 0.668 0.254 0.079
Batch: 100 | Loss: 1.423 | Acc: 72.602,88.506,92.071,% | Adaptive Acc: 85.149% | clf_exit: 0.668 0.256 0.076
Batch: 120 | Loss: 1.416 | Acc: 72.695,88.630,92.097,% | Adaptive Acc: 85.143% | clf_exit: 0.670 0.255 0.075
Batch: 140 | Loss: 1.416 | Acc: 72.512,88.736,92.271,% | Adaptive Acc: 84.996% | clf_exit: 0.673 0.253 0.074
Batch: 160 | Loss: 1.419 | Acc: 72.355,88.626,92.314,% | Adaptive Acc: 84.686% | clf_exit: 0.673 0.253 0.073
Batch: 180 | Loss: 1.416 | Acc: 72.298,88.730,92.364,% | Adaptive Acc: 84.660% | clf_exit: 0.674 0.253 0.074
Batch: 200 | Loss: 1.414 | Acc: 72.186,88.771,92.428,% | Adaptive Acc: 84.686% | clf_exit: 0.673 0.253 0.074
Batch: 220 | Loss: 1.410 | Acc: 72.267,88.815,92.463,% | Adaptive Acc: 84.746% | clf_exit: 0.673 0.253 0.074
Batch: 240 | Loss: 1.416 | Acc: 72.196,88.738,92.392,% | Adaptive Acc: 84.693% | clf_exit: 0.672 0.253 0.075
Batch: 260 | Loss: 1.412 | Acc: 72.222,88.829,92.400,% | Adaptive Acc: 84.788% | clf_exit: 0.672 0.254 0.074
Batch: 280 | Loss: 1.415 | Acc: 72.203,88.812,92.388,% | Adaptive Acc: 84.767% | clf_exit: 0.671 0.254 0.075
Batch: 300 | Loss: 1.421 | Acc: 72.077,88.725,92.315,% | Adaptive Acc: 84.692% | clf_exit: 0.670 0.255 0.075
Batch: 320 | Loss: 1.420 | Acc: 72.114,88.749,92.282,% | Adaptive Acc: 84.674% | clf_exit: 0.671 0.255 0.075
Batch: 340 | Loss: 1.416 | Acc: 72.132,88.817,92.304,% | Adaptive Acc: 84.712% | clf_exit: 0.671 0.255 0.075
Batch: 360 | Loss: 1.418 | Acc: 72.120,88.807,92.250,% | Adaptive Acc: 84.678% | clf_exit: 0.671 0.255 0.075
Batch: 380 | Loss: 1.417 | Acc: 72.115,88.851,92.327,% | Adaptive Acc: 84.730% | clf_exit: 0.671 0.255 0.075
Batch: 0 | Loss: 1.581 | Acc: 68.750,89.062,87.500,% | Adaptive Acc: 78.906% | clf_exit: 0.734 0.227 0.039
Batch: 20 | Loss: 1.742 | Acc: 69.903,85.193,87.091,% | Adaptive Acc: 80.022% | clf_exit: 0.727 0.216 0.057
Batch: 40 | Loss: 1.748 | Acc: 70.541,85.042,87.100,% | Adaptive Acc: 80.126% | clf_exit: 0.732 0.209 0.059
Batch: 60 | Loss: 1.737 | Acc: 70.684,84.951,87.308,% | Adaptive Acc: 80.046% | clf_exit: 0.730 0.210 0.060
Train classifier parameters

Epoch: 91
Batch: 0 | Loss: 1.448 | Acc: 75.000,88.281,90.625,% | Adaptive Acc: 83.594% | clf_exit: 0.727 0.195 0.078
Batch: 20 | Loss: 1.436 | Acc: 71.949,88.356,92.708,% | Adaptive Acc: 83.780% | clf_exit: 0.681 0.246 0.073
Batch: 40 | Loss: 1.436 | Acc: 71.475,88.662,92.283,% | Adaptive Acc: 84.223% | clf_exit: 0.677 0.247 0.076
Batch: 60 | Loss: 1.417 | Acc: 71.683,88.755,92.533,% | Adaptive Acc: 84.798% | clf_exit: 0.673 0.252 0.075
Batch: 80 | Loss: 1.407 | Acc: 72.097,88.821,92.515,% | Adaptive Acc: 84.944% | clf_exit: 0.674 0.250 0.076
Batch: 100 | Loss: 1.396 | Acc: 72.208,88.861,92.690,% | Adaptive Acc: 85.141% | clf_exit: 0.671 0.253 0.075
Batch: 120 | Loss: 1.408 | Acc: 72.114,88.862,92.497,% | Adaptive Acc: 84.975% | clf_exit: 0.672 0.253 0.075
Batch: 140 | Loss: 1.399 | Acc: 72.379,88.952,92.514,% | Adaptive Acc: 85.189% | clf_exit: 0.674 0.252 0.075
Batch: 160 | Loss: 1.403 | Acc: 72.370,88.961,92.484,% | Adaptive Acc: 85.137% | clf_exit: 0.675 0.251 0.074
Batch: 180 | Loss: 1.406 | Acc: 72.402,88.985,92.382,% | Adaptive Acc: 85.031% | clf_exit: 0.674 0.251 0.075
Batch: 200 | Loss: 1.405 | Acc: 72.369,89.051,92.394,% | Adaptive Acc: 85.040% | clf_exit: 0.673 0.252 0.075
Batch: 220 | Loss: 1.400 | Acc: 72.448,89.123,92.428,% | Adaptive Acc: 85.040% | clf_exit: 0.674 0.251 0.074
Batch: 240 | Loss: 1.401 | Acc: 72.322,89.121,92.405,% | Adaptive Acc: 84.994% | clf_exit: 0.675 0.251 0.074
Batch: 260 | Loss: 1.404 | Acc: 72.186,89.089,92.382,% | Adaptive Acc: 84.920% | clf_exit: 0.674 0.252 0.074
Batch: 280 | Loss: 1.408 | Acc: 72.111,89.065,92.329,% | Adaptive Acc: 84.895% | clf_exit: 0.673 0.253 0.073
Batch: 300 | Loss: 1.411 | Acc: 72.000,89.021,92.320,% | Adaptive Acc: 84.814% | clf_exit: 0.673 0.253 0.074
Batch: 320 | Loss: 1.406 | Acc: 72.121,89.016,92.334,% | Adaptive Acc: 84.881% | clf_exit: 0.673 0.253 0.074
Batch: 340 | Loss: 1.405 | Acc: 72.150,89.076,92.330,% | Adaptive Acc: 84.893% | clf_exit: 0.673 0.252 0.075
Batch: 360 | Loss: 1.407 | Acc: 72.126,88.978,92.302,% | Adaptive Acc: 84.851% | clf_exit: 0.673 0.252 0.075
Batch: 380 | Loss: 1.405 | Acc: 72.193,89.021,92.339,% | Adaptive Acc: 84.881% | clf_exit: 0.674 0.252 0.075
Batch: 0 | Loss: 1.589 | Acc: 69.531,89.062,86.719,% | Adaptive Acc: 78.125% | clf_exit: 0.750 0.203 0.047
Batch: 20 | Loss: 1.742 | Acc: 69.531,85.007,86.905,% | Adaptive Acc: 79.688% | clf_exit: 0.737 0.208 0.055
Batch: 40 | Loss: 1.749 | Acc: 69.989,85.042,86.986,% | Adaptive Acc: 79.668% | clf_exit: 0.736 0.207 0.058
Batch: 60 | Loss: 1.739 | Acc: 70.402,84.977,87.282,% | Adaptive Acc: 79.752% | clf_exit: 0.732 0.208 0.059
Train classifier parameters

Epoch: 92
Batch: 0 | Loss: 1.296 | Acc: 75.000,93.750,96.875,% | Adaptive Acc: 87.500% | clf_exit: 0.688 0.258 0.055
Batch: 20 | Loss: 1.394 | Acc: 71.912,89.211,92.634,% | Adaptive Acc: 84.821% | clf_exit: 0.670 0.256 0.073
Batch: 40 | Loss: 1.396 | Acc: 71.589,89.310,92.912,% | Adaptive Acc: 84.699% | clf_exit: 0.675 0.256 0.069
Batch: 60 | Loss: 1.406 | Acc: 71.709,88.973,92.713,% | Adaptive Acc: 84.759% | clf_exit: 0.671 0.258 0.071
Batch: 80 | Loss: 1.399 | Acc: 72.020,89.178,92.670,% | Adaptive Acc: 84.606% | clf_exit: 0.672 0.257 0.071
Batch: 100 | Loss: 1.397 | Acc: 72.184,89.117,92.675,% | Adaptive Acc: 84.677% | clf_exit: 0.674 0.255 0.071
Batch: 120 | Loss: 1.385 | Acc: 72.527,89.230,92.775,% | Adaptive Acc: 84.904% | clf_exit: 0.675 0.255 0.070
Batch: 140 | Loss: 1.383 | Acc: 72.579,89.207,92.647,% | Adaptive Acc: 84.940% | clf_exit: 0.676 0.252 0.071
Batch: 160 | Loss: 1.384 | Acc: 72.530,89.276,92.697,% | Adaptive Acc: 85.030% | clf_exit: 0.676 0.253 0.072
Batch: 180 | Loss: 1.386 | Acc: 72.479,89.227,92.649,% | Adaptive Acc: 85.027% | clf_exit: 0.676 0.252 0.071
Batch: 200 | Loss: 1.392 | Acc: 72.415,89.140,92.603,% | Adaptive Acc: 84.919% | clf_exit: 0.678 0.251 0.071
Batch: 220 | Loss: 1.388 | Acc: 72.494,89.165,92.605,% | Adaptive Acc: 84.979% | clf_exit: 0.678 0.250 0.071
Batch: 240 | Loss: 1.387 | Acc: 72.553,89.189,92.557,% | Adaptive Acc: 84.946% | clf_exit: 0.679 0.250 0.071
Batch: 260 | Loss: 1.388 | Acc: 72.543,89.143,92.577,% | Adaptive Acc: 84.950% | clf_exit: 0.678 0.250 0.072
Batch: 280 | Loss: 1.386 | Acc: 72.570,89.171,92.585,% | Adaptive Acc: 84.953% | clf_exit: 0.680 0.248 0.072
Batch: 300 | Loss: 1.388 | Acc: 72.454,89.200,92.569,% | Adaptive Acc: 84.902% | clf_exit: 0.679 0.250 0.072
Batch: 320 | Loss: 1.389 | Acc: 72.420,89.191,92.553,% | Adaptive Acc: 84.867% | clf_exit: 0.678 0.250 0.072
Batch: 340 | Loss: 1.394 | Acc: 72.283,89.117,92.499,% | Adaptive Acc: 84.794% | clf_exit: 0.678 0.250 0.072
Batch: 360 | Loss: 1.395 | Acc: 72.329,89.127,92.536,% | Adaptive Acc: 84.840% | clf_exit: 0.678 0.250 0.073
Batch: 380 | Loss: 1.393 | Acc: 72.295,89.165,92.561,% | Adaptive Acc: 84.822% | clf_exit: 0.677 0.251 0.073
Batch: 0 | Loss: 1.561 | Acc: 68.750,89.844,87.500,% | Adaptive Acc: 78.125% | clf_exit: 0.758 0.203 0.039
Batch: 20 | Loss: 1.732 | Acc: 69.829,85.268,87.165,% | Adaptive Acc: 79.836% | clf_exit: 0.735 0.212 0.053
Batch: 40 | Loss: 1.737 | Acc: 70.770,85.194,87.176,% | Adaptive Acc: 79.821% | clf_exit: 0.740 0.203 0.057
Batch: 60 | Loss: 1.725 | Acc: 70.914,85.041,87.423,% | Adaptive Acc: 79.931% | clf_exit: 0.735 0.207 0.059
Train classifier parameters

Epoch: 93
Batch: 0 | Loss: 1.364 | Acc: 72.656,87.500,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.594 0.328 0.078
Batch: 20 | Loss: 1.433 | Acc: 71.280,89.100,92.113,% | Adaptive Acc: 83.854% | clf_exit: 0.671 0.256 0.073
Batch: 40 | Loss: 1.435 | Acc: 71.551,88.624,91.997,% | Adaptive Acc: 83.861% | clf_exit: 0.670 0.258 0.072
Batch: 60 | Loss: 1.417 | Acc: 72.272,89.127,92.072,% | Adaptive Acc: 84.349% | clf_exit: 0.673 0.257 0.069
Batch: 80 | Loss: 1.411 | Acc: 71.904,89.091,92.284,% | Adaptive Acc: 84.443% | clf_exit: 0.674 0.255 0.071
Batch: 100 | Loss: 1.399 | Acc: 72.030,89.248,92.505,% | Adaptive Acc: 84.452% | clf_exit: 0.677 0.252 0.071
Batch: 120 | Loss: 1.395 | Acc: 72.178,89.321,92.446,% | Adaptive Acc: 84.452% | clf_exit: 0.677 0.251 0.072
Batch: 140 | Loss: 1.400 | Acc: 72.169,89.245,92.315,% | Adaptive Acc: 84.519% | clf_exit: 0.676 0.251 0.073
Batch: 160 | Loss: 1.391 | Acc: 72.394,89.359,92.343,% | Adaptive Acc: 84.584% | clf_exit: 0.677 0.251 0.072
Batch: 180 | Loss: 1.394 | Acc: 72.276,89.296,92.360,% | Adaptive Acc: 84.643% | clf_exit: 0.677 0.251 0.072
Batch: 200 | Loss: 1.398 | Acc: 72.135,89.280,92.281,% | Adaptive Acc: 84.530% | clf_exit: 0.677 0.251 0.072
Batch: 220 | Loss: 1.393 | Acc: 72.168,89.303,92.364,% | Adaptive Acc: 84.591% | clf_exit: 0.677 0.251 0.072
Batch: 240 | Loss: 1.396 | Acc: 72.202,89.280,92.278,% | Adaptive Acc: 84.573% | clf_exit: 0.676 0.250 0.073
Batch: 260 | Loss: 1.397 | Acc: 72.141,89.233,92.325,% | Adaptive Acc: 84.519% | clf_exit: 0.676 0.250 0.073
Batch: 280 | Loss: 1.395 | Acc: 72.195,89.316,92.346,% | Adaptive Acc: 84.609% | clf_exit: 0.676 0.251 0.073
Batch: 300 | Loss: 1.395 | Acc: 72.129,89.306,92.354,% | Adaptive Acc: 84.635% | clf_exit: 0.676 0.251 0.073
Batch: 320 | Loss: 1.394 | Acc: 72.123,89.291,92.380,% | Adaptive Acc: 84.708% | clf_exit: 0.676 0.251 0.073
Batch: 340 | Loss: 1.398 | Acc: 72.090,89.253,92.375,% | Adaptive Acc: 84.691% | clf_exit: 0.675 0.252 0.073
Batch: 360 | Loss: 1.398 | Acc: 72.100,89.257,92.376,% | Adaptive Acc: 84.695% | clf_exit: 0.676 0.251 0.073
Batch: 380 | Loss: 1.398 | Acc: 72.094,89.239,92.341,% | Adaptive Acc: 84.668% | clf_exit: 0.676 0.251 0.073
Batch: 0 | Loss: 1.558 | Acc: 70.312,90.625,86.719,% | Adaptive Acc: 78.906% | clf_exit: 0.734 0.219 0.047
Batch: 20 | Loss: 1.726 | Acc: 70.015,85.342,87.016,% | Adaptive Acc: 80.283% | clf_exit: 0.732 0.209 0.058
Batch: 40 | Loss: 1.733 | Acc: 70.560,85.213,87.005,% | Adaptive Acc: 80.069% | clf_exit: 0.738 0.204 0.057
Batch: 60 | Loss: 1.724 | Acc: 70.889,85.182,87.244,% | Adaptive Acc: 80.136% | clf_exit: 0.735 0.206 0.059
Train classifier parameters

Epoch: 94
Batch: 0 | Loss: 1.442 | Acc: 74.219,85.938,90.625,% | Adaptive Acc: 85.156% | clf_exit: 0.609 0.289 0.102
Batch: 20 | Loss: 1.378 | Acc: 72.284,89.435,92.671,% | Adaptive Acc: 84.673% | clf_exit: 0.685 0.248 0.068
Batch: 40 | Loss: 1.386 | Acc: 72.504,89.348,92.664,% | Adaptive Acc: 84.832% | clf_exit: 0.684 0.245 0.070
Batch: 60 | Loss: 1.399 | Acc: 72.093,89.306,92.649,% | Adaptive Acc: 84.900% | clf_exit: 0.675 0.252 0.073
Batch: 80 | Loss: 1.396 | Acc: 71.981,89.198,92.699,% | Adaptive Acc: 84.799% | clf_exit: 0.678 0.249 0.073
Batch: 100 | Loss: 1.382 | Acc: 72.416,89.325,92.791,% | Adaptive Acc: 85.063% | clf_exit: 0.679 0.250 0.071
Batch: 120 | Loss: 1.384 | Acc: 72.605,89.321,92.710,% | Adaptive Acc: 84.943% | clf_exit: 0.680 0.251 0.069
Batch: 140 | Loss: 1.383 | Acc: 72.612,89.351,92.686,% | Adaptive Acc: 84.885% | clf_exit: 0.679 0.251 0.070
Batch: 160 | Loss: 1.388 | Acc: 72.501,89.203,92.610,% | Adaptive Acc: 84.802% | clf_exit: 0.680 0.249 0.071
Batch: 180 | Loss: 1.386 | Acc: 72.514,89.300,92.649,% | Adaptive Acc: 84.828% | clf_exit: 0.679 0.250 0.071
Batch: 200 | Loss: 1.387 | Acc: 72.524,89.300,92.677,% | Adaptive Acc: 84.857% | clf_exit: 0.680 0.249 0.071
Batch: 220 | Loss: 1.394 | Acc: 72.419,89.151,92.583,% | Adaptive Acc: 84.743% | clf_exit: 0.679 0.249 0.072
Batch: 240 | Loss: 1.393 | Acc: 72.436,89.153,92.567,% | Adaptive Acc: 84.735% | clf_exit: 0.679 0.249 0.072
Batch: 260 | Loss: 1.391 | Acc: 72.504,89.152,92.595,% | Adaptive Acc: 84.680% | clf_exit: 0.679 0.249 0.072
Batch: 280 | Loss: 1.390 | Acc: 72.523,89.182,92.607,% | Adaptive Acc: 84.706% | clf_exit: 0.680 0.249 0.072
Batch: 300 | Loss: 1.390 | Acc: 72.456,89.164,92.587,% | Adaptive Acc: 84.689% | clf_exit: 0.679 0.249 0.072
Batch: 320 | Loss: 1.387 | Acc: 72.547,89.213,92.594,% | Adaptive Acc: 84.779% | clf_exit: 0.680 0.248 0.072
Batch: 340 | Loss: 1.389 | Acc: 72.537,89.172,92.547,% | Adaptive Acc: 84.774% | clf_exit: 0.679 0.249 0.072
Batch: 360 | Loss: 1.391 | Acc: 72.459,89.156,92.532,% | Adaptive Acc: 84.734% | clf_exit: 0.678 0.250 0.073
Batch: 380 | Loss: 1.392 | Acc: 72.433,89.134,92.520,% | Adaptive Acc: 84.736% | clf_exit: 0.678 0.249 0.073
Batch: 0 | Loss: 1.569 | Acc: 69.531,89.062,86.719,% | Adaptive Acc: 78.906% | clf_exit: 0.742 0.211 0.047
Batch: 20 | Loss: 1.727 | Acc: 70.164,85.045,87.314,% | Adaptive Acc: 80.394% | clf_exit: 0.735 0.207 0.057
Batch: 40 | Loss: 1.735 | Acc: 70.770,84.832,87.233,% | Adaptive Acc: 80.202% | clf_exit: 0.737 0.204 0.059
Batch: 60 | Loss: 1.723 | Acc: 70.940,84.926,87.513,% | Adaptive Acc: 80.161% | clf_exit: 0.735 0.205 0.060
Train classifier parameters

Epoch: 95
Batch: 0 | Loss: 1.466 | Acc: 69.531,91.406,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.664 0.258 0.078
Batch: 20 | Loss: 1.318 | Acc: 73.065,90.885,92.783,% | Adaptive Acc: 86.012% | clf_exit: 0.681 0.246 0.073
Batch: 40 | Loss: 1.385 | Acc: 72.142,89.596,92.530,% | Adaptive Acc: 85.175% | clf_exit: 0.679 0.245 0.076
Batch: 60 | Loss: 1.373 | Acc: 72.592,89.703,92.546,% | Adaptive Acc: 85.156% | clf_exit: 0.684 0.243 0.072
Batch: 80 | Loss: 1.377 | Acc: 72.328,89.680,92.602,% | Adaptive Acc: 85.127% | clf_exit: 0.685 0.243 0.072
Batch: 100 | Loss: 1.379 | Acc: 72.386,89.511,92.520,% | Adaptive Acc: 85.241% | clf_exit: 0.683 0.245 0.072
Batch: 120 | Loss: 1.383 | Acc: 72.450,89.405,92.523,% | Adaptive Acc: 85.111% | clf_exit: 0.681 0.245 0.073
Batch: 140 | Loss: 1.385 | Acc: 72.451,89.356,92.520,% | Adaptive Acc: 85.084% | clf_exit: 0.680 0.248 0.073
Batch: 160 | Loss: 1.398 | Acc: 72.360,89.237,92.377,% | Adaptive Acc: 84.846% | clf_exit: 0.678 0.249 0.073
Batch: 180 | Loss: 1.390 | Acc: 72.522,89.330,92.477,% | Adaptive Acc: 85.031% | clf_exit: 0.678 0.249 0.073
Batch: 200 | Loss: 1.389 | Acc: 72.493,89.315,92.394,% | Adaptive Acc: 84.989% | clf_exit: 0.679 0.248 0.073
Batch: 220 | Loss: 1.392 | Acc: 72.426,89.278,92.410,% | Adaptive Acc: 84.870% | clf_exit: 0.681 0.246 0.073
Batch: 240 | Loss: 1.394 | Acc: 72.339,89.212,92.418,% | Adaptive Acc: 84.783% | clf_exit: 0.681 0.246 0.073
Batch: 260 | Loss: 1.393 | Acc: 72.324,89.176,92.451,% | Adaptive Acc: 84.782% | clf_exit: 0.681 0.247 0.073
Batch: 280 | Loss: 1.396 | Acc: 72.298,89.182,92.457,% | Adaptive Acc: 84.789% | clf_exit: 0.680 0.247 0.073
Batch: 300 | Loss: 1.394 | Acc: 72.389,89.182,92.411,% | Adaptive Acc: 84.754% | clf_exit: 0.681 0.246 0.073
Batch: 320 | Loss: 1.393 | Acc: 72.374,89.216,92.411,% | Adaptive Acc: 84.764% | clf_exit: 0.681 0.245 0.073
Batch: 340 | Loss: 1.389 | Acc: 72.436,89.253,92.483,% | Adaptive Acc: 84.797% | clf_exit: 0.682 0.245 0.073
Batch: 360 | Loss: 1.391 | Acc: 72.403,89.190,92.449,% | Adaptive Acc: 84.745% | clf_exit: 0.681 0.246 0.073
Batch: 380 | Loss: 1.390 | Acc: 72.392,89.179,92.489,% | Adaptive Acc: 84.726% | clf_exit: 0.681 0.246 0.073
Batch: 0 | Loss: 1.569 | Acc: 68.750,89.844,87.500,% | Adaptive Acc: 77.344% | clf_exit: 0.766 0.188 0.047
Batch: 20 | Loss: 1.728 | Acc: 69.680,85.528,87.351,% | Adaptive Acc: 80.506% | clf_exit: 0.731 0.214 0.054
Batch: 40 | Loss: 1.737 | Acc: 70.675,85.328,87.405,% | Adaptive Acc: 80.107% | clf_exit: 0.736 0.208 0.057
Batch: 60 | Loss: 1.728 | Acc: 70.876,85.297,87.526,% | Adaptive Acc: 80.097% | clf_exit: 0.733 0.209 0.058
Train classifier parameters

Epoch: 96
Batch: 0 | Loss: 1.436 | Acc: 71.875,89.844,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.672 0.250 0.078
Batch: 20 | Loss: 1.401 | Acc: 71.726,89.100,91.927,% | Adaptive Acc: 84.412% | clf_exit: 0.686 0.238 0.077
Batch: 40 | Loss: 1.393 | Acc: 72.637,89.101,92.378,% | Adaptive Acc: 85.042% | clf_exit: 0.680 0.247 0.073
Batch: 60 | Loss: 1.393 | Acc: 72.426,89.139,92.213,% | Adaptive Acc: 84.964% | clf_exit: 0.679 0.248 0.073
Batch: 80 | Loss: 1.399 | Acc: 72.078,89.226,92.188,% | Adaptive Acc: 84.770% | clf_exit: 0.679 0.248 0.073
Batch: 100 | Loss: 1.395 | Acc: 72.161,89.240,92.304,% | Adaptive Acc: 84.816% | clf_exit: 0.679 0.248 0.072
Batch: 120 | Loss: 1.400 | Acc: 72.127,89.050,92.265,% | Adaptive Acc: 84.666% | clf_exit: 0.679 0.247 0.073
Batch: 140 | Loss: 1.398 | Acc: 72.135,89.051,92.348,% | Adaptive Acc: 84.658% | clf_exit: 0.680 0.246 0.075
Batch: 160 | Loss: 1.391 | Acc: 72.195,89.160,92.454,% | Adaptive Acc: 84.797% | clf_exit: 0.679 0.247 0.074
Batch: 180 | Loss: 1.392 | Acc: 72.000,89.136,92.507,% | Adaptive Acc: 84.755% | clf_exit: 0.679 0.247 0.074
Batch: 200 | Loss: 1.394 | Acc: 71.980,89.175,92.471,% | Adaptive Acc: 84.721% | clf_exit: 0.680 0.245 0.075
Batch: 220 | Loss: 1.399 | Acc: 71.861,89.094,92.446,% | Adaptive Acc: 84.630% | clf_exit: 0.680 0.245 0.075
Batch: 240 | Loss: 1.390 | Acc: 72.108,89.176,92.492,% | Adaptive Acc: 84.796% | clf_exit: 0.681 0.245 0.074
Batch: 260 | Loss: 1.391 | Acc: 72.138,89.197,92.454,% | Adaptive Acc: 84.818% | clf_exit: 0.681 0.245 0.074
Batch: 280 | Loss: 1.391 | Acc: 72.181,89.182,92.435,% | Adaptive Acc: 84.842% | clf_exit: 0.681 0.245 0.074
Batch: 300 | Loss: 1.390 | Acc: 72.241,89.200,92.431,% | Adaptive Acc: 84.834% | clf_exit: 0.681 0.245 0.074
Batch: 320 | Loss: 1.391 | Acc: 72.250,89.201,92.416,% | Adaptive Acc: 84.779% | clf_exit: 0.681 0.245 0.074
Batch: 340 | Loss: 1.390 | Acc: 72.315,89.221,92.403,% | Adaptive Acc: 84.790% | clf_exit: 0.681 0.246 0.073
Batch: 360 | Loss: 1.389 | Acc: 72.329,89.223,92.376,% | Adaptive Acc: 84.736% | clf_exit: 0.682 0.245 0.073
Batch: 380 | Loss: 1.389 | Acc: 72.265,89.253,92.409,% | Adaptive Acc: 84.742% | clf_exit: 0.682 0.245 0.072
Batch: 0 | Loss: 1.568 | Acc: 68.750,90.625,88.281,% | Adaptive Acc: 78.906% | clf_exit: 0.734 0.219 0.047
Batch: 20 | Loss: 1.713 | Acc: 69.903,85.565,87.574,% | Adaptive Acc: 80.357% | clf_exit: 0.731 0.214 0.054
Batch: 40 | Loss: 1.722 | Acc: 70.751,85.404,87.576,% | Adaptive Acc: 80.145% | clf_exit: 0.739 0.204 0.057
Batch: 60 | Loss: 1.712 | Acc: 70.889,85.246,87.692,% | Adaptive Acc: 80.072% | clf_exit: 0.737 0.205 0.058
Train classifier parameters

Epoch: 97
Batch: 0 | Loss: 1.403 | Acc: 73.438,89.062,88.281,% | Adaptive Acc: 84.375% | clf_exit: 0.703 0.227 0.070
Batch: 20 | Loss: 1.388 | Acc: 72.656,88.951,92.746,% | Adaptive Acc: 85.156% | clf_exit: 0.683 0.246 0.071
Batch: 40 | Loss: 1.402 | Acc: 72.180,88.720,92.416,% | Adaptive Acc: 84.451% | clf_exit: 0.683 0.247 0.070
Batch: 60 | Loss: 1.377 | Acc: 72.618,89.088,92.777,% | Adaptive Acc: 84.810% | clf_exit: 0.686 0.245 0.069
Batch: 80 | Loss: 1.370 | Acc: 72.492,89.313,92.911,% | Adaptive Acc: 85.012% | clf_exit: 0.687 0.244 0.069
Batch: 100 | Loss: 1.366 | Acc: 72.718,89.356,92.891,% | Adaptive Acc: 85.025% | clf_exit: 0.687 0.242 0.070
Batch: 120 | Loss: 1.359 | Acc: 72.934,89.489,92.930,% | Adaptive Acc: 85.285% | clf_exit: 0.687 0.244 0.069
Batch: 140 | Loss: 1.358 | Acc: 72.834,89.473,92.847,% | Adaptive Acc: 85.245% | clf_exit: 0.686 0.245 0.069
Batch: 160 | Loss: 1.371 | Acc: 72.719,89.407,92.639,% | Adaptive Acc: 85.171% | clf_exit: 0.684 0.246 0.071
Batch: 180 | Loss: 1.374 | Acc: 72.743,89.442,92.619,% | Adaptive Acc: 85.126% | clf_exit: 0.684 0.244 0.071
Batch: 200 | Loss: 1.380 | Acc: 72.711,89.350,92.592,% | Adaptive Acc: 85.110% | clf_exit: 0.683 0.245 0.072
Batch: 220 | Loss: 1.377 | Acc: 72.699,89.402,92.629,% | Adaptive Acc: 85.177% | clf_exit: 0.682 0.247 0.071
Batch: 240 | Loss: 1.374 | Acc: 72.734,89.455,92.683,% | Adaptive Acc: 85.189% | clf_exit: 0.681 0.247 0.071
Batch: 260 | Loss: 1.377 | Acc: 72.587,89.398,92.705,% | Adaptive Acc: 85.090% | clf_exit: 0.681 0.248 0.071
Batch: 280 | Loss: 1.378 | Acc: 72.514,89.377,92.699,% | Adaptive Acc: 85.056% | clf_exit: 0.681 0.248 0.071
Batch: 300 | Loss: 1.378 | Acc: 72.456,89.371,92.691,% | Adaptive Acc: 85.060% | clf_exit: 0.680 0.249 0.071
Batch: 320 | Loss: 1.377 | Acc: 72.476,89.391,92.674,% | Adaptive Acc: 85.088% | clf_exit: 0.680 0.249 0.071
Batch: 340 | Loss: 1.376 | Acc: 72.505,89.395,92.659,% | Adaptive Acc: 85.092% | clf_exit: 0.680 0.249 0.071
Batch: 360 | Loss: 1.379 | Acc: 72.457,89.374,92.644,% | Adaptive Acc: 85.037% | clf_exit: 0.680 0.248 0.072
Batch: 380 | Loss: 1.377 | Acc: 72.515,89.352,92.698,% | Adaptive Acc: 85.080% | clf_exit: 0.680 0.248 0.072
Batch: 0 | Loss: 1.548 | Acc: 67.969,91.406,87.500,% | Adaptive Acc: 77.344% | clf_exit: 0.766 0.195 0.039
Batch: 20 | Loss: 1.716 | Acc: 70.015,85.714,87.463,% | Adaptive Acc: 80.060% | clf_exit: 0.735 0.211 0.054
Batch: 40 | Loss: 1.725 | Acc: 70.827,85.404,87.557,% | Adaptive Acc: 80.030% | clf_exit: 0.739 0.204 0.056
Batch: 60 | Loss: 1.714 | Acc: 71.004,85.374,87.718,% | Adaptive Acc: 80.110% | clf_exit: 0.735 0.208 0.058
Train classifier parameters

Epoch: 98
Batch: 0 | Loss: 1.448 | Acc: 73.438,86.719,90.625,% | Adaptive Acc: 84.375% | clf_exit: 0.609 0.305 0.086
Batch: 20 | Loss: 1.401 | Acc: 72.061,89.174,92.039,% | Adaptive Acc: 84.859% | clf_exit: 0.687 0.241 0.072
Batch: 40 | Loss: 1.376 | Acc: 72.675,88.967,92.569,% | Adaptive Acc: 85.099% | clf_exit: 0.687 0.245 0.068
Batch: 60 | Loss: 1.379 | Acc: 72.387,88.858,92.559,% | Adaptive Acc: 85.015% | clf_exit: 0.684 0.245 0.070
Batch: 80 | Loss: 1.381 | Acc: 72.483,89.188,92.631,% | Adaptive Acc: 84.973% | clf_exit: 0.683 0.246 0.071
Batch: 100 | Loss: 1.385 | Acc: 72.246,89.302,92.636,% | Adaptive Acc: 84.916% | clf_exit: 0.681 0.248 0.070
Batch: 120 | Loss: 1.379 | Acc: 72.327,89.398,92.756,% | Adaptive Acc: 84.995% | clf_exit: 0.680 0.249 0.070
Batch: 140 | Loss: 1.393 | Acc: 72.130,89.190,92.581,% | Adaptive Acc: 84.680% | clf_exit: 0.679 0.249 0.072
Batch: 160 | Loss: 1.407 | Acc: 71.788,89.024,92.503,% | Adaptive Acc: 84.467% | clf_exit: 0.677 0.249 0.074
Batch: 180 | Loss: 1.410 | Acc: 71.741,89.037,92.485,% | Adaptive Acc: 84.457% | clf_exit: 0.676 0.250 0.074
Batch: 200 | Loss: 1.401 | Acc: 71.926,89.226,92.588,% | Adaptive Acc: 84.655% | clf_exit: 0.678 0.249 0.073
Batch: 220 | Loss: 1.400 | Acc: 72.016,89.243,92.569,% | Adaptive Acc: 84.647% | clf_exit: 0.679 0.248 0.073
Batch: 240 | Loss: 1.397 | Acc: 72.079,89.254,92.564,% | Adaptive Acc: 84.751% | clf_exit: 0.679 0.247 0.073
Batch: 260 | Loss: 1.392 | Acc: 72.186,89.278,92.613,% | Adaptive Acc: 84.797% | clf_exit: 0.680 0.247 0.073
Batch: 280 | Loss: 1.392 | Acc: 72.159,89.299,92.538,% | Adaptive Acc: 84.798% | clf_exit: 0.679 0.247 0.074
Batch: 300 | Loss: 1.392 | Acc: 72.106,89.340,92.592,% | Adaptive Acc: 84.733% | clf_exit: 0.680 0.247 0.073
Batch: 320 | Loss: 1.390 | Acc: 72.123,89.364,92.652,% | Adaptive Acc: 84.777% | clf_exit: 0.680 0.247 0.073
Batch: 340 | Loss: 1.391 | Acc: 72.093,89.365,92.655,% | Adaptive Acc: 84.723% | clf_exit: 0.680 0.246 0.073
Batch: 360 | Loss: 1.389 | Acc: 72.161,89.374,92.648,% | Adaptive Acc: 84.786% | clf_exit: 0.681 0.246 0.073
Batch: 380 | Loss: 1.385 | Acc: 72.193,89.454,92.651,% | Adaptive Acc: 84.808% | clf_exit: 0.681 0.247 0.073
Batch: 0 | Loss: 1.546 | Acc: 69.531,89.844,86.719,% | Adaptive Acc: 78.906% | clf_exit: 0.750 0.195 0.055
Batch: 20 | Loss: 1.709 | Acc: 70.089,85.677,87.426,% | Adaptive Acc: 79.985% | clf_exit: 0.742 0.205 0.052
Batch: 40 | Loss: 1.717 | Acc: 70.903,85.442,87.348,% | Adaptive Acc: 79.859% | clf_exit: 0.747 0.198 0.055
Batch: 60 | Loss: 1.706 | Acc: 70.966,85.425,87.551,% | Adaptive Acc: 80.059% | clf_exit: 0.742 0.201 0.057
Train classifier parameters

Epoch: 99
Batch: 0 | Loss: 1.116 | Acc: 75.781,92.969,91.406,% | Adaptive Acc: 89.062% | clf_exit: 0.680 0.266 0.055
Batch: 20 | Loss: 1.343 | Acc: 72.582,90.327,93.043,% | Adaptive Acc: 85.863% | clf_exit: 0.679 0.254 0.067
Batch: 40 | Loss: 1.374 | Acc: 72.809,89.863,92.816,% | Adaptive Acc: 85.442% | clf_exit: 0.676 0.250 0.074
Batch: 60 | Loss: 1.349 | Acc: 73.297,90.266,92.853,% | Adaptive Acc: 85.502% | clf_exit: 0.683 0.248 0.069
Batch: 80 | Loss: 1.359 | Acc: 73.389,90.075,92.573,% | Adaptive Acc: 85.571% | clf_exit: 0.682 0.246 0.072
Batch: 100 | Loss: 1.357 | Acc: 73.275,89.944,92.613,% | Adaptive Acc: 85.381% | clf_exit: 0.683 0.247 0.070
Batch: 120 | Loss: 1.364 | Acc: 73.018,89.857,92.652,% | Adaptive Acc: 85.234% | clf_exit: 0.683 0.247 0.070
Batch: 140 | Loss: 1.359 | Acc: 73.055,89.855,92.692,% | Adaptive Acc: 85.212% | clf_exit: 0.687 0.244 0.069
Batch: 160 | Loss: 1.360 | Acc: 72.981,89.718,92.712,% | Adaptive Acc: 85.108% | clf_exit: 0.687 0.244 0.069
Batch: 180 | Loss: 1.361 | Acc: 72.898,89.662,92.723,% | Adaptive Acc: 84.958% | clf_exit: 0.688 0.244 0.069
Batch: 200 | Loss: 1.366 | Acc: 72.792,89.533,92.646,% | Adaptive Acc: 84.907% | clf_exit: 0.686 0.244 0.070
Batch: 220 | Loss: 1.370 | Acc: 72.709,89.476,92.590,% | Adaptive Acc: 84.782% | clf_exit: 0.685 0.245 0.070
Batch: 240 | Loss: 1.369 | Acc: 72.838,89.546,92.557,% | Adaptive Acc: 84.842% | clf_exit: 0.685 0.245 0.069
Batch: 260 | Loss: 1.374 | Acc: 72.686,89.503,92.544,% | Adaptive Acc: 84.743% | clf_exit: 0.685 0.246 0.069
Batch: 280 | Loss: 1.379 | Acc: 72.553,89.424,92.532,% | Adaptive Acc: 84.675% | clf_exit: 0.684 0.246 0.070
Batch: 300 | Loss: 1.379 | Acc: 72.495,89.460,92.540,% | Adaptive Acc: 84.718% | clf_exit: 0.682 0.248 0.070
Batch: 320 | Loss: 1.379 | Acc: 72.500,89.420,92.555,% | Adaptive Acc: 84.716% | clf_exit: 0.682 0.248 0.070
Batch: 340 | Loss: 1.380 | Acc: 72.533,89.441,92.501,% | Adaptive Acc: 84.719% | clf_exit: 0.682 0.248 0.070
Batch: 360 | Loss: 1.381 | Acc: 72.516,89.461,92.480,% | Adaptive Acc: 84.708% | clf_exit: 0.681 0.248 0.070
Batch: 380 | Loss: 1.379 | Acc: 72.566,89.493,92.485,% | Adaptive Acc: 84.760% | clf_exit: 0.682 0.248 0.070
Batch: 0 | Loss: 1.554 | Acc: 67.969,89.844,87.500,% | Adaptive Acc: 77.344% | clf_exit: 0.766 0.195 0.039
Batch: 20 | Loss: 1.707 | Acc: 69.829,85.826,87.351,% | Adaptive Acc: 80.283% | clf_exit: 0.737 0.211 0.052
Batch: 40 | Loss: 1.715 | Acc: 70.675,85.575,87.462,% | Adaptive Acc: 80.069% | clf_exit: 0.740 0.204 0.056
Batch: 60 | Loss: 1.706 | Acc: 70.966,85.451,87.602,% | Adaptive Acc: 80.110% | clf_exit: 0.736 0.206 0.058
Train all parameters

Epoch: 100
Batch: 0 | Loss: 1.323 | Acc: 72.656,91.406,92.969,% | Adaptive Acc: 89.844% | clf_exit: 0.664 0.258 0.078
Batch: 20 | Loss: 1.563 | Acc: 68.638,86.421,92.039,% | Adaptive Acc: 81.882% | clf_exit: 0.679 0.256 0.065
Batch: 40 | Loss: 1.898 | Acc: 66.139,82.412,86.738,% | Adaptive Acc: 78.163% | clf_exit: 0.647 0.269 0.084
Batch: 60 | Loss: 2.043 | Acc: 64.395,80.904,84.221,% | Adaptive Acc: 76.294% | clf_exit: 0.637 0.268 0.096
Batch: 80 | Loss: 2.069 | Acc: 64.786,80.363,83.468,% | Adaptive Acc: 76.148% | clf_exit: 0.637 0.264 0.099
Batch: 100 | Loss: 2.053 | Acc: 64.805,80.453,83.826,% | Adaptive Acc: 76.284% | clf_exit: 0.637 0.262 0.101
Batch: 120 | Loss: 2.020 | Acc: 65.199,80.876,84.226,% | Adaptive Acc: 76.640% | clf_exit: 0.640 0.262 0.099
Batch: 140 | Loss: 1.983 | Acc: 65.631,81.416,84.707,% | Adaptive Acc: 77.122% | clf_exit: 0.644 0.260 0.096
Batch: 160 | Loss: 1.966 | Acc: 65.834,81.672,85.006,% | Adaptive Acc: 77.290% | clf_exit: 0.645 0.259 0.096
Batch: 180 | Loss: 1.954 | Acc: 66.095,81.828,85.195,% | Adaptive Acc: 77.508% | clf_exit: 0.645 0.259 0.096
Batch: 200 | Loss: 1.937 | Acc: 66.297,82.074,85.370,% | Adaptive Acc: 77.756% | clf_exit: 0.646 0.259 0.095
Batch: 220 | Loss: 1.917 | Acc: 66.502,82.325,85.644,% | Adaptive Acc: 78.026% | clf_exit: 0.646 0.259 0.095
Batch: 240 | Loss: 1.899 | Acc: 66.756,82.527,85.792,% | Adaptive Acc: 78.232% | clf_exit: 0.649 0.257 0.094
Batch: 260 | Loss: 1.886 | Acc: 66.900,82.669,85.964,% | Adaptive Acc: 78.400% | clf_exit: 0.650 0.257 0.094
Batch: 280 | Loss: 1.876 | Acc: 67.032,82.871,86.140,% | Adaptive Acc: 78.578% | clf_exit: 0.651 0.256 0.093
Batch: 300 | Loss: 1.864 | Acc: 67.247,83.018,86.329,% | Adaptive Acc: 78.771% | clf_exit: 0.652 0.255 0.093
Batch: 320 | Loss: 1.855 | Acc: 67.341,83.105,86.434,% | Adaptive Acc: 78.848% | clf_exit: 0.654 0.254 0.092
Batch: 340 | Loss: 1.843 | Acc: 67.456,83.257,86.606,% | Adaptive Acc: 79.032% | clf_exit: 0.655 0.253 0.092
Batch: 360 | Loss: 1.827 | Acc: 67.592,83.442,86.831,% | Adaptive Acc: 79.194% | clf_exit: 0.657 0.253 0.090
Batch: 380 | Loss: 1.817 | Acc: 67.694,83.557,86.965,% | Adaptive Acc: 79.347% | clf_exit: 0.658 0.252 0.090
Batch: 0 | Loss: 2.645 | Acc: 60.156,74.219,82.031,% | Adaptive Acc: 64.844% | clf_exit: 0.812 0.148 0.039
Batch: 20 | Loss: 2.393 | Acc: 63.951,78.757,82.403,% | Adaptive Acc: 72.321% | clf_exit: 0.759 0.190 0.051
Batch: 40 | Loss: 2.325 | Acc: 64.596,78.735,82.260,% | Adaptive Acc: 72.332% | clf_exit: 0.767 0.180 0.053
Batch: 60 | Loss: 2.333 | Acc: 64.677,78.932,82.236,% | Adaptive Acc: 72.374% | clf_exit: 0.764 0.182 0.053
Train all parameters

Epoch: 101
Batch: 0 | Loss: 1.840 | Acc: 65.625,82.031,88.281,% | Adaptive Acc: 78.906% | clf_exit: 0.656 0.242 0.102
Batch: 20 | Loss: 1.523 | Acc: 70.275,87.240,90.588,% | Adaptive Acc: 83.073% | clf_exit: 0.677 0.248 0.075
Batch: 40 | Loss: 1.513 | Acc: 70.446,87.481,90.892,% | Adaptive Acc: 82.946% | clf_exit: 0.679 0.245 0.075
Batch: 60 | Loss: 1.515 | Acc: 70.620,87.462,90.920,% | Adaptive Acc: 83.017% | clf_exit: 0.680 0.246 0.075
Batch: 80 | Loss: 1.492 | Acc: 70.853,87.539,91.281,% | Adaptive Acc: 83.073% | clf_exit: 0.687 0.241 0.072
Batch: 100 | Loss: 1.504 | Acc: 70.931,87.570,91.190,% | Adaptive Acc: 83.029% | clf_exit: 0.688 0.239 0.073
Batch: 120 | Loss: 1.509 | Acc: 70.726,87.526,91.025,% | Adaptive Acc: 82.942% | clf_exit: 0.686 0.241 0.073
Batch: 140 | Loss: 1.504 | Acc: 70.817,87.655,91.068,% | Adaptive Acc: 83.056% | clf_exit: 0.688 0.241 0.071
Batch: 160 | Loss: 1.500 | Acc: 70.720,87.767,91.130,% | Adaptive Acc: 83.128% | clf_exit: 0.687 0.241 0.072
Batch: 180 | Loss: 1.488 | Acc: 70.973,87.979,91.212,% | Adaptive Acc: 83.274% | clf_exit: 0.688 0.240 0.072
Batch: 200 | Loss: 1.487 | Acc: 70.923,88.005,91.290,% | Adaptive Acc: 83.190% | clf_exit: 0.689 0.240 0.071
Batch: 220 | Loss: 1.485 | Acc: 71.118,87.903,91.297,% | Adaptive Acc: 83.180% | clf_exit: 0.690 0.239 0.071
Batch: 240 | Loss: 1.484 | Acc: 71.110,87.908,91.322,% | Adaptive Acc: 83.153% | clf_exit: 0.690 0.239 0.071
Batch: 260 | Loss: 1.483 | Acc: 71.076,87.940,91.322,% | Adaptive Acc: 83.142% | clf_exit: 0.690 0.239 0.071
Batch: 280 | Loss: 1.487 | Acc: 71.080,87.931,91.309,% | Adaptive Acc: 83.113% | clf_exit: 0.690 0.240 0.071
Batch: 300 | Loss: 1.482 | Acc: 71.268,87.907,91.339,% | Adaptive Acc: 83.215% | clf_exit: 0.689 0.240 0.071
Batch: 320 | Loss: 1.486 | Acc: 71.118,87.853,91.319,% | Adaptive Acc: 83.165% | clf_exit: 0.689 0.240 0.071
Batch: 340 | Loss: 1.486 | Acc: 71.046,87.768,91.294,% | Adaptive Acc: 83.110% | clf_exit: 0.689 0.240 0.071
Batch: 360 | Loss: 1.486 | Acc: 70.990,87.749,91.326,% | Adaptive Acc: 83.144% | clf_exit: 0.688 0.241 0.071
Batch: 380 | Loss: 1.487 | Acc: 71.012,87.752,91.345,% | Adaptive Acc: 83.157% | clf_exit: 0.689 0.241 0.071
Batch: 0 | Loss: 1.709 | Acc: 62.500,85.156,89.844,% | Adaptive Acc: 73.438% | clf_exit: 0.766 0.180 0.055
Batch: 20 | Loss: 1.937 | Acc: 62.686,82.515,87.686,% | Adaptive Acc: 72.247% | clf_exit: 0.753 0.208 0.038
Batch: 40 | Loss: 1.884 | Acc: 64.101,83.194,87.824,% | Adaptive Acc: 73.533% | clf_exit: 0.759 0.198 0.043
Batch: 60 | Loss: 1.880 | Acc: 64.127,83.414,87.756,% | Adaptive Acc: 73.873% | clf_exit: 0.757 0.199 0.044
Train all parameters

Epoch: 102
Batch: 0 | Loss: 1.546 | Acc: 66.406,87.500,92.188,% | Adaptive Acc: 81.250% | clf_exit: 0.719 0.234 0.047
Batch: 20 | Loss: 1.477 | Acc: 70.126,88.653,92.374,% | Adaptive Acc: 82.775% | clf_exit: 0.686 0.254 0.061
Batch: 40 | Loss: 1.432 | Acc: 71.627,88.777,92.302,% | Adaptive Acc: 83.537% | clf_exit: 0.695 0.240 0.065
Batch: 60 | Loss: 1.398 | Acc: 71.952,89.267,92.751,% | Adaptive Acc: 83.696% | clf_exit: 0.698 0.238 0.064
Batch: 80 | Loss: 1.397 | Acc: 71.981,89.159,92.814,% | Adaptive Acc: 83.883% | clf_exit: 0.701 0.237 0.062
Batch: 100 | Loss: 1.394 | Acc: 72.208,89.186,92.752,% | Adaptive Acc: 84.042% | clf_exit: 0.701 0.236 0.062
Batch: 120 | Loss: 1.408 | Acc: 71.927,88.933,92.581,% | Adaptive Acc: 83.852% | clf_exit: 0.697 0.239 0.064
Batch: 140 | Loss: 1.415 | Acc: 71.653,88.841,92.459,% | Adaptive Acc: 83.799% | clf_exit: 0.695 0.240 0.065
Batch: 160 | Loss: 1.413 | Acc: 71.501,88.912,92.401,% | Adaptive Acc: 83.802% | clf_exit: 0.695 0.241 0.064
Batch: 180 | Loss: 1.407 | Acc: 71.556,89.032,92.459,% | Adaptive Acc: 83.810% | clf_exit: 0.695 0.241 0.064
Batch: 200 | Loss: 1.394 | Acc: 71.704,89.070,92.631,% | Adaptive Acc: 83.854% | clf_exit: 0.696 0.241 0.063
Batch: 220 | Loss: 1.389 | Acc: 71.780,89.055,92.707,% | Adaptive Acc: 83.930% | clf_exit: 0.697 0.241 0.063
Batch: 240 | Loss: 1.391 | Acc: 71.693,89.033,92.690,% | Adaptive Acc: 83.863% | clf_exit: 0.697 0.241 0.063
Batch: 260 | Loss: 1.396 | Acc: 71.645,88.949,92.702,% | Adaptive Acc: 83.776% | clf_exit: 0.697 0.240 0.063
Batch: 280 | Loss: 1.401 | Acc: 71.547,88.857,92.649,% | Adaptive Acc: 83.730% | clf_exit: 0.696 0.240 0.064
Batch: 300 | Loss: 1.398 | Acc: 71.618,88.834,92.665,% | Adaptive Acc: 83.762% | clf_exit: 0.696 0.240 0.063
Batch: 320 | Loss: 1.395 | Acc: 71.746,88.826,92.689,% | Adaptive Acc: 83.891% | clf_exit: 0.697 0.240 0.063
Batch: 340 | Loss: 1.394 | Acc: 71.795,88.776,92.724,% | Adaptive Acc: 83.940% | clf_exit: 0.697 0.239 0.064
Batch: 360 | Loss: 1.390 | Acc: 71.869,88.781,92.718,% | Adaptive Acc: 83.959% | clf_exit: 0.699 0.238 0.063
Batch: 380 | Loss: 1.394 | Acc: 71.883,88.730,92.651,% | Adaptive Acc: 83.907% | clf_exit: 0.698 0.239 0.063
Batch: 0 | Loss: 1.736 | Acc: 61.719,85.156,89.844,% | Adaptive Acc: 68.750% | clf_exit: 0.781 0.172 0.047
Batch: 20 | Loss: 1.931 | Acc: 65.141,82.924,87.351,% | Adaptive Acc: 74.777% | clf_exit: 0.754 0.192 0.054
Batch: 40 | Loss: 1.903 | Acc: 65.835,83.308,87.519,% | Adaptive Acc: 75.095% | clf_exit: 0.763 0.187 0.050
Batch: 60 | Loss: 1.893 | Acc: 65.599,83.184,87.602,% | Adaptive Acc: 75.154% | clf_exit: 0.759 0.191 0.050
Train all parameters

Epoch: 103
Batch: 0 | Loss: 1.254 | Acc: 81.250,89.844,94.531,% | Adaptive Acc: 91.406% | clf_exit: 0.742 0.156 0.102
Batch: 20 | Loss: 1.375 | Acc: 71.689,88.988,93.304,% | Adaptive Acc: 83.705% | clf_exit: 0.709 0.229 0.062
Batch: 40 | Loss: 1.356 | Acc: 72.199,89.310,93.598,% | Adaptive Acc: 83.880% | clf_exit: 0.707 0.236 0.057
Batch: 60 | Loss: 1.348 | Acc: 71.926,89.447,93.609,% | Adaptive Acc: 84.362% | clf_exit: 0.704 0.237 0.059
Batch: 80 | Loss: 1.326 | Acc: 72.232,89.786,93.769,% | Adaptive Acc: 84.770% | clf_exit: 0.705 0.236 0.058
Batch: 100 | Loss: 1.320 | Acc: 72.594,89.859,93.704,% | Adaptive Acc: 85.002% | clf_exit: 0.704 0.237 0.058
Batch: 120 | Loss: 1.325 | Acc: 72.417,89.631,93.640,% | Adaptive Acc: 84.601% | clf_exit: 0.707 0.234 0.059
Batch: 140 | Loss: 1.332 | Acc: 72.379,89.567,93.523,% | Adaptive Acc: 84.475% | clf_exit: 0.709 0.232 0.059
Batch: 160 | Loss: 1.331 | Acc: 72.482,89.591,93.498,% | Adaptive Acc: 84.385% | clf_exit: 0.712 0.230 0.058
Batch: 180 | Loss: 1.327 | Acc: 72.531,89.624,93.487,% | Adaptive Acc: 84.414% | clf_exit: 0.713 0.230 0.057
Batch: 200 | Loss: 1.332 | Acc: 72.357,89.579,93.451,% | Adaptive Acc: 84.270% | clf_exit: 0.712 0.232 0.057
Batch: 220 | Loss: 1.334 | Acc: 72.327,89.628,93.499,% | Adaptive Acc: 84.329% | clf_exit: 0.710 0.233 0.057
Batch: 240 | Loss: 1.332 | Acc: 72.387,89.627,93.536,% | Adaptive Acc: 84.482% | clf_exit: 0.709 0.234 0.057
Batch: 260 | Loss: 1.335 | Acc: 72.327,89.529,93.472,% | Adaptive Acc: 84.534% | clf_exit: 0.708 0.234 0.058
Batch: 280 | Loss: 1.330 | Acc: 72.412,89.563,93.497,% | Adaptive Acc: 84.631% | clf_exit: 0.708 0.235 0.058
Batch: 300 | Loss: 1.329 | Acc: 72.534,89.558,93.485,% | Adaptive Acc: 84.658% | clf_exit: 0.708 0.234 0.058
Batch: 320 | Loss: 1.326 | Acc: 72.668,89.600,93.502,% | Adaptive Acc: 84.730% | clf_exit: 0.708 0.234 0.059
Batch: 340 | Loss: 1.327 | Acc: 72.697,89.596,93.480,% | Adaptive Acc: 84.726% | clf_exit: 0.709 0.233 0.059
Batch: 360 | Loss: 1.326 | Acc: 72.726,89.601,93.464,% | Adaptive Acc: 84.715% | clf_exit: 0.709 0.233 0.058
Batch: 380 | Loss: 1.325 | Acc: 72.749,89.587,93.453,% | Adaptive Acc: 84.672% | clf_exit: 0.710 0.232 0.058
Batch: 0 | Loss: 1.263 | Acc: 75.000,91.406,90.625,% | Adaptive Acc: 82.812% | clf_exit: 0.773 0.188 0.039
Batch: 20 | Loss: 1.604 | Acc: 70.982,85.305,89.435,% | Adaptive Acc: 79.911% | clf_exit: 0.746 0.210 0.044
Batch: 40 | Loss: 1.596 | Acc: 71.684,85.252,89.901,% | Adaptive Acc: 80.069% | clf_exit: 0.760 0.195 0.045
Batch: 60 | Loss: 1.606 | Acc: 71.696,85.105,89.754,% | Adaptive Acc: 80.085% | clf_exit: 0.757 0.199 0.044
Train all parameters

Epoch: 104
Batch: 0 | Loss: 1.115 | Acc: 76.562,92.188,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.695 0.250 0.055
Batch: 20 | Loss: 1.261 | Acc: 74.405,89.881,94.829,% | Adaptive Acc: 85.826% | clf_exit: 0.719 0.226 0.055
Batch: 40 | Loss: 1.238 | Acc: 74.371,90.511,94.512,% | Adaptive Acc: 86.300% | clf_exit: 0.715 0.229 0.056
Batch: 60 | Loss: 1.240 | Acc: 74.091,90.856,94.608,% | Adaptive Acc: 86.245% | clf_exit: 0.713 0.230 0.057
Batch: 80 | Loss: 1.250 | Acc: 73.843,90.548,94.454,% | Adaptive Acc: 85.947% | clf_exit: 0.712 0.231 0.057
Batch: 100 | Loss: 1.244 | Acc: 74.002,90.602,94.554,% | Adaptive Acc: 86.139% | clf_exit: 0.712 0.230 0.058
Batch: 120 | Loss: 1.248 | Acc: 73.754,90.476,94.576,% | Adaptive Acc: 85.789% | clf_exit: 0.714 0.228 0.058
Batch: 140 | Loss: 1.254 | Acc: 73.809,90.442,94.548,% | Adaptive Acc: 85.755% | clf_exit: 0.715 0.227 0.058
Batch: 160 | Loss: 1.255 | Acc: 73.738,90.407,94.585,% | Adaptive Acc: 85.680% | clf_exit: 0.715 0.227 0.058
Batch: 180 | Loss: 1.258 | Acc: 73.714,90.357,94.566,% | Adaptive Acc: 85.670% | clf_exit: 0.712 0.229 0.058
Batch: 200 | Loss: 1.257 | Acc: 73.733,90.403,94.527,% | Adaptive Acc: 85.669% | clf_exit: 0.711 0.230 0.058
Batch: 220 | Loss: 1.258 | Acc: 73.840,90.363,94.471,% | Adaptive Acc: 85.609% | clf_exit: 0.712 0.230 0.058
Batch: 240 | Loss: 1.261 | Acc: 73.817,90.281,94.411,% | Adaptive Acc: 85.561% | clf_exit: 0.712 0.231 0.057
Batch: 260 | Loss: 1.262 | Acc: 73.818,90.239,94.361,% | Adaptive Acc: 85.551% | clf_exit: 0.712 0.230 0.058
Batch: 280 | Loss: 1.268 | Acc: 73.741,90.163,94.284,% | Adaptive Acc: 85.526% | clf_exit: 0.712 0.230 0.058
Batch: 300 | Loss: 1.272 | Acc: 73.718,90.160,94.207,% | Adaptive Acc: 85.481% | clf_exit: 0.712 0.230 0.057
Batch: 320 | Loss: 1.273 | Acc: 73.669,90.172,94.203,% | Adaptive Acc: 85.426% | clf_exit: 0.714 0.229 0.057
Batch: 340 | Loss: 1.273 | Acc: 73.637,90.158,94.227,% | Adaptive Acc: 85.374% | clf_exit: 0.714 0.229 0.057
Batch: 360 | Loss: 1.276 | Acc: 73.518,90.132,94.204,% | Adaptive Acc: 85.284% | clf_exit: 0.713 0.230 0.057
Batch: 380 | Loss: 1.274 | Acc: 73.581,90.207,94.215,% | Adaptive Acc: 85.326% | clf_exit: 0.713 0.229 0.057
Batch: 0 | Loss: 1.268 | Acc: 71.875,92.188,94.531,% | Adaptive Acc: 82.812% | clf_exit: 0.797 0.156 0.047
Batch: 20 | Loss: 1.629 | Acc: 69.382,85.677,90.327,% | Adaptive Acc: 80.432% | clf_exit: 0.730 0.212 0.059
Batch: 40 | Loss: 1.620 | Acc: 69.703,86.090,89.844,% | Adaptive Acc: 80.335% | clf_exit: 0.735 0.209 0.056
Batch: 60 | Loss: 1.604 | Acc: 69.736,86.424,89.793,% | Adaptive Acc: 80.443% | clf_exit: 0.731 0.213 0.056
Train all parameters

Epoch: 105
Batch: 0 | Loss: 1.267 | Acc: 75.000,88.281,92.969,% | Adaptive Acc: 84.375% | clf_exit: 0.695 0.234 0.070
Batch: 20 | Loss: 1.245 | Acc: 75.074,90.625,94.122,% | Adaptive Acc: 86.235% | clf_exit: 0.707 0.227 0.066
Batch: 40 | Loss: 1.252 | Acc: 74.371,90.187,94.264,% | Adaptive Acc: 85.385% | clf_exit: 0.716 0.223 0.061
Batch: 60 | Loss: 1.251 | Acc: 74.219,90.484,94.416,% | Adaptive Acc: 85.579% | clf_exit: 0.711 0.231 0.058
Batch: 80 | Loss: 1.234 | Acc: 74.064,90.683,94.666,% | Adaptive Acc: 85.590% | clf_exit: 0.714 0.232 0.054
Batch: 100 | Loss: 1.228 | Acc: 74.025,90.857,94.701,% | Adaptive Acc: 85.690% | clf_exit: 0.714 0.232 0.054
Batch: 120 | Loss: 1.232 | Acc: 73.960,90.754,94.770,% | Adaptive Acc: 85.628% | clf_exit: 0.716 0.231 0.054
Batch: 140 | Loss: 1.230 | Acc: 74.008,90.719,94.742,% | Adaptive Acc: 85.594% | clf_exit: 0.717 0.230 0.053
Batch: 160 | Loss: 1.228 | Acc: 73.996,90.805,94.842,% | Adaptive Acc: 85.729% | clf_exit: 0.715 0.231 0.054
Batch: 180 | Loss: 1.228 | Acc: 73.878,90.854,94.829,% | Adaptive Acc: 85.627% | clf_exit: 0.716 0.231 0.053
Batch: 200 | Loss: 1.228 | Acc: 73.811,90.761,94.796,% | Adaptive Acc: 85.568% | clf_exit: 0.717 0.230 0.053
Batch: 220 | Loss: 1.234 | Acc: 73.738,90.692,94.726,% | Adaptive Acc: 85.453% | clf_exit: 0.717 0.230 0.053
Batch: 240 | Loss: 1.239 | Acc: 73.687,90.631,94.632,% | Adaptive Acc: 85.409% | clf_exit: 0.717 0.230 0.053
Batch: 260 | Loss: 1.243 | Acc: 73.716,90.565,94.558,% | Adaptive Acc: 85.393% | clf_exit: 0.717 0.229 0.053
Batch: 280 | Loss: 1.248 | Acc: 73.649,90.480,94.501,% | Adaptive Acc: 85.329% | clf_exit: 0.717 0.229 0.054
Batch: 300 | Loss: 1.252 | Acc: 73.596,90.412,94.443,% | Adaptive Acc: 85.294% | clf_exit: 0.716 0.230 0.054
Batch: 320 | Loss: 1.255 | Acc: 73.574,90.345,94.412,% | Adaptive Acc: 85.324% | clf_exit: 0.716 0.230 0.054
Batch: 340 | Loss: 1.253 | Acc: 73.639,90.362,94.412,% | Adaptive Acc: 85.342% | clf_exit: 0.716 0.230 0.054
Batch: 360 | Loss: 1.248 | Acc: 73.673,90.450,94.494,% | Adaptive Acc: 85.405% | clf_exit: 0.716 0.230 0.054
Batch: 380 | Loss: 1.247 | Acc: 73.706,90.447,94.498,% | Adaptive Acc: 85.429% | clf_exit: 0.717 0.229 0.054
Batch: 0 | Loss: 1.600 | Acc: 68.750,90.625,88.281,% | Adaptive Acc: 80.469% | clf_exit: 0.734 0.227 0.039
Batch: 20 | Loss: 1.726 | Acc: 68.229,85.379,88.839,% | Adaptive Acc: 77.865% | clf_exit: 0.765 0.199 0.036
Batch: 40 | Loss: 1.703 | Acc: 69.569,85.823,88.700,% | Adaptive Acc: 78.563% | clf_exit: 0.775 0.191 0.034
Batch: 60 | Loss: 1.682 | Acc: 69.454,85.899,88.755,% | Adaptive Acc: 78.458% | clf_exit: 0.774 0.192 0.034
Train all parameters

Epoch: 106
Batch: 0 | Loss: 1.319 | Acc: 68.750,89.844,92.969,% | Adaptive Acc: 84.375% | clf_exit: 0.672 0.289 0.039
Batch: 20 | Loss: 1.178 | Acc: 73.921,91.555,95.871,% | Adaptive Acc: 86.533% | clf_exit: 0.712 0.236 0.052
Batch: 40 | Loss: 1.195 | Acc: 73.247,91.692,95.579,% | Adaptive Acc: 85.842% | clf_exit: 0.711 0.240 0.050
Batch: 60 | Loss: 1.205 | Acc: 73.233,91.265,95.351,% | Adaptive Acc: 85.989% | clf_exit: 0.710 0.237 0.053
Batch: 80 | Loss: 1.201 | Acc: 73.563,91.310,95.351,% | Adaptive Acc: 86.063% | clf_exit: 0.710 0.238 0.052
Batch: 100 | Loss: 1.201 | Acc: 73.793,91.128,95.328,% | Adaptive Acc: 86.146% | clf_exit: 0.712 0.235 0.053
Batch: 120 | Loss: 1.196 | Acc: 73.896,91.174,95.371,% | Adaptive Acc: 86.157% | clf_exit: 0.713 0.233 0.054
Batch: 140 | Loss: 1.197 | Acc: 73.715,91.096,95.335,% | Adaptive Acc: 85.960% | clf_exit: 0.714 0.232 0.054
Batch: 160 | Loss: 1.198 | Acc: 73.869,91.067,95.254,% | Adaptive Acc: 85.947% | clf_exit: 0.714 0.233 0.054
Batch: 180 | Loss: 1.198 | Acc: 73.848,91.083,95.291,% | Adaptive Acc: 85.994% | clf_exit: 0.715 0.231 0.054
Batch: 200 | Loss: 1.192 | Acc: 73.916,91.192,95.344,% | Adaptive Acc: 86.074% | clf_exit: 0.716 0.231 0.054
Batch: 220 | Loss: 1.188 | Acc: 74.060,91.251,95.373,% | Adaptive Acc: 86.058% | clf_exit: 0.720 0.228 0.053
Batch: 240 | Loss: 1.190 | Acc: 74.066,91.157,95.316,% | Adaptive Acc: 86.028% | clf_exit: 0.720 0.228 0.052
Batch: 260 | Loss: 1.192 | Acc: 74.027,91.107,95.262,% | Adaptive Acc: 85.943% | clf_exit: 0.721 0.226 0.052
Batch: 280 | Loss: 1.194 | Acc: 74.052,91.134,95.274,% | Adaptive Acc: 86.038% | clf_exit: 0.721 0.228 0.052
Batch: 300 | Loss: 1.192 | Acc: 74.115,91.142,95.242,% | Adaptive Acc: 86.041% | clf_exit: 0.722 0.227 0.052
Batch: 320 | Loss: 1.195 | Acc: 74.121,91.087,95.181,% | Adaptive Acc: 86.013% | clf_exit: 0.722 0.226 0.052
Batch: 340 | Loss: 1.202 | Acc: 74.063,90.969,95.109,% | Adaptive Acc: 85.956% | clf_exit: 0.721 0.227 0.052
Batch: 360 | Loss: 1.207 | Acc: 73.970,90.937,95.055,% | Adaptive Acc: 85.894% | clf_exit: 0.720 0.227 0.052
Batch: 380 | Loss: 1.210 | Acc: 73.940,90.879,95.027,% | Adaptive Acc: 85.837% | clf_exit: 0.720 0.227 0.053
Batch: 0 | Loss: 1.639 | Acc: 66.406,85.156,88.281,% | Adaptive Acc: 79.688% | clf_exit: 0.719 0.203 0.078
Batch: 20 | Loss: 1.753 | Acc: 65.811,84.301,89.360,% | Adaptive Acc: 74.888% | clf_exit: 0.770 0.184 0.047
Batch: 40 | Loss: 1.736 | Acc: 66.806,84.546,89.348,% | Adaptive Acc: 75.534% | clf_exit: 0.779 0.179 0.042
Batch: 60 | Loss: 1.723 | Acc: 66.189,84.721,89.498,% | Adaptive Acc: 75.499% | clf_exit: 0.773 0.184 0.044
Train all parameters

Epoch: 107
Batch: 0 | Loss: 1.108 | Acc: 79.688,89.844,96.875,% | Adaptive Acc: 89.062% | clf_exit: 0.711 0.203 0.086
Batch: 20 | Loss: 1.081 | Acc: 76.711,92.560,95.796,% | Adaptive Acc: 87.798% | clf_exit: 0.749 0.206 0.046
Batch: 40 | Loss: 1.133 | Acc: 75.381,92.035,95.675,% | Adaptive Acc: 86.757% | clf_exit: 0.744 0.211 0.045
Batch: 60 | Loss: 1.138 | Acc: 75.154,91.880,95.620,% | Adaptive Acc: 86.475% | clf_exit: 0.740 0.214 0.046
Batch: 80 | Loss: 1.156 | Acc: 74.990,91.696,95.592,% | Adaptive Acc: 86.294% | clf_exit: 0.736 0.216 0.048
Batch: 100 | Loss: 1.164 | Acc: 74.760,91.569,95.645,% | Adaptive Acc: 86.262% | clf_exit: 0.733 0.218 0.049
Batch: 120 | Loss: 1.173 | Acc: 74.354,91.393,95.700,% | Adaptive Acc: 86.176% | clf_exit: 0.729 0.221 0.050
Batch: 140 | Loss: 1.174 | Acc: 74.363,91.384,95.634,% | Adaptive Acc: 86.170% | clf_exit: 0.728 0.221 0.050
Batch: 160 | Loss: 1.173 | Acc: 74.272,91.363,95.647,% | Adaptive Acc: 86.243% | clf_exit: 0.729 0.221 0.050
Batch: 180 | Loss: 1.173 | Acc: 74.253,91.329,95.653,% | Adaptive Acc: 86.296% | clf_exit: 0.729 0.221 0.050
Batch: 200 | Loss: 1.179 | Acc: 74.110,91.259,95.530,% | Adaptive Acc: 86.190% | clf_exit: 0.727 0.222 0.051
Batch: 220 | Loss: 1.186 | Acc: 74.091,91.166,95.450,% | Adaptive Acc: 86.157% | clf_exit: 0.727 0.222 0.051
Batch: 240 | Loss: 1.187 | Acc: 74.154,91.153,95.345,% | Adaptive Acc: 86.187% | clf_exit: 0.727 0.222 0.052
Batch: 260 | Loss: 1.193 | Acc: 74.054,91.047,95.274,% | Adaptive Acc: 86.129% | clf_exit: 0.727 0.221 0.052
Batch: 280 | Loss: 1.193 | Acc: 74.102,90.984,95.224,% | Adaptive Acc: 86.135% | clf_exit: 0.726 0.221 0.053
Batch: 300 | Loss: 1.192 | Acc: 74.136,91.017,95.193,% | Adaptive Acc: 86.145% | clf_exit: 0.727 0.222 0.052
Batch: 320 | Loss: 1.192 | Acc: 74.112,91.014,95.210,% | Adaptive Acc: 86.132% | clf_exit: 0.727 0.221 0.052
Batch: 340 | Loss: 1.195 | Acc: 74.084,90.937,95.173,% | Adaptive Acc: 86.116% | clf_exit: 0.726 0.222 0.053
Batch: 360 | Loss: 1.196 | Acc: 74.128,90.900,95.139,% | Adaptive Acc: 86.074% | clf_exit: 0.725 0.222 0.053
Batch: 380 | Loss: 1.195 | Acc: 74.198,90.900,95.118,% | Adaptive Acc: 86.085% | clf_exit: 0.726 0.221 0.053
Batch: 0 | Loss: 1.595 | Acc: 63.281,90.625,92.188,% | Adaptive Acc: 77.344% | clf_exit: 0.719 0.227 0.055
Batch: 20 | Loss: 1.654 | Acc: 68.638,86.682,89.397,% | Adaptive Acc: 79.092% | clf_exit: 0.737 0.211 0.052
Batch: 40 | Loss: 1.595 | Acc: 70.027,86.852,89.996,% | Adaptive Acc: 80.678% | clf_exit: 0.740 0.208 0.052
Batch: 60 | Loss: 1.579 | Acc: 69.954,87.244,90.023,% | Adaptive Acc: 80.546% | clf_exit: 0.742 0.209 0.049
Train all parameters

Epoch: 108
Batch: 0 | Loss: 1.208 | Acc: 71.875,89.844,93.750,% | Adaptive Acc: 82.031% | clf_exit: 0.719 0.227 0.055
Batch: 20 | Loss: 1.128 | Acc: 74.330,92.746,95.833,% | Adaptive Acc: 86.830% | clf_exit: 0.722 0.229 0.049
Batch: 40 | Loss: 1.134 | Acc: 74.848,92.264,95.922,% | Adaptive Acc: 87.138% | clf_exit: 0.717 0.234 0.049
Batch: 60 | Loss: 1.125 | Acc: 74.718,92.175,96.004,% | Adaptive Acc: 86.847% | clf_exit: 0.721 0.233 0.046
Batch: 80 | Loss: 1.123 | Acc: 74.875,92.178,96.046,% | Adaptive Acc: 87.076% | clf_exit: 0.723 0.229 0.047
Batch: 100 | Loss: 1.132 | Acc: 74.760,92.033,96.016,% | Adaptive Acc: 86.850% | clf_exit: 0.725 0.229 0.047
Batch: 120 | Loss: 1.133 | Acc: 74.690,91.929,95.997,% | Adaptive Acc: 86.796% | clf_exit: 0.724 0.229 0.047
Batch: 140 | Loss: 1.131 | Acc: 74.867,91.910,95.928,% | Adaptive Acc: 86.796% | clf_exit: 0.725 0.227 0.048
Batch: 160 | Loss: 1.137 | Acc: 74.782,91.799,95.972,% | Adaptive Acc: 86.821% | clf_exit: 0.724 0.228 0.049
Batch: 180 | Loss: 1.138 | Acc: 74.763,91.782,95.999,% | Adaptive Acc: 86.939% | clf_exit: 0.722 0.229 0.049
Batch: 200 | Loss: 1.137 | Acc: 74.841,91.791,96.051,% | Adaptive Acc: 86.925% | clf_exit: 0.723 0.228 0.049
Batch: 220 | Loss: 1.139 | Acc: 74.841,91.792,96.044,% | Adaptive Acc: 86.828% | clf_exit: 0.724 0.228 0.048
Batch: 240 | Loss: 1.138 | Acc: 74.822,91.753,96.058,% | Adaptive Acc: 86.806% | clf_exit: 0.726 0.226 0.048
Batch: 260 | Loss: 1.141 | Acc: 74.832,91.673,95.971,% | Adaptive Acc: 86.719% | clf_exit: 0.727 0.225 0.048
Batch: 280 | Loss: 1.144 | Acc: 74.778,91.645,95.891,% | Adaptive Acc: 86.624% | clf_exit: 0.727 0.225 0.048
Batch: 300 | Loss: 1.146 | Acc: 74.787,91.614,95.839,% | Adaptive Acc: 86.568% | clf_exit: 0.727 0.225 0.048
Batch: 320 | Loss: 1.150 | Acc: 74.817,91.596,95.785,% | Adaptive Acc: 86.558% | clf_exit: 0.727 0.225 0.048
Batch: 340 | Loss: 1.155 | Acc: 74.759,91.521,95.734,% | Adaptive Acc: 86.478% | clf_exit: 0.727 0.224 0.049
Batch: 360 | Loss: 1.159 | Acc: 74.699,91.476,95.709,% | Adaptive Acc: 86.429% | clf_exit: 0.726 0.225 0.049
Batch: 380 | Loss: 1.162 | Acc: 74.637,91.431,95.661,% | Adaptive Acc: 86.393% | clf_exit: 0.726 0.225 0.049
Batch: 0 | Loss: 1.511 | Acc: 67.969,86.719,91.406,% | Adaptive Acc: 77.344% | clf_exit: 0.750 0.180 0.070
Batch: 20 | Loss: 1.607 | Acc: 70.499,85.863,90.067,% | Adaptive Acc: 80.394% | clf_exit: 0.731 0.212 0.057
Batch: 40 | Loss: 1.599 | Acc: 71.265,85.747,90.034,% | Adaptive Acc: 80.945% | clf_exit: 0.728 0.212 0.059
Batch: 60 | Loss: 1.598 | Acc: 70.927,85.822,90.036,% | Adaptive Acc: 80.789% | clf_exit: 0.728 0.214 0.058
Train all parameters

Epoch: 109
Batch: 0 | Loss: 1.137 | Acc: 75.000,92.188,93.750,% | Adaptive Acc: 86.719% | clf_exit: 0.758 0.188 0.055
Batch: 20 | Loss: 1.142 | Acc: 74.814,91.741,95.610,% | Adaptive Acc: 86.644% | clf_exit: 0.719 0.232 0.049
Batch: 40 | Loss: 1.140 | Acc: 74.962,91.940,95.808,% | Adaptive Acc: 87.043% | clf_exit: 0.724 0.225 0.050
Batch: 60 | Loss: 1.139 | Acc: 75.179,91.906,95.863,% | Adaptive Acc: 86.924% | clf_exit: 0.723 0.228 0.049
Batch: 80 | Loss: 1.136 | Acc: 75.212,91.985,95.824,% | Adaptive Acc: 87.047% | clf_exit: 0.722 0.229 0.049
Batch: 100 | Loss: 1.135 | Acc: 75.131,92.110,95.885,% | Adaptive Acc: 86.897% | clf_exit: 0.725 0.226 0.049
Batch: 120 | Loss: 1.138 | Acc: 75.213,91.916,95.874,% | Adaptive Acc: 86.803% | clf_exit: 0.727 0.224 0.049
Batch: 140 | Loss: 1.152 | Acc: 74.789,91.733,95.711,% | Adaptive Acc: 86.436% | clf_exit: 0.727 0.224 0.049
Batch: 160 | Loss: 1.159 | Acc: 74.646,91.552,95.681,% | Adaptive Acc: 86.398% | clf_exit: 0.727 0.223 0.050
Batch: 180 | Loss: 1.158 | Acc: 74.573,91.549,95.697,% | Adaptive Acc: 86.404% | clf_exit: 0.728 0.222 0.050
Batch: 200 | Loss: 1.159 | Acc: 74.526,91.554,95.662,% | Adaptive Acc: 86.350% | clf_exit: 0.729 0.221 0.050
Batch: 220 | Loss: 1.157 | Acc: 74.576,91.562,95.662,% | Adaptive Acc: 86.503% | clf_exit: 0.728 0.222 0.050
Batch: 240 | Loss: 1.161 | Acc: 74.566,91.491,95.578,% | Adaptive Acc: 86.411% | clf_exit: 0.729 0.221 0.050
Batch: 260 | Loss: 1.164 | Acc: 74.500,91.439,95.567,% | Adaptive Acc: 86.342% | clf_exit: 0.729 0.220 0.051
Batch: 280 | Loss: 1.162 | Acc: 74.647,91.462,95.538,% | Adaptive Acc: 86.360% | clf_exit: 0.730 0.219 0.051
Batch: 300 | Loss: 1.160 | Acc: 74.702,91.515,95.538,% | Adaptive Acc: 86.387% | clf_exit: 0.730 0.220 0.050
Batch: 320 | Loss: 1.162 | Acc: 74.718,91.462,95.502,% | Adaptive Acc: 86.388% | clf_exit: 0.731 0.219 0.050
Batch: 340 | Loss: 1.159 | Acc: 74.734,91.502,95.519,% | Adaptive Acc: 86.357% | clf_exit: 0.731 0.219 0.050
Batch: 360 | Loss: 1.159 | Acc: 74.753,91.473,95.525,% | Adaptive Acc: 86.368% | clf_exit: 0.732 0.219 0.049
Batch: 380 | Loss: 1.159 | Acc: 74.811,91.472,95.536,% | Adaptive Acc: 86.362% | clf_exit: 0.732 0.219 0.049
Batch: 0 | Loss: 1.297 | Acc: 73.438,91.406,92.969,% | Adaptive Acc: 79.688% | clf_exit: 0.773 0.203 0.023
Batch: 20 | Loss: 1.543 | Acc: 72.135,86.421,89.993,% | Adaptive Acc: 80.097% | clf_exit: 0.767 0.192 0.041
Batch: 40 | Loss: 1.518 | Acc: 72.046,86.909,89.920,% | Adaptive Acc: 80.202% | clf_exit: 0.778 0.185 0.037
Batch: 60 | Loss: 1.503 | Acc: 72.157,87.026,90.036,% | Adaptive Acc: 80.635% | clf_exit: 0.776 0.187 0.037
Train all parameters

Epoch: 110
Batch: 0 | Loss: 1.041 | Acc: 77.344,93.750,97.656,% | Adaptive Acc: 89.062% | clf_exit: 0.719 0.211 0.070
Batch: 20 | Loss: 1.097 | Acc: 76.004,92.746,96.280,% | Adaptive Acc: 86.607% | clf_exit: 0.749 0.207 0.044
Batch: 40 | Loss: 1.106 | Acc: 75.057,92.283,96.646,% | Adaptive Acc: 86.147% | clf_exit: 0.742 0.216 0.042
Batch: 60 | Loss: 1.097 | Acc: 75.051,92.533,96.657,% | Adaptive Acc: 86.527% | clf_exit: 0.745 0.212 0.043
Batch: 80 | Loss: 1.085 | Acc: 75.511,92.554,96.750,% | Adaptive Acc: 86.892% | clf_exit: 0.742 0.215 0.043
Batch: 100 | Loss: 1.082 | Acc: 75.565,92.621,96.767,% | Adaptive Acc: 86.889% | clf_exit: 0.743 0.215 0.042
Batch: 120 | Loss: 1.094 | Acc: 75.426,92.497,96.623,% | Adaptive Acc: 86.932% | clf_exit: 0.738 0.217 0.045
Batch: 140 | Loss: 1.095 | Acc: 75.410,92.426,96.731,% | Adaptive Acc: 86.979% | clf_exit: 0.736 0.219 0.044
Batch: 160 | Loss: 1.100 | Acc: 75.296,92.265,96.666,% | Adaptive Acc: 86.918% | clf_exit: 0.738 0.218 0.044
Batch: 180 | Loss: 1.099 | Acc: 75.229,92.347,96.672,% | Adaptive Acc: 86.956% | clf_exit: 0.736 0.219 0.045
Batch: 200 | Loss: 1.100 | Acc: 75.264,92.285,96.611,% | Adaptive Acc: 86.936% | clf_exit: 0.737 0.218 0.045
Batch: 220 | Loss: 1.101 | Acc: 75.194,92.283,96.575,% | Adaptive Acc: 86.910% | clf_exit: 0.737 0.218 0.045
Batch: 240 | Loss: 1.102 | Acc: 75.227,92.233,96.518,% | Adaptive Acc: 86.939% | clf_exit: 0.736 0.219 0.045
Batch: 260 | Loss: 1.106 | Acc: 75.233,92.146,96.486,% | Adaptive Acc: 86.901% | clf_exit: 0.737 0.217 0.045
Batch: 280 | Loss: 1.113 | Acc: 75.131,92.015,96.402,% | Adaptive Acc: 86.783% | clf_exit: 0.736 0.219 0.045
Batch: 300 | Loss: 1.113 | Acc: 75.234,91.990,96.369,% | Adaptive Acc: 86.817% | clf_exit: 0.737 0.218 0.046
Batch: 320 | Loss: 1.117 | Acc: 75.253,91.998,96.318,% | Adaptive Acc: 86.797% | clf_exit: 0.737 0.218 0.046
Batch: 340 | Loss: 1.117 | Acc: 75.172,91.993,96.300,% | Adaptive Acc: 86.826% | clf_exit: 0.735 0.218 0.046
Batch: 360 | Loss: 1.120 | Acc: 75.106,91.982,96.198,% | Adaptive Acc: 86.807% | clf_exit: 0.735 0.219 0.046
Batch: 380 | Loss: 1.125 | Acc: 74.994,91.902,96.127,% | Adaptive Acc: 86.631% | clf_exit: 0.735 0.219 0.046
Batch: 0 | Loss: 1.588 | Acc: 72.656,89.844,89.062,% | Adaptive Acc: 86.719% | clf_exit: 0.695 0.258 0.047
Batch: 20 | Loss: 1.587 | Acc: 71.131,85.751,89.546,% | Adaptive Acc: 80.357% | clf_exit: 0.735 0.224 0.041
Batch: 40 | Loss: 1.580 | Acc: 71.208,86.490,89.672,% | Adaptive Acc: 80.240% | clf_exit: 0.746 0.213 0.041
Batch: 60 | Loss: 1.562 | Acc: 71.388,86.591,89.780,% | Adaptive Acc: 80.443% | clf_exit: 0.748 0.214 0.038
Train all parameters

Epoch: 111
Batch: 0 | Loss: 1.152 | Acc: 67.969,92.188,93.750,% | Adaptive Acc: 85.938% | clf_exit: 0.680 0.258 0.062
Batch: 20 | Loss: 1.108 | Acc: 74.851,92.262,95.908,% | Adaptive Acc: 87.686% | clf_exit: 0.725 0.229 0.046
Batch: 40 | Loss: 1.079 | Acc: 75.991,92.721,96.227,% | Adaptive Acc: 87.938% | clf_exit: 0.735 0.217 0.049
Batch: 60 | Loss: 1.084 | Acc: 75.730,92.674,96.260,% | Adaptive Acc: 87.487% | clf_exit: 0.733 0.220 0.046
Batch: 80 | Loss: 1.091 | Acc: 75.444,92.486,96.373,% | Adaptive Acc: 87.143% | clf_exit: 0.739 0.215 0.046
Batch: 100 | Loss: 1.097 | Acc: 75.193,92.280,96.372,% | Adaptive Acc: 86.920% | clf_exit: 0.737 0.218 0.045
Batch: 120 | Loss: 1.099 | Acc: 75.245,92.336,96.397,% | Adaptive Acc: 86.964% | clf_exit: 0.736 0.219 0.045
Batch: 140 | Loss: 1.099 | Acc: 75.316,92.260,96.365,% | Adaptive Acc: 87.018% | clf_exit: 0.737 0.218 0.045
Batch: 160 | Loss: 1.098 | Acc: 75.408,92.265,96.361,% | Adaptive Acc: 86.995% | clf_exit: 0.738 0.218 0.044
Batch: 180 | Loss: 1.097 | Acc: 75.324,92.317,96.366,% | Adaptive Acc: 86.948% | clf_exit: 0.738 0.217 0.045
Batch: 200 | Loss: 1.098 | Acc: 75.311,92.246,96.346,% | Adaptive Acc: 86.956% | clf_exit: 0.739 0.216 0.045
Batch: 220 | Loss: 1.097 | Acc: 75.290,92.230,96.334,% | Adaptive Acc: 86.963% | clf_exit: 0.739 0.216 0.045
Batch: 240 | Loss: 1.106 | Acc: 75.214,92.097,96.204,% | Adaptive Acc: 86.829% | clf_exit: 0.738 0.216 0.045
Batch: 260 | Loss: 1.113 | Acc: 75.117,92.038,96.127,% | Adaptive Acc: 86.791% | clf_exit: 0.736 0.217 0.046
Batch: 280 | Loss: 1.115 | Acc: 75.131,91.985,96.063,% | Adaptive Acc: 86.816% | clf_exit: 0.736 0.218 0.047
Batch: 300 | Loss: 1.118 | Acc: 75.166,91.969,96.037,% | Adaptive Acc: 86.846% | clf_exit: 0.735 0.218 0.047
Batch: 320 | Loss: 1.121 | Acc: 75.139,91.932,96.011,% | Adaptive Acc: 86.804% | clf_exit: 0.736 0.217 0.047
Batch: 340 | Loss: 1.123 | Acc: 75.128,91.922,96.027,% | Adaptive Acc: 86.792% | clf_exit: 0.735 0.218 0.047
Batch: 360 | Loss: 1.122 | Acc: 75.177,91.956,96.027,% | Adaptive Acc: 86.788% | clf_exit: 0.736 0.217 0.047
Batch: 380 | Loss: 1.123 | Acc: 75.182,91.892,95.977,% | Adaptive Acc: 86.723% | clf_exit: 0.736 0.217 0.047
Batch: 0 | Loss: 1.371 | Acc: 73.438,87.500,89.062,% | Adaptive Acc: 80.469% | clf_exit: 0.742 0.211 0.047
Batch: 20 | Loss: 1.479 | Acc: 73.549,87.128,90.104,% | Adaptive Acc: 81.027% | clf_exit: 0.774 0.181 0.045
Batch: 40 | Loss: 1.492 | Acc: 73.457,87.138,90.053,% | Adaptive Acc: 80.793% | clf_exit: 0.776 0.178 0.045
Batch: 60 | Loss: 1.470 | Acc: 73.566,87.244,90.254,% | Adaptive Acc: 81.404% | clf_exit: 0.776 0.179 0.044
Train all parameters

Epoch: 112
Batch: 0 | Loss: 1.018 | Acc: 75.000,92.969,96.875,% | Adaptive Acc: 88.281% | clf_exit: 0.789 0.180 0.031
Batch: 20 | Loss: 1.020 | Acc: 75.744,93.304,97.024,% | Adaptive Acc: 87.835% | clf_exit: 0.737 0.228 0.035
Batch: 40 | Loss: 1.055 | Acc: 74.829,93.102,97.066,% | Adaptive Acc: 87.309% | clf_exit: 0.740 0.222 0.038
Batch: 60 | Loss: 1.069 | Acc: 75.218,92.764,96.798,% | Adaptive Acc: 87.167% | clf_exit: 0.738 0.221 0.040
Batch: 80 | Loss: 1.074 | Acc: 75.482,92.525,96.692,% | Adaptive Acc: 87.027% | clf_exit: 0.742 0.217 0.041
Batch: 100 | Loss: 1.085 | Acc: 75.387,92.474,96.635,% | Adaptive Acc: 86.904% | clf_exit: 0.741 0.216 0.043
Batch: 120 | Loss: 1.082 | Acc: 75.523,92.485,96.584,% | Adaptive Acc: 86.938% | clf_exit: 0.742 0.215 0.042
Batch: 140 | Loss: 1.087 | Acc: 75.371,92.398,96.548,% | Adaptive Acc: 86.929% | clf_exit: 0.741 0.217 0.042
Batch: 160 | Loss: 1.084 | Acc: 75.437,92.450,96.574,% | Adaptive Acc: 86.971% | clf_exit: 0.741 0.217 0.043
Batch: 180 | Loss: 1.088 | Acc: 75.388,92.265,96.551,% | Adaptive Acc: 86.844% | clf_exit: 0.741 0.215 0.043
Batch: 200 | Loss: 1.100 | Acc: 75.260,92.016,96.377,% | Adaptive Acc: 86.707% | clf_exit: 0.741 0.216 0.043
Batch: 220 | Loss: 1.110 | Acc: 75.095,91.958,96.217,% | Adaptive Acc: 86.556% | clf_exit: 0.740 0.216 0.044
Batch: 240 | Loss: 1.109 | Acc: 75.188,92.009,96.230,% | Adaptive Acc: 86.647% | clf_exit: 0.739 0.216 0.044
Batch: 260 | Loss: 1.112 | Acc: 75.132,91.975,96.228,% | Adaptive Acc: 86.623% | clf_exit: 0.739 0.217 0.045
Batch: 280 | Loss: 1.114 | Acc: 75.136,91.937,96.219,% | Adaptive Acc: 86.699% | clf_exit: 0.737 0.218 0.045
Batch: 300 | Loss: 1.120 | Acc: 75.034,91.868,96.148,% | Adaptive Acc: 86.607% | clf_exit: 0.736 0.219 0.045
Batch: 320 | Loss: 1.120 | Acc: 75.153,91.832,96.162,% | Adaptive Acc: 86.621% | clf_exit: 0.736 0.218 0.045
Batch: 340 | Loss: 1.120 | Acc: 75.170,91.826,96.158,% | Adaptive Acc: 86.652% | clf_exit: 0.736 0.219 0.045
Batch: 360 | Loss: 1.123 | Acc: 75.156,91.753,96.102,% | Adaptive Acc: 86.617% | clf_exit: 0.736 0.219 0.046
Batch: 380 | Loss: 1.125 | Acc: 75.172,91.728,96.108,% | Adaptive Acc: 86.624% | clf_exit: 0.736 0.218 0.046
Batch: 0 | Loss: 1.447 | Acc: 69.531,87.500,92.188,% | Adaptive Acc: 79.688% | clf_exit: 0.758 0.195 0.047
Batch: 20 | Loss: 1.586 | Acc: 72.619,86.161,89.546,% | Adaptive Acc: 80.171% | clf_exit: 0.775 0.181 0.044
Batch: 40 | Loss: 1.545 | Acc: 73.190,86.662,90.187,% | Adaptive Acc: 80.869% | clf_exit: 0.780 0.179 0.040
Batch: 60 | Loss: 1.533 | Acc: 73.002,86.860,89.895,% | Adaptive Acc: 80.879% | clf_exit: 0.780 0.180 0.040
Train all parameters

Epoch: 113
Batch: 0 | Loss: 1.103 | Acc: 79.688,89.844,95.312,% | Adaptive Acc: 90.625% | clf_exit: 0.766 0.203 0.031
Batch: 20 | Loss: 1.098 | Acc: 76.228,91.927,96.354,% | Adaptive Acc: 87.351% | clf_exit: 0.743 0.208 0.049
Batch: 40 | Loss: 1.098 | Acc: 75.743,92.321,96.113,% | Adaptive Acc: 87.005% | clf_exit: 0.736 0.216 0.048
Batch: 60 | Loss: 1.082 | Acc: 75.884,92.482,96.427,% | Adaptive Acc: 86.988% | clf_exit: 0.739 0.216 0.046
Batch: 80 | Loss: 1.067 | Acc: 76.177,92.660,96.615,% | Adaptive Acc: 87.336% | clf_exit: 0.740 0.217 0.044
Batch: 100 | Loss: 1.075 | Acc: 75.990,92.536,96.651,% | Adaptive Acc: 87.160% | clf_exit: 0.739 0.219 0.043
Batch: 120 | Loss: 1.084 | Acc: 75.859,92.394,96.604,% | Adaptive Acc: 87.067% | clf_exit: 0.736 0.222 0.043
Batch: 140 | Loss: 1.087 | Acc: 75.715,92.304,96.592,% | Adaptive Acc: 86.913% | clf_exit: 0.736 0.221 0.042
Batch: 160 | Loss: 1.086 | Acc: 75.689,92.294,96.569,% | Adaptive Acc: 86.874% | clf_exit: 0.737 0.221 0.042
Batch: 180 | Loss: 1.087 | Acc: 75.742,92.295,96.530,% | Adaptive Acc: 86.978% | clf_exit: 0.737 0.220 0.043
Batch: 200 | Loss: 1.088 | Acc: 75.735,92.292,96.510,% | Adaptive Acc: 86.878% | clf_exit: 0.738 0.220 0.042
Batch: 220 | Loss: 1.090 | Acc: 75.696,92.223,96.454,% | Adaptive Acc: 86.878% | clf_exit: 0.739 0.219 0.043
Batch: 240 | Loss: 1.092 | Acc: 75.645,92.188,96.434,% | Adaptive Acc: 86.962% | clf_exit: 0.738 0.219 0.043
Batch: 260 | Loss: 1.093 | Acc: 75.542,92.170,96.423,% | Adaptive Acc: 86.952% | clf_exit: 0.739 0.218 0.043
Batch: 280 | Loss: 1.096 | Acc: 75.484,92.118,96.369,% | Adaptive Acc: 86.861% | clf_exit: 0.740 0.217 0.044
Batch: 300 | Loss: 1.100 | Acc: 75.470,92.084,96.314,% | Adaptive Acc: 86.859% | clf_exit: 0.740 0.217 0.044
Batch: 320 | Loss: 1.102 | Acc: 75.477,92.076,96.264,% | Adaptive Acc: 86.848% | clf_exit: 0.740 0.217 0.044
Batch: 340 | Loss: 1.103 | Acc: 75.513,92.043,96.217,% | Adaptive Acc: 86.808% | clf_exit: 0.741 0.215 0.044
Batch: 360 | Loss: 1.106 | Acc: 75.485,91.997,96.193,% | Adaptive Acc: 86.794% | clf_exit: 0.740 0.216 0.044
Batch: 380 | Loss: 1.107 | Acc: 75.439,92.019,96.170,% | Adaptive Acc: 86.803% | clf_exit: 0.740 0.216 0.044
Batch: 0 | Loss: 1.386 | Acc: 71.875,87.500,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.758 0.211 0.031
Batch: 20 | Loss: 1.509 | Acc: 72.098,87.054,89.546,% | Adaptive Acc: 80.432% | clf_exit: 0.767 0.193 0.040
Batch: 40 | Loss: 1.493 | Acc: 72.542,87.043,89.710,% | Adaptive Acc: 80.678% | clf_exit: 0.777 0.184 0.040
Batch: 60 | Loss: 1.469 | Acc: 72.605,87.180,89.959,% | Adaptive Acc: 80.917% | clf_exit: 0.773 0.190 0.037
Train all parameters

Epoch: 114
Batch: 0 | Loss: 1.042 | Acc: 76.562,92.188,97.656,% | Adaptive Acc: 85.156% | clf_exit: 0.734 0.219 0.047
Batch: 20 | Loss: 1.080 | Acc: 76.972,91.964,96.689,% | Adaptive Acc: 87.760% | clf_exit: 0.734 0.218 0.049
Batch: 40 | Loss: 1.093 | Acc: 75.877,92.283,96.742,% | Adaptive Acc: 87.157% | clf_exit: 0.733 0.222 0.046
Batch: 60 | Loss: 1.090 | Acc: 75.615,92.239,96.849,% | Adaptive Acc: 87.398% | clf_exit: 0.733 0.220 0.047
Batch: 80 | Loss: 1.081 | Acc: 75.781,92.274,96.817,% | Adaptive Acc: 87.307% | clf_exit: 0.736 0.218 0.046
Batch: 100 | Loss: 1.090 | Acc: 75.364,92.226,96.689,% | Adaptive Acc: 87.175% | clf_exit: 0.736 0.218 0.046
Batch: 120 | Loss: 1.090 | Acc: 75.226,92.246,96.701,% | Adaptive Acc: 87.203% | clf_exit: 0.736 0.218 0.045
Batch: 140 | Loss: 1.088 | Acc: 75.360,92.343,96.676,% | Adaptive Acc: 87.162% | clf_exit: 0.738 0.217 0.045
Batch: 160 | Loss: 1.098 | Acc: 75.262,92.149,96.560,% | Adaptive Acc: 87.015% | clf_exit: 0.738 0.217 0.045
Batch: 180 | Loss: 1.103 | Acc: 75.207,92.101,96.512,% | Adaptive Acc: 86.926% | clf_exit: 0.736 0.218 0.046
Batch: 200 | Loss: 1.097 | Acc: 75.268,92.238,96.529,% | Adaptive Acc: 87.057% | clf_exit: 0.737 0.217 0.046
Batch: 220 | Loss: 1.100 | Acc: 75.219,92.184,96.550,% | Adaptive Acc: 87.033% | clf_exit: 0.736 0.218 0.046
Batch: 240 | Loss: 1.102 | Acc: 75.191,92.194,96.548,% | Adaptive Acc: 87.001% | clf_exit: 0.735 0.219 0.046
Batch: 260 | Loss: 1.102 | Acc: 75.236,92.134,96.510,% | Adaptive Acc: 87.003% | clf_exit: 0.736 0.217 0.046
Batch: 280 | Loss: 1.100 | Acc: 75.378,92.132,96.455,% | Adaptive Acc: 87.002% | clf_exit: 0.738 0.216 0.046
Batch: 300 | Loss: 1.101 | Acc: 75.343,92.115,96.405,% | Adaptive Acc: 86.908% | clf_exit: 0.738 0.216 0.046
Batch: 320 | Loss: 1.099 | Acc: 75.382,92.168,96.388,% | Adaptive Acc: 86.901% | clf_exit: 0.739 0.215 0.046
Batch: 340 | Loss: 1.101 | Acc: 75.348,92.112,96.362,% | Adaptive Acc: 86.909% | clf_exit: 0.738 0.216 0.046
Batch: 360 | Loss: 1.099 | Acc: 75.439,92.159,96.369,% | Adaptive Acc: 86.957% | clf_exit: 0.738 0.216 0.046
Batch: 380 | Loss: 1.100 | Acc: 75.445,92.167,96.340,% | Adaptive Acc: 86.950% | clf_exit: 0.739 0.215 0.046
Batch: 0 | Loss: 1.218 | Acc: 77.344,89.062,91.406,% | Adaptive Acc: 84.375% | clf_exit: 0.758 0.188 0.055
Batch: 20 | Loss: 1.594 | Acc: 72.619,85.603,89.993,% | Adaptive Acc: 80.394% | clf_exit: 0.781 0.180 0.039
Batch: 40 | Loss: 1.572 | Acc: 72.732,85.652,89.863,% | Adaptive Acc: 80.507% | clf_exit: 0.781 0.179 0.039
Batch: 60 | Loss: 1.548 | Acc: 72.605,85.822,89.703,% | Adaptive Acc: 80.597% | clf_exit: 0.780 0.180 0.040
Train all parameters

Epoch: 115
Batch: 0 | Loss: 1.165 | Acc: 75.000,91.406,93.750,% | Adaptive Acc: 87.500% | clf_exit: 0.688 0.266 0.047
Batch: 20 | Loss: 1.098 | Acc: 75.521,92.113,96.019,% | Adaptive Acc: 86.384% | clf_exit: 0.743 0.214 0.044
Batch: 40 | Loss: 1.090 | Acc: 75.705,92.264,96.322,% | Adaptive Acc: 87.081% | clf_exit: 0.742 0.216 0.042
Batch: 60 | Loss: 1.085 | Acc: 75.512,92.239,96.363,% | Adaptive Acc: 87.141% | clf_exit: 0.743 0.214 0.042
Batch: 80 | Loss: 1.092 | Acc: 75.376,92.130,96.325,% | Adaptive Acc: 86.844% | clf_exit: 0.740 0.217 0.043
Batch: 100 | Loss: 1.087 | Acc: 75.371,92.164,96.395,% | Adaptive Acc: 86.889% | clf_exit: 0.739 0.217 0.044
Batch: 120 | Loss: 1.090 | Acc: 75.420,92.162,96.391,% | Adaptive Acc: 86.951% | clf_exit: 0.738 0.218 0.044
Batch: 140 | Loss: 1.095 | Acc: 75.294,92.055,96.382,% | Adaptive Acc: 86.957% | clf_exit: 0.737 0.219 0.044
Batch: 160 | Loss: 1.092 | Acc: 75.301,92.110,96.419,% | Adaptive Acc: 86.952% | clf_exit: 0.739 0.217 0.044
Batch: 180 | Loss: 1.087 | Acc: 75.384,92.231,96.543,% | Adaptive Acc: 86.982% | clf_exit: 0.741 0.217 0.042
Batch: 200 | Loss: 1.092 | Acc: 75.276,92.199,96.463,% | Adaptive Acc: 86.913% | clf_exit: 0.740 0.218 0.042
Batch: 220 | Loss: 1.091 | Acc: 75.364,92.226,96.433,% | Adaptive Acc: 86.941% | clf_exit: 0.741 0.217 0.042
Batch: 240 | Loss: 1.093 | Acc: 75.379,92.158,96.421,% | Adaptive Acc: 87.001% | clf_exit: 0.740 0.218 0.042
Batch: 260 | Loss: 1.092 | Acc: 75.500,92.128,96.387,% | Adaptive Acc: 87.039% | clf_exit: 0.739 0.218 0.043
Batch: 280 | Loss: 1.096 | Acc: 75.392,92.046,96.375,% | Adaptive Acc: 86.930% | clf_exit: 0.739 0.219 0.043
Batch: 300 | Loss: 1.096 | Acc: 75.363,92.060,96.387,% | Adaptive Acc: 86.864% | clf_exit: 0.739 0.218 0.043
Batch: 320 | Loss: 1.096 | Acc: 75.338,92.085,96.374,% | Adaptive Acc: 86.857% | clf_exit: 0.739 0.218 0.043
Batch: 340 | Loss: 1.099 | Acc: 75.321,92.059,96.325,% | Adaptive Acc: 86.861% | clf_exit: 0.739 0.218 0.043
Batch: 360 | Loss: 1.101 | Acc: 75.327,92.040,96.284,% | Adaptive Acc: 86.846% | clf_exit: 0.739 0.218 0.043
Batch: 380 | Loss: 1.103 | Acc: 75.375,92.028,96.229,% | Adaptive Acc: 86.856% | clf_exit: 0.740 0.217 0.043
Batch: 0 | Loss: 1.372 | Acc: 73.438,89.844,93.750,% | Adaptive Acc: 84.375% | clf_exit: 0.750 0.203 0.047
Batch: 20 | Loss: 1.484 | Acc: 72.173,87.202,91.220,% | Adaptive Acc: 81.510% | clf_exit: 0.763 0.196 0.041
Batch: 40 | Loss: 1.469 | Acc: 72.675,87.538,91.216,% | Adaptive Acc: 81.898% | clf_exit: 0.765 0.192 0.042
Batch: 60 | Loss: 1.461 | Acc: 73.143,87.756,91.048,% | Adaptive Acc: 82.262% | clf_exit: 0.763 0.195 0.043
Train all parameters

Epoch: 116
Batch: 0 | Loss: 1.382 | Acc: 73.438,89.062,91.406,% | Adaptive Acc: 85.938% | clf_exit: 0.711 0.219 0.070
Batch: 20 | Loss: 1.112 | Acc: 74.665,92.634,96.540,% | Adaptive Acc: 87.240% | clf_exit: 0.729 0.220 0.051
Batch: 40 | Loss: 1.078 | Acc: 75.743,92.664,96.932,% | Adaptive Acc: 87.309% | clf_exit: 0.731 0.224 0.045
Batch: 60 | Loss: 1.063 | Acc: 75.768,93.007,97.067,% | Adaptive Acc: 87.654% | clf_exit: 0.735 0.222 0.043
Batch: 80 | Loss: 1.077 | Acc: 75.328,92.660,96.817,% | Adaptive Acc: 87.375% | clf_exit: 0.737 0.220 0.043
Batch: 100 | Loss: 1.078 | Acc: 75.255,92.628,96.736,% | Adaptive Acc: 87.461% | clf_exit: 0.735 0.221 0.043
Batch: 120 | Loss: 1.083 | Acc: 75.149,92.504,96.707,% | Adaptive Acc: 87.332% | clf_exit: 0.735 0.222 0.043
Batch: 140 | Loss: 1.085 | Acc: 75.205,92.520,96.676,% | Adaptive Acc: 87.212% | clf_exit: 0.740 0.217 0.043
Batch: 160 | Loss: 1.088 | Acc: 75.204,92.445,96.521,% | Adaptive Acc: 87.088% | clf_exit: 0.739 0.218 0.043
Batch: 180 | Loss: 1.084 | Acc: 75.350,92.490,96.594,% | Adaptive Acc: 87.228% | clf_exit: 0.740 0.217 0.043
Batch: 200 | Loss: 1.080 | Acc: 75.365,92.510,96.646,% | Adaptive Acc: 87.243% | clf_exit: 0.740 0.218 0.043
Batch: 220 | Loss: 1.083 | Acc: 75.343,92.509,96.631,% | Adaptive Acc: 87.214% | clf_exit: 0.739 0.217 0.043
Batch: 240 | Loss: 1.083 | Acc: 75.379,92.479,96.583,% | Adaptive Acc: 87.192% | clf_exit: 0.739 0.217 0.043
Batch: 260 | Loss: 1.087 | Acc: 75.398,92.430,96.528,% | Adaptive Acc: 87.222% | clf_exit: 0.739 0.217 0.043
Batch: 280 | Loss: 1.087 | Acc: 75.398,92.407,96.539,% | Adaptive Acc: 87.169% | clf_exit: 0.740 0.217 0.043
Batch: 300 | Loss: 1.089 | Acc: 75.395,92.413,96.517,% | Adaptive Acc: 87.176% | clf_exit: 0.740 0.217 0.043
Batch: 320 | Loss: 1.090 | Acc: 75.387,92.414,96.500,% | Adaptive Acc: 87.132% | clf_exit: 0.740 0.217 0.043
Batch: 340 | Loss: 1.091 | Acc: 75.433,92.391,96.474,% | Adaptive Acc: 87.099% | clf_exit: 0.740 0.217 0.043
Batch: 360 | Loss: 1.091 | Acc: 75.474,92.352,96.457,% | Adaptive Acc: 87.080% | clf_exit: 0.741 0.215 0.043
Batch: 380 | Loss: 1.093 | Acc: 75.465,92.290,96.422,% | Adaptive Acc: 87.047% | clf_exit: 0.741 0.216 0.043
Batch: 0 | Loss: 1.378 | Acc: 69.531,92.188,92.188,% | Adaptive Acc: 78.125% | clf_exit: 0.789 0.141 0.070
Batch: 20 | Loss: 1.564 | Acc: 69.531,87.240,90.365,% | Adaptive Acc: 78.720% | clf_exit: 0.767 0.188 0.045
Batch: 40 | Loss: 1.558 | Acc: 70.484,87.290,90.454,% | Adaptive Acc: 79.745% | clf_exit: 0.771 0.183 0.045
Batch: 60 | Loss: 1.545 | Acc: 70.914,87.141,90.292,% | Adaptive Acc: 79.777% | clf_exit: 0.771 0.186 0.043
Train all parameters

Epoch: 117
Batch: 0 | Loss: 0.872 | Acc: 78.125,96.094,96.094,% | Adaptive Acc: 86.719% | clf_exit: 0.773 0.219 0.008
Batch: 20 | Loss: 1.017 | Acc: 75.260,94.159,96.540,% | Adaptive Acc: 87.426% | clf_exit: 0.758 0.203 0.039
Batch: 40 | Loss: 1.033 | Acc: 75.476,93.674,96.570,% | Adaptive Acc: 87.576% | clf_exit: 0.755 0.207 0.038
Batch: 60 | Loss: 1.040 | Acc: 75.845,93.212,96.580,% | Adaptive Acc: 87.526% | clf_exit: 0.748 0.215 0.037
Batch: 80 | Loss: 1.047 | Acc: 75.694,93.046,96.672,% | Adaptive Acc: 87.384% | clf_exit: 0.747 0.215 0.037
Batch: 100 | Loss: 1.047 | Acc: 75.905,92.953,96.720,% | Adaptive Acc: 87.423% | clf_exit: 0.749 0.214 0.037
Batch: 120 | Loss: 1.057 | Acc: 75.730,92.730,96.617,% | Adaptive Acc: 87.268% | clf_exit: 0.748 0.213 0.040
Batch: 140 | Loss: 1.064 | Acc: 75.643,92.703,96.637,% | Adaptive Acc: 87.145% | clf_exit: 0.748 0.212 0.041
Batch: 160 | Loss: 1.072 | Acc: 75.602,92.624,96.647,% | Adaptive Acc: 87.170% | clf_exit: 0.746 0.213 0.041
Batch: 180 | Loss: 1.078 | Acc: 75.423,92.550,96.642,% | Adaptive Acc: 87.021% | clf_exit: 0.745 0.214 0.042
Batch: 200 | Loss: 1.078 | Acc: 75.470,92.557,96.638,% | Adaptive Acc: 87.037% | clf_exit: 0.745 0.213 0.041
Batch: 220 | Loss: 1.075 | Acc: 75.647,92.523,96.652,% | Adaptive Acc: 87.125% | clf_exit: 0.745 0.213 0.042
Batch: 240 | Loss: 1.076 | Acc: 75.668,92.515,96.638,% | Adaptive Acc: 87.088% | clf_exit: 0.745 0.213 0.041
Batch: 260 | Loss: 1.078 | Acc: 75.659,92.499,96.582,% | Adaptive Acc: 87.078% | clf_exit: 0.745 0.214 0.042
Batch: 280 | Loss: 1.081 | Acc: 75.634,92.466,96.558,% | Adaptive Acc: 87.080% | clf_exit: 0.743 0.215 0.042
Batch: 300 | Loss: 1.081 | Acc: 75.646,92.424,96.543,% | Adaptive Acc: 87.017% | clf_exit: 0.744 0.215 0.041
Batch: 320 | Loss: 1.088 | Acc: 75.528,92.355,96.539,% | Adaptive Acc: 87.016% | clf_exit: 0.742 0.216 0.042
Batch: 340 | Loss: 1.088 | Acc: 75.518,92.339,96.524,% | Adaptive Acc: 86.982% | clf_exit: 0.742 0.216 0.042
Batch: 360 | Loss: 1.090 | Acc: 75.554,92.302,96.511,% | Adaptive Acc: 86.985% | clf_exit: 0.742 0.215 0.042
Batch: 380 | Loss: 1.088 | Acc: 75.595,92.319,96.502,% | Adaptive Acc: 86.942% | clf_exit: 0.743 0.215 0.042
Batch: 0 | Loss: 1.471 | Acc: 71.094,86.719,89.844,% | Adaptive Acc: 80.469% | clf_exit: 0.797 0.156 0.047
Batch: 20 | Loss: 1.569 | Acc: 71.429,86.272,89.695,% | Adaptive Acc: 79.688% | clf_exit: 0.778 0.180 0.041
Batch: 40 | Loss: 1.558 | Acc: 72.294,86.128,89.615,% | Adaptive Acc: 79.707% | clf_exit: 0.787 0.174 0.038
Batch: 60 | Loss: 1.548 | Acc: 72.464,86.232,89.754,% | Adaptive Acc: 80.072% | clf_exit: 0.788 0.173 0.038
Train all parameters

Epoch: 118
Batch: 0 | Loss: 0.938 | Acc: 77.344,96.875,96.875,% | Adaptive Acc: 90.625% | clf_exit: 0.781 0.188 0.031
Batch: 20 | Loss: 1.042 | Acc: 76.972,93.378,96.912,% | Adaptive Acc: 88.467% | clf_exit: 0.740 0.215 0.045
Batch: 40 | Loss: 1.068 | Acc: 76.315,92.931,96.570,% | Adaptive Acc: 87.900% | clf_exit: 0.739 0.216 0.045
Batch: 60 | Loss: 1.085 | Acc: 75.845,92.418,96.414,% | Adaptive Acc: 87.449% | clf_exit: 0.740 0.216 0.044
Batch: 80 | Loss: 1.085 | Acc: 75.714,92.506,96.383,% | Adaptive Acc: 87.288% | clf_exit: 0.743 0.213 0.044
Batch: 100 | Loss: 1.083 | Acc: 75.905,92.450,96.295,% | Adaptive Acc: 87.268% | clf_exit: 0.741 0.215 0.044
Batch: 120 | Loss: 1.077 | Acc: 75.975,92.575,96.397,% | Adaptive Acc: 87.481% | clf_exit: 0.741 0.215 0.043
Batch: 140 | Loss: 1.077 | Acc: 75.914,92.570,96.432,% | Adaptive Acc: 87.450% | clf_exit: 0.742 0.214 0.044
Batch: 160 | Loss: 1.077 | Acc: 75.995,92.576,96.472,% | Adaptive Acc: 87.432% | clf_exit: 0.742 0.214 0.044
Batch: 180 | Loss: 1.080 | Acc: 75.928,92.464,96.443,% | Adaptive Acc: 87.336% | clf_exit: 0.742 0.214 0.044
Batch: 200 | Loss: 1.079 | Acc: 75.882,92.452,96.521,% | Adaptive Acc: 87.325% | clf_exit: 0.742 0.214 0.044
Batch: 220 | Loss: 1.085 | Acc: 75.714,92.385,96.507,% | Adaptive Acc: 87.185% | clf_exit: 0.741 0.215 0.044
Batch: 240 | Loss: 1.081 | Acc: 75.801,92.437,96.509,% | Adaptive Acc: 87.218% | clf_exit: 0.742 0.215 0.043
Batch: 260 | Loss: 1.084 | Acc: 75.754,92.406,96.465,% | Adaptive Acc: 87.243% | clf_exit: 0.741 0.215 0.043
Batch: 280 | Loss: 1.087 | Acc: 75.698,92.324,96.444,% | Adaptive Acc: 87.144% | clf_exit: 0.742 0.215 0.043
Batch: 300 | Loss: 1.086 | Acc: 75.719,92.315,96.457,% | Adaptive Acc: 87.124% | clf_exit: 0.743 0.214 0.043
Batch: 320 | Loss: 1.086 | Acc: 75.706,92.316,96.449,% | Adaptive Acc: 87.069% | clf_exit: 0.744 0.213 0.043
Batch: 340 | Loss: 1.086 | Acc: 75.655,92.300,96.405,% | Adaptive Acc: 86.987% | clf_exit: 0.745 0.212 0.043
Batch: 360 | Loss: 1.089 | Acc: 75.591,92.276,96.373,% | Adaptive Acc: 86.952% | clf_exit: 0.744 0.212 0.044
Batch: 380 | Loss: 1.090 | Acc: 75.552,92.237,96.371,% | Adaptive Acc: 86.989% | clf_exit: 0.743 0.213 0.044
Batch: 0 | Loss: 1.272 | Acc: 71.875,89.062,92.969,% | Adaptive Acc: 80.469% | clf_exit: 0.773 0.172 0.055
Batch: 20 | Loss: 1.468 | Acc: 71.652,87.798,90.811,% | Adaptive Acc: 80.283% | clf_exit: 0.779 0.176 0.045
Batch: 40 | Loss: 1.471 | Acc: 72.561,87.767,90.625,% | Adaptive Acc: 80.488% | clf_exit: 0.788 0.167 0.046
Batch: 60 | Loss: 1.464 | Acc: 72.759,87.718,90.676,% | Adaptive Acc: 80.789% | clf_exit: 0.785 0.171 0.044
Train all parameters

Epoch: 119
Batch: 0 | Loss: 0.950 | Acc: 78.906,94.531,97.656,% | Adaptive Acc: 89.062% | clf_exit: 0.766 0.203 0.031
Batch: 20 | Loss: 1.065 | Acc: 75.707,92.634,97.433,% | Adaptive Acc: 87.835% | clf_exit: 0.737 0.221 0.042
Batch: 40 | Loss: 1.077 | Acc: 75.629,92.645,97.123,% | Adaptive Acc: 87.481% | clf_exit: 0.741 0.216 0.043
Batch: 60 | Loss: 1.069 | Acc: 75.679,92.585,96.952,% | Adaptive Acc: 87.321% | clf_exit: 0.744 0.213 0.043
Batch: 80 | Loss: 1.070 | Acc: 75.473,92.564,97.039,% | Adaptive Acc: 87.162% | clf_exit: 0.742 0.216 0.042
Batch: 100 | Loss: 1.061 | Acc: 75.681,92.644,97.084,% | Adaptive Acc: 87.322% | clf_exit: 0.742 0.216 0.042
Batch: 120 | Loss: 1.055 | Acc: 75.749,92.691,97.133,% | Adaptive Acc: 87.293% | clf_exit: 0.744 0.214 0.042
Batch: 140 | Loss: 1.054 | Acc: 75.892,92.620,97.091,% | Adaptive Acc: 87.439% | clf_exit: 0.744 0.214 0.042
Batch: 160 | Loss: 1.052 | Acc: 75.956,92.770,97.035,% | Adaptive Acc: 87.573% | clf_exit: 0.744 0.214 0.042
Batch: 180 | Loss: 1.050 | Acc: 76.032,92.749,97.009,% | Adaptive Acc: 87.634% | clf_exit: 0.744 0.214 0.042
Batch: 200 | Loss: 1.054 | Acc: 75.960,92.611,96.968,% | Adaptive Acc: 87.527% | clf_exit: 0.747 0.211 0.041
Batch: 220 | Loss: 1.061 | Acc: 75.781,92.456,96.882,% | Adaptive Acc: 87.412% | clf_exit: 0.746 0.211 0.042
Batch: 240 | Loss: 1.069 | Acc: 75.629,92.395,96.804,% | Adaptive Acc: 87.302% | clf_exit: 0.746 0.211 0.043
Batch: 260 | Loss: 1.069 | Acc: 75.700,92.388,96.779,% | Adaptive Acc: 87.278% | clf_exit: 0.746 0.211 0.043
Batch: 280 | Loss: 1.067 | Acc: 75.737,92.427,96.794,% | Adaptive Acc: 87.280% | clf_exit: 0.746 0.211 0.043
Batch: 300 | Loss: 1.067 | Acc: 75.716,92.431,96.763,% | Adaptive Acc: 87.222% | clf_exit: 0.746 0.211 0.043
Batch: 320 | Loss: 1.071 | Acc: 75.708,92.385,96.695,% | Adaptive Acc: 87.203% | clf_exit: 0.745 0.212 0.043
Batch: 340 | Loss: 1.071 | Acc: 75.774,92.385,96.705,% | Adaptive Acc: 87.204% | clf_exit: 0.746 0.211 0.044
Batch: 360 | Loss: 1.071 | Acc: 75.794,92.382,96.728,% | Adaptive Acc: 87.169% | clf_exit: 0.746 0.210 0.043
Batch: 380 | Loss: 1.070 | Acc: 75.853,92.380,96.723,% | Adaptive Acc: 87.184% | clf_exit: 0.747 0.210 0.043
Batch: 0 | Loss: 1.723 | Acc: 67.188,83.594,87.500,% | Adaptive Acc: 71.875% | clf_exit: 0.734 0.203 0.062
Batch: 20 | Loss: 1.651 | Acc: 70.164,85.379,89.100,% | Adaptive Acc: 79.278% | clf_exit: 0.749 0.200 0.051
Batch: 40 | Loss: 1.620 | Acc: 71.056,85.442,89.596,% | Adaptive Acc: 79.745% | clf_exit: 0.758 0.196 0.046
Batch: 60 | Loss: 1.625 | Acc: 71.043,85.207,89.652,% | Adaptive Acc: 79.969% | clf_exit: 0.755 0.197 0.048
Train all parameters

Epoch: 120
Batch: 0 | Loss: 0.936 | Acc: 80.469,92.188,96.875,% | Adaptive Acc: 91.406% | clf_exit: 0.742 0.203 0.055
Batch: 20 | Loss: 1.013 | Acc: 77.269,93.452,97.321,% | Adaptive Acc: 88.616% | clf_exit: 0.752 0.205 0.043
Batch: 40 | Loss: 1.020 | Acc: 77.134,93.464,97.370,% | Adaptive Acc: 88.415% | clf_exit: 0.748 0.209 0.042
Batch: 60 | Loss: 1.029 | Acc: 76.422,93.417,97.323,% | Adaptive Acc: 87.999% | clf_exit: 0.744 0.213 0.043
Batch: 80 | Loss: 1.024 | Acc: 76.418,93.287,97.232,% | Adaptive Acc: 87.915% | clf_exit: 0.746 0.212 0.042
Batch: 100 | Loss: 1.040 | Acc: 76.369,93.007,97.030,% | Adaptive Acc: 88.003% | clf_exit: 0.745 0.212 0.043
Batch: 120 | Loss: 1.049 | Acc: 76.001,92.820,97.056,% | Adaptive Acc: 87.636% | clf_exit: 0.744 0.214 0.042
Batch: 140 | Loss: 1.052 | Acc: 76.031,92.797,97.030,% | Adaptive Acc: 87.622% | clf_exit: 0.745 0.212 0.044
Batch: 160 | Loss: 1.058 | Acc: 75.835,92.818,96.972,% | Adaptive Acc: 87.534% | clf_exit: 0.743 0.214 0.043
Batch: 180 | Loss: 1.059 | Acc: 75.837,92.757,96.927,% | Adaptive Acc: 87.491% | clf_exit: 0.743 0.213 0.044
Batch: 200 | Loss: 1.056 | Acc: 75.906,92.771,96.933,% | Adaptive Acc: 87.539% | clf_exit: 0.743 0.214 0.043
Batch: 220 | Loss: 1.055 | Acc: 75.855,92.774,96.956,% | Adaptive Acc: 87.493% | clf_exit: 0.744 0.214 0.042
Batch: 240 | Loss: 1.053 | Acc: 76.008,92.742,96.933,% | Adaptive Acc: 87.532% | clf_exit: 0.745 0.213 0.043
Batch: 260 | Loss: 1.054 | Acc: 75.997,92.732,96.929,% | Adaptive Acc: 87.524% | clf_exit: 0.744 0.213 0.043
Batch: 280 | Loss: 1.050 | Acc: 76.190,92.774,96.925,% | Adaptive Acc: 87.600% | clf_exit: 0.746 0.212 0.042
Batch: 300 | Loss: 1.053 | Acc: 76.147,92.748,96.859,% | Adaptive Acc: 87.578% | clf_exit: 0.746 0.212 0.043
Batch: 320 | Loss: 1.053 | Acc: 76.185,92.730,96.829,% | Adaptive Acc: 87.554% | clf_exit: 0.747 0.211 0.042
Batch: 340 | Loss: 1.056 | Acc: 76.150,92.749,96.802,% | Adaptive Acc: 87.518% | clf_exit: 0.747 0.211 0.043
Batch: 360 | Loss: 1.059 | Acc: 76.123,92.698,96.717,% | Adaptive Acc: 87.450% | clf_exit: 0.747 0.211 0.043
Batch: 380 | Loss: 1.060 | Acc: 76.111,92.663,96.678,% | Adaptive Acc: 87.451% | clf_exit: 0.747 0.210 0.043
Batch: 0 | Loss: 1.355 | Acc: 75.000,89.844,90.625,% | Adaptive Acc: 84.375% | clf_exit: 0.719 0.219 0.062
Batch: 20 | Loss: 1.483 | Acc: 72.321,87.240,90.997,% | Adaptive Acc: 80.952% | clf_exit: 0.772 0.186 0.042
Batch: 40 | Loss: 1.492 | Acc: 72.961,87.138,90.777,% | Adaptive Acc: 81.098% | clf_exit: 0.778 0.184 0.038
Batch: 60 | Loss: 1.496 | Acc: 72.656,87.077,90.625,% | Adaptive Acc: 80.981% | clf_exit: 0.776 0.185 0.038
Train all parameters

Epoch: 121
Batch: 0 | Loss: 1.193 | Acc: 71.094,91.406,97.656,% | Adaptive Acc: 82.031% | clf_exit: 0.703 0.242 0.055
Batch: 20 | Loss: 1.058 | Acc: 75.223,93.006,96.540,% | Adaptive Acc: 87.165% | clf_exit: 0.744 0.212 0.044
Batch: 40 | Loss: 1.048 | Acc: 76.391,93.045,96.704,% | Adaptive Acc: 87.500% | clf_exit: 0.750 0.208 0.042
Batch: 60 | Loss: 1.052 | Acc: 76.140,92.994,96.875,% | Adaptive Acc: 87.205% | clf_exit: 0.748 0.212 0.041
Batch: 80 | Loss: 1.058 | Acc: 75.829,92.641,96.933,% | Adaptive Acc: 87.133% | clf_exit: 0.745 0.214 0.041
Batch: 100 | Loss: 1.056 | Acc: 75.843,92.559,96.867,% | Adaptive Acc: 87.020% | clf_exit: 0.749 0.211 0.040
Batch: 120 | Loss: 1.052 | Acc: 75.820,92.530,96.920,% | Adaptive Acc: 87.158% | clf_exit: 0.751 0.209 0.040
Batch: 140 | Loss: 1.052 | Acc: 75.942,92.564,96.809,% | Adaptive Acc: 87.096% | clf_exit: 0.752 0.207 0.041
Batch: 160 | Loss: 1.057 | Acc: 75.956,92.479,96.729,% | Adaptive Acc: 87.107% | clf_exit: 0.752 0.207 0.041
Batch: 180 | Loss: 1.061 | Acc: 75.863,92.429,96.698,% | Adaptive Acc: 87.090% | clf_exit: 0.749 0.210 0.041
Batch: 200 | Loss: 1.062 | Acc: 75.781,92.428,96.681,% | Adaptive Acc: 87.115% | clf_exit: 0.749 0.210 0.041
Batch: 220 | Loss: 1.065 | Acc: 75.672,92.449,96.681,% | Adaptive Acc: 87.079% | clf_exit: 0.747 0.212 0.041
Batch: 240 | Loss: 1.068 | Acc: 75.713,92.440,96.671,% | Adaptive Acc: 87.143% | clf_exit: 0.746 0.213 0.041
Batch: 260 | Loss: 1.068 | Acc: 75.691,92.433,96.698,% | Adaptive Acc: 87.159% | clf_exit: 0.746 0.213 0.041
Batch: 280 | Loss: 1.067 | Acc: 75.829,92.491,96.655,% | Adaptive Acc: 87.186% | clf_exit: 0.747 0.212 0.041
Batch: 300 | Loss: 1.066 | Acc: 75.812,92.517,96.673,% | Adaptive Acc: 87.176% | clf_exit: 0.747 0.212 0.041
Batch: 320 | Loss: 1.064 | Acc: 75.815,92.497,96.668,% | Adaptive Acc: 87.145% | clf_exit: 0.747 0.212 0.041
Batch: 340 | Loss: 1.066 | Acc: 75.825,92.433,96.673,% | Adaptive Acc: 87.140% | clf_exit: 0.747 0.212 0.041
Batch: 360 | Loss: 1.066 | Acc: 75.844,92.430,96.641,% | Adaptive Acc: 87.097% | clf_exit: 0.748 0.211 0.041
Batch: 380 | Loss: 1.066 | Acc: 75.904,92.440,96.619,% | Adaptive Acc: 87.125% | clf_exit: 0.748 0.211 0.041
Batch: 0 | Loss: 1.554 | Acc: 65.625,89.062,89.062,% | Adaptive Acc: 71.094% | clf_exit: 0.852 0.109 0.039
Batch: 20 | Loss: 1.741 | Acc: 69.568,84.635,88.728,% | Adaptive Acc: 77.121% | clf_exit: 0.815 0.155 0.030
Batch: 40 | Loss: 1.688 | Acc: 70.446,84.851,89.043,% | Adaptive Acc: 77.287% | clf_exit: 0.827 0.143 0.030
Batch: 60 | Loss: 1.654 | Acc: 70.594,85.207,89.293,% | Adaptive Acc: 77.677% | clf_exit: 0.824 0.147 0.029
Train all parameters

Epoch: 122
Batch: 0 | Loss: 1.242 | Acc: 72.656,90.625,94.531,% | Adaptive Acc: 85.156% | clf_exit: 0.742 0.211 0.047
Batch: 20 | Loss: 1.085 | Acc: 75.223,92.746,96.949,% | Adaptive Acc: 86.868% | clf_exit: 0.752 0.208 0.040
Batch: 40 | Loss: 1.078 | Acc: 76.181,92.607,96.627,% | Adaptive Acc: 86.719% | clf_exit: 0.756 0.203 0.042
Batch: 60 | Loss: 1.081 | Acc: 76.089,92.431,96.504,% | Adaptive Acc: 87.077% | clf_exit: 0.750 0.208 0.042
Batch: 80 | Loss: 1.082 | Acc: 75.733,92.226,96.508,% | Adaptive Acc: 86.902% | clf_exit: 0.748 0.210 0.042
Batch: 100 | Loss: 1.085 | Acc: 75.719,92.087,96.511,% | Adaptive Acc: 86.928% | clf_exit: 0.747 0.210 0.043
Batch: 120 | Loss: 1.088 | Acc: 75.646,92.175,96.468,% | Adaptive Acc: 86.951% | clf_exit: 0.746 0.212 0.042
Batch: 140 | Loss: 1.079 | Acc: 75.715,92.210,96.565,% | Adaptive Acc: 86.968% | clf_exit: 0.746 0.212 0.042
Batch: 160 | Loss: 1.081 | Acc: 75.655,92.241,96.555,% | Adaptive Acc: 87.024% | clf_exit: 0.746 0.213 0.042
Batch: 180 | Loss: 1.079 | Acc: 75.613,92.321,96.569,% | Adaptive Acc: 86.995% | clf_exit: 0.747 0.211 0.042
Batch: 200 | Loss: 1.082 | Acc: 75.707,92.277,96.533,% | Adaptive Acc: 86.940% | clf_exit: 0.748 0.210 0.042
Batch: 220 | Loss: 1.082 | Acc: 75.742,92.304,96.483,% | Adaptive Acc: 86.980% | clf_exit: 0.748 0.211 0.042
Batch: 240 | Loss: 1.083 | Acc: 75.713,92.330,96.473,% | Adaptive Acc: 86.929% | clf_exit: 0.746 0.212 0.042
Batch: 260 | Loss: 1.083 | Acc: 75.682,92.331,96.516,% | Adaptive Acc: 86.904% | clf_exit: 0.745 0.213 0.042
Batch: 280 | Loss: 1.082 | Acc: 75.726,92.377,96.519,% | Adaptive Acc: 86.955% | clf_exit: 0.745 0.213 0.041
Batch: 300 | Loss: 1.080 | Acc: 75.742,92.419,96.519,% | Adaptive Acc: 86.999% | clf_exit: 0.746 0.213 0.041
Batch: 320 | Loss: 1.077 | Acc: 75.774,92.445,96.551,% | Adaptive Acc: 87.033% | clf_exit: 0.746 0.213 0.041
Batch: 340 | Loss: 1.075 | Acc: 75.852,92.444,96.563,% | Adaptive Acc: 87.115% | clf_exit: 0.746 0.214 0.041
Batch: 360 | Loss: 1.075 | Acc: 75.848,92.434,96.535,% | Adaptive Acc: 87.072% | clf_exit: 0.747 0.212 0.041
Batch: 380 | Loss: 1.077 | Acc: 75.820,92.405,96.504,% | Adaptive Acc: 87.055% | clf_exit: 0.746 0.213 0.042
Batch: 0 | Loss: 1.317 | Acc: 72.656,90.625,91.406,% | Adaptive Acc: 82.812% | clf_exit: 0.758 0.188 0.055
Batch: 20 | Loss: 1.486 | Acc: 73.289,87.909,89.844,% | Adaptive Acc: 80.655% | clf_exit: 0.792 0.175 0.033
Batch: 40 | Loss: 1.485 | Acc: 73.704,87.557,89.996,% | Adaptive Acc: 80.697% | clf_exit: 0.797 0.167 0.036
Batch: 60 | Loss: 1.488 | Acc: 73.706,87.743,90.049,% | Adaptive Acc: 80.725% | clf_exit: 0.798 0.169 0.034
Train all parameters

Epoch: 123
Batch: 0 | Loss: 1.270 | Acc: 73.438,91.406,96.094,% | Adaptive Acc: 85.938% | clf_exit: 0.688 0.250 0.062
Batch: 20 | Loss: 1.037 | Acc: 76.488,93.155,97.173,% | Adaptive Acc: 87.686% | clf_exit: 0.750 0.206 0.044
Batch: 40 | Loss: 1.020 | Acc: 77.020,92.988,96.875,% | Adaptive Acc: 88.014% | clf_exit: 0.753 0.204 0.043
Batch: 60 | Loss: 1.014 | Acc: 76.883,93.251,97.029,% | Adaptive Acc: 88.153% | clf_exit: 0.752 0.207 0.042
Batch: 80 | Loss: 1.027 | Acc: 76.505,93.210,97.087,% | Adaptive Acc: 87.847% | clf_exit: 0.751 0.209 0.041
Batch: 100 | Loss: 1.029 | Acc: 76.408,93.131,97.115,% | Adaptive Acc: 88.011% | clf_exit: 0.748 0.211 0.041
Batch: 120 | Loss: 1.036 | Acc: 76.194,92.969,97.172,% | Adaptive Acc: 87.791% | clf_exit: 0.748 0.211 0.041
Batch: 140 | Loss: 1.040 | Acc: 76.114,92.936,97.163,% | Adaptive Acc: 87.722% | clf_exit: 0.748 0.211 0.041
Batch: 160 | Loss: 1.045 | Acc: 76.014,92.872,97.181,% | Adaptive Acc: 87.646% | clf_exit: 0.748 0.211 0.041
Batch: 180 | Loss: 1.047 | Acc: 75.915,92.736,97.099,% | Adaptive Acc: 87.539% | clf_exit: 0.748 0.210 0.042
Batch: 200 | Loss: 1.050 | Acc: 75.956,92.669,97.003,% | Adaptive Acc: 87.446% | clf_exit: 0.749 0.209 0.042
Batch: 220 | Loss: 1.058 | Acc: 75.753,92.594,96.921,% | Adaptive Acc: 87.260% | clf_exit: 0.748 0.210 0.042
Batch: 240 | Loss: 1.063 | Acc: 75.690,92.567,96.800,% | Adaptive Acc: 87.176% | clf_exit: 0.748 0.210 0.042
Batch: 260 | Loss: 1.062 | Acc: 75.748,92.565,96.803,% | Adaptive Acc: 87.255% | clf_exit: 0.748 0.210 0.042
Batch: 280 | Loss: 1.062 | Acc: 75.748,92.588,96.775,% | Adaptive Acc: 87.222% | clf_exit: 0.748 0.209 0.042
Batch: 300 | Loss: 1.060 | Acc: 75.812,92.598,96.756,% | Adaptive Acc: 87.305% | clf_exit: 0.748 0.209 0.042
Batch: 320 | Loss: 1.062 | Acc: 75.825,92.553,96.719,% | Adaptive Acc: 87.305% | clf_exit: 0.749 0.209 0.042
Batch: 340 | Loss: 1.060 | Acc: 75.907,92.522,96.692,% | Adaptive Acc: 87.308% | clf_exit: 0.749 0.209 0.042
Batch: 360 | Loss: 1.062 | Acc: 75.905,92.510,96.672,% | Adaptive Acc: 87.329% | clf_exit: 0.749 0.209 0.042
Batch: 380 | Loss: 1.064 | Acc: 75.923,92.493,96.651,% | Adaptive Acc: 87.315% | clf_exit: 0.748 0.209 0.043
Batch: 0 | Loss: 1.343 | Acc: 73.438,89.844,91.406,% | Adaptive Acc: 82.031% | clf_exit: 0.773 0.203 0.023
Batch: 20 | Loss: 1.453 | Acc: 72.768,87.165,90.402,% | Adaptive Acc: 80.469% | clf_exit: 0.797 0.173 0.031
Batch: 40 | Loss: 1.441 | Acc: 73.552,87.233,90.568,% | Adaptive Acc: 81.212% | clf_exit: 0.795 0.175 0.031
Batch: 60 | Loss: 1.432 | Acc: 73.796,87.269,90.484,% | Adaptive Acc: 81.160% | clf_exit: 0.796 0.174 0.030
Train all parameters

Epoch: 124
Batch: 0 | Loss: 0.849 | Acc: 79.688,93.750,99.219,% | Adaptive Acc: 89.062% | clf_exit: 0.797 0.156 0.047
Batch: 20 | Loss: 1.002 | Acc: 76.414,93.676,97.545,% | Adaptive Acc: 87.351% | clf_exit: 0.758 0.205 0.037
Batch: 40 | Loss: 1.000 | Acc: 76.410,93.731,97.409,% | Adaptive Acc: 87.557% | clf_exit: 0.759 0.204 0.038
Batch: 60 | Loss: 1.004 | Acc: 76.742,93.481,97.413,% | Adaptive Acc: 87.782% | clf_exit: 0.757 0.204 0.039
Batch: 80 | Loss: 1.015 | Acc: 76.485,93.509,97.290,% | Adaptive Acc: 87.654% | clf_exit: 0.753 0.207 0.040
Batch: 100 | Loss: 1.017 | Acc: 76.570,93.317,97.246,% | Adaptive Acc: 87.577% | clf_exit: 0.753 0.207 0.040
Batch: 120 | Loss: 1.005 | Acc: 76.698,93.382,97.359,% | Adaptive Acc: 87.862% | clf_exit: 0.754 0.207 0.040
Batch: 140 | Loss: 1.010 | Acc: 76.540,93.251,97.291,% | Adaptive Acc: 87.699% | clf_exit: 0.754 0.206 0.039
Batch: 160 | Loss: 1.012 | Acc: 76.456,93.207,97.229,% | Adaptive Acc: 87.704% | clf_exit: 0.754 0.208 0.039
Batch: 180 | Loss: 1.017 | Acc: 76.398,93.116,97.156,% | Adaptive Acc: 87.664% | clf_exit: 0.753 0.208 0.039
Batch: 200 | Loss: 1.026 | Acc: 76.349,93.058,97.042,% | Adaptive Acc: 87.652% | clf_exit: 0.753 0.208 0.039
Batch: 220 | Loss: 1.025 | Acc: 76.403,93.075,97.034,% | Adaptive Acc: 87.627% | clf_exit: 0.753 0.208 0.039
Batch: 240 | Loss: 1.027 | Acc: 76.387,93.024,97.034,% | Adaptive Acc: 87.620% | clf_exit: 0.752 0.208 0.039
Batch: 260 | Loss: 1.032 | Acc: 76.398,92.903,96.938,% | Adaptive Acc: 87.506% | clf_exit: 0.753 0.208 0.039
Batch: 280 | Loss: 1.032 | Acc: 76.426,92.852,96.894,% | Adaptive Acc: 87.494% | clf_exit: 0.754 0.208 0.038
Batch: 300 | Loss: 1.035 | Acc: 76.394,92.808,96.865,% | Adaptive Acc: 87.440% | clf_exit: 0.753 0.208 0.039
Batch: 320 | Loss: 1.040 | Acc: 76.312,92.742,96.841,% | Adaptive Acc: 87.344% | clf_exit: 0.753 0.208 0.040
Batch: 340 | Loss: 1.042 | Acc: 76.249,92.712,96.838,% | Adaptive Acc: 87.294% | clf_exit: 0.752 0.209 0.039
Batch: 360 | Loss: 1.045 | Acc: 76.195,92.713,96.799,% | Adaptive Acc: 87.225% | clf_exit: 0.751 0.209 0.040
Batch: 380 | Loss: 1.047 | Acc: 76.154,92.743,96.746,% | Adaptive Acc: 87.197% | clf_exit: 0.751 0.209 0.040
Batch: 0 | Loss: 1.282 | Acc: 74.219,89.844,89.844,% | Adaptive Acc: 82.031% | clf_exit: 0.820 0.148 0.031
Batch: 20 | Loss: 1.500 | Acc: 72.321,87.798,90.551,% | Adaptive Acc: 81.176% | clf_exit: 0.785 0.187 0.028
Batch: 40 | Loss: 1.463 | Acc: 73.285,88.053,90.473,% | Adaptive Acc: 81.402% | clf_exit: 0.799 0.174 0.027
Batch: 60 | Loss: 1.457 | Acc: 73.425,87.974,90.382,% | Adaptive Acc: 81.557% | clf_exit: 0.797 0.175 0.028
Train all parameters

Epoch: 125
Batch: 0 | Loss: 1.067 | Acc: 78.125,94.531,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.742 0.227 0.031
Batch: 20 | Loss: 1.039 | Acc: 76.414,93.192,96.949,% | Adaptive Acc: 86.868% | clf_exit: 0.756 0.206 0.038
Batch: 40 | Loss: 1.009 | Acc: 76.658,93.579,97.256,% | Adaptive Acc: 87.786% | clf_exit: 0.755 0.209 0.036
Batch: 60 | Loss: 1.004 | Acc: 76.511,93.584,97.272,% | Adaptive Acc: 87.935% | clf_exit: 0.756 0.207 0.037
Batch: 80 | Loss: 1.008 | Acc: 76.562,93.441,97.309,% | Adaptive Acc: 87.895% | clf_exit: 0.755 0.208 0.037
Batch: 100 | Loss: 1.006 | Acc: 76.942,93.332,97.254,% | Adaptive Acc: 87.949% | clf_exit: 0.756 0.207 0.037
Batch: 120 | Loss: 1.012 | Acc: 76.847,93.272,97.282,% | Adaptive Acc: 87.720% | clf_exit: 0.757 0.207 0.036
Batch: 140 | Loss: 1.018 | Acc: 76.707,93.229,97.207,% | Adaptive Acc: 87.661% | clf_exit: 0.756 0.207 0.037
Batch: 160 | Loss: 1.023 | Acc: 76.567,93.163,97.200,% | Adaptive Acc: 87.694% | clf_exit: 0.753 0.208 0.038
Batch: 180 | Loss: 1.027 | Acc: 76.446,93.064,97.151,% | Adaptive Acc: 87.634% | clf_exit: 0.753 0.209 0.038
Batch: 200 | Loss: 1.031 | Acc: 76.306,92.988,97.108,% | Adaptive Acc: 87.554% | clf_exit: 0.753 0.208 0.039
Batch: 220 | Loss: 1.034 | Acc: 76.198,92.944,97.045,% | Adaptive Acc: 87.525% | clf_exit: 0.751 0.209 0.040
Batch: 240 | Loss: 1.035 | Acc: 76.232,92.920,97.008,% | Adaptive Acc: 87.626% | clf_exit: 0.749 0.210 0.040
Batch: 260 | Loss: 1.040 | Acc: 76.218,92.921,96.920,% | Adaptive Acc: 87.566% | clf_exit: 0.749 0.210 0.040
Batch: 280 | Loss: 1.042 | Acc: 76.109,92.869,96.933,% | Adaptive Acc: 87.517% | clf_exit: 0.749 0.210 0.041
Batch: 300 | Loss: 1.042 | Acc: 76.137,92.816,96.909,% | Adaptive Acc: 87.492% | clf_exit: 0.749 0.210 0.041
Batch: 320 | Loss: 1.046 | Acc: 76.064,92.791,96.880,% | Adaptive Acc: 87.410% | clf_exit: 0.749 0.210 0.041
Batch: 340 | Loss: 1.047 | Acc: 76.049,92.740,96.854,% | Adaptive Acc: 87.390% | clf_exit: 0.749 0.211 0.041
Batch: 360 | Loss: 1.051 | Acc: 76.032,92.677,96.795,% | Adaptive Acc: 87.349% | clf_exit: 0.749 0.211 0.041
Batch: 380 | Loss: 1.053 | Acc: 76.036,92.653,96.770,% | Adaptive Acc: 87.346% | clf_exit: 0.749 0.210 0.041
Batch: 0 | Loss: 1.416 | Acc: 73.438,86.719,89.062,% | Adaptive Acc: 83.594% | clf_exit: 0.727 0.188 0.086
Batch: 20 | Loss: 1.531 | Acc: 72.656,85.714,90.030,% | Adaptive Acc: 81.101% | clf_exit: 0.782 0.180 0.038
Batch: 40 | Loss: 1.534 | Acc: 72.675,86.128,89.958,% | Adaptive Acc: 80.659% | clf_exit: 0.790 0.175 0.036
Batch: 60 | Loss: 1.542 | Acc: 72.861,85.861,89.933,% | Adaptive Acc: 80.648% | clf_exit: 0.787 0.175 0.038
Train all parameters

Epoch: 126
Batch: 0 | Loss: 0.914 | Acc: 81.250,94.531,96.875,% | Adaptive Acc: 93.750% | clf_exit: 0.758 0.195 0.047
Batch: 20 | Loss: 1.010 | Acc: 76.339,93.527,97.321,% | Adaptive Acc: 87.686% | clf_exit: 0.759 0.206 0.036
Batch: 40 | Loss: 1.022 | Acc: 76.753,93.559,97.199,% | Adaptive Acc: 88.034% | clf_exit: 0.756 0.203 0.040
Batch: 60 | Loss: 1.025 | Acc: 76.524,93.315,97.016,% | Adaptive Acc: 87.923% | clf_exit: 0.756 0.204 0.040
Batch: 80 | Loss: 1.025 | Acc: 76.283,93.268,97.145,% | Adaptive Acc: 87.828% | clf_exit: 0.756 0.204 0.040
Batch: 100 | Loss: 1.022 | Acc: 76.377,93.332,97.192,% | Adaptive Acc: 87.933% | clf_exit: 0.755 0.205 0.041
Batch: 120 | Loss: 1.017 | Acc: 76.517,93.408,97.243,% | Adaptive Acc: 88.068% | clf_exit: 0.754 0.205 0.041
Batch: 140 | Loss: 1.013 | Acc: 76.612,93.495,97.274,% | Adaptive Acc: 88.115% | clf_exit: 0.752 0.207 0.041
Batch: 160 | Loss: 1.009 | Acc: 76.562,93.483,97.351,% | Adaptive Acc: 88.077% | clf_exit: 0.753 0.207 0.040
Batch: 180 | Loss: 1.013 | Acc: 76.550,93.392,97.276,% | Adaptive Acc: 87.936% | clf_exit: 0.753 0.207 0.040
Batch: 200 | Loss: 1.019 | Acc: 76.485,93.334,97.264,% | Adaptive Acc: 87.935% | clf_exit: 0.752 0.208 0.039
Batch: 220 | Loss: 1.026 | Acc: 76.418,93.266,97.186,% | Adaptive Acc: 87.740% | clf_exit: 0.752 0.209 0.039
Batch: 240 | Loss: 1.028 | Acc: 76.374,93.189,97.167,% | Adaptive Acc: 87.717% | clf_exit: 0.752 0.209 0.040
Batch: 260 | Loss: 1.029 | Acc: 76.431,93.166,97.129,% | Adaptive Acc: 87.722% | clf_exit: 0.752 0.208 0.040
Batch: 280 | Loss: 1.032 | Acc: 76.454,93.124,97.081,% | Adaptive Acc: 87.647% | clf_exit: 0.753 0.208 0.040
Batch: 300 | Loss: 1.035 | Acc: 76.443,93.054,96.992,% | Adaptive Acc: 87.625% | clf_exit: 0.752 0.208 0.040
Batch: 320 | Loss: 1.039 | Acc: 76.448,93.003,96.938,% | Adaptive Acc: 87.588% | clf_exit: 0.752 0.208 0.040
Batch: 340 | Loss: 1.041 | Acc: 76.443,93.010,96.928,% | Adaptive Acc: 87.596% | clf_exit: 0.752 0.208 0.040
Batch: 360 | Loss: 1.043 | Acc: 76.396,92.949,96.925,% | Adaptive Acc: 87.554% | clf_exit: 0.752 0.208 0.040
Batch: 380 | Loss: 1.044 | Acc: 76.386,92.899,96.900,% | Adaptive Acc: 87.547% | clf_exit: 0.752 0.208 0.040
Batch: 0 | Loss: 1.578 | Acc: 67.188,85.156,89.062,% | Adaptive Acc: 75.781% | clf_exit: 0.766 0.180 0.055
Batch: 20 | Loss: 1.593 | Acc: 71.949,86.049,89.174,% | Adaptive Acc: 79.688% | clf_exit: 0.781 0.181 0.038
Batch: 40 | Loss: 1.559 | Acc: 72.466,86.414,89.806,% | Adaptive Acc: 80.202% | clf_exit: 0.784 0.179 0.037
Batch: 60 | Loss: 1.546 | Acc: 72.400,86.616,89.997,% | Adaptive Acc: 80.341% | clf_exit: 0.783 0.180 0.037
Train all parameters

Epoch: 127
Batch: 0 | Loss: 0.879 | Acc: 78.906,94.531,96.875,% | Adaptive Acc: 89.062% | clf_exit: 0.758 0.195 0.047
Batch: 20 | Loss: 1.038 | Acc: 75.893,92.820,96.838,% | Adaptive Acc: 86.607% | clf_exit: 0.760 0.201 0.039
Batch: 40 | Loss: 1.065 | Acc: 75.152,92.550,97.123,% | Adaptive Acc: 86.643% | clf_exit: 0.748 0.212 0.040
Batch: 60 | Loss: 1.042 | Acc: 75.576,92.802,97.387,% | Adaptive Acc: 87.167% | clf_exit: 0.749 0.210 0.040
Batch: 80 | Loss: 1.036 | Acc: 75.810,92.949,97.280,% | Adaptive Acc: 87.191% | clf_exit: 0.752 0.209 0.039
Batch: 100 | Loss: 1.032 | Acc: 75.928,93.054,97.107,% | Adaptive Acc: 87.353% | clf_exit: 0.750 0.211 0.038
Batch: 120 | Loss: 1.029 | Acc: 76.162,93.040,97.127,% | Adaptive Acc: 87.468% | clf_exit: 0.749 0.212 0.038
Batch: 140 | Loss: 1.035 | Acc: 76.108,92.902,97.002,% | Adaptive Acc: 87.417% | clf_exit: 0.749 0.211 0.039
Batch: 160 | Loss: 1.041 | Acc: 75.995,92.818,96.948,% | Adaptive Acc: 87.354% | clf_exit: 0.748 0.212 0.039
Batch: 180 | Loss: 1.045 | Acc: 75.846,92.831,96.940,% | Adaptive Acc: 87.297% | clf_exit: 0.746 0.214 0.040
Batch: 200 | Loss: 1.044 | Acc: 76.046,92.817,96.887,% | Adaptive Acc: 87.403% | clf_exit: 0.747 0.213 0.040
Batch: 220 | Loss: 1.039 | Acc: 76.322,92.806,96.903,% | Adaptive Acc: 87.500% | clf_exit: 0.749 0.211 0.040
Batch: 240 | Loss: 1.041 | Acc: 76.306,92.768,96.911,% | Adaptive Acc: 87.458% | clf_exit: 0.749 0.211 0.040
Batch: 260 | Loss: 1.045 | Acc: 76.245,92.726,96.869,% | Adaptive Acc: 87.341% | clf_exit: 0.750 0.210 0.040
Batch: 280 | Loss: 1.042 | Acc: 76.335,92.771,96.844,% | Adaptive Acc: 87.317% | clf_exit: 0.751 0.209 0.040
Batch: 300 | Loss: 1.043 | Acc: 76.303,92.803,96.813,% | Adaptive Acc: 87.349% | clf_exit: 0.751 0.209 0.040
Batch: 320 | Loss: 1.045 | Acc: 76.222,92.757,96.800,% | Adaptive Acc: 87.305% | clf_exit: 0.751 0.209 0.040
Batch: 340 | Loss: 1.046 | Acc: 76.173,92.705,96.806,% | Adaptive Acc: 87.296% | clf_exit: 0.751 0.208 0.040
Batch: 360 | Loss: 1.048 | Acc: 76.130,92.692,96.786,% | Adaptive Acc: 87.236% | clf_exit: 0.751 0.209 0.041
Batch: 380 | Loss: 1.054 | Acc: 76.001,92.628,96.736,% | Adaptive Acc: 87.186% | clf_exit: 0.750 0.209 0.041
Batch: 0 | Loss: 1.474 | Acc: 75.781,89.844,88.281,% | Adaptive Acc: 82.031% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.616 | Acc: 72.656,86.384,88.579,% | Adaptive Acc: 79.688% | clf_exit: 0.812 0.163 0.024
Batch: 40 | Loss: 1.602 | Acc: 72.942,86.357,88.338,% | Adaptive Acc: 79.573% | clf_exit: 0.815 0.158 0.027
Batch: 60 | Loss: 1.586 | Acc: 73.066,86.706,88.345,% | Adaptive Acc: 79.752% | clf_exit: 0.816 0.158 0.026
Train all parameters

Epoch: 128
Batch: 0 | Loss: 0.915 | Acc: 77.344,96.094,97.656,% | Adaptive Acc: 88.281% | clf_exit: 0.727 0.250 0.023
Batch: 20 | Loss: 0.977 | Acc: 77.567,94.122,97.731,% | Adaptive Acc: 88.393% | clf_exit: 0.756 0.209 0.036
Batch: 40 | Loss: 0.984 | Acc: 77.039,94.036,97.732,% | Adaptive Acc: 87.938% | clf_exit: 0.754 0.208 0.037
Batch: 60 | Loss: 0.989 | Acc: 76.972,93.904,97.528,% | Adaptive Acc: 87.987% | clf_exit: 0.756 0.206 0.038
Batch: 80 | Loss: 1.016 | Acc: 76.524,93.490,97.309,% | Adaptive Acc: 87.780% | clf_exit: 0.750 0.211 0.040
Batch: 100 | Loss: 1.021 | Acc: 76.555,93.472,97.262,% | Adaptive Acc: 87.763% | clf_exit: 0.749 0.211 0.040
Batch: 120 | Loss: 1.023 | Acc: 76.763,93.388,97.237,% | Adaptive Acc: 87.829% | clf_exit: 0.750 0.211 0.040
Batch: 140 | Loss: 1.018 | Acc: 77.045,93.423,97.235,% | Adaptive Acc: 88.021% | clf_exit: 0.749 0.211 0.040
Batch: 160 | Loss: 1.022 | Acc: 76.849,93.304,97.200,% | Adaptive Acc: 87.869% | clf_exit: 0.751 0.209 0.039
Batch: 180 | Loss: 1.027 | Acc: 76.770,93.219,97.173,% | Adaptive Acc: 87.841% | clf_exit: 0.752 0.208 0.040
Batch: 200 | Loss: 1.031 | Acc: 76.644,93.167,97.155,% | Adaptive Acc: 87.846% | clf_exit: 0.750 0.210 0.040
Batch: 220 | Loss: 1.035 | Acc: 76.555,93.170,97.080,% | Adaptive Acc: 87.747% | clf_exit: 0.752 0.208 0.040
Batch: 240 | Loss: 1.036 | Acc: 76.481,93.095,97.060,% | Adaptive Acc: 87.613% | clf_exit: 0.751 0.209 0.040
Batch: 260 | Loss: 1.037 | Acc: 76.452,92.987,97.034,% | Adaptive Acc: 87.560% | clf_exit: 0.751 0.210 0.040
Batch: 280 | Loss: 1.037 | Acc: 76.479,92.972,97.000,% | Adaptive Acc: 87.525% | clf_exit: 0.752 0.209 0.040
Batch: 300 | Loss: 1.035 | Acc: 76.518,92.966,97.000,% | Adaptive Acc: 87.495% | clf_exit: 0.752 0.209 0.039
Batch: 320 | Loss: 1.035 | Acc: 76.526,92.908,96.987,% | Adaptive Acc: 87.461% | clf_exit: 0.753 0.208 0.039
Batch: 340 | Loss: 1.040 | Acc: 76.434,92.847,96.928,% | Adaptive Acc: 87.434% | clf_exit: 0.751 0.209 0.040
Batch: 360 | Loss: 1.042 | Acc: 76.430,92.848,96.866,% | Adaptive Acc: 87.429% | clf_exit: 0.751 0.210 0.040
Batch: 380 | Loss: 1.043 | Acc: 76.407,92.835,96.840,% | Adaptive Acc: 87.430% | clf_exit: 0.751 0.209 0.040
Batch: 0 | Loss: 1.475 | Acc: 74.219,86.719,89.844,% | Adaptive Acc: 82.031% | clf_exit: 0.773 0.195 0.031
Batch: 20 | Loss: 1.545 | Acc: 70.499,87.128,91.220,% | Adaptive Acc: 79.018% | clf_exit: 0.784 0.178 0.038
Batch: 40 | Loss: 1.523 | Acc: 70.979,87.138,90.949,% | Adaptive Acc: 79.268% | clf_exit: 0.788 0.172 0.040
Batch: 60 | Loss: 1.505 | Acc: 71.017,87.013,91.150,% | Adaptive Acc: 79.483% | clf_exit: 0.784 0.174 0.042
Train all parameters

Epoch: 129
Batch: 0 | Loss: 1.025 | Acc: 73.438,92.969,99.219,% | Adaptive Acc: 84.375% | clf_exit: 0.789 0.188 0.023
Batch: 20 | Loss: 1.000 | Acc: 76.674,93.713,97.805,% | Adaptive Acc: 87.463% | clf_exit: 0.759 0.211 0.031
Batch: 40 | Loss: 1.036 | Acc: 76.220,93.102,97.466,% | Adaptive Acc: 87.462% | clf_exit: 0.750 0.212 0.038
Batch: 60 | Loss: 1.032 | Acc: 76.230,93.174,97.106,% | Adaptive Acc: 87.398% | clf_exit: 0.750 0.213 0.037
Batch: 80 | Loss: 1.030 | Acc: 76.543,93.094,96.991,% | Adaptive Acc: 87.471% | clf_exit: 0.749 0.215 0.036
Batch: 100 | Loss: 1.037 | Acc: 76.230,92.938,97.014,% | Adaptive Acc: 87.322% | clf_exit: 0.751 0.213 0.036
Batch: 120 | Loss: 1.038 | Acc: 76.285,92.749,96.940,% | Adaptive Acc: 87.352% | clf_exit: 0.751 0.212 0.037
Batch: 140 | Loss: 1.040 | Acc: 76.225,92.703,96.892,% | Adaptive Acc: 87.323% | clf_exit: 0.751 0.211 0.038
Batch: 160 | Loss: 1.041 | Acc: 76.228,92.712,96.904,% | Adaptive Acc: 87.432% | clf_exit: 0.748 0.213 0.039
Batch: 180 | Loss: 1.047 | Acc: 76.174,92.598,96.823,% | Adaptive Acc: 87.405% | clf_exit: 0.748 0.212 0.040
Batch: 200 | Loss: 1.046 | Acc: 76.294,92.689,96.770,% | Adaptive Acc: 87.376% | clf_exit: 0.749 0.211 0.040
Batch: 220 | Loss: 1.045 | Acc: 76.368,92.711,96.818,% | Adaptive Acc: 87.408% | clf_exit: 0.748 0.212 0.040
Batch: 240 | Loss: 1.044 | Acc: 76.481,92.774,96.794,% | Adaptive Acc: 87.445% | clf_exit: 0.750 0.211 0.040
Batch: 260 | Loss: 1.043 | Acc: 76.509,92.801,96.803,% | Adaptive Acc: 87.506% | clf_exit: 0.749 0.211 0.040
Batch: 280 | Loss: 1.044 | Acc: 76.449,92.782,96.800,% | Adaptive Acc: 87.514% | clf_exit: 0.750 0.210 0.040
Batch: 300 | Loss: 1.041 | Acc: 76.482,92.782,96.823,% | Adaptive Acc: 87.544% | clf_exit: 0.750 0.210 0.040
Batch: 320 | Loss: 1.044 | Acc: 76.416,92.786,96.783,% | Adaptive Acc: 87.478% | clf_exit: 0.750 0.210 0.040
Batch: 340 | Loss: 1.048 | Acc: 76.306,92.760,96.767,% | Adaptive Acc: 87.429% | clf_exit: 0.749 0.211 0.040
Batch: 360 | Loss: 1.049 | Acc: 76.264,92.746,96.741,% | Adaptive Acc: 87.377% | clf_exit: 0.749 0.211 0.040
Batch: 380 | Loss: 1.047 | Acc: 76.302,92.786,96.734,% | Adaptive Acc: 87.383% | clf_exit: 0.750 0.211 0.039
Batch: 0 | Loss: 1.510 | Acc: 73.438,89.062,86.719,% | Adaptive Acc: 82.812% | clf_exit: 0.766 0.211 0.023
Batch: 20 | Loss: 1.516 | Acc: 72.879,87.314,89.918,% | Adaptive Acc: 81.287% | clf_exit: 0.788 0.179 0.033
Batch: 40 | Loss: 1.472 | Acc: 73.838,87.500,90.091,% | Adaptive Acc: 81.402% | clf_exit: 0.794 0.175 0.031
Batch: 60 | Loss: 1.455 | Acc: 73.758,87.487,90.484,% | Adaptive Acc: 81.327% | clf_exit: 0.796 0.175 0.030
Train all parameters

Epoch: 130
Batch: 0 | Loss: 0.912 | Acc: 81.250,92.969,95.312,% | Adaptive Acc: 92.188% | clf_exit: 0.797 0.164 0.039
Batch: 20 | Loss: 1.010 | Acc: 75.595,93.527,97.210,% | Adaptive Acc: 87.351% | clf_exit: 0.765 0.196 0.039
Batch: 40 | Loss: 1.025 | Acc: 75.667,93.140,97.275,% | Adaptive Acc: 87.195% | clf_exit: 0.753 0.206 0.041
Batch: 60 | Loss: 1.041 | Acc: 75.897,92.956,97.080,% | Adaptive Acc: 87.359% | clf_exit: 0.749 0.208 0.043
Batch: 80 | Loss: 1.040 | Acc: 75.993,93.065,97.039,% | Adaptive Acc: 87.384% | clf_exit: 0.750 0.208 0.042
Batch: 100 | Loss: 1.041 | Acc: 76.122,92.984,96.968,% | Adaptive Acc: 87.314% | clf_exit: 0.754 0.204 0.042
Batch: 120 | Loss: 1.034 | Acc: 76.240,93.079,97.030,% | Adaptive Acc: 87.410% | clf_exit: 0.754 0.206 0.041
Batch: 140 | Loss: 1.032 | Acc: 76.269,93.102,97.074,% | Adaptive Acc: 87.511% | clf_exit: 0.754 0.205 0.040
Batch: 160 | Loss: 1.029 | Acc: 76.427,93.061,97.025,% | Adaptive Acc: 87.612% | clf_exit: 0.754 0.206 0.040
Batch: 180 | Loss: 1.027 | Acc: 76.468,93.103,97.013,% | Adaptive Acc: 87.634% | clf_exit: 0.754 0.207 0.039
Batch: 200 | Loss: 1.031 | Acc: 76.496,93.035,96.945,% | Adaptive Acc: 87.671% | clf_exit: 0.754 0.207 0.039
Batch: 220 | Loss: 1.029 | Acc: 76.548,93.114,96.924,% | Adaptive Acc: 87.666% | clf_exit: 0.753 0.208 0.039
Batch: 240 | Loss: 1.032 | Acc: 76.449,93.047,96.904,% | Adaptive Acc: 87.613% | clf_exit: 0.753 0.207 0.040
Batch: 260 | Loss: 1.033 | Acc: 76.431,93.017,96.875,% | Adaptive Acc: 87.602% | clf_exit: 0.752 0.208 0.040
Batch: 280 | Loss: 1.033 | Acc: 76.465,92.997,96.831,% | Adaptive Acc: 87.494% | clf_exit: 0.753 0.207 0.040
Batch: 300 | Loss: 1.036 | Acc: 76.451,92.951,96.789,% | Adaptive Acc: 87.492% | clf_exit: 0.752 0.208 0.040
Batch: 320 | Loss: 1.035 | Acc: 76.455,92.927,96.807,% | Adaptive Acc: 87.471% | clf_exit: 0.753 0.207 0.040
Batch: 340 | Loss: 1.035 | Acc: 76.432,92.950,96.813,% | Adaptive Acc: 87.459% | clf_exit: 0.753 0.207 0.040
Batch: 360 | Loss: 1.038 | Acc: 76.370,92.932,96.797,% | Adaptive Acc: 87.446% | clf_exit: 0.753 0.207 0.040
Batch: 380 | Loss: 1.039 | Acc: 76.343,92.893,96.775,% | Adaptive Acc: 87.406% | clf_exit: 0.753 0.207 0.040
Batch: 0 | Loss: 1.435 | Acc: 71.875,89.844,91.406,% | Adaptive Acc: 80.469% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 1.480 | Acc: 71.838,86.979,91.034,% | Adaptive Acc: 80.469% | clf_exit: 0.794 0.172 0.035
Batch: 40 | Loss: 1.464 | Acc: 72.771,87.252,91.254,% | Adaptive Acc: 81.079% | clf_exit: 0.797 0.164 0.039
Batch: 60 | Loss: 1.477 | Acc: 73.079,87.282,90.971,% | Adaptive Acc: 80.751% | clf_exit: 0.795 0.166 0.039
Train all parameters

Epoch: 131
Batch: 0 | Loss: 0.981 | Acc: 78.906,91.406,96.875,% | Adaptive Acc: 89.062% | clf_exit: 0.695 0.234 0.070
Batch: 20 | Loss: 1.030 | Acc: 76.823,93.006,97.061,% | Adaptive Acc: 87.500% | clf_exit: 0.744 0.221 0.035
Batch: 40 | Loss: 1.018 | Acc: 77.115,92.683,97.046,% | Adaptive Acc: 87.386% | clf_exit: 0.755 0.208 0.037
Batch: 60 | Loss: 1.014 | Acc: 77.062,92.943,97.234,% | Adaptive Acc: 87.590% | clf_exit: 0.756 0.208 0.037
Batch: 80 | Loss: 1.009 | Acc: 77.402,93.007,97.377,% | Adaptive Acc: 87.905% | clf_exit: 0.753 0.210 0.037
Batch: 100 | Loss: 1.007 | Acc: 77.321,93.193,97.331,% | Adaptive Acc: 87.980% | clf_exit: 0.753 0.209 0.038
Batch: 120 | Loss: 1.007 | Acc: 77.085,93.137,97.359,% | Adaptive Acc: 87.913% | clf_exit: 0.754 0.208 0.038
Batch: 140 | Loss: 1.006 | Acc: 77.017,93.168,97.302,% | Adaptive Acc: 87.871% | clf_exit: 0.755 0.206 0.038
Batch: 160 | Loss: 1.020 | Acc: 76.951,92.935,97.122,% | Adaptive Acc: 87.874% | clf_exit: 0.754 0.206 0.039
Batch: 180 | Loss: 1.024 | Acc: 76.839,92.904,97.065,% | Adaptive Acc: 87.772% | clf_exit: 0.756 0.205 0.039
Batch: 200 | Loss: 1.030 | Acc: 76.671,92.864,97.023,% | Adaptive Acc: 87.655% | clf_exit: 0.755 0.206 0.039
Batch: 220 | Loss: 1.032 | Acc: 76.665,92.796,97.045,% | Adaptive Acc: 87.610% | clf_exit: 0.754 0.206 0.040
Batch: 240 | Loss: 1.033 | Acc: 76.640,92.787,97.005,% | Adaptive Acc: 87.597% | clf_exit: 0.754 0.206 0.040
Batch: 260 | Loss: 1.032 | Acc: 76.658,92.765,97.031,% | Adaptive Acc: 87.659% | clf_exit: 0.754 0.205 0.041
Batch: 280 | Loss: 1.035 | Acc: 76.629,92.746,96.992,% | Adaptive Acc: 87.586% | clf_exit: 0.755 0.204 0.040
Batch: 300 | Loss: 1.038 | Acc: 76.524,92.722,96.974,% | Adaptive Acc: 87.526% | clf_exit: 0.755 0.205 0.040
Batch: 320 | Loss: 1.038 | Acc: 76.545,92.713,97.002,% | Adaptive Acc: 87.507% | clf_exit: 0.754 0.205 0.040
Batch: 340 | Loss: 1.039 | Acc: 76.569,92.682,96.994,% | Adaptive Acc: 87.447% | clf_exit: 0.755 0.205 0.040
Batch: 360 | Loss: 1.043 | Acc: 76.500,92.631,96.920,% | Adaptive Acc: 87.377% | clf_exit: 0.755 0.205 0.040
Batch: 380 | Loss: 1.045 | Acc: 76.466,92.612,96.898,% | Adaptive Acc: 87.350% | clf_exit: 0.755 0.205 0.041
Batch: 0 | Loss: 1.366 | Acc: 75.000,88.281,89.062,% | Adaptive Acc: 82.031% | clf_exit: 0.781 0.180 0.039
Batch: 20 | Loss: 1.488 | Acc: 72.879,87.128,90.923,% | Adaptive Acc: 80.432% | clf_exit: 0.797 0.163 0.040
Batch: 40 | Loss: 1.475 | Acc: 73.304,87.119,90.682,% | Adaptive Acc: 81.079% | clf_exit: 0.803 0.160 0.037
Batch: 60 | Loss: 1.466 | Acc: 73.604,87.065,90.779,% | Adaptive Acc: 81.199% | clf_exit: 0.801 0.164 0.035
Train all parameters

Epoch: 132
Batch: 0 | Loss: 0.973 | Acc: 78.125,92.969,97.656,% | Adaptive Acc: 87.500% | clf_exit: 0.789 0.195 0.016
Batch: 20 | Loss: 0.993 | Acc: 77.344,93.378,97.210,% | Adaptive Acc: 88.170% | clf_exit: 0.754 0.205 0.041
Batch: 40 | Loss: 0.992 | Acc: 77.096,93.540,97.466,% | Adaptive Acc: 87.519% | clf_exit: 0.753 0.209 0.038
Batch: 60 | Loss: 0.985 | Acc: 77.049,93.558,97.503,% | Adaptive Acc: 87.615% | clf_exit: 0.752 0.213 0.035
Batch: 80 | Loss: 0.979 | Acc: 77.392,93.519,97.405,% | Adaptive Acc: 87.712% | clf_exit: 0.757 0.208 0.035
Batch: 100 | Loss: 0.982 | Acc: 77.398,93.448,97.401,% | Adaptive Acc: 87.856% | clf_exit: 0.758 0.208 0.035
Batch: 120 | Loss: 0.981 | Acc: 77.182,93.556,97.469,% | Adaptive Acc: 87.816% | clf_exit: 0.757 0.209 0.034
Batch: 140 | Loss: 0.988 | Acc: 77.111,93.412,97.401,% | Adaptive Acc: 87.683% | clf_exit: 0.758 0.208 0.035
Batch: 160 | Loss: 0.995 | Acc: 77.130,93.250,97.302,% | Adaptive Acc: 87.612% | clf_exit: 0.757 0.208 0.035
Batch: 180 | Loss: 1.002 | Acc: 77.016,93.206,97.229,% | Adaptive Acc: 87.660% | clf_exit: 0.755 0.209 0.035
Batch: 200 | Loss: 1.003 | Acc: 77.107,93.144,97.163,% | Adaptive Acc: 87.644% | clf_exit: 0.755 0.209 0.035
Batch: 220 | Loss: 1.011 | Acc: 76.997,93.039,97.115,% | Adaptive Acc: 87.557% | clf_exit: 0.755 0.209 0.036
Batch: 240 | Loss: 1.016 | Acc: 76.900,92.962,97.057,% | Adaptive Acc: 87.526% | clf_exit: 0.754 0.210 0.036
Batch: 260 | Loss: 1.021 | Acc: 76.808,92.942,97.019,% | Adaptive Acc: 87.512% | clf_exit: 0.753 0.210 0.037
Batch: 280 | Loss: 1.020 | Acc: 76.768,92.966,97.039,% | Adaptive Acc: 87.553% | clf_exit: 0.752 0.211 0.037
Batch: 300 | Loss: 1.021 | Acc: 76.755,92.927,97.013,% | Adaptive Acc: 87.500% | clf_exit: 0.752 0.210 0.037
Batch: 320 | Loss: 1.025 | Acc: 76.728,92.920,96.972,% | Adaptive Acc: 87.471% | clf_exit: 0.752 0.211 0.038
Batch: 340 | Loss: 1.027 | Acc: 76.700,92.889,96.960,% | Adaptive Acc: 87.406% | clf_exit: 0.753 0.210 0.038
Batch: 360 | Loss: 1.025 | Acc: 76.703,92.921,96.968,% | Adaptive Acc: 87.431% | clf_exit: 0.753 0.209 0.038
Batch: 380 | Loss: 1.024 | Acc: 76.714,92.922,96.992,% | Adaptive Acc: 87.441% | clf_exit: 0.753 0.209 0.038
Batch: 0 | Loss: 1.339 | Acc: 74.219,88.281,90.625,% | Adaptive Acc: 82.031% | clf_exit: 0.805 0.156 0.039
Batch: 20 | Loss: 1.567 | Acc: 71.391,87.202,90.402,% | Adaptive Acc: 78.906% | clf_exit: 0.804 0.166 0.030
Batch: 40 | Loss: 1.525 | Acc: 72.180,87.252,90.835,% | Adaptive Acc: 79.459% | clf_exit: 0.803 0.166 0.032
Batch: 60 | Loss: 1.502 | Acc: 72.118,87.295,90.830,% | Adaptive Acc: 79.854% | clf_exit: 0.795 0.171 0.034
Train all parameters

Epoch: 133
Batch: 0 | Loss: 0.999 | Acc: 76.562,92.969,97.656,% | Adaptive Acc: 85.156% | clf_exit: 0.797 0.164 0.039
Batch: 20 | Loss: 0.992 | Acc: 76.637,93.378,97.396,% | Adaptive Acc: 87.612% | clf_exit: 0.756 0.207 0.037
Batch: 40 | Loss: 1.005 | Acc: 76.372,93.045,96.989,% | Adaptive Acc: 86.986% | clf_exit: 0.759 0.205 0.036
Batch: 60 | Loss: 0.993 | Acc: 76.895,93.302,97.285,% | Adaptive Acc: 87.756% | clf_exit: 0.759 0.205 0.036
Batch: 80 | Loss: 0.995 | Acc: 76.726,93.268,97.242,% | Adaptive Acc: 87.596% | clf_exit: 0.759 0.204 0.037
Batch: 100 | Loss: 0.995 | Acc: 76.856,93.270,97.254,% | Adaptive Acc: 87.546% | clf_exit: 0.759 0.204 0.037
Batch: 120 | Loss: 0.990 | Acc: 76.795,93.350,97.430,% | Adaptive Acc: 87.739% | clf_exit: 0.760 0.204 0.037
Batch: 140 | Loss: 0.995 | Acc: 76.745,93.334,97.340,% | Adaptive Acc: 87.644% | clf_exit: 0.760 0.203 0.038
Batch: 160 | Loss: 0.995 | Acc: 76.936,93.342,97.317,% | Adaptive Acc: 87.796% | clf_exit: 0.759 0.203 0.038
Batch: 180 | Loss: 0.996 | Acc: 76.934,93.323,97.246,% | Adaptive Acc: 87.724% | clf_exit: 0.760 0.203 0.038
Batch: 200 | Loss: 1.002 | Acc: 76.811,93.245,97.147,% | Adaptive Acc: 87.582% | clf_exit: 0.760 0.202 0.038
Batch: 220 | Loss: 1.007 | Acc: 76.623,93.170,97.140,% | Adaptive Acc: 87.504% | clf_exit: 0.759 0.202 0.038
Batch: 240 | Loss: 1.011 | Acc: 76.494,93.137,97.131,% | Adaptive Acc: 87.471% | clf_exit: 0.758 0.203 0.039
Batch: 260 | Loss: 1.012 | Acc: 76.542,93.085,97.156,% | Adaptive Acc: 87.479% | clf_exit: 0.758 0.203 0.039
Batch: 280 | Loss: 1.013 | Acc: 76.571,93.077,97.172,% | Adaptive Acc: 87.522% | clf_exit: 0.758 0.203 0.039
Batch: 300 | Loss: 1.018 | Acc: 76.531,93.015,97.124,% | Adaptive Acc: 87.420% | clf_exit: 0.758 0.203 0.039
Batch: 320 | Loss: 1.019 | Acc: 76.555,92.947,97.082,% | Adaptive Acc: 87.400% | clf_exit: 0.758 0.202 0.040
Batch: 340 | Loss: 1.022 | Acc: 76.569,92.918,97.070,% | Adaptive Acc: 87.379% | clf_exit: 0.758 0.202 0.040
Batch: 360 | Loss: 1.024 | Acc: 76.537,92.904,97.055,% | Adaptive Acc: 87.368% | clf_exit: 0.758 0.202 0.040
Batch: 380 | Loss: 1.027 | Acc: 76.528,92.827,97.019,% | Adaptive Acc: 87.367% | clf_exit: 0.757 0.203 0.041
Batch: 0 | Loss: 1.345 | Acc: 73.438,87.500,91.406,% | Adaptive Acc: 80.469% | clf_exit: 0.820 0.125 0.055
Batch: 20 | Loss: 1.576 | Acc: 71.317,87.128,90.365,% | Adaptive Acc: 79.315% | clf_exit: 0.794 0.173 0.033
Batch: 40 | Loss: 1.541 | Acc: 71.665,87.576,90.568,% | Adaptive Acc: 79.497% | clf_exit: 0.799 0.167 0.034
Batch: 60 | Loss: 1.526 | Acc: 71.952,87.487,90.779,% | Adaptive Acc: 79.559% | clf_exit: 0.794 0.171 0.034
Train all parameters

Epoch: 134
Batch: 0 | Loss: 1.054 | Acc: 76.562,91.406,96.094,% | Adaptive Acc: 84.375% | clf_exit: 0.773 0.211 0.016
Batch: 20 | Loss: 0.961 | Acc: 77.158,94.568,97.768,% | Adaptive Acc: 88.542% | clf_exit: 0.747 0.221 0.032
Batch: 40 | Loss: 0.974 | Acc: 76.753,93.693,97.752,% | Adaptive Acc: 87.995% | clf_exit: 0.745 0.218 0.037
Batch: 60 | Loss: 0.957 | Acc: 77.446,93.712,97.720,% | Adaptive Acc: 88.307% | clf_exit: 0.749 0.215 0.036
Batch: 80 | Loss: 0.966 | Acc: 77.508,93.422,97.531,% | Adaptive Acc: 88.108% | clf_exit: 0.753 0.211 0.036
Batch: 100 | Loss: 0.985 | Acc: 77.135,93.224,97.293,% | Adaptive Acc: 88.088% | clf_exit: 0.750 0.214 0.037
Batch: 120 | Loss: 0.993 | Acc: 77.092,93.214,97.237,% | Adaptive Acc: 88.094% | clf_exit: 0.749 0.215 0.037
Batch: 140 | Loss: 1.007 | Acc: 76.956,93.085,97.008,% | Adaptive Acc: 87.799% | clf_exit: 0.750 0.213 0.037
Batch: 160 | Loss: 1.012 | Acc: 77.038,93.012,96.943,% | Adaptive Acc: 87.883% | clf_exit: 0.751 0.211 0.038
Batch: 180 | Loss: 1.017 | Acc: 76.934,92.969,96.931,% | Adaptive Acc: 87.737% | clf_exit: 0.751 0.211 0.038
Batch: 200 | Loss: 1.024 | Acc: 76.753,92.949,96.879,% | Adaptive Acc: 87.628% | clf_exit: 0.750 0.212 0.038
Batch: 220 | Loss: 1.024 | Acc: 76.753,93.008,96.893,% | Adaptive Acc: 87.606% | clf_exit: 0.751 0.211 0.038
Batch: 240 | Loss: 1.030 | Acc: 76.588,92.907,96.826,% | Adaptive Acc: 87.484% | clf_exit: 0.751 0.211 0.038
Batch: 260 | Loss: 1.029 | Acc: 76.491,92.918,96.869,% | Adaptive Acc: 87.485% | clf_exit: 0.751 0.210 0.039
Batch: 280 | Loss: 1.030 | Acc: 76.421,92.905,96.892,% | Adaptive Acc: 87.467% | clf_exit: 0.751 0.211 0.039
Batch: 300 | Loss: 1.029 | Acc: 76.464,92.862,96.870,% | Adaptive Acc: 87.471% | clf_exit: 0.751 0.210 0.039
Batch: 320 | Loss: 1.028 | Acc: 76.528,92.825,96.841,% | Adaptive Acc: 87.429% | clf_exit: 0.753 0.208 0.039
Batch: 340 | Loss: 1.030 | Acc: 76.471,92.802,96.841,% | Adaptive Acc: 87.411% | clf_exit: 0.753 0.208 0.039
Batch: 360 | Loss: 1.030 | Acc: 76.476,92.809,96.860,% | Adaptive Acc: 87.435% | clf_exit: 0.753 0.208 0.039
Batch: 380 | Loss: 1.029 | Acc: 76.487,92.833,96.881,% | Adaptive Acc: 87.441% | clf_exit: 0.753 0.208 0.039
Batch: 0 | Loss: 1.404 | Acc: 69.531,86.719,91.406,% | Adaptive Acc: 78.906% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 1.485 | Acc: 72.619,86.942,91.034,% | Adaptive Acc: 80.357% | clf_exit: 0.791 0.177 0.032
Batch: 40 | Loss: 1.469 | Acc: 73.438,87.043,91.235,% | Adaptive Acc: 80.259% | clf_exit: 0.812 0.159 0.030
Batch: 60 | Loss: 1.466 | Acc: 73.361,87.231,91.137,% | Adaptive Acc: 80.418% | clf_exit: 0.811 0.157 0.032
Train classifier parameters

Epoch: 135
Batch: 0 | Loss: 0.890 | Acc: 75.000,95.312,99.219,% | Adaptive Acc: 90.625% | clf_exit: 0.742 0.227 0.031
Batch: 20 | Loss: 1.175 | Acc: 73.214,91.518,95.945,% | Adaptive Acc: 84.561% | clf_exit: 0.746 0.209 0.045
Batch: 40 | Loss: 1.308 | Acc: 71.970,89.996,94.398,% | Adaptive Acc: 83.270% | clf_exit: 0.732 0.216 0.051
Batch: 60 | Loss: 1.345 | Acc: 71.542,89.575,94.006,% | Adaptive Acc: 83.299% | clf_exit: 0.724 0.223 0.053
Batch: 80 | Loss: 1.374 | Acc: 71.017,89.333,93.750,% | Adaptive Acc: 82.938% | clf_exit: 0.722 0.224 0.054
Batch: 100 | Loss: 1.386 | Acc: 71.109,89.001,93.611,% | Adaptive Acc: 82.890% | clf_exit: 0.718 0.226 0.056
Batch: 120 | Loss: 1.388 | Acc: 70.952,89.030,93.601,% | Adaptive Acc: 82.961% | clf_exit: 0.714 0.228 0.058
Batch: 140 | Loss: 1.390 | Acc: 70.850,88.885,93.645,% | Adaptive Acc: 82.957% | clf_exit: 0.711 0.230 0.058
Batch: 160 | Loss: 1.391 | Acc: 70.812,88.844,93.634,% | Adaptive Acc: 83.075% | clf_exit: 0.709 0.232 0.059
Batch: 180 | Loss: 1.386 | Acc: 70.856,88.842,93.677,% | Adaptive Acc: 83.136% | clf_exit: 0.709 0.232 0.059
Batch: 200 | Loss: 1.394 | Acc: 70.752,88.783,93.598,% | Adaptive Acc: 83.190% | clf_exit: 0.707 0.232 0.060
Batch: 220 | Loss: 1.390 | Acc: 70.715,88.847,93.598,% | Adaptive Acc: 83.230% | clf_exit: 0.707 0.233 0.061
Batch: 240 | Loss: 1.386 | Acc: 70.815,88.897,93.637,% | Adaptive Acc: 83.390% | clf_exit: 0.706 0.233 0.061
Batch: 260 | Loss: 1.387 | Acc: 70.857,88.877,93.567,% | Adaptive Acc: 83.462% | clf_exit: 0.705 0.234 0.061
Batch: 280 | Loss: 1.383 | Acc: 70.896,88.937,93.644,% | Adaptive Acc: 83.572% | clf_exit: 0.703 0.236 0.061
Batch: 300 | Loss: 1.385 | Acc: 70.938,88.946,93.602,% | Adaptive Acc: 83.586% | clf_exit: 0.703 0.237 0.061
Batch: 320 | Loss: 1.384 | Acc: 70.979,88.977,93.592,% | Adaptive Acc: 83.625% | clf_exit: 0.703 0.236 0.060
Batch: 340 | Loss: 1.381 | Acc: 71.055,88.985,93.622,% | Adaptive Acc: 83.727% | clf_exit: 0.702 0.237 0.061
Batch: 360 | Loss: 1.381 | Acc: 71.135,88.998,93.618,% | Adaptive Acc: 83.810% | clf_exit: 0.701 0.238 0.061
Batch: 380 | Loss: 1.384 | Acc: 71.079,88.989,93.584,% | Adaptive Acc: 83.809% | clf_exit: 0.700 0.238 0.062
Batch: 0 | Loss: 1.523 | Acc: 68.750,86.719,91.406,% | Adaptive Acc: 80.469% | clf_exit: 0.742 0.195 0.062
Batch: 20 | Loss: 1.729 | Acc: 69.159,84.338,88.244,% | Adaptive Acc: 77.902% | clf_exit: 0.751 0.200 0.049
Batch: 40 | Loss: 1.699 | Acc: 70.293,85.061,88.529,% | Adaptive Acc: 78.659% | clf_exit: 0.752 0.200 0.048
Batch: 60 | Loss: 1.703 | Acc: 70.261,84.874,88.358,% | Adaptive Acc: 78.842% | clf_exit: 0.743 0.207 0.049
Train classifier parameters

Epoch: 136
Batch: 0 | Loss: 1.178 | Acc: 76.562,92.969,96.875,% | Adaptive Acc: 84.375% | clf_exit: 0.703 0.242 0.055
Batch: 20 | Loss: 1.335 | Acc: 72.731,89.509,93.899,% | Adaptive Acc: 85.938% | clf_exit: 0.683 0.251 0.066
Batch: 40 | Loss: 1.354 | Acc: 72.332,88.777,93.540,% | Adaptive Acc: 85.194% | clf_exit: 0.688 0.246 0.066
Batch: 60 | Loss: 1.336 | Acc: 72.426,89.242,93.827,% | Adaptive Acc: 85.489% | clf_exit: 0.686 0.248 0.066
Batch: 80 | Loss: 1.340 | Acc: 72.078,89.198,93.789,% | Adaptive Acc: 85.069% | clf_exit: 0.686 0.249 0.065
Batch: 100 | Loss: 1.360 | Acc: 71.914,88.946,93.626,% | Adaptive Acc: 84.932% | clf_exit: 0.684 0.251 0.065
Batch: 120 | Loss: 1.354 | Acc: 72.024,89.004,93.660,% | Adaptive Acc: 85.008% | clf_exit: 0.685 0.250 0.065
Batch: 140 | Loss: 1.347 | Acc: 72.052,89.146,93.695,% | Adaptive Acc: 84.996% | clf_exit: 0.685 0.250 0.065
Batch: 160 | Loss: 1.349 | Acc: 72.006,89.164,93.808,% | Adaptive Acc: 85.142% | clf_exit: 0.684 0.250 0.066
Batch: 180 | Loss: 1.351 | Acc: 72.086,89.170,93.707,% | Adaptive Acc: 85.048% | clf_exit: 0.683 0.251 0.066
Batch: 200 | Loss: 1.347 | Acc: 72.190,89.222,93.703,% | Adaptive Acc: 85.110% | clf_exit: 0.683 0.251 0.066
Batch: 220 | Loss: 1.352 | Acc: 72.052,89.232,93.637,% | Adaptive Acc: 85.050% | clf_exit: 0.683 0.251 0.066
Batch: 240 | Loss: 1.348 | Acc: 72.170,89.335,93.656,% | Adaptive Acc: 85.085% | clf_exit: 0.684 0.251 0.066
Batch: 260 | Loss: 1.346 | Acc: 72.234,89.362,93.666,% | Adaptive Acc: 85.177% | clf_exit: 0.684 0.251 0.065
Batch: 280 | Loss: 1.349 | Acc: 72.170,89.321,93.653,% | Adaptive Acc: 85.090% | clf_exit: 0.684 0.251 0.065
Batch: 300 | Loss: 1.349 | Acc: 72.202,89.335,93.654,% | Adaptive Acc: 85.110% | clf_exit: 0.685 0.251 0.065
Batch: 320 | Loss: 1.349 | Acc: 72.194,89.320,93.643,% | Adaptive Acc: 85.091% | clf_exit: 0.685 0.250 0.065
Batch: 340 | Loss: 1.349 | Acc: 72.177,89.271,93.674,% | Adaptive Acc: 85.067% | clf_exit: 0.684 0.251 0.065
Batch: 360 | Loss: 1.351 | Acc: 72.152,89.296,93.692,% | Adaptive Acc: 85.048% | clf_exit: 0.685 0.250 0.065
Batch: 380 | Loss: 1.351 | Acc: 72.154,89.304,93.719,% | Adaptive Acc: 85.027% | clf_exit: 0.686 0.250 0.064
Batch: 0 | Loss: 1.525 | Acc: 68.750,85.938,91.406,% | Adaptive Acc: 80.469% | clf_exit: 0.742 0.195 0.062
Batch: 20 | Loss: 1.703 | Acc: 69.866,84.635,88.170,% | Adaptive Acc: 78.646% | clf_exit: 0.739 0.210 0.051
Batch: 40 | Loss: 1.676 | Acc: 70.903,85.232,88.586,% | Adaptive Acc: 79.306% | clf_exit: 0.744 0.206 0.049
Batch: 60 | Loss: 1.680 | Acc: 70.786,84.990,88.384,% | Adaptive Acc: 79.419% | clf_exit: 0.735 0.215 0.050
Train classifier parameters

Epoch: 137
Batch: 0 | Loss: 1.316 | Acc: 68.750,88.281,96.875,% | Adaptive Acc: 83.594% | clf_exit: 0.734 0.195 0.070
Batch: 20 | Loss: 1.349 | Acc: 72.545,89.025,93.973,% | Adaptive Acc: 84.896% | clf_exit: 0.686 0.247 0.067
Batch: 40 | Loss: 1.355 | Acc: 72.580,89.101,94.150,% | Adaptive Acc: 85.461% | clf_exit: 0.687 0.248 0.066
Batch: 60 | Loss: 1.339 | Acc: 72.900,89.395,94.352,% | Adaptive Acc: 85.400% | clf_exit: 0.699 0.236 0.066
Batch: 80 | Loss: 1.334 | Acc: 72.868,89.323,94.145,% | Adaptive Acc: 85.648% | clf_exit: 0.690 0.243 0.067
Batch: 100 | Loss: 1.335 | Acc: 72.679,89.511,94.152,% | Adaptive Acc: 85.566% | clf_exit: 0.688 0.245 0.067
Batch: 120 | Loss: 1.320 | Acc: 72.960,89.676,94.228,% | Adaptive Acc: 85.731% | clf_exit: 0.687 0.248 0.065
Batch: 140 | Loss: 1.315 | Acc: 73.050,89.733,94.276,% | Adaptive Acc: 85.827% | clf_exit: 0.687 0.247 0.066
Batch: 160 | Loss: 1.321 | Acc: 72.884,89.630,94.211,% | Adaptive Acc: 85.739% | clf_exit: 0.684 0.250 0.066
Batch: 180 | Loss: 1.320 | Acc: 72.928,89.550,94.212,% | Adaptive Acc: 85.653% | clf_exit: 0.686 0.248 0.066
Batch: 200 | Loss: 1.325 | Acc: 72.932,89.509,94.108,% | Adaptive Acc: 85.619% | clf_exit: 0.685 0.249 0.066
Batch: 220 | Loss: 1.325 | Acc: 72.769,89.635,94.104,% | Adaptive Acc: 85.549% | clf_exit: 0.685 0.249 0.066
Batch: 240 | Loss: 1.328 | Acc: 72.776,89.542,94.074,% | Adaptive Acc: 85.542% | clf_exit: 0.685 0.249 0.066
Batch: 260 | Loss: 1.326 | Acc: 72.734,89.598,94.058,% | Adaptive Acc: 85.587% | clf_exit: 0.684 0.249 0.066
Batch: 280 | Loss: 1.327 | Acc: 72.751,89.560,94.047,% | Adaptive Acc: 85.534% | clf_exit: 0.685 0.248 0.067
Batch: 300 | Loss: 1.329 | Acc: 72.716,89.514,94.007,% | Adaptive Acc: 85.512% | clf_exit: 0.685 0.249 0.067
Batch: 320 | Loss: 1.329 | Acc: 72.817,89.532,93.976,% | Adaptive Acc: 85.582% | clf_exit: 0.684 0.249 0.067
Batch: 340 | Loss: 1.327 | Acc: 72.904,89.562,93.945,% | Adaptive Acc: 85.608% | clf_exit: 0.685 0.248 0.067
Batch: 360 | Loss: 1.328 | Acc: 72.944,89.539,93.973,% | Adaptive Acc: 85.587% | clf_exit: 0.685 0.248 0.067
Batch: 380 | Loss: 1.325 | Acc: 72.956,89.581,94.012,% | Adaptive Acc: 85.609% | clf_exit: 0.686 0.248 0.066
Batch: 0 | Loss: 1.477 | Acc: 67.969,86.719,90.625,% | Adaptive Acc: 79.688% | clf_exit: 0.750 0.188 0.062
Batch: 20 | Loss: 1.676 | Acc: 70.722,85.007,88.244,% | Adaptive Acc: 78.423% | clf_exit: 0.749 0.205 0.046
Batch: 40 | Loss: 1.646 | Acc: 71.303,85.518,88.643,% | Adaptive Acc: 79.440% | clf_exit: 0.754 0.197 0.049
Batch: 60 | Loss: 1.647 | Acc: 71.145,85.374,88.435,% | Adaptive Acc: 79.636% | clf_exit: 0.745 0.206 0.049
Train classifier parameters

Epoch: 138
Batch: 0 | Loss: 1.506 | Acc: 74.219,88.281,89.062,% | Adaptive Acc: 82.031% | clf_exit: 0.641 0.320 0.039
Batch: 20 | Loss: 1.295 | Acc: 74.368,90.141,93.973,% | Adaptive Acc: 86.830% | clf_exit: 0.678 0.261 0.061
Batch: 40 | Loss: 1.279 | Acc: 74.181,90.072,94.284,% | Adaptive Acc: 86.623% | clf_exit: 0.688 0.251 0.061
Batch: 60 | Loss: 1.295 | Acc: 73.591,89.818,94.262,% | Adaptive Acc: 86.373% | clf_exit: 0.686 0.252 0.062
Batch: 80 | Loss: 1.307 | Acc: 73.196,89.709,94.088,% | Adaptive Acc: 86.130% | clf_exit: 0.685 0.252 0.062
Batch: 100 | Loss: 1.308 | Acc: 73.113,89.588,94.114,% | Adaptive Acc: 86.092% | clf_exit: 0.687 0.249 0.063
Batch: 120 | Loss: 1.313 | Acc: 73.082,89.527,93.995,% | Adaptive Acc: 86.034% | clf_exit: 0.687 0.248 0.065
Batch: 140 | Loss: 1.311 | Acc: 73.116,89.633,94.066,% | Adaptive Acc: 86.054% | clf_exit: 0.688 0.247 0.065
Batch: 160 | Loss: 1.314 | Acc: 72.899,89.635,94.056,% | Adaptive Acc: 85.971% | clf_exit: 0.687 0.249 0.065
Batch: 180 | Loss: 1.319 | Acc: 72.889,89.589,94.044,% | Adaptive Acc: 85.899% | clf_exit: 0.688 0.248 0.065
Batch: 200 | Loss: 1.321 | Acc: 72.963,89.618,94.003,% | Adaptive Acc: 85.836% | clf_exit: 0.689 0.246 0.065
Batch: 220 | Loss: 1.319 | Acc: 73.049,89.632,94.036,% | Adaptive Acc: 85.881% | clf_exit: 0.688 0.247 0.065
Batch: 240 | Loss: 1.319 | Acc: 73.029,89.669,93.961,% | Adaptive Acc: 85.808% | clf_exit: 0.689 0.246 0.065
Batch: 260 | Loss: 1.316 | Acc: 73.084,89.691,93.980,% | Adaptive Acc: 85.770% | clf_exit: 0.689 0.246 0.065
Batch: 280 | Loss: 1.319 | Acc: 73.032,89.621,93.964,% | Adaptive Acc: 85.707% | clf_exit: 0.689 0.246 0.065
Batch: 300 | Loss: 1.317 | Acc: 73.061,89.652,94.012,% | Adaptive Acc: 85.701% | clf_exit: 0.689 0.246 0.065
Batch: 320 | Loss: 1.318 | Acc: 73.016,89.637,94.049,% | Adaptive Acc: 85.728% | clf_exit: 0.688 0.247 0.065
Batch: 340 | Loss: 1.318 | Acc: 73.069,89.617,94.036,% | Adaptive Acc: 85.734% | clf_exit: 0.688 0.247 0.064
Batch: 360 | Loss: 1.319 | Acc: 73.022,89.606,94.038,% | Adaptive Acc: 85.725% | clf_exit: 0.688 0.247 0.065
Batch: 380 | Loss: 1.317 | Acc: 73.056,89.626,94.010,% | Adaptive Acc: 85.767% | clf_exit: 0.688 0.247 0.064
Batch: 0 | Loss: 1.435 | Acc: 67.188,88.281,91.406,% | Adaptive Acc: 80.469% | clf_exit: 0.750 0.195 0.055
Batch: 20 | Loss: 1.661 | Acc: 70.387,85.268,88.318,% | Adaptive Acc: 79.092% | clf_exit: 0.751 0.200 0.049
Batch: 40 | Loss: 1.630 | Acc: 71.361,85.976,88.777,% | Adaptive Acc: 79.821% | clf_exit: 0.756 0.195 0.049
Batch: 60 | Loss: 1.629 | Acc: 71.209,85.733,88.627,% | Adaptive Acc: 79.880% | clf_exit: 0.749 0.203 0.049
Train classifier parameters

Epoch: 139
Batch: 0 | Loss: 1.140 | Acc: 77.344,91.406,96.094,% | Adaptive Acc: 87.500% | clf_exit: 0.703 0.242 0.055
Batch: 20 | Loss: 1.306 | Acc: 72.024,89.993,93.638,% | Adaptive Acc: 85.305% | clf_exit: 0.681 0.252 0.067
Batch: 40 | Loss: 1.332 | Acc: 71.665,89.787,93.693,% | Adaptive Acc: 84.832% | clf_exit: 0.682 0.252 0.066
Batch: 60 | Loss: 1.324 | Acc: 72.170,89.895,93.699,% | Adaptive Acc: 85.041% | clf_exit: 0.686 0.247 0.066
Batch: 80 | Loss: 1.327 | Acc: 72.483,89.728,93.731,% | Adaptive Acc: 85.012% | clf_exit: 0.688 0.245 0.066
Batch: 100 | Loss: 1.329 | Acc: 72.509,89.751,93.750,% | Adaptive Acc: 85.071% | clf_exit: 0.688 0.247 0.065
Batch: 120 | Loss: 1.322 | Acc: 72.669,89.715,93.827,% | Adaptive Acc: 85.143% | clf_exit: 0.690 0.245 0.065
Batch: 140 | Loss: 1.315 | Acc: 72.834,89.877,93.889,% | Adaptive Acc: 85.317% | clf_exit: 0.691 0.246 0.063
Batch: 160 | Loss: 1.306 | Acc: 73.059,90.018,93.978,% | Adaptive Acc: 85.496% | clf_exit: 0.691 0.245 0.063
Batch: 180 | Loss: 1.298 | Acc: 73.360,90.103,94.035,% | Adaptive Acc: 85.614% | clf_exit: 0.693 0.244 0.063
Batch: 200 | Loss: 1.303 | Acc: 73.255,90.034,94.014,% | Adaptive Acc: 85.576% | clf_exit: 0.692 0.244 0.063
Batch: 220 | Loss: 1.302 | Acc: 73.328,90.035,93.966,% | Adaptive Acc: 85.602% | clf_exit: 0.693 0.244 0.063
Batch: 240 | Loss: 1.304 | Acc: 73.249,90.054,93.925,% | Adaptive Acc: 85.565% | clf_exit: 0.693 0.245 0.062
Batch: 260 | Loss: 1.303 | Acc: 73.279,90.062,93.945,% | Adaptive Acc: 85.617% | clf_exit: 0.694 0.244 0.063
Batch: 280 | Loss: 1.302 | Acc: 73.221,90.075,93.953,% | Adaptive Acc: 85.659% | clf_exit: 0.693 0.244 0.063
Batch: 300 | Loss: 1.304 | Acc: 73.235,90.059,93.921,% | Adaptive Acc: 85.608% | clf_exit: 0.694 0.243 0.063
Batch: 320 | Loss: 1.301 | Acc: 73.304,90.075,93.945,% | Adaptive Acc: 85.626% | clf_exit: 0.695 0.242 0.063
Batch: 340 | Loss: 1.305 | Acc: 73.289,89.979,93.906,% | Adaptive Acc: 85.557% | clf_exit: 0.695 0.242 0.063
Batch: 360 | Loss: 1.302 | Acc: 73.386,89.982,93.923,% | Adaptive Acc: 85.593% | clf_exit: 0.695 0.242 0.063
Batch: 380 | Loss: 1.303 | Acc: 73.353,89.987,93.961,% | Adaptive Acc: 85.659% | clf_exit: 0.694 0.242 0.063
Batch: 0 | Loss: 1.453 | Acc: 67.969,88.281,90.625,% | Adaptive Acc: 79.688% | clf_exit: 0.750 0.180 0.070
Batch: 20 | Loss: 1.656 | Acc: 70.945,85.119,88.430,% | Adaptive Acc: 79.204% | clf_exit: 0.747 0.206 0.047
Batch: 40 | Loss: 1.626 | Acc: 71.513,85.766,88.891,% | Adaptive Acc: 79.954% | clf_exit: 0.753 0.199 0.048
Batch: 60 | Loss: 1.628 | Acc: 71.427,85.553,88.717,% | Adaptive Acc: 80.059% | clf_exit: 0.747 0.203 0.050
Train classifier parameters

Epoch: 140
Batch: 0 | Loss: 1.177 | Acc: 75.781,90.625,95.312,% | Adaptive Acc: 86.719% | clf_exit: 0.711 0.234 0.055
Batch: 20 | Loss: 1.363 | Acc: 72.731,88.690,93.490,% | Adaptive Acc: 84.524% | clf_exit: 0.699 0.241 0.059
Batch: 40 | Loss: 1.328 | Acc: 72.961,89.482,93.979,% | Adaptive Acc: 85.252% | clf_exit: 0.697 0.241 0.062
Batch: 60 | Loss: 1.317 | Acc: 72.938,89.831,94.032,% | Adaptive Acc: 85.822% | clf_exit: 0.691 0.245 0.064
Batch: 80 | Loss: 1.317 | Acc: 73.196,89.689,93.866,% | Adaptive Acc: 85.581% | clf_exit: 0.693 0.243 0.065
Batch: 100 | Loss: 1.312 | Acc: 73.105,89.890,94.028,% | Adaptive Acc: 85.644% | clf_exit: 0.694 0.241 0.065
Batch: 120 | Loss: 1.312 | Acc: 73.134,89.773,94.008,% | Adaptive Acc: 85.550% | clf_exit: 0.693 0.243 0.064
Batch: 140 | Loss: 1.316 | Acc: 73.039,89.716,93.961,% | Adaptive Acc: 85.566% | clf_exit: 0.692 0.243 0.065
Batch: 160 | Loss: 1.319 | Acc: 73.103,89.635,93.915,% | Adaptive Acc: 85.525% | clf_exit: 0.694 0.241 0.064
Batch: 180 | Loss: 1.309 | Acc: 73.235,89.740,94.005,% | Adaptive Acc: 85.575% | clf_exit: 0.696 0.241 0.064
Batch: 200 | Loss: 1.307 | Acc: 73.340,89.824,94.034,% | Adaptive Acc: 85.673% | clf_exit: 0.697 0.240 0.064
Batch: 220 | Loss: 1.301 | Acc: 73.512,89.847,94.058,% | Adaptive Acc: 85.736% | clf_exit: 0.697 0.240 0.064
Batch: 240 | Loss: 1.300 | Acc: 73.444,89.808,94.087,% | Adaptive Acc: 85.727% | clf_exit: 0.697 0.239 0.064
Batch: 260 | Loss: 1.302 | Acc: 73.342,89.862,94.109,% | Adaptive Acc: 85.662% | clf_exit: 0.696 0.240 0.064
Batch: 280 | Loss: 1.299 | Acc: 73.362,89.888,94.111,% | Adaptive Acc: 85.684% | clf_exit: 0.697 0.239 0.064
Batch: 300 | Loss: 1.298 | Acc: 73.393,89.901,94.119,% | Adaptive Acc: 85.717% | clf_exit: 0.697 0.239 0.064
Batch: 320 | Loss: 1.297 | Acc: 73.428,89.953,94.139,% | Adaptive Acc: 85.762% | clf_exit: 0.697 0.239 0.064
Batch: 340 | Loss: 1.294 | Acc: 73.442,89.993,94.162,% | Adaptive Acc: 85.766% | clf_exit: 0.697 0.239 0.064
Batch: 360 | Loss: 1.293 | Acc: 73.502,89.991,94.131,% | Adaptive Acc: 85.756% | clf_exit: 0.698 0.239 0.064
Batch: 380 | Loss: 1.294 | Acc: 73.474,90.008,94.113,% | Adaptive Acc: 85.761% | clf_exit: 0.697 0.239 0.063
Batch: 0 | Loss: 1.455 | Acc: 66.406,86.719,90.625,% | Adaptive Acc: 80.469% | clf_exit: 0.750 0.195 0.055
Batch: 20 | Loss: 1.649 | Acc: 70.759,85.007,88.393,% | Adaptive Acc: 79.129% | clf_exit: 0.753 0.199 0.048
Batch: 40 | Loss: 1.619 | Acc: 71.570,85.899,88.948,% | Adaptive Acc: 80.069% | clf_exit: 0.755 0.197 0.049
Batch: 60 | Loss: 1.619 | Acc: 71.414,85.797,88.883,% | Adaptive Acc: 80.213% | clf_exit: 0.749 0.203 0.048
Train classifier parameters

Epoch: 141
Batch: 0 | Loss: 1.377 | Acc: 72.656,85.156,93.750,% | Adaptive Acc: 80.469% | clf_exit: 0.727 0.180 0.094
Batch: 20 | Loss: 1.264 | Acc: 73.996,90.551,94.271,% | Adaptive Acc: 86.049% | clf_exit: 0.698 0.244 0.058
Batch: 40 | Loss: 1.278 | Acc: 73.418,90.492,94.379,% | Adaptive Acc: 85.728% | clf_exit: 0.694 0.247 0.059
Batch: 60 | Loss: 1.269 | Acc: 73.386,90.228,94.531,% | Adaptive Acc: 85.681% | clf_exit: 0.698 0.244 0.059
Batch: 80 | Loss: 1.267 | Acc: 73.495,90.278,94.560,% | Adaptive Acc: 85.966% | clf_exit: 0.697 0.243 0.060
Batch: 100 | Loss: 1.261 | Acc: 73.639,90.223,94.524,% | Adaptive Acc: 85.953% | clf_exit: 0.699 0.244 0.058
Batch: 120 | Loss: 1.270 | Acc: 73.560,90.128,94.454,% | Adaptive Acc: 85.931% | clf_exit: 0.697 0.245 0.058
Batch: 140 | Loss: 1.273 | Acc: 73.365,90.099,94.492,% | Adaptive Acc: 85.877% | clf_exit: 0.698 0.242 0.060
Batch: 160 | Loss: 1.276 | Acc: 73.476,90.198,94.424,% | Adaptive Acc: 85.942% | clf_exit: 0.698 0.241 0.060
Batch: 180 | Loss: 1.279 | Acc: 73.554,90.180,94.363,% | Adaptive Acc: 85.881% | clf_exit: 0.699 0.241 0.060
Batch: 200 | Loss: 1.282 | Acc: 73.546,90.131,94.352,% | Adaptive Acc: 85.926% | clf_exit: 0.698 0.241 0.061
Batch: 220 | Loss: 1.285 | Acc: 73.494,90.077,94.337,% | Adaptive Acc: 85.846% | clf_exit: 0.698 0.242 0.061
Batch: 240 | Loss: 1.288 | Acc: 73.425,90.064,94.324,% | Adaptive Acc: 85.856% | clf_exit: 0.697 0.242 0.061
Batch: 260 | Loss: 1.290 | Acc: 73.294,90.026,94.307,% | Adaptive Acc: 85.824% | clf_exit: 0.697 0.242 0.061
Batch: 280 | Loss: 1.288 | Acc: 73.265,90.019,94.345,% | Adaptive Acc: 85.787% | clf_exit: 0.696 0.242 0.061
Batch: 300 | Loss: 1.288 | Acc: 73.365,89.984,94.316,% | Adaptive Acc: 85.803% | clf_exit: 0.697 0.242 0.061
Batch: 320 | Loss: 1.290 | Acc: 73.367,89.956,94.215,% | Adaptive Acc: 85.796% | clf_exit: 0.697 0.242 0.062
Batch: 340 | Loss: 1.289 | Acc: 73.456,89.949,94.192,% | Adaptive Acc: 85.757% | clf_exit: 0.698 0.241 0.062
Batch: 360 | Loss: 1.289 | Acc: 73.433,89.965,94.191,% | Adaptive Acc: 85.747% | clf_exit: 0.698 0.241 0.062
Batch: 380 | Loss: 1.289 | Acc: 73.454,89.965,94.220,% | Adaptive Acc: 85.812% | clf_exit: 0.698 0.240 0.062
Batch: 0 | Loss: 1.429 | Acc: 69.531,88.281,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.734 0.203 0.062
Batch: 20 | Loss: 1.647 | Acc: 70.796,85.231,88.467,% | Adaptive Acc: 79.315% | clf_exit: 0.753 0.201 0.046
Batch: 40 | Loss: 1.613 | Acc: 71.646,85.976,88.853,% | Adaptive Acc: 80.221% | clf_exit: 0.755 0.198 0.047
Batch: 60 | Loss: 1.609 | Acc: 71.644,85.835,88.768,% | Adaptive Acc: 80.379% | clf_exit: 0.748 0.205 0.048
Train classifier parameters

Epoch: 142
Batch: 0 | Loss: 1.009 | Acc: 78.906,93.750,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.719 0.242 0.039
Batch: 20 | Loss: 1.297 | Acc: 73.735,89.993,93.787,% | Adaptive Acc: 85.045% | clf_exit: 0.695 0.245 0.060
Batch: 40 | Loss: 1.293 | Acc: 73.914,90.301,93.960,% | Adaptive Acc: 85.537% | clf_exit: 0.696 0.240 0.064
Batch: 60 | Loss: 1.292 | Acc: 73.937,89.997,93.891,% | Adaptive Acc: 85.464% | clf_exit: 0.699 0.237 0.063
Batch: 80 | Loss: 1.309 | Acc: 73.254,89.670,93.682,% | Adaptive Acc: 85.253% | clf_exit: 0.696 0.239 0.065
Batch: 100 | Loss: 1.300 | Acc: 73.468,89.836,93.851,% | Adaptive Acc: 85.342% | clf_exit: 0.697 0.239 0.063
Batch: 120 | Loss: 1.309 | Acc: 73.153,89.708,93.795,% | Adaptive Acc: 85.124% | clf_exit: 0.696 0.241 0.063
Batch: 140 | Loss: 1.313 | Acc: 72.983,89.689,93.866,% | Adaptive Acc: 85.117% | clf_exit: 0.697 0.240 0.063
Batch: 160 | Loss: 1.311 | Acc: 72.952,89.727,93.896,% | Adaptive Acc: 85.210% | clf_exit: 0.695 0.242 0.062
Batch: 180 | Loss: 1.308 | Acc: 73.027,89.835,93.957,% | Adaptive Acc: 85.234% | clf_exit: 0.696 0.242 0.062
Batch: 200 | Loss: 1.304 | Acc: 73.142,89.805,94.049,% | Adaptive Acc: 85.378% | clf_exit: 0.697 0.240 0.063
Batch: 220 | Loss: 1.305 | Acc: 73.165,89.780,94.043,% | Adaptive Acc: 85.400% | clf_exit: 0.697 0.241 0.062
Batch: 240 | Loss: 1.306 | Acc: 73.042,89.753,94.029,% | Adaptive Acc: 85.341% | clf_exit: 0.697 0.241 0.062
Batch: 260 | Loss: 1.306 | Acc: 73.108,89.724,94.058,% | Adaptive Acc: 85.393% | clf_exit: 0.696 0.242 0.062
Batch: 280 | Loss: 1.305 | Acc: 73.068,89.716,94.078,% | Adaptive Acc: 85.418% | clf_exit: 0.696 0.242 0.062
Batch: 300 | Loss: 1.301 | Acc: 73.160,89.781,94.111,% | Adaptive Acc: 85.525% | clf_exit: 0.697 0.242 0.062
Batch: 320 | Loss: 1.294 | Acc: 73.345,89.810,94.137,% | Adaptive Acc: 85.604% | clf_exit: 0.698 0.240 0.062
Batch: 340 | Loss: 1.290 | Acc: 73.392,89.874,94.199,% | Adaptive Acc: 85.699% | clf_exit: 0.698 0.241 0.061
Batch: 360 | Loss: 1.287 | Acc: 73.539,89.941,94.209,% | Adaptive Acc: 85.786% | clf_exit: 0.698 0.241 0.061
Batch: 380 | Loss: 1.284 | Acc: 73.550,89.983,94.242,% | Adaptive Acc: 85.806% | clf_exit: 0.699 0.240 0.061
Batch: 0 | Loss: 1.418 | Acc: 68.750,88.281,92.188,% | Adaptive Acc: 79.688% | clf_exit: 0.750 0.180 0.070
Batch: 20 | Loss: 1.652 | Acc: 71.317,85.156,88.356,% | Adaptive Acc: 78.981% | clf_exit: 0.758 0.201 0.041
Batch: 40 | Loss: 1.622 | Acc: 71.970,85.957,88.700,% | Adaptive Acc: 79.878% | clf_exit: 0.762 0.194 0.044
Batch: 60 | Loss: 1.623 | Acc: 71.747,85.720,88.563,% | Adaptive Acc: 80.136% | clf_exit: 0.756 0.198 0.046
Train classifier parameters

Epoch: 143
Batch: 0 | Loss: 1.243 | Acc: 71.875,91.406,96.875,% | Adaptive Acc: 88.281% | clf_exit: 0.648 0.281 0.070
Batch: 20 | Loss: 1.219 | Acc: 74.628,90.737,94.345,% | Adaptive Acc: 86.049% | clf_exit: 0.702 0.237 0.061
Batch: 40 | Loss: 1.257 | Acc: 74.466,90.511,94.226,% | Adaptive Acc: 86.014% | clf_exit: 0.705 0.232 0.063
Batch: 60 | Loss: 1.241 | Acc: 74.680,90.587,94.493,% | Adaptive Acc: 86.552% | clf_exit: 0.705 0.232 0.063
Batch: 80 | Loss: 1.254 | Acc: 74.624,90.316,94.367,% | Adaptive Acc: 86.593% | clf_exit: 0.704 0.233 0.064
Batch: 100 | Loss: 1.256 | Acc: 74.435,90.362,94.291,% | Adaptive Acc: 86.518% | clf_exit: 0.702 0.235 0.063
Batch: 120 | Loss: 1.258 | Acc: 74.329,90.476,94.157,% | Adaptive Acc: 86.532% | clf_exit: 0.698 0.239 0.062
Batch: 140 | Loss: 1.257 | Acc: 74.285,90.525,94.238,% | Adaptive Acc: 86.602% | clf_exit: 0.699 0.239 0.062
Batch: 160 | Loss: 1.259 | Acc: 74.209,90.596,94.177,% | Adaptive Acc: 86.617% | clf_exit: 0.697 0.240 0.062
Batch: 180 | Loss: 1.260 | Acc: 74.141,90.586,94.156,% | Adaptive Acc: 86.460% | clf_exit: 0.698 0.240 0.062
Batch: 200 | Loss: 1.261 | Acc: 74.118,90.547,94.158,% | Adaptive Acc: 86.439% | clf_exit: 0.698 0.241 0.062
Batch: 220 | Loss: 1.265 | Acc: 73.996,90.480,94.086,% | Adaptive Acc: 86.280% | clf_exit: 0.698 0.240 0.061
Batch: 240 | Loss: 1.270 | Acc: 73.937,90.460,94.068,% | Adaptive Acc: 86.252% | clf_exit: 0.698 0.241 0.062
Batch: 260 | Loss: 1.270 | Acc: 73.952,90.427,94.094,% | Adaptive Acc: 86.201% | clf_exit: 0.699 0.240 0.062
Batch: 280 | Loss: 1.269 | Acc: 74.016,90.439,94.075,% | Adaptive Acc: 86.204% | clf_exit: 0.700 0.239 0.061
Batch: 300 | Loss: 1.270 | Acc: 74.042,90.391,94.056,% | Adaptive Acc: 86.166% | clf_exit: 0.701 0.238 0.061
Batch: 320 | Loss: 1.269 | Acc: 74.026,90.374,94.086,% | Adaptive Acc: 86.234% | clf_exit: 0.701 0.238 0.061
Batch: 340 | Loss: 1.270 | Acc: 73.985,90.339,94.121,% | Adaptive Acc: 86.222% | clf_exit: 0.700 0.239 0.061
Batch: 360 | Loss: 1.273 | Acc: 73.933,90.320,94.109,% | Adaptive Acc: 86.147% | clf_exit: 0.701 0.238 0.061
Batch: 380 | Loss: 1.275 | Acc: 73.948,90.250,94.082,% | Adaptive Acc: 86.122% | clf_exit: 0.700 0.238 0.062
Batch: 0 | Loss: 1.409 | Acc: 69.531,89.062,90.625,% | Adaptive Acc: 78.906% | clf_exit: 0.750 0.188 0.062
Batch: 20 | Loss: 1.635 | Acc: 71.243,85.379,88.244,% | Adaptive Acc: 78.832% | clf_exit: 0.756 0.202 0.042
Batch: 40 | Loss: 1.604 | Acc: 71.970,86.242,88.891,% | Adaptive Acc: 79.954% | clf_exit: 0.759 0.196 0.045
Batch: 60 | Loss: 1.601 | Acc: 71.811,86.053,88.781,% | Adaptive Acc: 80.097% | clf_exit: 0.754 0.200 0.046
Train classifier parameters

Epoch: 144
Batch: 0 | Loss: 1.191 | Acc: 71.875,90.625,95.312,% | Adaptive Acc: 82.031% | clf_exit: 0.703 0.242 0.055
Batch: 20 | Loss: 1.198 | Acc: 75.484,90.923,94.531,% | Adaptive Acc: 86.421% | clf_exit: 0.715 0.222 0.062
Batch: 40 | Loss: 1.236 | Acc: 74.886,90.473,94.512,% | Adaptive Acc: 86.566% | clf_exit: 0.711 0.225 0.065
Batch: 60 | Loss: 1.245 | Acc: 74.283,90.228,94.365,% | Adaptive Acc: 86.309% | clf_exit: 0.709 0.228 0.063
Batch: 80 | Loss: 1.260 | Acc: 74.064,90.104,94.213,% | Adaptive Acc: 86.198% | clf_exit: 0.702 0.235 0.063
Batch: 100 | Loss: 1.272 | Acc: 73.755,90.014,94.137,% | Adaptive Acc: 85.876% | clf_exit: 0.701 0.236 0.062
Batch: 120 | Loss: 1.272 | Acc: 73.676,90.089,94.189,% | Adaptive Acc: 85.795% | clf_exit: 0.702 0.236 0.062
Batch: 140 | Loss: 1.270 | Acc: 73.831,90.176,94.210,% | Adaptive Acc: 85.871% | clf_exit: 0.703 0.237 0.061
Batch: 160 | Loss: 1.267 | Acc: 73.894,90.188,94.250,% | Adaptive Acc: 85.938% | clf_exit: 0.703 0.236 0.061
Batch: 180 | Loss: 1.274 | Acc: 73.554,90.172,94.233,% | Adaptive Acc: 85.851% | clf_exit: 0.702 0.236 0.061
Batch: 200 | Loss: 1.269 | Acc: 73.706,90.225,94.236,% | Adaptive Acc: 85.945% | clf_exit: 0.703 0.236 0.060
Batch: 220 | Loss: 1.266 | Acc: 73.763,90.254,94.273,% | Adaptive Acc: 86.036% | clf_exit: 0.703 0.236 0.061
Batch: 240 | Loss: 1.265 | Acc: 73.859,90.311,94.291,% | Adaptive Acc: 86.057% | clf_exit: 0.702 0.237 0.060
Batch: 260 | Loss: 1.266 | Acc: 73.782,90.281,94.256,% | Adaptive Acc: 86.039% | clf_exit: 0.702 0.237 0.061
Batch: 280 | Loss: 1.266 | Acc: 73.804,90.269,94.239,% | Adaptive Acc: 85.993% | clf_exit: 0.703 0.236 0.060
Batch: 300 | Loss: 1.267 | Acc: 73.780,90.236,94.246,% | Adaptive Acc: 85.935% | clf_exit: 0.704 0.236 0.060
Batch: 320 | Loss: 1.269 | Acc: 73.766,90.155,94.186,% | Adaptive Acc: 85.864% | clf_exit: 0.704 0.236 0.060
Batch: 340 | Loss: 1.268 | Acc: 73.717,90.192,94.217,% | Adaptive Acc: 85.889% | clf_exit: 0.703 0.237 0.060
Batch: 360 | Loss: 1.266 | Acc: 73.784,90.253,94.246,% | Adaptive Acc: 85.929% | clf_exit: 0.703 0.237 0.060
Batch: 380 | Loss: 1.270 | Acc: 73.677,90.219,94.207,% | Adaptive Acc: 85.870% | clf_exit: 0.702 0.237 0.061
Batch: 0 | Loss: 1.431 | Acc: 69.531,88.281,90.625,% | Adaptive Acc: 82.031% | clf_exit: 0.742 0.195 0.062
Batch: 20 | Loss: 1.635 | Acc: 71.354,85.491,88.356,% | Adaptive Acc: 79.464% | clf_exit: 0.754 0.201 0.045
Batch: 40 | Loss: 1.602 | Acc: 71.818,86.223,88.834,% | Adaptive Acc: 80.240% | clf_exit: 0.758 0.195 0.047
Batch: 60 | Loss: 1.599 | Acc: 71.824,86.117,88.730,% | Adaptive Acc: 80.456% | clf_exit: 0.752 0.201 0.047
Train classifier parameters

Epoch: 145
Batch: 0 | Loss: 1.047 | Acc: 74.219,92.969,96.875,% | Adaptive Acc: 89.844% | clf_exit: 0.711 0.258 0.031
Batch: 20 | Loss: 1.270 | Acc: 74.293,89.807,94.010,% | Adaptive Acc: 86.161% | clf_exit: 0.705 0.230 0.066
Batch: 40 | Loss: 1.271 | Acc: 73.761,90.091,94.188,% | Adaptive Acc: 85.690% | clf_exit: 0.702 0.237 0.062
Batch: 60 | Loss: 1.273 | Acc: 73.886,90.305,94.109,% | Adaptive Acc: 85.912% | clf_exit: 0.701 0.237 0.062
Batch: 80 | Loss: 1.291 | Acc: 73.785,89.979,93.760,% | Adaptive Acc: 85.716% | clf_exit: 0.699 0.239 0.062
Batch: 100 | Loss: 1.277 | Acc: 73.933,90.153,93.943,% | Adaptive Acc: 85.852% | clf_exit: 0.699 0.239 0.062
Batch: 120 | Loss: 1.280 | Acc: 73.844,90.257,93.970,% | Adaptive Acc: 85.802% | clf_exit: 0.699 0.238 0.063
Batch: 140 | Loss: 1.274 | Acc: 73.892,90.326,94.121,% | Adaptive Acc: 85.926% | clf_exit: 0.699 0.239 0.062
Batch: 160 | Loss: 1.275 | Acc: 73.957,90.310,94.128,% | Adaptive Acc: 85.967% | clf_exit: 0.700 0.238 0.062
Batch: 180 | Loss: 1.273 | Acc: 73.951,90.284,94.238,% | Adaptive Acc: 85.963% | clf_exit: 0.702 0.236 0.062
Batch: 200 | Loss: 1.274 | Acc: 73.884,90.299,94.236,% | Adaptive Acc: 85.895% | clf_exit: 0.702 0.236 0.062
Batch: 220 | Loss: 1.272 | Acc: 73.894,90.296,94.217,% | Adaptive Acc: 85.902% | clf_exit: 0.702 0.237 0.061
Batch: 240 | Loss: 1.273 | Acc: 73.856,90.246,94.217,% | Adaptive Acc: 85.785% | clf_exit: 0.703 0.237 0.061
Batch: 260 | Loss: 1.274 | Acc: 73.886,90.284,94.241,% | Adaptive Acc: 85.887% | clf_exit: 0.702 0.237 0.061
Batch: 280 | Loss: 1.277 | Acc: 73.843,90.202,94.214,% | Adaptive Acc: 85.832% | clf_exit: 0.703 0.237 0.061
Batch: 300 | Loss: 1.273 | Acc: 73.858,90.236,94.248,% | Adaptive Acc: 85.906% | clf_exit: 0.702 0.237 0.061
Batch: 320 | Loss: 1.274 | Acc: 73.820,90.182,94.225,% | Adaptive Acc: 85.884% | clf_exit: 0.702 0.238 0.060
Batch: 340 | Loss: 1.273 | Acc: 73.889,90.169,94.215,% | Adaptive Acc: 85.885% | clf_exit: 0.702 0.238 0.060
Batch: 360 | Loss: 1.272 | Acc: 73.892,90.188,94.226,% | Adaptive Acc: 85.914% | clf_exit: 0.702 0.238 0.060
Batch: 380 | Loss: 1.272 | Acc: 73.915,90.151,94.203,% | Adaptive Acc: 85.913% | clf_exit: 0.702 0.237 0.061
Batch: 0 | Loss: 1.428 | Acc: 70.312,89.062,89.844,% | Adaptive Acc: 79.688% | clf_exit: 0.742 0.203 0.055
Batch: 20 | Loss: 1.628 | Acc: 70.945,85.714,88.467,% | Adaptive Acc: 79.390% | clf_exit: 0.757 0.201 0.042
Batch: 40 | Loss: 1.597 | Acc: 71.913,86.395,88.834,% | Adaptive Acc: 80.221% | clf_exit: 0.759 0.196 0.045
Batch: 60 | Loss: 1.596 | Acc: 71.824,86.168,88.845,% | Adaptive Acc: 80.405% | clf_exit: 0.752 0.203 0.045
Train classifier parameters

Epoch: 146
Batch: 0 | Loss: 1.189 | Acc: 77.344,89.844,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.695 0.227 0.078
Batch: 20 | Loss: 1.231 | Acc: 74.256,91.257,94.792,% | Adaptive Acc: 87.314% | clf_exit: 0.702 0.243 0.056
Batch: 40 | Loss: 1.249 | Acc: 73.876,90.796,94.436,% | Adaptive Acc: 86.890% | clf_exit: 0.701 0.237 0.062
Batch: 60 | Loss: 1.243 | Acc: 74.398,90.830,94.416,% | Adaptive Acc: 86.706% | clf_exit: 0.708 0.232 0.060
Batch: 80 | Loss: 1.255 | Acc: 74.402,90.625,94.184,% | Adaptive Acc: 86.256% | clf_exit: 0.707 0.232 0.061
Batch: 100 | Loss: 1.263 | Acc: 74.312,90.501,94.183,% | Adaptive Acc: 86.154% | clf_exit: 0.707 0.232 0.061
Batch: 120 | Loss: 1.267 | Acc: 74.186,90.451,94.189,% | Adaptive Acc: 85.970% | clf_exit: 0.707 0.233 0.061
Batch: 140 | Loss: 1.265 | Acc: 74.180,90.387,94.188,% | Adaptive Acc: 85.932% | clf_exit: 0.708 0.231 0.061
Batch: 160 | Loss: 1.269 | Acc: 74.015,90.314,94.192,% | Adaptive Acc: 85.797% | clf_exit: 0.708 0.231 0.061
Batch: 180 | Loss: 1.270 | Acc: 73.865,90.275,94.208,% | Adaptive Acc: 85.709% | clf_exit: 0.708 0.232 0.061
Batch: 200 | Loss: 1.262 | Acc: 74.133,90.333,94.306,% | Adaptive Acc: 85.875% | clf_exit: 0.709 0.231 0.060
Batch: 220 | Loss: 1.266 | Acc: 74.031,90.279,94.301,% | Adaptive Acc: 85.807% | clf_exit: 0.708 0.232 0.061
Batch: 240 | Loss: 1.266 | Acc: 73.969,90.359,94.334,% | Adaptive Acc: 85.759% | clf_exit: 0.708 0.232 0.061
Batch: 260 | Loss: 1.268 | Acc: 73.872,90.362,94.283,% | Adaptive Acc: 85.716% | clf_exit: 0.707 0.232 0.061
Batch: 280 | Loss: 1.271 | Acc: 73.827,90.247,94.239,% | Adaptive Acc: 85.687% | clf_exit: 0.707 0.233 0.061
Batch: 300 | Loss: 1.270 | Acc: 73.853,90.259,94.209,% | Adaptive Acc: 85.751% | clf_exit: 0.706 0.233 0.061
Batch: 320 | Loss: 1.270 | Acc: 73.842,90.238,94.198,% | Adaptive Acc: 85.755% | clf_exit: 0.706 0.233 0.061
Batch: 340 | Loss: 1.269 | Acc: 73.838,90.265,94.227,% | Adaptive Acc: 85.761% | clf_exit: 0.706 0.233 0.061
Batch: 360 | Loss: 1.272 | Acc: 73.764,90.235,94.213,% | Adaptive Acc: 85.775% | clf_exit: 0.705 0.234 0.061
Batch: 380 | Loss: 1.269 | Acc: 73.786,90.303,94.250,% | Adaptive Acc: 85.802% | clf_exit: 0.705 0.234 0.061
Batch: 0 | Loss: 1.425 | Acc: 70.312,87.500,89.844,% | Adaptive Acc: 78.906% | clf_exit: 0.758 0.180 0.062
Batch: 20 | Loss: 1.631 | Acc: 71.094,85.193,88.356,% | Adaptive Acc: 79.390% | clf_exit: 0.764 0.195 0.041
Batch: 40 | Loss: 1.599 | Acc: 71.818,85.995,88.891,% | Adaptive Acc: 80.259% | clf_exit: 0.765 0.191 0.043
Batch: 60 | Loss: 1.596 | Acc: 71.798,85.822,88.832,% | Adaptive Acc: 80.277% | clf_exit: 0.761 0.194 0.045
Train classifier parameters

Epoch: 147
Batch: 0 | Loss: 1.063 | Acc: 73.438,93.750,96.875,% | Adaptive Acc: 89.844% | clf_exit: 0.711 0.227 0.062
Batch: 20 | Loss: 1.209 | Acc: 73.884,91.183,95.015,% | Adaptive Acc: 86.533% | clf_exit: 0.714 0.235 0.050
Batch: 40 | Loss: 1.260 | Acc: 73.190,90.587,94.493,% | Adaptive Acc: 86.204% | clf_exit: 0.707 0.235 0.058
Batch: 60 | Loss: 1.250 | Acc: 73.783,90.574,94.506,% | Adaptive Acc: 85.976% | clf_exit: 0.709 0.233 0.058
Batch: 80 | Loss: 1.237 | Acc: 74.055,90.712,94.686,% | Adaptive Acc: 86.198% | clf_exit: 0.709 0.234 0.057
Batch: 100 | Loss: 1.257 | Acc: 73.933,90.455,94.392,% | Adaptive Acc: 86.038% | clf_exit: 0.707 0.236 0.057
Batch: 120 | Loss: 1.251 | Acc: 74.148,90.586,94.434,% | Adaptive Acc: 86.222% | clf_exit: 0.705 0.237 0.058
Batch: 140 | Loss: 1.253 | Acc: 74.113,90.619,94.415,% | Adaptive Acc: 86.287% | clf_exit: 0.705 0.236 0.059
Batch: 160 | Loss: 1.255 | Acc: 74.097,90.543,94.357,% | Adaptive Acc: 86.180% | clf_exit: 0.705 0.236 0.059
Batch: 180 | Loss: 1.258 | Acc: 74.025,90.422,94.350,% | Adaptive Acc: 86.032% | clf_exit: 0.706 0.235 0.059
Batch: 200 | Loss: 1.258 | Acc: 73.974,90.442,94.317,% | Adaptive Acc: 85.976% | clf_exit: 0.706 0.235 0.059
Batch: 220 | Loss: 1.260 | Acc: 73.943,90.424,94.305,% | Adaptive Acc: 85.899% | clf_exit: 0.706 0.234 0.060
Batch: 240 | Loss: 1.262 | Acc: 73.849,90.379,94.301,% | Adaptive Acc: 85.788% | clf_exit: 0.706 0.234 0.060
Batch: 260 | Loss: 1.261 | Acc: 73.979,90.389,94.271,% | Adaptive Acc: 85.836% | clf_exit: 0.707 0.233 0.060
Batch: 280 | Loss: 1.265 | Acc: 73.957,90.302,94.239,% | Adaptive Acc: 85.804% | clf_exit: 0.707 0.233 0.060
Batch: 300 | Loss: 1.268 | Acc: 73.879,90.301,94.251,% | Adaptive Acc: 85.777% | clf_exit: 0.706 0.234 0.060
Batch: 320 | Loss: 1.266 | Acc: 73.890,90.374,94.259,% | Adaptive Acc: 85.806% | clf_exit: 0.707 0.233 0.060
Batch: 340 | Loss: 1.269 | Acc: 73.822,90.284,94.227,% | Adaptive Acc: 85.766% | clf_exit: 0.706 0.233 0.061
Batch: 360 | Loss: 1.269 | Acc: 73.805,90.270,94.224,% | Adaptive Acc: 85.762% | clf_exit: 0.706 0.234 0.061
Batch: 380 | Loss: 1.270 | Acc: 73.817,90.240,94.197,% | Adaptive Acc: 85.782% | clf_exit: 0.706 0.234 0.061
Batch: 0 | Loss: 1.389 | Acc: 71.094,89.062,90.625,% | Adaptive Acc: 81.250% | clf_exit: 0.750 0.180 0.070
Batch: 20 | Loss: 1.624 | Acc: 71.689,85.342,88.393,% | Adaptive Acc: 79.353% | clf_exit: 0.762 0.195 0.043
Batch: 40 | Loss: 1.590 | Acc: 72.409,86.319,88.815,% | Adaptive Acc: 80.412% | clf_exit: 0.763 0.192 0.046
Batch: 60 | Loss: 1.588 | Acc: 72.272,86.117,88.742,% | Adaptive Acc: 80.571% | clf_exit: 0.757 0.196 0.047
Train classifier parameters

Epoch: 148
Batch: 0 | Loss: 1.139 | Acc: 77.344,92.969,96.094,% | Adaptive Acc: 86.719% | clf_exit: 0.742 0.211 0.047
Batch: 20 | Loss: 1.293 | Acc: 73.549,89.621,94.420,% | Adaptive Acc: 85.677% | clf_exit: 0.701 0.238 0.061
Batch: 40 | Loss: 1.269 | Acc: 73.933,90.225,94.379,% | Adaptive Acc: 86.300% | clf_exit: 0.703 0.239 0.058
Batch: 60 | Loss: 1.274 | Acc: 73.758,90.356,94.416,% | Adaptive Acc: 86.040% | clf_exit: 0.705 0.236 0.059
Batch: 80 | Loss: 1.275 | Acc: 73.939,90.403,94.223,% | Adaptive Acc: 86.082% | clf_exit: 0.707 0.234 0.060
Batch: 100 | Loss: 1.266 | Acc: 74.064,90.517,94.199,% | Adaptive Acc: 86.154% | clf_exit: 0.706 0.233 0.061
Batch: 120 | Loss: 1.270 | Acc: 74.012,90.502,94.208,% | Adaptive Acc: 85.989% | clf_exit: 0.706 0.233 0.061
Batch: 140 | Loss: 1.268 | Acc: 74.036,90.448,94.188,% | Adaptive Acc: 86.026% | clf_exit: 0.707 0.233 0.060
Batch: 160 | Loss: 1.265 | Acc: 73.962,90.567,94.187,% | Adaptive Acc: 85.933% | clf_exit: 0.707 0.233 0.060
Batch: 180 | Loss: 1.268 | Acc: 73.839,90.534,94.251,% | Adaptive Acc: 85.899% | clf_exit: 0.706 0.235 0.059
Batch: 200 | Loss: 1.273 | Acc: 73.752,90.512,94.197,% | Adaptive Acc: 85.844% | clf_exit: 0.705 0.236 0.059
Batch: 220 | Loss: 1.273 | Acc: 73.876,90.494,94.174,% | Adaptive Acc: 85.870% | clf_exit: 0.706 0.235 0.059
Batch: 240 | Loss: 1.273 | Acc: 73.901,90.528,94.194,% | Adaptive Acc: 85.886% | clf_exit: 0.706 0.235 0.059
Batch: 260 | Loss: 1.271 | Acc: 74.033,90.493,94.199,% | Adaptive Acc: 85.949% | clf_exit: 0.705 0.236 0.059
Batch: 280 | Loss: 1.274 | Acc: 73.980,90.422,94.117,% | Adaptive Acc: 85.885% | clf_exit: 0.705 0.236 0.060
Batch: 300 | Loss: 1.271 | Acc: 73.990,90.487,94.155,% | Adaptive Acc: 85.995% | clf_exit: 0.705 0.236 0.060
Batch: 320 | Loss: 1.270 | Acc: 74.026,90.481,94.154,% | Adaptive Acc: 85.986% | clf_exit: 0.704 0.236 0.060
Batch: 340 | Loss: 1.272 | Acc: 74.024,90.423,94.165,% | Adaptive Acc: 85.979% | clf_exit: 0.705 0.235 0.060
Batch: 360 | Loss: 1.269 | Acc: 74.069,90.417,94.183,% | Adaptive Acc: 85.981% | clf_exit: 0.705 0.235 0.060
Batch: 380 | Loss: 1.268 | Acc: 74.088,90.459,94.189,% | Adaptive Acc: 86.026% | clf_exit: 0.705 0.235 0.060
Batch: 0 | Loss: 1.414 | Acc: 69.531,87.500,90.625,% | Adaptive Acc: 79.688% | clf_exit: 0.766 0.188 0.047
Batch: 20 | Loss: 1.626 | Acc: 71.615,85.565,88.504,% | Adaptive Acc: 79.278% | clf_exit: 0.759 0.199 0.042
Batch: 40 | Loss: 1.595 | Acc: 72.027,86.223,88.967,% | Adaptive Acc: 80.354% | clf_exit: 0.758 0.197 0.045
Batch: 60 | Loss: 1.592 | Acc: 71.990,86.168,88.845,% | Adaptive Acc: 80.456% | clf_exit: 0.753 0.202 0.045
Train classifier parameters

Epoch: 149
Batch: 0 | Loss: 1.051 | Acc: 75.781,93.750,96.094,% | Adaptive Acc: 87.500% | clf_exit: 0.719 0.250 0.031
Batch: 20 | Loss: 1.221 | Acc: 74.479,90.811,94.606,% | Adaptive Acc: 86.421% | clf_exit: 0.725 0.222 0.054
Batch: 40 | Loss: 1.247 | Acc: 74.181,90.644,94.741,% | Adaptive Acc: 85.976% | clf_exit: 0.715 0.228 0.058
Batch: 60 | Loss: 1.268 | Acc: 73.783,90.318,94.557,% | Adaptive Acc: 85.771% | clf_exit: 0.713 0.228 0.059
Batch: 80 | Loss: 1.258 | Acc: 74.113,90.191,94.454,% | Adaptive Acc: 85.735% | clf_exit: 0.713 0.229 0.058
Batch: 100 | Loss: 1.264 | Acc: 73.878,90.254,94.462,% | Adaptive Acc: 85.721% | clf_exit: 0.713 0.228 0.059
Batch: 120 | Loss: 1.265 | Acc: 74.096,90.212,94.447,% | Adaptive Acc: 85.918% | clf_exit: 0.712 0.228 0.060
Batch: 140 | Loss: 1.267 | Acc: 74.119,90.137,94.310,% | Adaptive Acc: 85.882% | clf_exit: 0.711 0.231 0.059
Batch: 160 | Loss: 1.261 | Acc: 74.209,90.183,94.318,% | Adaptive Acc: 85.777% | clf_exit: 0.713 0.228 0.059
Batch: 180 | Loss: 1.265 | Acc: 74.068,90.198,94.294,% | Adaptive Acc: 85.713% | clf_exit: 0.712 0.229 0.059
Batch: 200 | Loss: 1.263 | Acc: 74.094,90.279,94.321,% | Adaptive Acc: 85.774% | clf_exit: 0.711 0.230 0.059
Batch: 220 | Loss: 1.264 | Acc: 74.095,90.236,94.301,% | Adaptive Acc: 85.839% | clf_exit: 0.710 0.231 0.059
Batch: 240 | Loss: 1.262 | Acc: 74.151,90.252,94.304,% | Adaptive Acc: 85.895% | clf_exit: 0.711 0.230 0.059
Batch: 260 | Loss: 1.267 | Acc: 74.036,90.215,94.301,% | Adaptive Acc: 85.881% | clf_exit: 0.709 0.231 0.060
Batch: 280 | Loss: 1.264 | Acc: 74.044,90.311,94.364,% | Adaptive Acc: 85.943% | clf_exit: 0.709 0.231 0.060
Batch: 300 | Loss: 1.264 | Acc: 73.985,90.295,94.373,% | Adaptive Acc: 85.914% | clf_exit: 0.709 0.231 0.060
Batch: 320 | Loss: 1.261 | Acc: 74.068,90.348,94.414,% | Adaptive Acc: 85.964% | clf_exit: 0.709 0.230 0.060
Batch: 340 | Loss: 1.262 | Acc: 73.974,90.352,94.440,% | Adaptive Acc: 85.942% | clf_exit: 0.709 0.230 0.060
Batch: 360 | Loss: 1.263 | Acc: 73.961,90.303,94.399,% | Adaptive Acc: 85.933% | clf_exit: 0.709 0.231 0.060
Batch: 380 | Loss: 1.261 | Acc: 74.040,90.334,94.380,% | Adaptive Acc: 85.968% | clf_exit: 0.709 0.231 0.060
Batch: 0 | Loss: 1.415 | Acc: 69.531,88.281,89.844,% | Adaptive Acc: 78.906% | clf_exit: 0.758 0.188 0.055
Batch: 20 | Loss: 1.628 | Acc: 71.875,85.751,88.430,% | Adaptive Acc: 79.204% | clf_exit: 0.759 0.199 0.043
Batch: 40 | Loss: 1.596 | Acc: 72.046,86.509,88.986,% | Adaptive Acc: 80.145% | clf_exit: 0.762 0.192 0.045
Batch: 60 | Loss: 1.594 | Acc: 71.977,86.245,88.896,% | Adaptive Acc: 80.187% | clf_exit: 0.756 0.197 0.047
Train all parameters

Epoch: 150
Batch: 0 | Loss: 1.028 | Acc: 75.000,92.188,97.656,% | Adaptive Acc: 92.188% | clf_exit: 0.711 0.234 0.055
Batch: 20 | Loss: 1.112 | Acc: 74.926,93.080,96.615,% | Adaptive Acc: 88.207% | clf_exit: 0.725 0.230 0.045
Batch: 40 | Loss: 1.023 | Acc: 76.296,93.941,97.523,% | Adaptive Acc: 88.700% | clf_exit: 0.741 0.219 0.040
Batch: 60 | Loss: 0.987 | Acc: 76.972,94.237,97.746,% | Adaptive Acc: 88.870% | clf_exit: 0.749 0.212 0.038
Batch: 80 | Loss: 0.981 | Acc: 76.784,94.396,97.926,% | Adaptive Acc: 88.580% | clf_exit: 0.749 0.215 0.035
Batch: 100 | Loss: 0.960 | Acc: 77.003,94.578,98.066,% | Adaptive Acc: 88.591% | clf_exit: 0.755 0.211 0.034
Batch: 120 | Loss: 0.947 | Acc: 77.266,94.673,98.199,% | Adaptive Acc: 88.707% | clf_exit: 0.757 0.210 0.033
Batch: 140 | Loss: 0.936 | Acc: 77.543,94.703,98.305,% | Adaptive Acc: 88.785% | clf_exit: 0.760 0.207 0.033
Batch: 160 | Loss: 0.923 | Acc: 77.771,94.885,98.408,% | Adaptive Acc: 88.839% | clf_exit: 0.762 0.206 0.032
Batch: 180 | Loss: 0.916 | Acc: 77.849,94.907,98.455,% | Adaptive Acc: 88.950% | clf_exit: 0.765 0.204 0.031
Batch: 200 | Loss: 0.912 | Acc: 77.892,94.885,98.527,% | Adaptive Acc: 89.016% | clf_exit: 0.765 0.203 0.032
Batch: 220 | Loss: 0.904 | Acc: 78.097,94.980,98.586,% | Adaptive Acc: 89.130% | clf_exit: 0.766 0.202 0.032
Batch: 240 | Loss: 0.899 | Acc: 78.141,95.037,98.609,% | Adaptive Acc: 89.150% | clf_exit: 0.767 0.202 0.031
Batch: 260 | Loss: 0.895 | Acc: 78.164,95.103,98.650,% | Adaptive Acc: 89.212% | clf_exit: 0.767 0.203 0.030
Batch: 280 | Loss: 0.892 | Acc: 78.228,95.168,98.696,% | Adaptive Acc: 89.246% | clf_exit: 0.768 0.203 0.030
Batch: 300 | Loss: 0.887 | Acc: 78.268,95.224,98.741,% | Adaptive Acc: 89.257% | clf_exit: 0.769 0.202 0.029
Batch: 320 | Loss: 0.883 | Acc: 78.346,95.254,98.764,% | Adaptive Acc: 89.311% | clf_exit: 0.769 0.201 0.029
Batch: 340 | Loss: 0.881 | Acc: 78.384,95.264,98.772,% | Adaptive Acc: 89.353% | clf_exit: 0.770 0.201 0.029
Batch: 360 | Loss: 0.879 | Acc: 78.426,95.310,98.799,% | Adaptive Acc: 89.359% | clf_exit: 0.771 0.200 0.029
Batch: 380 | Loss: 0.879 | Acc: 78.408,95.286,98.807,% | Adaptive Acc: 89.343% | clf_exit: 0.771 0.200 0.029
Batch: 0 | Loss: 0.942 | Acc: 79.688,95.312,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.805 0.164 0.031
Batch: 20 | Loss: 1.218 | Acc: 76.860,89.993,93.006,% | Adaptive Acc: 83.743% | clf_exit: 0.814 0.159 0.028
Batch: 40 | Loss: 1.197 | Acc: 77.534,90.187,93.274,% | Adaptive Acc: 84.356% | clf_exit: 0.821 0.152 0.026
Batch: 60 | Loss: 1.195 | Acc: 77.587,90.113,93.199,% | Adaptive Acc: 84.516% | clf_exit: 0.817 0.157 0.026
Train all parameters

Epoch: 151
Batch: 0 | Loss: 0.842 | Acc: 81.250,94.531,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.789 0.180 0.031
Batch: 20 | Loss: 0.839 | Acc: 78.274,95.685,99.368,% | Adaptive Acc: 89.546% | clf_exit: 0.777 0.200 0.023
Batch: 40 | Loss: 0.833 | Acc: 78.468,96.113,99.409,% | Adaptive Acc: 89.920% | clf_exit: 0.773 0.205 0.022
Batch: 60 | Loss: 0.823 | Acc: 78.791,96.171,99.398,% | Adaptive Acc: 89.985% | clf_exit: 0.777 0.201 0.022
Batch: 80 | Loss: 0.821 | Acc: 78.877,96.065,99.354,% | Adaptive Acc: 89.882% | clf_exit: 0.779 0.198 0.023
Batch: 100 | Loss: 0.818 | Acc: 78.945,96.094,99.350,% | Adaptive Acc: 89.813% | clf_exit: 0.778 0.200 0.023
Batch: 120 | Loss: 0.822 | Acc: 78.822,96.113,99.348,% | Adaptive Acc: 89.721% | clf_exit: 0.777 0.201 0.022
Batch: 140 | Loss: 0.822 | Acc: 78.873,96.116,99.330,% | Adaptive Acc: 89.783% | clf_exit: 0.778 0.200 0.023
Batch: 160 | Loss: 0.819 | Acc: 79.042,96.069,99.321,% | Adaptive Acc: 89.781% | clf_exit: 0.778 0.199 0.023
Batch: 180 | Loss: 0.820 | Acc: 79.057,96.089,99.331,% | Adaptive Acc: 89.775% | clf_exit: 0.778 0.198 0.023
Batch: 200 | Loss: 0.822 | Acc: 78.914,96.020,99.343,% | Adaptive Acc: 89.735% | clf_exit: 0.777 0.199 0.024
Batch: 220 | Loss: 0.824 | Acc: 78.885,96.016,99.346,% | Adaptive Acc: 89.734% | clf_exit: 0.777 0.200 0.023
Batch: 240 | Loss: 0.822 | Acc: 78.922,96.029,99.355,% | Adaptive Acc: 89.763% | clf_exit: 0.777 0.200 0.023
Batch: 260 | Loss: 0.821 | Acc: 78.951,95.986,99.350,% | Adaptive Acc: 89.766% | clf_exit: 0.777 0.199 0.024
Batch: 280 | Loss: 0.822 | Acc: 78.867,95.988,99.355,% | Adaptive Acc: 89.774% | clf_exit: 0.777 0.199 0.024
Batch: 300 | Loss: 0.818 | Acc: 78.971,96.000,99.369,% | Adaptive Acc: 89.794% | clf_exit: 0.778 0.199 0.024
Batch: 320 | Loss: 0.819 | Acc: 78.948,95.965,99.372,% | Adaptive Acc: 89.771% | clf_exit: 0.778 0.198 0.024
Batch: 340 | Loss: 0.818 | Acc: 78.989,95.938,99.365,% | Adaptive Acc: 89.731% | clf_exit: 0.778 0.198 0.023
Batch: 360 | Loss: 0.820 | Acc: 78.960,95.934,99.353,% | Adaptive Acc: 89.705% | clf_exit: 0.778 0.198 0.023
Batch: 380 | Loss: 0.819 | Acc: 78.996,95.956,99.360,% | Adaptive Acc: 89.737% | clf_exit: 0.779 0.198 0.023
Batch: 0 | Loss: 1.043 | Acc: 82.031,91.406,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.781 0.180 0.039
Batch: 20 | Loss: 1.212 | Acc: 76.897,89.993,92.969,% | Adaptive Acc: 84.375% | clf_exit: 0.814 0.156 0.031
Batch: 40 | Loss: 1.195 | Acc: 77.496,90.072,93.236,% | Adaptive Acc: 84.280% | clf_exit: 0.825 0.148 0.027
Batch: 60 | Loss: 1.187 | Acc: 77.421,90.228,93.404,% | Adaptive Acc: 84.490% | clf_exit: 0.821 0.152 0.027
Train all parameters

Epoch: 152
Batch: 0 | Loss: 0.656 | Acc: 82.812,97.656,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.797 0.195 0.008
Batch: 20 | Loss: 0.774 | Acc: 80.394,96.726,99.293,% | Adaptive Acc: 91.109% | clf_exit: 0.779 0.197 0.024
Batch: 40 | Loss: 0.776 | Acc: 79.935,96.761,99.333,% | Adaptive Acc: 90.720% | clf_exit: 0.780 0.196 0.024
Batch: 60 | Loss: 0.785 | Acc: 79.470,96.747,99.449,% | Adaptive Acc: 90.459% | clf_exit: 0.780 0.196 0.024
Batch: 80 | Loss: 0.787 | Acc: 79.533,96.615,99.479,% | Adaptive Acc: 90.172% | clf_exit: 0.786 0.191 0.023
Batch: 100 | Loss: 0.785 | Acc: 79.688,96.658,99.497,% | Adaptive Acc: 90.145% | clf_exit: 0.787 0.191 0.022
Batch: 120 | Loss: 0.783 | Acc: 79.771,96.662,99.516,% | Adaptive Acc: 90.141% | clf_exit: 0.788 0.190 0.022
Batch: 140 | Loss: 0.785 | Acc: 79.782,96.565,99.518,% | Adaptive Acc: 90.254% | clf_exit: 0.787 0.191 0.022
Batch: 160 | Loss: 0.780 | Acc: 80.003,96.569,99.524,% | Adaptive Acc: 90.305% | clf_exit: 0.789 0.191 0.021
Batch: 180 | Loss: 0.778 | Acc: 79.998,96.581,99.525,% | Adaptive Acc: 90.254% | clf_exit: 0.789 0.190 0.021
Batch: 200 | Loss: 0.780 | Acc: 79.866,96.560,99.537,% | Adaptive Acc: 90.182% | clf_exit: 0.789 0.189 0.021
Batch: 220 | Loss: 0.783 | Acc: 79.857,96.465,99.526,% | Adaptive Acc: 90.165% | clf_exit: 0.789 0.189 0.021
Batch: 240 | Loss: 0.782 | Acc: 79.882,96.434,99.517,% | Adaptive Acc: 90.187% | clf_exit: 0.789 0.190 0.021
Batch: 260 | Loss: 0.784 | Acc: 79.780,96.408,99.488,% | Adaptive Acc: 90.188% | clf_exit: 0.788 0.191 0.022
Batch: 280 | Loss: 0.786 | Acc: 79.682,96.380,99.488,% | Adaptive Acc: 90.091% | clf_exit: 0.789 0.190 0.022
Batch: 300 | Loss: 0.788 | Acc: 79.636,96.377,99.496,% | Adaptive Acc: 90.028% | clf_exit: 0.789 0.190 0.022
Batch: 320 | Loss: 0.790 | Acc: 79.622,96.349,99.477,% | Adaptive Acc: 90.058% | clf_exit: 0.788 0.190 0.022
Batch: 340 | Loss: 0.791 | Acc: 79.568,96.334,99.468,% | Adaptive Acc: 90.043% | clf_exit: 0.788 0.191 0.022
Batch: 360 | Loss: 0.792 | Acc: 79.488,96.334,99.470,% | Adaptive Acc: 89.980% | clf_exit: 0.787 0.191 0.022
Batch: 380 | Loss: 0.792 | Acc: 79.482,96.323,99.461,% | Adaptive Acc: 89.961% | clf_exit: 0.787 0.191 0.022
Batch: 0 | Loss: 0.980 | Acc: 79.688,91.406,94.531,% | Adaptive Acc: 87.500% | clf_exit: 0.836 0.133 0.031
Batch: 20 | Loss: 1.205 | Acc: 76.674,90.290,93.229,% | Adaptive Acc: 84.561% | clf_exit: 0.807 0.164 0.029
Batch: 40 | Loss: 1.202 | Acc: 77.287,90.244,93.312,% | Adaptive Acc: 84.432% | clf_exit: 0.816 0.159 0.026
Batch: 60 | Loss: 1.195 | Acc: 77.344,90.190,93.276,% | Adaptive Acc: 84.695% | clf_exit: 0.813 0.161 0.026
Train all parameters

Epoch: 153
Batch: 0 | Loss: 0.654 | Acc: 81.250,98.438,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.766 0.219 0.016
Batch: 20 | Loss: 0.770 | Acc: 79.836,96.652,99.777,% | Adaptive Acc: 90.960% | clf_exit: 0.772 0.201 0.026
Batch: 40 | Loss: 0.780 | Acc: 79.707,96.494,99.657,% | Adaptive Acc: 90.568% | clf_exit: 0.784 0.191 0.025
Batch: 60 | Loss: 0.765 | Acc: 80.277,96.709,99.667,% | Adaptive Acc: 90.932% | clf_exit: 0.788 0.189 0.023
Batch: 80 | Loss: 0.772 | Acc: 80.170,96.576,99.633,% | Adaptive Acc: 90.702% | clf_exit: 0.787 0.190 0.022
Batch: 100 | Loss: 0.771 | Acc: 80.183,96.589,99.644,% | Adaptive Acc: 90.726% | clf_exit: 0.786 0.192 0.022
Batch: 120 | Loss: 0.775 | Acc: 79.901,96.649,99.638,% | Adaptive Acc: 90.444% | clf_exit: 0.786 0.193 0.021
Batch: 140 | Loss: 0.780 | Acc: 79.776,96.642,99.634,% | Adaptive Acc: 90.359% | clf_exit: 0.786 0.193 0.022
Batch: 160 | Loss: 0.780 | Acc: 79.731,96.661,99.655,% | Adaptive Acc: 90.368% | clf_exit: 0.786 0.192 0.022
Batch: 180 | Loss: 0.784 | Acc: 79.502,96.590,99.620,% | Adaptive Acc: 90.245% | clf_exit: 0.786 0.192 0.022
Batch: 200 | Loss: 0.784 | Acc: 79.544,96.572,99.615,% | Adaptive Acc: 90.194% | clf_exit: 0.787 0.192 0.022
Batch: 220 | Loss: 0.784 | Acc: 79.535,96.543,99.618,% | Adaptive Acc: 90.215% | clf_exit: 0.785 0.193 0.022
Batch: 240 | Loss: 0.785 | Acc: 79.532,96.541,99.598,% | Adaptive Acc: 90.165% | clf_exit: 0.785 0.193 0.022
Batch: 260 | Loss: 0.785 | Acc: 79.574,96.501,99.605,% | Adaptive Acc: 90.185% | clf_exit: 0.785 0.194 0.022
Batch: 280 | Loss: 0.786 | Acc: 79.557,96.450,99.600,% | Adaptive Acc: 90.163% | clf_exit: 0.785 0.193 0.021
Batch: 300 | Loss: 0.783 | Acc: 79.623,96.465,99.600,% | Adaptive Acc: 90.220% | clf_exit: 0.786 0.193 0.021
Batch: 320 | Loss: 0.784 | Acc: 79.610,96.456,99.596,% | Adaptive Acc: 90.194% | clf_exit: 0.785 0.194 0.021
Batch: 340 | Loss: 0.785 | Acc: 79.619,96.449,99.597,% | Adaptive Acc: 90.219% | clf_exit: 0.785 0.194 0.021
Batch: 360 | Loss: 0.784 | Acc: 79.597,96.455,99.593,% | Adaptive Acc: 90.203% | clf_exit: 0.785 0.194 0.021
Batch: 380 | Loss: 0.785 | Acc: 79.587,96.438,99.586,% | Adaptive Acc: 90.182% | clf_exit: 0.785 0.194 0.021
Batch: 0 | Loss: 0.988 | Acc: 78.906,92.969,94.531,% | Adaptive Acc: 85.938% | clf_exit: 0.828 0.133 0.039
Batch: 20 | Loss: 1.186 | Acc: 76.637,90.774,93.452,% | Adaptive Acc: 84.784% | clf_exit: 0.821 0.154 0.025
Batch: 40 | Loss: 1.175 | Acc: 77.172,90.625,93.559,% | Adaptive Acc: 84.489% | clf_exit: 0.830 0.146 0.024
Batch: 60 | Loss: 1.175 | Acc: 77.318,90.510,93.494,% | Adaptive Acc: 84.670% | clf_exit: 0.829 0.145 0.025
Train all parameters

Epoch: 154
Batch: 0 | Loss: 0.689 | Acc: 84.375,96.094,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.812 0.180 0.008
Batch: 20 | Loss: 0.805 | Acc: 78.460,96.503,99.479,% | Adaptive Acc: 89.509% | clf_exit: 0.788 0.196 0.017
Batch: 40 | Loss: 0.778 | Acc: 79.497,96.875,99.638,% | Adaptive Acc: 90.130% | clf_exit: 0.789 0.195 0.016
Batch: 60 | Loss: 0.776 | Acc: 79.649,96.901,99.667,% | Adaptive Acc: 90.113% | clf_exit: 0.791 0.192 0.017
Batch: 80 | Loss: 0.778 | Acc: 79.668,96.788,99.633,% | Adaptive Acc: 90.017% | clf_exit: 0.790 0.192 0.019
Batch: 100 | Loss: 0.775 | Acc: 79.633,96.720,99.629,% | Adaptive Acc: 90.006% | clf_exit: 0.788 0.193 0.019
Batch: 120 | Loss: 0.775 | Acc: 79.681,96.720,99.651,% | Adaptive Acc: 90.134% | clf_exit: 0.786 0.194 0.020
Batch: 140 | Loss: 0.770 | Acc: 79.743,96.770,99.662,% | Adaptive Acc: 90.182% | clf_exit: 0.787 0.193 0.020
Batch: 160 | Loss: 0.772 | Acc: 79.717,96.763,99.670,% | Adaptive Acc: 90.101% | clf_exit: 0.787 0.193 0.020
Batch: 180 | Loss: 0.772 | Acc: 79.713,96.746,99.694,% | Adaptive Acc: 90.129% | clf_exit: 0.788 0.192 0.020
Batch: 200 | Loss: 0.771 | Acc: 79.742,96.708,99.705,% | Adaptive Acc: 90.205% | clf_exit: 0.787 0.192 0.021
Batch: 220 | Loss: 0.773 | Acc: 79.702,96.691,99.692,% | Adaptive Acc: 90.190% | clf_exit: 0.787 0.193 0.021
Batch: 240 | Loss: 0.770 | Acc: 79.785,96.713,99.695,% | Adaptive Acc: 90.246% | clf_exit: 0.787 0.193 0.021
Batch: 260 | Loss: 0.771 | Acc: 79.720,96.713,99.686,% | Adaptive Acc: 90.221% | clf_exit: 0.787 0.192 0.021
Batch: 280 | Loss: 0.771 | Acc: 79.704,96.717,99.672,% | Adaptive Acc: 90.183% | clf_exit: 0.787 0.192 0.021
Batch: 300 | Loss: 0.773 | Acc: 79.599,96.709,99.668,% | Adaptive Acc: 90.124% | clf_exit: 0.787 0.192 0.020
Batch: 320 | Loss: 0.771 | Acc: 79.619,96.729,99.669,% | Adaptive Acc: 90.129% | clf_exit: 0.788 0.192 0.020
Batch: 340 | Loss: 0.773 | Acc: 79.527,96.701,99.668,% | Adaptive Acc: 90.061% | clf_exit: 0.787 0.193 0.020
Batch: 360 | Loss: 0.773 | Acc: 79.562,96.678,99.654,% | Adaptive Acc: 90.064% | clf_exit: 0.788 0.192 0.020
Batch: 380 | Loss: 0.772 | Acc: 79.577,96.676,99.660,% | Adaptive Acc: 90.090% | clf_exit: 0.788 0.192 0.020
Batch: 0 | Loss: 0.976 | Acc: 81.250,92.969,92.969,% | Adaptive Acc: 87.500% | clf_exit: 0.805 0.164 0.031
Batch: 20 | Loss: 1.165 | Acc: 77.604,90.513,93.490,% | Adaptive Acc: 85.268% | clf_exit: 0.810 0.164 0.026
Batch: 40 | Loss: 1.170 | Acc: 77.992,90.206,93.674,% | Adaptive Acc: 84.832% | clf_exit: 0.823 0.153 0.024
Batch: 60 | Loss: 1.166 | Acc: 78.215,90.087,93.443,% | Adaptive Acc: 85.015% | clf_exit: 0.821 0.155 0.024
Train all parameters

Epoch: 155
Batch: 0 | Loss: 0.916 | Acc: 75.781,96.875,99.219,% | Adaptive Acc: 89.062% | clf_exit: 0.773 0.180 0.047
Batch: 20 | Loss: 0.760 | Acc: 80.432,97.284,99.702,% | Adaptive Acc: 90.365% | clf_exit: 0.783 0.199 0.018
Batch: 40 | Loss: 0.750 | Acc: 80.412,97.428,99.695,% | Adaptive Acc: 90.492% | clf_exit: 0.789 0.194 0.016
Batch: 60 | Loss: 0.771 | Acc: 79.764,97.029,99.641,% | Adaptive Acc: 90.151% | clf_exit: 0.788 0.195 0.017
Batch: 80 | Loss: 0.771 | Acc: 79.601,96.923,99.653,% | Adaptive Acc: 89.853% | clf_exit: 0.791 0.190 0.018
Batch: 100 | Loss: 0.776 | Acc: 79.510,96.821,99.652,% | Adaptive Acc: 89.890% | clf_exit: 0.789 0.192 0.019
Batch: 120 | Loss: 0.776 | Acc: 79.410,96.817,99.638,% | Adaptive Acc: 89.928% | clf_exit: 0.788 0.193 0.019
Batch: 140 | Loss: 0.769 | Acc: 79.615,96.858,99.668,% | Adaptive Acc: 90.082% | clf_exit: 0.787 0.194 0.019
Batch: 160 | Loss: 0.766 | Acc: 79.586,96.846,99.665,% | Adaptive Acc: 90.086% | clf_exit: 0.789 0.192 0.019
Batch: 180 | Loss: 0.768 | Acc: 79.515,96.810,99.676,% | Adaptive Acc: 90.025% | clf_exit: 0.788 0.193 0.018
Batch: 200 | Loss: 0.767 | Acc: 79.672,96.778,99.677,% | Adaptive Acc: 90.034% | clf_exit: 0.788 0.193 0.018
Batch: 220 | Loss: 0.769 | Acc: 79.656,96.772,99.682,% | Adaptive Acc: 90.031% | clf_exit: 0.788 0.193 0.019
Batch: 240 | Loss: 0.767 | Acc: 79.723,96.781,99.682,% | Adaptive Acc: 90.097% | clf_exit: 0.789 0.192 0.019
Batch: 260 | Loss: 0.767 | Acc: 79.723,96.764,99.686,% | Adaptive Acc: 90.083% | clf_exit: 0.789 0.192 0.019
Batch: 280 | Loss: 0.765 | Acc: 79.768,96.803,99.689,% | Adaptive Acc: 90.111% | clf_exit: 0.789 0.192 0.019
Batch: 300 | Loss: 0.763 | Acc: 79.877,96.810,99.689,% | Adaptive Acc: 90.158% | clf_exit: 0.789 0.192 0.019
Batch: 320 | Loss: 0.765 | Acc: 79.848,96.800,99.693,% | Adaptive Acc: 90.131% | clf_exit: 0.789 0.192 0.019
Batch: 340 | Loss: 0.766 | Acc: 79.781,96.772,99.679,% | Adaptive Acc: 90.038% | clf_exit: 0.789 0.192 0.019
Batch: 360 | Loss: 0.766 | Acc: 79.768,96.795,99.665,% | Adaptive Acc: 90.049% | clf_exit: 0.789 0.192 0.019
Batch: 380 | Loss: 0.767 | Acc: 79.767,96.762,99.660,% | Adaptive Acc: 90.043% | clf_exit: 0.789 0.192 0.019
Batch: 0 | Loss: 1.092 | Acc: 82.031,91.406,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.805 0.164 0.031
Batch: 20 | Loss: 1.206 | Acc: 77.493,90.402,93.601,% | Adaptive Acc: 83.705% | clf_exit: 0.827 0.148 0.026
Batch: 40 | Loss: 1.207 | Acc: 77.477,90.149,93.464,% | Adaptive Acc: 83.537% | clf_exit: 0.832 0.144 0.024
Batch: 60 | Loss: 1.198 | Acc: 77.485,90.292,93.443,% | Adaptive Acc: 83.850% | clf_exit: 0.829 0.148 0.023
Train all parameters

Epoch: 156
Batch: 0 | Loss: 0.746 | Acc: 78.906,96.875,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.828 0.164 0.008
Batch: 20 | Loss: 0.735 | Acc: 80.208,96.615,99.554,% | Adaptive Acc: 90.216% | clf_exit: 0.802 0.182 0.015
Batch: 40 | Loss: 0.731 | Acc: 80.678,96.856,99.638,% | Adaptive Acc: 90.320% | clf_exit: 0.797 0.190 0.014
Batch: 60 | Loss: 0.726 | Acc: 80.610,97.029,99.667,% | Adaptive Acc: 90.561% | clf_exit: 0.796 0.189 0.015
Batch: 80 | Loss: 0.737 | Acc: 80.314,97.106,99.711,% | Adaptive Acc: 90.519% | clf_exit: 0.791 0.193 0.016
Batch: 100 | Loss: 0.741 | Acc: 80.121,97.138,99.714,% | Adaptive Acc: 90.493% | clf_exit: 0.788 0.195 0.017
Batch: 120 | Loss: 0.748 | Acc: 79.855,97.107,99.703,% | Adaptive Acc: 90.367% | clf_exit: 0.788 0.195 0.017
Batch: 140 | Loss: 0.747 | Acc: 79.904,97.097,99.673,% | Adaptive Acc: 90.448% | clf_exit: 0.788 0.195 0.017
Batch: 160 | Loss: 0.748 | Acc: 79.940,97.098,99.675,% | Adaptive Acc: 90.489% | clf_exit: 0.790 0.194 0.016
Batch: 180 | Loss: 0.755 | Acc: 79.791,96.996,99.681,% | Adaptive Acc: 90.336% | clf_exit: 0.789 0.194 0.017
Batch: 200 | Loss: 0.754 | Acc: 79.789,97.023,99.689,% | Adaptive Acc: 90.318% | clf_exit: 0.789 0.193 0.017
Batch: 220 | Loss: 0.756 | Acc: 79.723,96.970,99.678,% | Adaptive Acc: 90.286% | clf_exit: 0.790 0.193 0.017
Batch: 240 | Loss: 0.757 | Acc: 79.762,96.956,99.689,% | Adaptive Acc: 90.285% | clf_exit: 0.789 0.193 0.017
Batch: 260 | Loss: 0.756 | Acc: 79.807,96.974,99.701,% | Adaptive Acc: 90.296% | clf_exit: 0.790 0.193 0.017
Batch: 280 | Loss: 0.754 | Acc: 79.946,96.967,99.691,% | Adaptive Acc: 90.286% | clf_exit: 0.791 0.191 0.018
Batch: 300 | Loss: 0.753 | Acc: 80.030,96.994,99.696,% | Adaptive Acc: 90.363% | clf_exit: 0.792 0.191 0.018
Batch: 320 | Loss: 0.755 | Acc: 79.977,96.929,99.696,% | Adaptive Acc: 90.270% | clf_exit: 0.792 0.191 0.017
Batch: 340 | Loss: 0.756 | Acc: 79.967,96.893,99.684,% | Adaptive Acc: 90.295% | clf_exit: 0.791 0.191 0.018
Batch: 360 | Loss: 0.754 | Acc: 80.038,96.923,99.675,% | Adaptive Acc: 90.365% | clf_exit: 0.791 0.191 0.018
Batch: 380 | Loss: 0.757 | Acc: 79.977,96.889,99.674,% | Adaptive Acc: 90.322% | clf_exit: 0.791 0.191 0.018
Batch: 0 | Loss: 1.036 | Acc: 77.344,93.750,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.852 0.117 0.031
Batch: 20 | Loss: 1.226 | Acc: 76.600,90.476,92.932,% | Adaptive Acc: 83.296% | clf_exit: 0.837 0.141 0.022
Batch: 40 | Loss: 1.218 | Acc: 76.944,90.034,93.255,% | Adaptive Acc: 83.098% | clf_exit: 0.840 0.140 0.020
Batch: 60 | Loss: 1.207 | Acc: 77.600,90.087,93.302,% | Adaptive Acc: 83.312% | clf_exit: 0.841 0.138 0.020
Train all parameters

Epoch: 157
Batch: 0 | Loss: 0.764 | Acc: 78.125,95.312,100.000,% | Adaptive Acc: 85.156% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 0.756 | Acc: 79.911,96.949,99.665,% | Adaptive Acc: 89.695% | clf_exit: 0.797 0.185 0.019
Batch: 40 | Loss: 0.750 | Acc: 79.840,96.875,99.714,% | Adaptive Acc: 90.034% | clf_exit: 0.796 0.186 0.019
Batch: 60 | Loss: 0.752 | Acc: 79.841,96.888,99.744,% | Adaptive Acc: 89.985% | clf_exit: 0.796 0.185 0.019
Batch: 80 | Loss: 0.756 | Acc: 79.649,96.846,99.759,% | Adaptive Acc: 90.037% | clf_exit: 0.794 0.185 0.020
Batch: 100 | Loss: 0.753 | Acc: 79.633,96.914,99.776,% | Adaptive Acc: 90.138% | clf_exit: 0.793 0.187 0.020
Batch: 120 | Loss: 0.756 | Acc: 79.584,96.888,99.755,% | Adaptive Acc: 90.205% | clf_exit: 0.792 0.189 0.020
Batch: 140 | Loss: 0.763 | Acc: 79.516,96.842,99.740,% | Adaptive Acc: 90.154% | clf_exit: 0.790 0.191 0.019
Batch: 160 | Loss: 0.757 | Acc: 79.620,96.909,99.723,% | Adaptive Acc: 90.208% | clf_exit: 0.790 0.191 0.019
Batch: 180 | Loss: 0.758 | Acc: 79.580,96.892,99.732,% | Adaptive Acc: 90.241% | clf_exit: 0.789 0.192 0.019
Batch: 200 | Loss: 0.754 | Acc: 79.656,96.976,99.743,% | Adaptive Acc: 90.275% | clf_exit: 0.790 0.191 0.019
Batch: 220 | Loss: 0.752 | Acc: 79.758,96.995,99.745,% | Adaptive Acc: 90.310% | clf_exit: 0.791 0.191 0.018
Batch: 240 | Loss: 0.753 | Acc: 79.807,96.963,99.750,% | Adaptive Acc: 90.236% | clf_exit: 0.791 0.191 0.018
Batch: 260 | Loss: 0.752 | Acc: 79.852,96.962,99.761,% | Adaptive Acc: 90.293% | clf_exit: 0.791 0.191 0.018
Batch: 280 | Loss: 0.754 | Acc: 79.771,96.942,99.761,% | Adaptive Acc: 90.233% | clf_exit: 0.791 0.191 0.018
Batch: 300 | Loss: 0.752 | Acc: 79.885,96.966,99.756,% | Adaptive Acc: 90.275% | clf_exit: 0.791 0.191 0.018
Batch: 320 | Loss: 0.751 | Acc: 79.853,96.958,99.757,% | Adaptive Acc: 90.257% | clf_exit: 0.792 0.190 0.018
Batch: 340 | Loss: 0.750 | Acc: 79.907,96.960,99.753,% | Adaptive Acc: 90.336% | clf_exit: 0.792 0.190 0.018
Batch: 360 | Loss: 0.750 | Acc: 79.939,96.979,99.753,% | Adaptive Acc: 90.389% | clf_exit: 0.792 0.190 0.018
Batch: 380 | Loss: 0.753 | Acc: 79.849,96.920,99.727,% | Adaptive Acc: 90.346% | clf_exit: 0.791 0.190 0.018
Batch: 0 | Loss: 0.966 | Acc: 83.594,91.406,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.820 0.148 0.031
Batch: 20 | Loss: 1.172 | Acc: 77.641,90.439,93.341,% | Adaptive Acc: 84.524% | clf_exit: 0.814 0.162 0.024
Batch: 40 | Loss: 1.173 | Acc: 77.954,90.358,93.350,% | Adaptive Acc: 84.604% | clf_exit: 0.825 0.152 0.024
Batch: 60 | Loss: 1.172 | Acc: 77.907,90.254,93.340,% | Adaptive Acc: 84.631% | clf_exit: 0.826 0.151 0.023
Train all parameters

Epoch: 158
Batch: 0 | Loss: 0.697 | Acc: 84.375,97.656,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.758 0.227 0.016
Batch: 20 | Loss: 0.733 | Acc: 80.915,97.098,99.591,% | Adaptive Acc: 91.071% | clf_exit: 0.797 0.184 0.019
Batch: 40 | Loss: 0.748 | Acc: 80.393,97.046,99.714,% | Adaptive Acc: 90.968% | clf_exit: 0.790 0.194 0.017
Batch: 60 | Loss: 0.748 | Acc: 80.110,97.170,99.744,% | Adaptive Acc: 90.689% | clf_exit: 0.791 0.193 0.016
Batch: 80 | Loss: 0.749 | Acc: 80.044,97.203,99.720,% | Adaptive Acc: 90.731% | clf_exit: 0.786 0.198 0.016
Batch: 100 | Loss: 0.742 | Acc: 80.144,97.223,99.722,% | Adaptive Acc: 90.787% | clf_exit: 0.788 0.195 0.017
Batch: 120 | Loss: 0.746 | Acc: 79.926,97.095,99.716,% | Adaptive Acc: 90.586% | clf_exit: 0.788 0.196 0.017
Batch: 140 | Loss: 0.745 | Acc: 79.915,97.113,99.734,% | Adaptive Acc: 90.570% | clf_exit: 0.788 0.196 0.017
Batch: 160 | Loss: 0.747 | Acc: 80.003,97.118,99.728,% | Adaptive Acc: 90.620% | clf_exit: 0.786 0.197 0.016
Batch: 180 | Loss: 0.749 | Acc: 80.028,97.086,99.732,% | Adaptive Acc: 90.552% | clf_exit: 0.787 0.197 0.017
Batch: 200 | Loss: 0.748 | Acc: 80.173,97.097,99.740,% | Adaptive Acc: 90.574% | clf_exit: 0.788 0.195 0.017
Batch: 220 | Loss: 0.749 | Acc: 80.112,97.020,99.738,% | Adaptive Acc: 90.537% | clf_exit: 0.787 0.196 0.017
Batch: 240 | Loss: 0.749 | Acc: 80.080,97.044,99.731,% | Adaptive Acc: 90.528% | clf_exit: 0.788 0.195 0.017
Batch: 260 | Loss: 0.751 | Acc: 80.098,97.004,99.734,% | Adaptive Acc: 90.427% | clf_exit: 0.789 0.194 0.017
Batch: 280 | Loss: 0.751 | Acc: 80.130,96.953,99.736,% | Adaptive Acc: 90.403% | clf_exit: 0.789 0.194 0.017
Batch: 300 | Loss: 0.752 | Acc: 80.090,96.945,99.725,% | Adaptive Acc: 90.441% | clf_exit: 0.789 0.194 0.017
Batch: 320 | Loss: 0.751 | Acc: 80.101,96.916,99.732,% | Adaptive Acc: 90.382% | clf_exit: 0.791 0.192 0.017
Batch: 340 | Loss: 0.750 | Acc: 80.150,96.941,99.739,% | Adaptive Acc: 90.437% | clf_exit: 0.791 0.192 0.017
Batch: 360 | Loss: 0.750 | Acc: 80.133,96.936,99.736,% | Adaptive Acc: 90.435% | clf_exit: 0.791 0.192 0.017
Batch: 380 | Loss: 0.750 | Acc: 80.130,96.939,99.744,% | Adaptive Acc: 90.426% | clf_exit: 0.791 0.192 0.017
Batch: 0 | Loss: 1.007 | Acc: 80.469,92.188,93.750,% | Adaptive Acc: 86.719% | clf_exit: 0.852 0.109 0.039
Batch: 20 | Loss: 1.218 | Acc: 76.972,90.513,93.229,% | Adaptive Acc: 84.301% | clf_exit: 0.828 0.149 0.023
Batch: 40 | Loss: 1.208 | Acc: 77.496,90.282,93.121,% | Adaptive Acc: 84.318% | clf_exit: 0.833 0.143 0.024
Batch: 60 | Loss: 1.198 | Acc: 77.536,90.254,93.110,% | Adaptive Acc: 84.247% | clf_exit: 0.832 0.145 0.023
Train all parameters

Epoch: 159
Batch: 0 | Loss: 0.615 | Acc: 84.375,96.094,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.812 0.172 0.016
Batch: 20 | Loss: 0.716 | Acc: 81.324,97.173,99.851,% | Adaptive Acc: 90.811% | clf_exit: 0.797 0.187 0.016
Batch: 40 | Loss: 0.720 | Acc: 81.136,97.142,99.829,% | Adaptive Acc: 90.187% | clf_exit: 0.806 0.179 0.016
Batch: 60 | Loss: 0.724 | Acc: 80.968,97.182,99.795,% | Adaptive Acc: 90.459% | clf_exit: 0.801 0.183 0.016
Batch: 80 | Loss: 0.727 | Acc: 80.816,97.280,99.836,% | Adaptive Acc: 90.548% | clf_exit: 0.798 0.186 0.016
Batch: 100 | Loss: 0.727 | Acc: 80.848,97.339,99.791,% | Adaptive Acc: 90.586% | clf_exit: 0.797 0.187 0.015
Batch: 120 | Loss: 0.728 | Acc: 80.727,97.295,99.787,% | Adaptive Acc: 90.593% | clf_exit: 0.797 0.186 0.016
Batch: 140 | Loss: 0.732 | Acc: 80.552,97.235,99.762,% | Adaptive Acc: 90.514% | clf_exit: 0.797 0.187 0.016
Batch: 160 | Loss: 0.732 | Acc: 80.566,97.224,99.767,% | Adaptive Acc: 90.475% | clf_exit: 0.798 0.186 0.016
Batch: 180 | Loss: 0.732 | Acc: 80.568,97.229,99.758,% | Adaptive Acc: 90.556% | clf_exit: 0.797 0.186 0.016
Batch: 200 | Loss: 0.735 | Acc: 80.531,97.225,99.763,% | Adaptive Acc: 90.497% | clf_exit: 0.797 0.187 0.016
Batch: 220 | Loss: 0.736 | Acc: 80.440,97.186,99.760,% | Adaptive Acc: 90.452% | clf_exit: 0.796 0.188 0.016
Batch: 240 | Loss: 0.738 | Acc: 80.333,97.202,99.747,% | Adaptive Acc: 90.411% | clf_exit: 0.796 0.187 0.017
Batch: 260 | Loss: 0.740 | Acc: 80.220,97.186,99.728,% | Adaptive Acc: 90.401% | clf_exit: 0.795 0.188 0.017
Batch: 280 | Loss: 0.741 | Acc: 80.232,97.184,99.733,% | Adaptive Acc: 90.450% | clf_exit: 0.794 0.190 0.017
Batch: 300 | Loss: 0.743 | Acc: 80.121,97.192,99.733,% | Adaptive Acc: 90.394% | clf_exit: 0.794 0.189 0.017
Batch: 320 | Loss: 0.743 | Acc: 80.099,97.208,99.737,% | Adaptive Acc: 90.394% | clf_exit: 0.794 0.189 0.017
Batch: 340 | Loss: 0.743 | Acc: 80.104,97.203,99.739,% | Adaptive Acc: 90.389% | clf_exit: 0.794 0.189 0.017
Batch: 360 | Loss: 0.742 | Acc: 80.172,97.176,99.734,% | Adaptive Acc: 90.426% | clf_exit: 0.794 0.189 0.017
Batch: 380 | Loss: 0.743 | Acc: 80.235,97.129,99.738,% | Adaptive Acc: 90.445% | clf_exit: 0.794 0.189 0.017
Batch: 0 | Loss: 0.959 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 87.500% | clf_exit: 0.844 0.125 0.031
Batch: 20 | Loss: 1.188 | Acc: 78.497,90.179,93.229,% | Adaptive Acc: 85.045% | clf_exit: 0.820 0.161 0.019
Batch: 40 | Loss: 1.178 | Acc: 78.735,90.396,93.159,% | Adaptive Acc: 84.775% | clf_exit: 0.825 0.153 0.021
Batch: 60 | Loss: 1.169 | Acc: 78.906,90.420,93.225,% | Adaptive Acc: 85.092% | clf_exit: 0.822 0.155 0.022
Train all parameters

Epoch: 160
Batch: 0 | Loss: 0.664 | Acc: 82.031,99.219,99.219,% | Adaptive Acc: 91.406% | clf_exit: 0.797 0.203 0.000
Batch: 20 | Loss: 0.718 | Acc: 81.324,97.582,99.740,% | Adaptive Acc: 91.034% | clf_exit: 0.794 0.190 0.016
Batch: 40 | Loss: 0.731 | Acc: 80.373,97.389,99.733,% | Adaptive Acc: 90.549% | clf_exit: 0.791 0.193 0.015
Batch: 60 | Loss: 0.717 | Acc: 80.520,97.618,99.808,% | Adaptive Acc: 90.753% | clf_exit: 0.794 0.191 0.015
Batch: 80 | Loss: 0.720 | Acc: 80.498,97.560,99.788,% | Adaptive Acc: 90.702% | clf_exit: 0.795 0.191 0.015
Batch: 100 | Loss: 0.716 | Acc: 80.523,97.556,99.783,% | Adaptive Acc: 90.563% | clf_exit: 0.798 0.188 0.014
Batch: 120 | Loss: 0.718 | Acc: 80.604,97.482,99.787,% | Adaptive Acc: 90.670% | clf_exit: 0.796 0.189 0.015
Batch: 140 | Loss: 0.723 | Acc: 80.568,97.451,99.778,% | Adaptive Acc: 90.586% | clf_exit: 0.796 0.189 0.015
Batch: 160 | Loss: 0.734 | Acc: 80.265,97.273,99.762,% | Adaptive Acc: 90.460% | clf_exit: 0.794 0.191 0.016
Batch: 180 | Loss: 0.734 | Acc: 80.266,97.242,99.763,% | Adaptive Acc: 90.483% | clf_exit: 0.794 0.190 0.016
Batch: 200 | Loss: 0.736 | Acc: 80.208,97.198,99.775,% | Adaptive Acc: 90.407% | clf_exit: 0.794 0.190 0.016
Batch: 220 | Loss: 0.739 | Acc: 80.165,97.175,99.781,% | Adaptive Acc: 90.378% | clf_exit: 0.793 0.190 0.016
Batch: 240 | Loss: 0.741 | Acc: 80.125,97.154,99.773,% | Adaptive Acc: 90.349% | clf_exit: 0.793 0.190 0.017
Batch: 260 | Loss: 0.742 | Acc: 80.104,97.070,99.764,% | Adaptive Acc: 90.272% | clf_exit: 0.793 0.189 0.017
Batch: 280 | Loss: 0.743 | Acc: 80.118,97.070,99.766,% | Adaptive Acc: 90.225% | clf_exit: 0.794 0.189 0.017
Batch: 300 | Loss: 0.742 | Acc: 80.157,97.064,99.772,% | Adaptive Acc: 90.256% | clf_exit: 0.794 0.189 0.017
Batch: 320 | Loss: 0.742 | Acc: 80.182,97.070,99.776,% | Adaptive Acc: 90.267% | clf_exit: 0.794 0.189 0.017
Batch: 340 | Loss: 0.741 | Acc: 80.242,97.049,99.773,% | Adaptive Acc: 90.325% | clf_exit: 0.793 0.189 0.017
Batch: 360 | Loss: 0.741 | Acc: 80.209,97.065,99.771,% | Adaptive Acc: 90.296% | clf_exit: 0.794 0.189 0.017
Batch: 380 | Loss: 0.741 | Acc: 80.182,97.053,99.766,% | Adaptive Acc: 90.307% | clf_exit: 0.794 0.189 0.017
Batch: 0 | Loss: 1.002 | Acc: 79.688,90.625,93.750,% | Adaptive Acc: 87.500% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.222 | Acc: 77.158,90.141,93.452,% | Adaptive Acc: 83.296% | clf_exit: 0.853 0.126 0.020
Batch: 40 | Loss: 1.211 | Acc: 76.963,90.091,93.407,% | Adaptive Acc: 83.441% | clf_exit: 0.850 0.128 0.022
Batch: 60 | Loss: 1.197 | Acc: 77.139,90.202,93.507,% | Adaptive Acc: 83.478% | clf_exit: 0.850 0.128 0.022
Train all parameters

Epoch: 161
Batch: 0 | Loss: 0.792 | Acc: 78.125,98.438,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.758 0.203 0.039
Batch: 20 | Loss: 0.704 | Acc: 81.324,97.768,99.888,% | Adaptive Acc: 90.662% | clf_exit: 0.804 0.178 0.018
Batch: 40 | Loss: 0.714 | Acc: 81.117,97.694,99.829,% | Adaptive Acc: 90.587% | clf_exit: 0.802 0.181 0.017
Batch: 60 | Loss: 0.716 | Acc: 81.096,97.605,99.846,% | Adaptive Acc: 90.638% | clf_exit: 0.803 0.180 0.017
Batch: 80 | Loss: 0.711 | Acc: 81.086,97.569,99.836,% | Adaptive Acc: 90.683% | clf_exit: 0.804 0.179 0.017
Batch: 100 | Loss: 0.713 | Acc: 81.002,97.610,99.822,% | Adaptive Acc: 90.787% | clf_exit: 0.800 0.183 0.017
Batch: 120 | Loss: 0.723 | Acc: 80.695,97.385,99.806,% | Adaptive Acc: 90.567% | clf_exit: 0.799 0.184 0.017
Batch: 140 | Loss: 0.723 | Acc: 80.591,97.363,99.812,% | Adaptive Acc: 90.703% | clf_exit: 0.798 0.185 0.017
Batch: 160 | Loss: 0.727 | Acc: 80.537,97.375,99.820,% | Adaptive Acc: 90.581% | clf_exit: 0.799 0.184 0.017
Batch: 180 | Loss: 0.724 | Acc: 80.568,97.410,99.823,% | Adaptive Acc: 90.616% | clf_exit: 0.800 0.183 0.017
Batch: 200 | Loss: 0.725 | Acc: 80.504,97.442,99.829,% | Adaptive Acc: 90.617% | clf_exit: 0.800 0.184 0.017
Batch: 220 | Loss: 0.728 | Acc: 80.419,97.373,99.820,% | Adaptive Acc: 90.547% | clf_exit: 0.799 0.184 0.017
Batch: 240 | Loss: 0.726 | Acc: 80.511,97.339,99.828,% | Adaptive Acc: 90.528% | clf_exit: 0.800 0.184 0.017
Batch: 260 | Loss: 0.726 | Acc: 80.457,97.345,99.823,% | Adaptive Acc: 90.451% | clf_exit: 0.800 0.183 0.016
Batch: 280 | Loss: 0.729 | Acc: 80.352,97.278,99.822,% | Adaptive Acc: 90.378% | clf_exit: 0.799 0.184 0.017
Batch: 300 | Loss: 0.729 | Acc: 80.329,97.306,99.816,% | Adaptive Acc: 90.420% | clf_exit: 0.798 0.186 0.017
Batch: 320 | Loss: 0.729 | Acc: 80.301,97.316,99.808,% | Adaptive Acc: 90.460% | clf_exit: 0.797 0.187 0.016
Batch: 340 | Loss: 0.730 | Acc: 80.233,97.306,99.814,% | Adaptive Acc: 90.405% | clf_exit: 0.797 0.186 0.016
Batch: 360 | Loss: 0.731 | Acc: 80.233,97.308,99.807,% | Adaptive Acc: 90.389% | clf_exit: 0.797 0.186 0.017
Batch: 380 | Loss: 0.730 | Acc: 80.270,97.295,99.805,% | Adaptive Acc: 90.404% | clf_exit: 0.797 0.186 0.017
Batch: 0 | Loss: 1.050 | Acc: 79.688,90.625,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.828 0.141 0.031
Batch: 20 | Loss: 1.240 | Acc: 76.674,89.844,93.415,% | Adaptive Acc: 83.519% | clf_exit: 0.834 0.142 0.024
Batch: 40 | Loss: 1.224 | Acc: 77.382,89.977,93.369,% | Adaptive Acc: 84.051% | clf_exit: 0.835 0.138 0.026
Batch: 60 | Loss: 1.207 | Acc: 77.690,89.959,93.404,% | Adaptive Acc: 84.401% | clf_exit: 0.834 0.141 0.025
Train all parameters

Epoch: 162
Batch: 0 | Loss: 0.839 | Acc: 78.906,96.875,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.789 0.195 0.016
Batch: 20 | Loss: 0.702 | Acc: 82.031,97.656,99.591,% | Adaptive Acc: 91.220% | clf_exit: 0.810 0.176 0.013
Batch: 40 | Loss: 0.701 | Acc: 81.155,97.542,99.771,% | Adaptive Acc: 90.816% | clf_exit: 0.808 0.176 0.016
Batch: 60 | Loss: 0.710 | Acc: 80.853,97.400,99.731,% | Adaptive Acc: 90.753% | clf_exit: 0.803 0.181 0.016
Batch: 80 | Loss: 0.720 | Acc: 80.565,97.396,99.730,% | Adaptive Acc: 90.577% | clf_exit: 0.802 0.183 0.015
Batch: 100 | Loss: 0.731 | Acc: 80.337,97.293,99.714,% | Adaptive Acc: 90.610% | clf_exit: 0.797 0.187 0.016
Batch: 120 | Loss: 0.726 | Acc: 80.533,97.346,99.729,% | Adaptive Acc: 90.767% | clf_exit: 0.797 0.187 0.016
Batch: 140 | Loss: 0.724 | Acc: 80.718,97.390,99.745,% | Adaptive Acc: 90.902% | clf_exit: 0.797 0.187 0.016
Batch: 160 | Loss: 0.726 | Acc: 80.658,97.375,99.753,% | Adaptive Acc: 90.780% | clf_exit: 0.798 0.186 0.016
Batch: 180 | Loss: 0.730 | Acc: 80.559,97.337,99.737,% | Adaptive Acc: 90.798% | clf_exit: 0.796 0.188 0.017
Batch: 200 | Loss: 0.730 | Acc: 80.605,97.310,99.736,% | Adaptive Acc: 90.823% | clf_exit: 0.796 0.188 0.017
Batch: 220 | Loss: 0.729 | Acc: 80.589,97.313,99.742,% | Adaptive Acc: 90.834% | clf_exit: 0.796 0.187 0.017
Batch: 240 | Loss: 0.728 | Acc: 80.628,97.300,99.741,% | Adaptive Acc: 90.764% | clf_exit: 0.797 0.187 0.016
Batch: 260 | Loss: 0.730 | Acc: 80.568,97.288,99.746,% | Adaptive Acc: 90.739% | clf_exit: 0.797 0.187 0.016
Batch: 280 | Loss: 0.729 | Acc: 80.533,97.292,99.747,% | Adaptive Acc: 90.756% | clf_exit: 0.797 0.186 0.016
Batch: 300 | Loss: 0.730 | Acc: 80.510,97.314,99.751,% | Adaptive Acc: 90.742% | clf_exit: 0.797 0.187 0.017
Batch: 320 | Loss: 0.732 | Acc: 80.469,97.289,99.740,% | Adaptive Acc: 90.705% | clf_exit: 0.796 0.187 0.017
Batch: 340 | Loss: 0.731 | Acc: 80.512,97.303,99.743,% | Adaptive Acc: 90.707% | clf_exit: 0.796 0.187 0.017
Batch: 360 | Loss: 0.733 | Acc: 80.428,97.241,99.725,% | Adaptive Acc: 90.670% | clf_exit: 0.796 0.187 0.017
Batch: 380 | Loss: 0.734 | Acc: 80.405,97.236,99.723,% | Adaptive Acc: 90.596% | clf_exit: 0.796 0.187 0.017
Batch: 0 | Loss: 1.194 | Acc: 75.781,89.062,93.750,% | Adaptive Acc: 84.375% | clf_exit: 0.812 0.172 0.016
Batch: 20 | Loss: 1.241 | Acc: 77.121,90.104,93.006,% | Adaptive Acc: 84.524% | clf_exit: 0.812 0.166 0.021
Batch: 40 | Loss: 1.241 | Acc: 77.115,89.863,93.083,% | Adaptive Acc: 84.242% | clf_exit: 0.821 0.160 0.019
Batch: 60 | Loss: 1.216 | Acc: 77.203,89.921,93.251,% | Adaptive Acc: 84.234% | clf_exit: 0.822 0.158 0.019
Train all parameters

Epoch: 163
Batch: 0 | Loss: 0.708 | Acc: 82.031,97.656,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.727 0.258 0.016
Batch: 20 | Loss: 0.720 | Acc: 81.213,97.842,99.814,% | Adaptive Acc: 91.146% | clf_exit: 0.797 0.190 0.013
Batch: 40 | Loss: 0.732 | Acc: 80.774,97.409,99.867,% | Adaptive Acc: 90.739% | clf_exit: 0.794 0.191 0.014
Batch: 60 | Loss: 0.718 | Acc: 80.853,97.605,99.859,% | Adaptive Acc: 90.907% | clf_exit: 0.796 0.189 0.014
Batch: 80 | Loss: 0.723 | Acc: 80.710,97.386,99.855,% | Adaptive Acc: 90.799% | clf_exit: 0.794 0.190 0.015
Batch: 100 | Loss: 0.722 | Acc: 80.678,97.362,99.838,% | Adaptive Acc: 90.718% | clf_exit: 0.796 0.188 0.017
Batch: 120 | Loss: 0.719 | Acc: 80.856,97.353,99.813,% | Adaptive Acc: 90.728% | clf_exit: 0.797 0.187 0.017
Batch: 140 | Loss: 0.719 | Acc: 80.701,97.357,99.834,% | Adaptive Acc: 90.714% | clf_exit: 0.796 0.188 0.016
Batch: 160 | Loss: 0.723 | Acc: 80.624,97.331,99.820,% | Adaptive Acc: 90.693% | clf_exit: 0.795 0.189 0.015
Batch: 180 | Loss: 0.725 | Acc: 80.533,97.320,99.819,% | Adaptive Acc: 90.685% | clf_exit: 0.795 0.189 0.016
Batch: 200 | Loss: 0.725 | Acc: 80.531,97.338,99.817,% | Adaptive Acc: 90.703% | clf_exit: 0.795 0.190 0.016
Batch: 220 | Loss: 0.727 | Acc: 80.440,97.331,99.813,% | Adaptive Acc: 90.643% | clf_exit: 0.795 0.190 0.015
Batch: 240 | Loss: 0.729 | Acc: 80.417,97.293,99.809,% | Adaptive Acc: 90.596% | clf_exit: 0.795 0.190 0.015
Batch: 260 | Loss: 0.729 | Acc: 80.427,97.276,99.805,% | Adaptive Acc: 90.535% | clf_exit: 0.796 0.189 0.015
Batch: 280 | Loss: 0.728 | Acc: 80.427,97.278,99.789,% | Adaptive Acc: 90.547% | clf_exit: 0.796 0.189 0.015
Batch: 300 | Loss: 0.727 | Acc: 80.458,97.308,99.795,% | Adaptive Acc: 90.537% | clf_exit: 0.797 0.188 0.015
Batch: 320 | Loss: 0.727 | Acc: 80.566,97.277,99.793,% | Adaptive Acc: 90.571% | clf_exit: 0.798 0.187 0.015
Batch: 340 | Loss: 0.725 | Acc: 80.634,97.260,99.798,% | Adaptive Acc: 90.602% | clf_exit: 0.798 0.187 0.015
Batch: 360 | Loss: 0.725 | Acc: 80.601,97.232,99.801,% | Adaptive Acc: 90.580% | clf_exit: 0.797 0.187 0.016
Batch: 380 | Loss: 0.727 | Acc: 80.567,97.222,99.801,% | Adaptive Acc: 90.578% | clf_exit: 0.797 0.187 0.016
Batch: 0 | Loss: 0.990 | Acc: 82.031,93.750,96.094,% | Adaptive Acc: 89.844% | clf_exit: 0.852 0.102 0.047
Batch: 20 | Loss: 1.189 | Acc: 77.790,90.662,93.155,% | Adaptive Acc: 84.524% | clf_exit: 0.824 0.153 0.024
Batch: 40 | Loss: 1.176 | Acc: 78.392,90.225,93.407,% | Adaptive Acc: 84.566% | clf_exit: 0.834 0.142 0.024
Batch: 60 | Loss: 1.158 | Acc: 78.599,90.318,93.481,% | Adaptive Acc: 84.900% | clf_exit: 0.832 0.145 0.023
Train all parameters

Epoch: 164
Batch: 0 | Loss: 0.867 | Acc: 75.781,96.875,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.797 0.203 0.000
Batch: 20 | Loss: 0.723 | Acc: 80.915,97.396,99.665,% | Adaptive Acc: 90.365% | clf_exit: 0.810 0.178 0.012
Batch: 40 | Loss: 0.726 | Acc: 80.793,97.504,99.771,% | Adaptive Acc: 90.777% | clf_exit: 0.799 0.187 0.015
Batch: 60 | Loss: 0.728 | Acc: 80.418,97.464,99.782,% | Adaptive Acc: 90.548% | clf_exit: 0.799 0.186 0.015
Batch: 80 | Loss: 0.721 | Acc: 80.469,97.598,99.749,% | Adaptive Acc: 90.721% | clf_exit: 0.797 0.188 0.015
Batch: 100 | Loss: 0.719 | Acc: 80.523,97.563,99.768,% | Adaptive Acc: 90.617% | clf_exit: 0.800 0.185 0.015
Batch: 120 | Loss: 0.724 | Acc: 80.327,97.566,99.761,% | Adaptive Acc: 90.586% | clf_exit: 0.797 0.188 0.015
Batch: 140 | Loss: 0.726 | Acc: 80.258,97.595,99.767,% | Adaptive Acc: 90.564% | clf_exit: 0.796 0.189 0.015
Batch: 160 | Loss: 0.728 | Acc: 80.202,97.549,99.777,% | Adaptive Acc: 90.644% | clf_exit: 0.794 0.191 0.015
Batch: 180 | Loss: 0.728 | Acc: 80.136,97.540,99.784,% | Adaptive Acc: 90.655% | clf_exit: 0.795 0.190 0.015
Batch: 200 | Loss: 0.727 | Acc: 80.306,97.551,99.790,% | Adaptive Acc: 90.637% | clf_exit: 0.796 0.189 0.015
Batch: 220 | Loss: 0.721 | Acc: 80.518,97.575,99.791,% | Adaptive Acc: 90.738% | clf_exit: 0.797 0.188 0.015
Batch: 240 | Loss: 0.719 | Acc: 80.621,97.559,99.796,% | Adaptive Acc: 90.849% | clf_exit: 0.797 0.188 0.015
Batch: 260 | Loss: 0.722 | Acc: 80.615,97.495,99.790,% | Adaptive Acc: 90.784% | clf_exit: 0.796 0.189 0.015
Batch: 280 | Loss: 0.723 | Acc: 80.566,97.484,99.789,% | Adaptive Acc: 90.803% | clf_exit: 0.796 0.189 0.015
Batch: 300 | Loss: 0.723 | Acc: 80.617,97.441,99.790,% | Adaptive Acc: 90.838% | clf_exit: 0.795 0.190 0.015
Batch: 320 | Loss: 0.723 | Acc: 80.600,97.430,99.793,% | Adaptive Acc: 90.861% | clf_exit: 0.795 0.190 0.015
Batch: 340 | Loss: 0.722 | Acc: 80.611,97.411,99.798,% | Adaptive Acc: 90.852% | clf_exit: 0.795 0.190 0.015
Batch: 360 | Loss: 0.721 | Acc: 80.648,97.392,99.805,% | Adaptive Acc: 90.876% | clf_exit: 0.795 0.189 0.015
Batch: 380 | Loss: 0.721 | Acc: 80.631,97.373,99.807,% | Adaptive Acc: 90.818% | clf_exit: 0.796 0.188 0.015
Batch: 0 | Loss: 1.078 | Acc: 82.031,89.844,92.969,% | Adaptive Acc: 85.156% | clf_exit: 0.836 0.141 0.023
Batch: 20 | Loss: 1.169 | Acc: 78.162,90.402,93.490,% | Adaptive Acc: 84.784% | clf_exit: 0.833 0.143 0.024
Batch: 40 | Loss: 1.174 | Acc: 78.525,90.282,93.674,% | Adaptive Acc: 85.080% | clf_exit: 0.835 0.141 0.024
Batch: 60 | Loss: 1.164 | Acc: 78.689,90.446,93.532,% | Adaptive Acc: 85.323% | clf_exit: 0.835 0.142 0.023
Train all parameters

Epoch: 165
Batch: 0 | Loss: 0.834 | Acc: 79.688,96.094,99.219,% | Adaptive Acc: 92.188% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 0.690 | Acc: 80.432,97.917,99.888,% | Adaptive Acc: 90.625% | clf_exit: 0.803 0.183 0.014
Batch: 40 | Loss: 0.713 | Acc: 80.069,97.542,99.886,% | Adaptive Acc: 90.796% | clf_exit: 0.796 0.186 0.018
Batch: 60 | Loss: 0.709 | Acc: 80.379,97.477,99.834,% | Adaptive Acc: 90.996% | clf_exit: 0.795 0.187 0.018
Batch: 80 | Loss: 0.711 | Acc: 80.208,97.560,99.846,% | Adaptive Acc: 90.779% | clf_exit: 0.794 0.190 0.016
Batch: 100 | Loss: 0.710 | Acc: 80.345,97.556,99.853,% | Adaptive Acc: 90.873% | clf_exit: 0.796 0.188 0.016
Batch: 120 | Loss: 0.701 | Acc: 80.772,97.605,99.858,% | Adaptive Acc: 91.019% | clf_exit: 0.799 0.186 0.016
Batch: 140 | Loss: 0.702 | Acc: 80.807,97.568,99.861,% | Adaptive Acc: 90.952% | clf_exit: 0.800 0.185 0.016
Batch: 160 | Loss: 0.708 | Acc: 80.736,97.477,99.864,% | Adaptive Acc: 90.892% | clf_exit: 0.798 0.186 0.016
Batch: 180 | Loss: 0.710 | Acc: 80.792,97.471,99.858,% | Adaptive Acc: 90.845% | clf_exit: 0.799 0.185 0.016
Batch: 200 | Loss: 0.712 | Acc: 80.718,97.477,99.852,% | Adaptive Acc: 90.819% | clf_exit: 0.798 0.186 0.016
Batch: 220 | Loss: 0.714 | Acc: 80.635,97.472,99.848,% | Adaptive Acc: 90.788% | clf_exit: 0.797 0.187 0.016
Batch: 240 | Loss: 0.714 | Acc: 80.654,97.442,99.848,% | Adaptive Acc: 90.725% | clf_exit: 0.798 0.186 0.016
Batch: 260 | Loss: 0.716 | Acc: 80.609,97.453,99.844,% | Adaptive Acc: 90.733% | clf_exit: 0.797 0.187 0.016
Batch: 280 | Loss: 0.715 | Acc: 80.627,97.456,99.842,% | Adaptive Acc: 90.750% | clf_exit: 0.798 0.186 0.016
Batch: 300 | Loss: 0.719 | Acc: 80.565,97.438,99.836,% | Adaptive Acc: 90.716% | clf_exit: 0.797 0.186 0.016
Batch: 320 | Loss: 0.718 | Acc: 80.598,97.454,99.832,% | Adaptive Acc: 90.739% | clf_exit: 0.798 0.186 0.016
Batch: 340 | Loss: 0.719 | Acc: 80.560,97.448,99.830,% | Adaptive Acc: 90.698% | clf_exit: 0.798 0.186 0.016
Batch: 360 | Loss: 0.721 | Acc: 80.542,97.427,99.820,% | Adaptive Acc: 90.679% | clf_exit: 0.797 0.187 0.016
Batch: 380 | Loss: 0.721 | Acc: 80.528,97.437,99.824,% | Adaptive Acc: 90.693% | clf_exit: 0.797 0.186 0.016
Batch: 0 | Loss: 1.117 | Acc: 75.000,90.625,94.531,% | Adaptive Acc: 82.812% | clf_exit: 0.859 0.125 0.016
Batch: 20 | Loss: 1.223 | Acc: 77.009,89.955,93.341,% | Adaptive Acc: 83.557% | clf_exit: 0.829 0.147 0.025
Batch: 40 | Loss: 1.224 | Acc: 77.287,90.053,93.407,% | Adaptive Acc: 83.822% | clf_exit: 0.834 0.143 0.023
Batch: 60 | Loss: 1.213 | Acc: 77.267,90.215,93.519,% | Adaptive Acc: 83.824% | clf_exit: 0.834 0.144 0.022
Train all parameters

Epoch: 166
Batch: 0 | Loss: 0.627 | Acc: 80.469,99.219,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.758 0.242 0.000
Batch: 20 | Loss: 0.706 | Acc: 80.580,98.177,99.851,% | Adaptive Acc: 91.555% | clf_exit: 0.786 0.200 0.014
Batch: 40 | Loss: 0.704 | Acc: 80.869,98.037,99.809,% | Adaptive Acc: 91.216% | clf_exit: 0.797 0.190 0.014
Batch: 60 | Loss: 0.700 | Acc: 80.879,97.912,99.808,% | Adaptive Acc: 91.393% | clf_exit: 0.796 0.189 0.015
Batch: 80 | Loss: 0.701 | Acc: 80.874,97.975,99.817,% | Adaptive Acc: 91.098% | clf_exit: 0.798 0.188 0.015
Batch: 100 | Loss: 0.707 | Acc: 80.593,97.857,99.814,% | Adaptive Acc: 90.803% | clf_exit: 0.799 0.186 0.015
Batch: 120 | Loss: 0.706 | Acc: 80.662,97.876,99.819,% | Adaptive Acc: 90.954% | clf_exit: 0.797 0.188 0.015
Batch: 140 | Loss: 0.709 | Acc: 80.696,97.784,99.828,% | Adaptive Acc: 90.841% | clf_exit: 0.798 0.187 0.015
Batch: 160 | Loss: 0.711 | Acc: 80.702,97.724,99.835,% | Adaptive Acc: 90.746% | clf_exit: 0.800 0.185 0.015
Batch: 180 | Loss: 0.711 | Acc: 80.723,97.661,99.823,% | Adaptive Acc: 90.651% | clf_exit: 0.801 0.184 0.015
Batch: 200 | Loss: 0.713 | Acc: 80.655,97.637,99.817,% | Adaptive Acc: 90.582% | clf_exit: 0.802 0.183 0.015
Batch: 220 | Loss: 0.714 | Acc: 80.674,97.582,99.820,% | Adaptive Acc: 90.607% | clf_exit: 0.800 0.185 0.015
Batch: 240 | Loss: 0.715 | Acc: 80.699,97.546,99.812,% | Adaptive Acc: 90.583% | clf_exit: 0.800 0.185 0.015
Batch: 260 | Loss: 0.714 | Acc: 80.759,97.545,99.808,% | Adaptive Acc: 90.601% | clf_exit: 0.801 0.184 0.015
Batch: 280 | Loss: 0.713 | Acc: 80.738,97.553,99.800,% | Adaptive Acc: 90.564% | clf_exit: 0.801 0.184 0.015
Batch: 300 | Loss: 0.715 | Acc: 80.723,97.542,99.811,% | Adaptive Acc: 90.583% | clf_exit: 0.801 0.184 0.015
Batch: 320 | Loss: 0.716 | Acc: 80.671,97.532,99.813,% | Adaptive Acc: 90.557% | clf_exit: 0.800 0.184 0.015
Batch: 340 | Loss: 0.717 | Acc: 80.645,97.530,99.810,% | Adaptive Acc: 90.540% | clf_exit: 0.800 0.184 0.015
Batch: 360 | Loss: 0.719 | Acc: 80.605,97.477,99.803,% | Adaptive Acc: 90.515% | clf_exit: 0.800 0.185 0.015
Batch: 380 | Loss: 0.719 | Acc: 80.629,97.494,99.793,% | Adaptive Acc: 90.527% | clf_exit: 0.800 0.185 0.015
Batch: 0 | Loss: 1.063 | Acc: 78.125,93.750,94.531,% | Adaptive Acc: 86.719% | clf_exit: 0.844 0.109 0.047
Batch: 20 | Loss: 1.214 | Acc: 77.121,90.067,93.266,% | Adaptive Acc: 84.784% | clf_exit: 0.820 0.160 0.021
Batch: 40 | Loss: 1.212 | Acc: 77.820,89.920,93.159,% | Adaptive Acc: 84.566% | clf_exit: 0.831 0.147 0.022
Batch: 60 | Loss: 1.202 | Acc: 78.087,89.908,93.174,% | Adaptive Acc: 84.580% | clf_exit: 0.831 0.147 0.022
Train all parameters

Epoch: 167
Batch: 0 | Loss: 0.655 | Acc: 81.250,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.773 0.219 0.008
Batch: 20 | Loss: 0.756 | Acc: 78.757,97.545,99.814,% | Adaptive Acc: 90.253% | clf_exit: 0.785 0.196 0.019
Batch: 40 | Loss: 0.738 | Acc: 79.421,97.504,99.829,% | Adaptive Acc: 90.396% | clf_exit: 0.789 0.194 0.017
Batch: 60 | Loss: 0.729 | Acc: 80.072,97.503,99.859,% | Adaptive Acc: 90.894% | clf_exit: 0.789 0.194 0.017
Batch: 80 | Loss: 0.727 | Acc: 80.131,97.463,99.836,% | Adaptive Acc: 90.673% | clf_exit: 0.792 0.192 0.016
Batch: 100 | Loss: 0.721 | Acc: 80.252,97.502,99.830,% | Adaptive Acc: 90.749% | clf_exit: 0.795 0.190 0.015
Batch: 120 | Loss: 0.724 | Acc: 80.210,97.559,99.826,% | Adaptive Acc: 90.741% | clf_exit: 0.795 0.190 0.015
Batch: 140 | Loss: 0.720 | Acc: 80.286,97.584,99.839,% | Adaptive Acc: 90.858% | clf_exit: 0.796 0.189 0.015
Batch: 160 | Loss: 0.718 | Acc: 80.284,97.593,99.820,% | Adaptive Acc: 90.829% | clf_exit: 0.796 0.189 0.015
Batch: 180 | Loss: 0.722 | Acc: 80.240,97.522,99.810,% | Adaptive Acc: 90.776% | clf_exit: 0.796 0.189 0.015
Batch: 200 | Loss: 0.718 | Acc: 80.360,97.528,99.817,% | Adaptive Acc: 90.742% | clf_exit: 0.798 0.187 0.015
Batch: 220 | Loss: 0.719 | Acc: 80.341,97.515,99.806,% | Adaptive Acc: 90.770% | clf_exit: 0.797 0.188 0.015
Batch: 240 | Loss: 0.720 | Acc: 80.310,97.510,99.802,% | Adaptive Acc: 90.729% | clf_exit: 0.797 0.188 0.015
Batch: 260 | Loss: 0.720 | Acc: 80.352,97.513,99.790,% | Adaptive Acc: 90.736% | clf_exit: 0.797 0.188 0.015
Batch: 280 | Loss: 0.721 | Acc: 80.338,97.492,99.797,% | Adaptive Acc: 90.725% | clf_exit: 0.797 0.188 0.015
Batch: 300 | Loss: 0.720 | Acc: 80.391,97.498,99.795,% | Adaptive Acc: 90.739% | clf_exit: 0.797 0.188 0.015
Batch: 320 | Loss: 0.719 | Acc: 80.405,97.498,99.793,% | Adaptive Acc: 90.761% | clf_exit: 0.797 0.188 0.015
Batch: 340 | Loss: 0.718 | Acc: 80.460,97.494,99.794,% | Adaptive Acc: 90.774% | clf_exit: 0.798 0.187 0.015
Batch: 360 | Loss: 0.718 | Acc: 80.406,97.498,99.799,% | Adaptive Acc: 90.770% | clf_exit: 0.797 0.188 0.015
Batch: 380 | Loss: 0.717 | Acc: 80.430,97.492,99.801,% | Adaptive Acc: 90.777% | clf_exit: 0.797 0.187 0.015
Batch: 0 | Loss: 1.109 | Acc: 79.688,92.188,93.750,% | Adaptive Acc: 84.375% | clf_exit: 0.836 0.148 0.016
Batch: 20 | Loss: 1.226 | Acc: 77.269,90.253,93.192,% | Adaptive Acc: 83.966% | clf_exit: 0.831 0.151 0.018
Batch: 40 | Loss: 1.203 | Acc: 77.877,90.111,93.426,% | Adaptive Acc: 84.070% | clf_exit: 0.840 0.139 0.022
Batch: 60 | Loss: 1.197 | Acc: 77.856,90.036,93.494,% | Adaptive Acc: 83.927% | clf_exit: 0.838 0.142 0.020
Train all parameters

Epoch: 168
Batch: 0 | Loss: 0.766 | Acc: 78.125,96.875,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.789 0.188 0.023
Batch: 20 | Loss: 0.709 | Acc: 80.320,98.103,99.851,% | Adaptive Acc: 91.667% | clf_exit: 0.787 0.201 0.012
Batch: 40 | Loss: 0.714 | Acc: 80.488,97.809,99.790,% | Adaptive Acc: 91.330% | clf_exit: 0.789 0.197 0.014
Batch: 60 | Loss: 0.704 | Acc: 80.507,97.887,99.834,% | Adaptive Acc: 91.265% | clf_exit: 0.790 0.197 0.013
Batch: 80 | Loss: 0.702 | Acc: 80.411,97.936,99.836,% | Adaptive Acc: 91.117% | clf_exit: 0.793 0.195 0.012
Batch: 100 | Loss: 0.709 | Acc: 80.268,97.873,99.853,% | Adaptive Acc: 90.857% | clf_exit: 0.794 0.193 0.013
Batch: 120 | Loss: 0.711 | Acc: 80.353,97.760,99.864,% | Adaptive Acc: 90.832% | clf_exit: 0.793 0.193 0.013
Batch: 140 | Loss: 0.716 | Acc: 80.269,97.739,99.850,% | Adaptive Acc: 90.691% | clf_exit: 0.795 0.191 0.014
Batch: 160 | Loss: 0.712 | Acc: 80.352,97.700,99.859,% | Adaptive Acc: 90.834% | clf_exit: 0.795 0.191 0.014
Batch: 180 | Loss: 0.710 | Acc: 80.551,97.665,99.853,% | Adaptive Acc: 90.875% | clf_exit: 0.796 0.190 0.014
Batch: 200 | Loss: 0.709 | Acc: 80.597,97.668,99.856,% | Adaptive Acc: 90.862% | clf_exit: 0.797 0.188 0.014
Batch: 220 | Loss: 0.707 | Acc: 80.709,97.677,99.859,% | Adaptive Acc: 90.880% | clf_exit: 0.798 0.188 0.015
Batch: 240 | Loss: 0.707 | Acc: 80.686,97.608,99.848,% | Adaptive Acc: 90.826% | clf_exit: 0.798 0.187 0.015
Batch: 260 | Loss: 0.709 | Acc: 80.669,97.596,99.847,% | Adaptive Acc: 90.802% | clf_exit: 0.798 0.187 0.014
Batch: 280 | Loss: 0.711 | Acc: 80.663,97.551,99.842,% | Adaptive Acc: 90.786% | clf_exit: 0.798 0.187 0.014
Batch: 300 | Loss: 0.710 | Acc: 80.656,97.539,99.834,% | Adaptive Acc: 90.789% | clf_exit: 0.798 0.187 0.014
Batch: 320 | Loss: 0.710 | Acc: 80.680,97.535,99.830,% | Adaptive Acc: 90.781% | clf_exit: 0.799 0.187 0.015
Batch: 340 | Loss: 0.714 | Acc: 80.599,97.489,99.819,% | Adaptive Acc: 90.712% | clf_exit: 0.798 0.187 0.015
Batch: 360 | Loss: 0.714 | Acc: 80.627,97.466,99.807,% | Adaptive Acc: 90.688% | clf_exit: 0.799 0.186 0.015
Batch: 380 | Loss: 0.715 | Acc: 80.596,97.459,99.797,% | Adaptive Acc: 90.713% | clf_exit: 0.798 0.186 0.015
Batch: 0 | Loss: 1.140 | Acc: 78.125,90.625,89.844,% | Adaptive Acc: 83.594% | clf_exit: 0.844 0.133 0.023
Batch: 20 | Loss: 1.230 | Acc: 77.344,89.881,92.671,% | Adaptive Acc: 83.705% | clf_exit: 0.828 0.153 0.019
Batch: 40 | Loss: 1.216 | Acc: 77.820,89.768,93.102,% | Adaptive Acc: 83.956% | clf_exit: 0.838 0.144 0.019
Batch: 60 | Loss: 1.202 | Acc: 78.023,89.793,93.199,% | Adaptive Acc: 84.055% | clf_exit: 0.840 0.141 0.019
Train all parameters

Epoch: 169
Batch: 0 | Loss: 0.563 | Acc: 83.594,98.438,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.836 0.164 0.000
Batch: 20 | Loss: 0.696 | Acc: 79.948,98.251,99.888,% | Adaptive Acc: 90.290% | clf_exit: 0.804 0.181 0.015
Batch: 40 | Loss: 0.702 | Acc: 80.164,97.999,99.886,% | Adaptive Acc: 90.168% | clf_exit: 0.806 0.181 0.013
Batch: 60 | Loss: 0.698 | Acc: 80.200,98.015,99.898,% | Adaptive Acc: 90.369% | clf_exit: 0.806 0.181 0.013
Batch: 80 | Loss: 0.693 | Acc: 80.430,97.868,99.894,% | Adaptive Acc: 90.413% | clf_exit: 0.809 0.178 0.012
Batch: 100 | Loss: 0.698 | Acc: 80.538,97.811,99.845,% | Adaptive Acc: 90.610% | clf_exit: 0.807 0.179 0.013
Batch: 120 | Loss: 0.699 | Acc: 80.624,97.760,99.839,% | Adaptive Acc: 90.683% | clf_exit: 0.806 0.180 0.014
Batch: 140 | Loss: 0.702 | Acc: 80.724,97.651,99.839,% | Adaptive Acc: 90.736% | clf_exit: 0.805 0.181 0.014
Batch: 160 | Loss: 0.704 | Acc: 80.745,97.579,99.806,% | Adaptive Acc: 90.761% | clf_exit: 0.804 0.181 0.014
Batch: 180 | Loss: 0.705 | Acc: 80.810,97.609,99.801,% | Adaptive Acc: 90.811% | clf_exit: 0.804 0.182 0.015
Batch: 200 | Loss: 0.707 | Acc: 80.756,97.563,99.806,% | Adaptive Acc: 90.815% | clf_exit: 0.802 0.183 0.015
Batch: 220 | Loss: 0.707 | Acc: 80.840,97.564,99.813,% | Adaptive Acc: 90.883% | clf_exit: 0.802 0.184 0.015
Batch: 240 | Loss: 0.707 | Acc: 80.845,97.533,99.802,% | Adaptive Acc: 90.939% | clf_exit: 0.800 0.185 0.015
Batch: 260 | Loss: 0.710 | Acc: 80.750,97.516,99.781,% | Adaptive Acc: 90.912% | clf_exit: 0.800 0.186 0.015
Batch: 280 | Loss: 0.712 | Acc: 80.713,97.506,99.789,% | Adaptive Acc: 90.864% | clf_exit: 0.800 0.185 0.015
Batch: 300 | Loss: 0.712 | Acc: 80.752,97.532,99.800,% | Adaptive Acc: 90.905% | clf_exit: 0.800 0.185 0.015
Batch: 320 | Loss: 0.713 | Acc: 80.695,97.527,99.800,% | Adaptive Acc: 90.864% | clf_exit: 0.799 0.186 0.015
Batch: 340 | Loss: 0.712 | Acc: 80.700,97.519,99.792,% | Adaptive Acc: 90.870% | clf_exit: 0.799 0.186 0.015
Batch: 360 | Loss: 0.712 | Acc: 80.705,97.509,99.786,% | Adaptive Acc: 90.861% | clf_exit: 0.800 0.185 0.015
Batch: 380 | Loss: 0.711 | Acc: 80.717,97.511,99.793,% | Adaptive Acc: 90.867% | clf_exit: 0.800 0.185 0.015
Batch: 0 | Loss: 1.001 | Acc: 81.250,93.750,92.188,% | Adaptive Acc: 89.062% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 1.169 | Acc: 78.869,90.104,93.452,% | Adaptive Acc: 85.231% | clf_exit: 0.831 0.150 0.020
Batch: 40 | Loss: 1.167 | Acc: 78.887,90.263,93.483,% | Adaptive Acc: 85.004% | clf_exit: 0.840 0.138 0.022
Batch: 60 | Loss: 1.166 | Acc: 79.022,90.138,93.417,% | Adaptive Acc: 85.092% | clf_exit: 0.838 0.140 0.021
Train all parameters

Epoch: 170
Batch: 0 | Loss: 0.908 | Acc: 76.562,95.312,99.219,% | Adaptive Acc: 88.281% | clf_exit: 0.812 0.156 0.031
Batch: 20 | Loss: 0.734 | Acc: 79.985,97.879,99.702,% | Adaptive Acc: 90.513% | clf_exit: 0.798 0.187 0.015
Batch: 40 | Loss: 0.725 | Acc: 80.431,97.504,99.733,% | Adaptive Acc: 90.434% | clf_exit: 0.800 0.186 0.015
Batch: 60 | Loss: 0.720 | Acc: 80.610,97.451,99.808,% | Adaptive Acc: 90.241% | clf_exit: 0.804 0.182 0.014
Batch: 80 | Loss: 0.716 | Acc: 80.748,97.502,99.788,% | Adaptive Acc: 90.287% | clf_exit: 0.804 0.181 0.015
Batch: 100 | Loss: 0.719 | Acc: 80.546,97.494,99.783,% | Adaptive Acc: 90.153% | clf_exit: 0.802 0.183 0.015
Batch: 120 | Loss: 0.721 | Acc: 80.501,97.534,99.793,% | Adaptive Acc: 90.322% | clf_exit: 0.801 0.184 0.015
Batch: 140 | Loss: 0.719 | Acc: 80.546,97.507,99.789,% | Adaptive Acc: 90.503% | clf_exit: 0.799 0.185 0.015
Batch: 160 | Loss: 0.717 | Acc: 80.576,97.433,99.772,% | Adaptive Acc: 90.509% | clf_exit: 0.799 0.186 0.015
Batch: 180 | Loss: 0.716 | Acc: 80.508,97.484,99.780,% | Adaptive Acc: 90.491% | clf_exit: 0.800 0.186 0.015
Batch: 200 | Loss: 0.716 | Acc: 80.461,97.512,99.778,% | Adaptive Acc: 90.493% | clf_exit: 0.799 0.186 0.015
Batch: 220 | Loss: 0.716 | Acc: 80.465,97.518,99.767,% | Adaptive Acc: 90.519% | clf_exit: 0.799 0.186 0.015
Batch: 240 | Loss: 0.715 | Acc: 80.478,97.546,99.763,% | Adaptive Acc: 90.573% | clf_exit: 0.799 0.186 0.015
Batch: 260 | Loss: 0.714 | Acc: 80.538,97.543,99.770,% | Adaptive Acc: 90.604% | clf_exit: 0.800 0.186 0.015
Batch: 280 | Loss: 0.713 | Acc: 80.583,97.551,99.769,% | Adaptive Acc: 90.611% | clf_exit: 0.800 0.185 0.015
Batch: 300 | Loss: 0.713 | Acc: 80.547,97.565,99.772,% | Adaptive Acc: 90.630% | clf_exit: 0.799 0.186 0.015
Batch: 320 | Loss: 0.714 | Acc: 80.549,97.530,99.761,% | Adaptive Acc: 90.662% | clf_exit: 0.799 0.186 0.015
Batch: 340 | Loss: 0.716 | Acc: 80.455,97.507,99.764,% | Adaptive Acc: 90.616% | clf_exit: 0.798 0.187 0.015
Batch: 360 | Loss: 0.717 | Acc: 80.428,97.492,99.764,% | Adaptive Acc: 90.597% | clf_exit: 0.798 0.187 0.015
Batch: 380 | Loss: 0.716 | Acc: 80.500,97.476,99.770,% | Adaptive Acc: 90.607% | clf_exit: 0.798 0.186 0.015
Batch: 0 | Loss: 0.974 | Acc: 82.031,92.969,96.094,% | Adaptive Acc: 89.844% | clf_exit: 0.836 0.148 0.016
Batch: 20 | Loss: 1.160 | Acc: 78.906,90.327,93.638,% | Adaptive Acc: 85.714% | clf_exit: 0.821 0.162 0.017
Batch: 40 | Loss: 1.167 | Acc: 78.982,90.225,93.540,% | Adaptive Acc: 85.309% | clf_exit: 0.833 0.148 0.019
Batch: 60 | Loss: 1.159 | Acc: 78.957,90.241,93.494,% | Adaptive Acc: 85.272% | clf_exit: 0.834 0.146 0.021
Train all parameters

Epoch: 171
Batch: 0 | Loss: 0.659 | Acc: 78.125,98.438,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.773 0.219 0.008
Batch: 20 | Loss: 0.668 | Acc: 81.957,98.065,99.926,% | Adaptive Acc: 91.332% | clf_exit: 0.814 0.174 0.012
Batch: 40 | Loss: 0.671 | Acc: 82.012,97.809,99.886,% | Adaptive Acc: 91.597% | clf_exit: 0.808 0.179 0.013
Batch: 60 | Loss: 0.680 | Acc: 81.532,97.836,99.846,% | Adaptive Acc: 91.496% | clf_exit: 0.806 0.180 0.013
Batch: 80 | Loss: 0.686 | Acc: 81.472,97.714,99.836,% | Adaptive Acc: 91.204% | clf_exit: 0.806 0.181 0.013
Batch: 100 | Loss: 0.690 | Acc: 81.250,97.641,99.830,% | Adaptive Acc: 91.097% | clf_exit: 0.807 0.180 0.014
Batch: 120 | Loss: 0.695 | Acc: 81.192,97.650,99.813,% | Adaptive Acc: 90.974% | clf_exit: 0.806 0.180 0.014
Batch: 140 | Loss: 0.697 | Acc: 81.139,97.629,99.812,% | Adaptive Acc: 90.985% | clf_exit: 0.804 0.181 0.015
Batch: 160 | Loss: 0.697 | Acc: 81.192,97.666,99.816,% | Adaptive Acc: 90.936% | clf_exit: 0.805 0.181 0.015
Batch: 180 | Loss: 0.696 | Acc: 81.013,97.678,99.827,% | Adaptive Acc: 90.901% | clf_exit: 0.804 0.182 0.014
Batch: 200 | Loss: 0.699 | Acc: 80.966,97.621,99.806,% | Adaptive Acc: 90.823% | clf_exit: 0.804 0.182 0.014
Batch: 220 | Loss: 0.699 | Acc: 81.049,97.621,99.791,% | Adaptive Acc: 90.834% | clf_exit: 0.804 0.182 0.014
Batch: 240 | Loss: 0.699 | Acc: 81.020,97.608,99.793,% | Adaptive Acc: 90.852% | clf_exit: 0.805 0.181 0.014
Batch: 260 | Loss: 0.700 | Acc: 81.031,97.614,99.802,% | Adaptive Acc: 90.882% | clf_exit: 0.804 0.182 0.014
Batch: 280 | Loss: 0.703 | Acc: 80.955,97.576,99.794,% | Adaptive Acc: 90.836% | clf_exit: 0.803 0.182 0.014
Batch: 300 | Loss: 0.704 | Acc: 80.879,97.581,99.800,% | Adaptive Acc: 90.809% | clf_exit: 0.802 0.183 0.014
Batch: 320 | Loss: 0.705 | Acc: 80.868,97.588,99.803,% | Adaptive Acc: 90.825% | clf_exit: 0.802 0.184 0.014
Batch: 340 | Loss: 0.706 | Acc: 80.831,97.558,99.801,% | Adaptive Acc: 90.774% | clf_exit: 0.803 0.183 0.014
Batch: 360 | Loss: 0.707 | Acc: 80.843,97.555,99.794,% | Adaptive Acc: 90.813% | clf_exit: 0.802 0.183 0.014
Batch: 380 | Loss: 0.709 | Acc: 80.799,97.554,99.791,% | Adaptive Acc: 90.771% | clf_exit: 0.802 0.184 0.014
Batch: 0 | Loss: 0.982 | Acc: 82.031,93.750,93.750,% | Adaptive Acc: 86.719% | clf_exit: 0.852 0.125 0.023
Batch: 20 | Loss: 1.180 | Acc: 78.199,89.993,93.266,% | Adaptive Acc: 84.152% | clf_exit: 0.832 0.147 0.021
Batch: 40 | Loss: 1.186 | Acc: 78.544,89.844,93.426,% | Adaptive Acc: 84.680% | clf_exit: 0.833 0.143 0.023
Batch: 60 | Loss: 1.176 | Acc: 78.919,89.959,93.391,% | Adaptive Acc: 85.041% | clf_exit: 0.832 0.146 0.023
Train all parameters

Epoch: 172
Batch: 0 | Loss: 0.723 | Acc: 75.781,99.219,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.766 0.227 0.008
Batch: 20 | Loss: 0.715 | Acc: 79.911,97.693,99.851,% | Adaptive Acc: 90.662% | clf_exit: 0.794 0.191 0.015
Batch: 40 | Loss: 0.718 | Acc: 80.259,97.275,99.848,% | Adaptive Acc: 90.492% | clf_exit: 0.800 0.185 0.015
Batch: 60 | Loss: 0.710 | Acc: 80.405,97.515,99.859,% | Adaptive Acc: 90.791% | clf_exit: 0.800 0.186 0.014
Batch: 80 | Loss: 0.709 | Acc: 80.758,97.521,99.826,% | Adaptive Acc: 90.596% | clf_exit: 0.803 0.183 0.014
Batch: 100 | Loss: 0.706 | Acc: 80.755,97.587,99.814,% | Adaptive Acc: 90.710% | clf_exit: 0.802 0.184 0.014
Batch: 120 | Loss: 0.709 | Acc: 80.617,97.618,99.787,% | Adaptive Acc: 90.477% | clf_exit: 0.804 0.183 0.013
Batch: 140 | Loss: 0.711 | Acc: 80.519,97.584,99.773,% | Adaptive Acc: 90.431% | clf_exit: 0.803 0.184 0.013
Batch: 160 | Loss: 0.710 | Acc: 80.566,97.588,99.777,% | Adaptive Acc: 90.426% | clf_exit: 0.804 0.183 0.013
Batch: 180 | Loss: 0.711 | Acc: 80.607,97.557,99.745,% | Adaptive Acc: 90.327% | clf_exit: 0.805 0.181 0.014
Batch: 200 | Loss: 0.713 | Acc: 80.581,97.532,99.751,% | Adaptive Acc: 90.310% | clf_exit: 0.805 0.181 0.014
Batch: 220 | Loss: 0.711 | Acc: 80.674,97.554,99.763,% | Adaptive Acc: 90.402% | clf_exit: 0.805 0.182 0.014
Batch: 240 | Loss: 0.711 | Acc: 80.699,97.588,99.767,% | Adaptive Acc: 90.466% | clf_exit: 0.804 0.182 0.014
Batch: 260 | Loss: 0.709 | Acc: 80.738,97.608,99.764,% | Adaptive Acc: 90.547% | clf_exit: 0.804 0.183 0.014
Batch: 280 | Loss: 0.711 | Acc: 80.738,97.553,99.761,% | Adaptive Acc: 90.539% | clf_exit: 0.804 0.182 0.014
Batch: 300 | Loss: 0.711 | Acc: 80.746,97.571,99.759,% | Adaptive Acc: 90.563% | clf_exit: 0.803 0.183 0.014
Batch: 320 | Loss: 0.711 | Acc: 80.722,97.588,99.761,% | Adaptive Acc: 90.581% | clf_exit: 0.803 0.183 0.014
Batch: 340 | Loss: 0.712 | Acc: 80.718,97.594,99.769,% | Adaptive Acc: 90.593% | clf_exit: 0.803 0.184 0.014
Batch: 360 | Loss: 0.712 | Acc: 80.739,97.596,99.771,% | Adaptive Acc: 90.634% | clf_exit: 0.803 0.183 0.014
Batch: 380 | Loss: 0.714 | Acc: 80.672,97.572,99.776,% | Adaptive Acc: 90.555% | clf_exit: 0.803 0.183 0.014
Batch: 0 | Loss: 0.994 | Acc: 79.688,91.406,95.312,% | Adaptive Acc: 84.375% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.179 | Acc: 78.534,90.737,93.601,% | Adaptive Acc: 84.784% | clf_exit: 0.839 0.141 0.019
Batch: 40 | Loss: 1.176 | Acc: 78.811,90.377,93.521,% | Adaptive Acc: 85.061% | clf_exit: 0.841 0.136 0.022
Batch: 60 | Loss: 1.157 | Acc: 79.060,90.356,93.584,% | Adaptive Acc: 85.246% | clf_exit: 0.839 0.139 0.022
Train all parameters

Epoch: 173
Batch: 0 | Loss: 0.750 | Acc: 78.906,99.219,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.789 0.195 0.016
Batch: 20 | Loss: 0.662 | Acc: 80.915,98.140,99.814,% | Adaptive Acc: 90.811% | clf_exit: 0.810 0.177 0.013
Batch: 40 | Loss: 0.679 | Acc: 80.831,98.095,99.867,% | Adaptive Acc: 90.682% | clf_exit: 0.805 0.184 0.011
Batch: 60 | Loss: 0.688 | Acc: 81.237,97.733,99.821,% | Adaptive Acc: 90.715% | clf_exit: 0.805 0.183 0.012
Batch: 80 | Loss: 0.689 | Acc: 81.433,97.676,99.817,% | Adaptive Acc: 90.750% | clf_exit: 0.807 0.182 0.012
Batch: 100 | Loss: 0.696 | Acc: 81.358,97.633,99.814,% | Adaptive Acc: 90.710% | clf_exit: 0.804 0.184 0.012
Batch: 120 | Loss: 0.692 | Acc: 81.366,97.714,99.826,% | Adaptive Acc: 90.799% | clf_exit: 0.806 0.181 0.013
Batch: 140 | Loss: 0.684 | Acc: 81.654,97.756,99.845,% | Adaptive Acc: 90.941% | clf_exit: 0.809 0.179 0.012
Batch: 160 | Loss: 0.686 | Acc: 81.561,97.729,99.845,% | Adaptive Acc: 90.771% | clf_exit: 0.811 0.177 0.012
Batch: 180 | Loss: 0.692 | Acc: 81.336,97.648,99.853,% | Adaptive Acc: 90.776% | clf_exit: 0.808 0.180 0.012
Batch: 200 | Loss: 0.693 | Acc: 81.316,97.641,99.845,% | Adaptive Acc: 90.738% | clf_exit: 0.809 0.179 0.013
Batch: 220 | Loss: 0.699 | Acc: 81.176,97.589,99.834,% | Adaptive Acc: 90.710% | clf_exit: 0.807 0.180 0.013
Batch: 240 | Loss: 0.699 | Acc: 81.137,97.559,99.831,% | Adaptive Acc: 90.680% | clf_exit: 0.807 0.180 0.013
Batch: 260 | Loss: 0.701 | Acc: 81.070,97.572,99.832,% | Adaptive Acc: 90.625% | clf_exit: 0.806 0.181 0.013
Batch: 280 | Loss: 0.700 | Acc: 81.089,97.584,99.833,% | Adaptive Acc: 90.669% | clf_exit: 0.806 0.181 0.013
Batch: 300 | Loss: 0.702 | Acc: 81.032,97.568,99.818,% | Adaptive Acc: 90.672% | clf_exit: 0.806 0.181 0.013
Batch: 320 | Loss: 0.705 | Acc: 80.956,97.552,99.822,% | Adaptive Acc: 90.591% | clf_exit: 0.805 0.182 0.013
Batch: 340 | Loss: 0.706 | Acc: 80.987,97.521,99.819,% | Adaptive Acc: 90.646% | clf_exit: 0.804 0.182 0.013
Batch: 360 | Loss: 0.705 | Acc: 80.995,97.503,99.812,% | Adaptive Acc: 90.670% | clf_exit: 0.804 0.182 0.014
Batch: 380 | Loss: 0.705 | Acc: 81.053,97.509,99.799,% | Adaptive Acc: 90.678% | clf_exit: 0.805 0.181 0.014
Batch: 0 | Loss: 0.938 | Acc: 77.344,94.531,96.875,% | Adaptive Acc: 85.938% | clf_exit: 0.852 0.133 0.016
Batch: 20 | Loss: 1.262 | Acc: 77.567,90.067,92.894,% | Adaptive Acc: 84.449% | clf_exit: 0.831 0.148 0.021
Batch: 40 | Loss: 1.240 | Acc: 78.182,89.463,93.159,% | Adaptive Acc: 84.299% | clf_exit: 0.836 0.141 0.023
Batch: 60 | Loss: 1.218 | Acc: 78.202,89.600,93.238,% | Adaptive Acc: 84.477% | clf_exit: 0.837 0.141 0.023
Train all parameters

Epoch: 174
Batch: 0 | Loss: 0.783 | Acc: 75.781,97.656,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.719 0.250 0.031
Batch: 20 | Loss: 0.704 | Acc: 80.804,98.065,99.814,% | Adaptive Acc: 91.071% | clf_exit: 0.801 0.186 0.013
Batch: 40 | Loss: 0.711 | Acc: 80.755,97.847,99.829,% | Adaptive Acc: 91.197% | clf_exit: 0.797 0.189 0.014
Batch: 60 | Loss: 0.705 | Acc: 80.955,97.900,99.846,% | Adaptive Acc: 91.368% | clf_exit: 0.798 0.189 0.013
Batch: 80 | Loss: 0.705 | Acc: 80.710,97.897,99.846,% | Adaptive Acc: 91.146% | clf_exit: 0.798 0.188 0.014
Batch: 100 | Loss: 0.698 | Acc: 80.964,97.912,99.845,% | Adaptive Acc: 91.120% | clf_exit: 0.800 0.186 0.014
Batch: 120 | Loss: 0.700 | Acc: 80.927,97.798,99.839,% | Adaptive Acc: 90.999% | clf_exit: 0.802 0.185 0.013
Batch: 140 | Loss: 0.701 | Acc: 80.879,97.773,99.834,% | Adaptive Acc: 91.024% | clf_exit: 0.801 0.186 0.013
Batch: 160 | Loss: 0.704 | Acc: 80.789,97.763,99.830,% | Adaptive Acc: 90.984% | clf_exit: 0.799 0.187 0.013
Batch: 180 | Loss: 0.704 | Acc: 80.745,97.799,99.836,% | Adaptive Acc: 90.919% | clf_exit: 0.800 0.187 0.013
Batch: 200 | Loss: 0.705 | Acc: 80.679,97.800,99.833,% | Adaptive Acc: 90.862% | clf_exit: 0.799 0.188 0.013
Batch: 220 | Loss: 0.703 | Acc: 80.734,97.798,99.823,% | Adaptive Acc: 90.865% | clf_exit: 0.801 0.186 0.013
Batch: 240 | Loss: 0.700 | Acc: 80.897,97.802,99.828,% | Adaptive Acc: 90.826% | clf_exit: 0.802 0.185 0.013
Batch: 260 | Loss: 0.703 | Acc: 80.867,97.728,99.811,% | Adaptive Acc: 90.790% | clf_exit: 0.803 0.184 0.013
Batch: 280 | Loss: 0.706 | Acc: 80.725,97.665,99.800,% | Adaptive Acc: 90.644% | clf_exit: 0.803 0.183 0.014
Batch: 300 | Loss: 0.706 | Acc: 80.757,97.630,99.803,% | Adaptive Acc: 90.685% | clf_exit: 0.803 0.184 0.014
Batch: 320 | Loss: 0.705 | Acc: 80.775,97.629,99.798,% | Adaptive Acc: 90.715% | clf_exit: 0.803 0.183 0.013
Batch: 340 | Loss: 0.706 | Acc: 80.753,97.617,99.798,% | Adaptive Acc: 90.689% | clf_exit: 0.803 0.183 0.013
Batch: 360 | Loss: 0.707 | Acc: 80.677,97.611,99.799,% | Adaptive Acc: 90.670% | clf_exit: 0.803 0.184 0.013
Batch: 380 | Loss: 0.707 | Acc: 80.657,97.611,99.799,% | Adaptive Acc: 90.678% | clf_exit: 0.802 0.184 0.014
Batch: 0 | Loss: 1.148 | Acc: 82.031,89.062,92.188,% | Adaptive Acc: 84.375% | clf_exit: 0.812 0.164 0.023
Batch: 20 | Loss: 1.217 | Acc: 77.493,89.993,92.932,% | Adaptive Acc: 83.557% | clf_exit: 0.822 0.161 0.017
Batch: 40 | Loss: 1.204 | Acc: 77.858,89.977,93.521,% | Adaptive Acc: 84.013% | clf_exit: 0.832 0.148 0.021
Batch: 60 | Loss: 1.191 | Acc: 78.023,90.061,93.519,% | Adaptive Acc: 84.324% | clf_exit: 0.830 0.149 0.021
Train all parameters

Epoch: 175
Batch: 0 | Loss: 0.638 | Acc: 78.906,98.438,100.000,% | Adaptive Acc: 87.500% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 0.736 | Acc: 78.795,97.768,99.777,% | Adaptive Acc: 89.509% | clf_exit: 0.804 0.180 0.015
Batch: 40 | Loss: 0.726 | Acc: 79.497,97.866,99.848,% | Adaptive Acc: 90.244% | clf_exit: 0.798 0.187 0.015
Batch: 60 | Loss: 0.713 | Acc: 80.008,98.015,99.872,% | Adaptive Acc: 90.394% | clf_exit: 0.803 0.184 0.014
Batch: 80 | Loss: 0.711 | Acc: 80.170,97.965,99.836,% | Adaptive Acc: 90.384% | clf_exit: 0.802 0.184 0.014
Batch: 100 | Loss: 0.700 | Acc: 80.685,98.051,99.853,% | Adaptive Acc: 90.726% | clf_exit: 0.802 0.185 0.013
Batch: 120 | Loss: 0.701 | Acc: 80.669,97.947,99.806,% | Adaptive Acc: 90.496% | clf_exit: 0.804 0.183 0.013
Batch: 140 | Loss: 0.702 | Acc: 80.591,97.933,99.823,% | Adaptive Acc: 90.520% | clf_exit: 0.803 0.184 0.013
Batch: 160 | Loss: 0.704 | Acc: 80.585,97.884,99.806,% | Adaptive Acc: 90.606% | clf_exit: 0.801 0.186 0.013
Batch: 180 | Loss: 0.701 | Acc: 80.792,97.859,99.810,% | Adaptive Acc: 90.621% | clf_exit: 0.803 0.183 0.013
Batch: 200 | Loss: 0.704 | Acc: 80.718,97.812,99.786,% | Adaptive Acc: 90.621% | clf_exit: 0.802 0.184 0.014
Batch: 220 | Loss: 0.702 | Acc: 80.759,97.784,99.781,% | Adaptive Acc: 90.643% | clf_exit: 0.803 0.184 0.014
Batch: 240 | Loss: 0.703 | Acc: 80.738,97.796,99.786,% | Adaptive Acc: 90.735% | clf_exit: 0.802 0.184 0.014
Batch: 260 | Loss: 0.701 | Acc: 80.852,97.767,99.787,% | Adaptive Acc: 90.805% | clf_exit: 0.802 0.184 0.014
Batch: 280 | Loss: 0.701 | Acc: 80.897,97.723,99.778,% | Adaptive Acc: 90.797% | clf_exit: 0.802 0.184 0.014
Batch: 300 | Loss: 0.703 | Acc: 80.848,97.724,99.772,% | Adaptive Acc: 90.796% | clf_exit: 0.802 0.184 0.014
Batch: 320 | Loss: 0.705 | Acc: 80.836,97.712,99.771,% | Adaptive Acc: 90.778% | clf_exit: 0.802 0.184 0.014
Batch: 340 | Loss: 0.704 | Acc: 80.927,97.709,99.775,% | Adaptive Acc: 90.815% | clf_exit: 0.803 0.184 0.014
Batch: 360 | Loss: 0.704 | Acc: 80.917,97.676,99.764,% | Adaptive Acc: 90.779% | clf_exit: 0.803 0.183 0.014
Batch: 380 | Loss: 0.704 | Acc: 80.936,97.675,99.762,% | Adaptive Acc: 90.791% | clf_exit: 0.803 0.183 0.014
Batch: 0 | Loss: 1.064 | Acc: 78.906,92.188,93.750,% | Adaptive Acc: 82.812% | clf_exit: 0.852 0.141 0.008
Batch: 20 | Loss: 1.195 | Acc: 77.418,89.918,93.341,% | Adaptive Acc: 84.189% | clf_exit: 0.829 0.149 0.022
Batch: 40 | Loss: 1.208 | Acc: 77.896,89.939,93.216,% | Adaptive Acc: 83.880% | clf_exit: 0.839 0.140 0.021
Batch: 60 | Loss: 1.202 | Acc: 77.818,89.946,93.327,% | Adaptive Acc: 83.927% | clf_exit: 0.837 0.141 0.022
Train all parameters

Epoch: 176
Batch: 0 | Loss: 0.658 | Acc: 82.812,99.219,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.812 0.164 0.023
Batch: 20 | Loss: 0.682 | Acc: 82.254,97.842,99.814,% | Adaptive Acc: 91.853% | clf_exit: 0.802 0.185 0.013
Batch: 40 | Loss: 0.685 | Acc: 82.260,97.942,99.752,% | Adaptive Acc: 91.692% | clf_exit: 0.803 0.185 0.012
Batch: 60 | Loss: 0.685 | Acc: 82.006,97.964,99.769,% | Adaptive Acc: 91.573% | clf_exit: 0.806 0.181 0.013
Batch: 80 | Loss: 0.687 | Acc: 81.665,97.868,99.788,% | Adaptive Acc: 91.223% | clf_exit: 0.808 0.178 0.013
Batch: 100 | Loss: 0.688 | Acc: 81.420,97.857,99.768,% | Adaptive Acc: 91.174% | clf_exit: 0.808 0.179 0.013
Batch: 120 | Loss: 0.695 | Acc: 81.321,97.798,99.761,% | Adaptive Acc: 91.245% | clf_exit: 0.805 0.183 0.013
Batch: 140 | Loss: 0.697 | Acc: 81.184,97.784,99.784,% | Adaptive Acc: 91.162% | clf_exit: 0.804 0.184 0.013
Batch: 160 | Loss: 0.699 | Acc: 81.114,97.792,99.791,% | Adaptive Acc: 91.105% | clf_exit: 0.804 0.183 0.013
Batch: 180 | Loss: 0.699 | Acc: 81.116,97.777,99.793,% | Adaptive Acc: 91.113% | clf_exit: 0.802 0.185 0.013
Batch: 200 | Loss: 0.702 | Acc: 80.974,97.750,99.798,% | Adaptive Acc: 90.994% | clf_exit: 0.802 0.185 0.013
Batch: 220 | Loss: 0.701 | Acc: 81.020,97.784,99.802,% | Adaptive Acc: 91.003% | clf_exit: 0.802 0.186 0.012
Batch: 240 | Loss: 0.698 | Acc: 81.088,97.789,99.809,% | Adaptive Acc: 91.027% | clf_exit: 0.803 0.185 0.012
Batch: 260 | Loss: 0.701 | Acc: 81.005,97.773,99.808,% | Adaptive Acc: 90.966% | clf_exit: 0.803 0.184 0.012
Batch: 280 | Loss: 0.699 | Acc: 80.994,97.756,99.814,% | Adaptive Acc: 90.986% | clf_exit: 0.803 0.184 0.013
Batch: 300 | Loss: 0.699 | Acc: 80.928,97.734,99.816,% | Adaptive Acc: 91.001% | clf_exit: 0.803 0.185 0.013
Batch: 320 | Loss: 0.701 | Acc: 80.817,97.739,99.815,% | Adaptive Acc: 90.910% | clf_exit: 0.803 0.184 0.013
Batch: 340 | Loss: 0.703 | Acc: 80.783,97.711,99.812,% | Adaptive Acc: 90.898% | clf_exit: 0.802 0.185 0.013
Batch: 360 | Loss: 0.703 | Acc: 80.787,97.702,99.816,% | Adaptive Acc: 90.906% | clf_exit: 0.801 0.186 0.013
Batch: 380 | Loss: 0.702 | Acc: 80.856,97.697,99.815,% | Adaptive Acc: 90.941% | clf_exit: 0.802 0.185 0.013
Batch: 0 | Loss: 0.983 | Acc: 81.250,92.969,94.531,% | Adaptive Acc: 85.938% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 1.211 | Acc: 78.088,90.141,92.969,% | Adaptive Acc: 84.487% | clf_exit: 0.816 0.161 0.023
Batch: 40 | Loss: 1.213 | Acc: 78.258,90.225,93.293,% | Adaptive Acc: 84.775% | clf_exit: 0.824 0.151 0.025
Batch: 60 | Loss: 1.205 | Acc: 78.291,90.074,93.212,% | Adaptive Acc: 84.746% | clf_exit: 0.825 0.150 0.024
Train all parameters

Epoch: 177
Batch: 0 | Loss: 0.774 | Acc: 80.469,97.656,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.805 0.156 0.039
Batch: 20 | Loss: 0.703 | Acc: 81.138,97.582,99.777,% | Adaptive Acc: 91.406% | clf_exit: 0.799 0.186 0.015
Batch: 40 | Loss: 0.707 | Acc: 81.098,97.389,99.733,% | Adaptive Acc: 91.139% | clf_exit: 0.805 0.181 0.014
Batch: 60 | Loss: 0.702 | Acc: 81.096,97.618,99.769,% | Adaptive Acc: 91.137% | clf_exit: 0.803 0.185 0.013
Batch: 80 | Loss: 0.690 | Acc: 81.366,97.782,99.797,% | Adaptive Acc: 91.223% | clf_exit: 0.804 0.184 0.012
Batch: 100 | Loss: 0.697 | Acc: 81.196,97.687,99.760,% | Adaptive Acc: 91.089% | clf_exit: 0.804 0.183 0.012
Batch: 120 | Loss: 0.702 | Acc: 80.966,97.650,99.774,% | Adaptive Acc: 91.032% | clf_exit: 0.804 0.183 0.013
Batch: 140 | Loss: 0.700 | Acc: 81.023,97.689,99.762,% | Adaptive Acc: 91.063% | clf_exit: 0.803 0.184 0.013
Batch: 160 | Loss: 0.696 | Acc: 81.114,97.647,99.782,% | Adaptive Acc: 91.067% | clf_exit: 0.803 0.184 0.013
Batch: 180 | Loss: 0.693 | Acc: 81.198,97.609,99.806,% | Adaptive Acc: 91.117% | clf_exit: 0.804 0.183 0.013
Batch: 200 | Loss: 0.696 | Acc: 81.157,97.520,99.794,% | Adaptive Acc: 91.053% | clf_exit: 0.803 0.184 0.013
Batch: 220 | Loss: 0.701 | Acc: 81.038,97.522,99.784,% | Adaptive Acc: 90.971% | clf_exit: 0.802 0.185 0.013
Batch: 240 | Loss: 0.702 | Acc: 80.981,97.556,99.786,% | Adaptive Acc: 90.939% | clf_exit: 0.801 0.186 0.013
Batch: 260 | Loss: 0.701 | Acc: 80.981,97.543,99.784,% | Adaptive Acc: 90.915% | clf_exit: 0.801 0.186 0.013
Batch: 280 | Loss: 0.702 | Acc: 80.980,97.562,99.766,% | Adaptive Acc: 90.978% | clf_exit: 0.800 0.187 0.013
Batch: 300 | Loss: 0.704 | Acc: 80.915,97.563,99.756,% | Adaptive Acc: 90.949% | clf_exit: 0.800 0.187 0.013
Batch: 320 | Loss: 0.702 | Acc: 81.036,97.588,99.752,% | Adaptive Acc: 91.027% | clf_exit: 0.801 0.186 0.014
Batch: 340 | Loss: 0.703 | Acc: 80.961,97.571,99.748,% | Adaptive Acc: 90.966% | clf_exit: 0.801 0.185 0.014
Batch: 360 | Loss: 0.705 | Acc: 80.962,97.535,99.734,% | Adaptive Acc: 90.967% | clf_exit: 0.800 0.186 0.014
Batch: 380 | Loss: 0.706 | Acc: 80.959,97.523,99.725,% | Adaptive Acc: 90.961% | clf_exit: 0.800 0.186 0.014
Batch: 0 | Loss: 1.022 | Acc: 79.688,92.969,92.969,% | Adaptive Acc: 86.719% | clf_exit: 0.828 0.148 0.023
Batch: 20 | Loss: 1.191 | Acc: 78.162,90.104,92.932,% | Adaptive Acc: 84.598% | clf_exit: 0.823 0.157 0.021
Batch: 40 | Loss: 1.180 | Acc: 78.258,90.130,93.369,% | Adaptive Acc: 84.851% | clf_exit: 0.835 0.142 0.023
Batch: 60 | Loss: 1.172 | Acc: 78.560,90.100,93.251,% | Adaptive Acc: 84.926% | clf_exit: 0.836 0.141 0.024
Train all parameters

Epoch: 178
Batch: 0 | Loss: 0.709 | Acc: 82.812,100.000,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.781 0.211 0.008
Batch: 20 | Loss: 0.689 | Acc: 80.618,97.917,99.888,% | Adaptive Acc: 90.513% | clf_exit: 0.818 0.171 0.012
Batch: 40 | Loss: 0.680 | Acc: 80.983,98.018,99.924,% | Adaptive Acc: 90.358% | clf_exit: 0.810 0.180 0.010
Batch: 60 | Loss: 0.682 | Acc: 81.199,97.900,99.885,% | Adaptive Acc: 90.523% | clf_exit: 0.808 0.180 0.011
Batch: 80 | Loss: 0.686 | Acc: 80.999,97.897,99.875,% | Adaptive Acc: 90.413% | clf_exit: 0.807 0.182 0.011
Batch: 100 | Loss: 0.691 | Acc: 80.755,97.881,99.899,% | Adaptive Acc: 90.339% | clf_exit: 0.805 0.184 0.012
Batch: 120 | Loss: 0.696 | Acc: 80.733,97.798,99.890,% | Adaptive Acc: 90.334% | clf_exit: 0.805 0.183 0.012
Batch: 140 | Loss: 0.693 | Acc: 80.967,97.800,99.867,% | Adaptive Acc: 90.459% | clf_exit: 0.804 0.184 0.012
Batch: 160 | Loss: 0.697 | Acc: 80.925,97.782,99.854,% | Adaptive Acc: 90.499% | clf_exit: 0.804 0.184 0.012
Batch: 180 | Loss: 0.699 | Acc: 80.883,97.717,99.845,% | Adaptive Acc: 90.496% | clf_exit: 0.803 0.185 0.012
Batch: 200 | Loss: 0.700 | Acc: 80.900,97.683,99.837,% | Adaptive Acc: 90.481% | clf_exit: 0.805 0.183 0.012
Batch: 220 | Loss: 0.701 | Acc: 80.868,97.667,99.830,% | Adaptive Acc: 90.508% | clf_exit: 0.804 0.183 0.013
Batch: 240 | Loss: 0.701 | Acc: 80.861,97.640,99.809,% | Adaptive Acc: 90.602% | clf_exit: 0.803 0.184 0.013
Batch: 260 | Loss: 0.701 | Acc: 80.873,97.647,99.802,% | Adaptive Acc: 90.622% | clf_exit: 0.803 0.184 0.013
Batch: 280 | Loss: 0.699 | Acc: 80.886,97.687,99.800,% | Adaptive Acc: 90.700% | clf_exit: 0.803 0.184 0.013
Batch: 300 | Loss: 0.698 | Acc: 80.928,97.685,99.800,% | Adaptive Acc: 90.700% | clf_exit: 0.804 0.183 0.013
Batch: 320 | Loss: 0.699 | Acc: 80.880,97.654,99.798,% | Adaptive Acc: 90.674% | clf_exit: 0.804 0.183 0.013
Batch: 340 | Loss: 0.701 | Acc: 80.904,97.629,99.789,% | Adaptive Acc: 90.682% | clf_exit: 0.805 0.182 0.013
Batch: 360 | Loss: 0.699 | Acc: 80.971,97.643,99.788,% | Adaptive Acc: 90.731% | clf_exit: 0.805 0.182 0.013
Batch: 380 | Loss: 0.700 | Acc: 80.959,97.615,99.783,% | Adaptive Acc: 90.711% | clf_exit: 0.805 0.181 0.013
Batch: 0 | Loss: 1.103 | Acc: 82.031,92.969,92.969,% | Adaptive Acc: 85.156% | clf_exit: 0.844 0.141 0.016
Batch: 20 | Loss: 1.247 | Acc: 77.567,89.249,92.894,% | Adaptive Acc: 83.036% | clf_exit: 0.847 0.131 0.022
Batch: 40 | Loss: 1.241 | Acc: 77.801,89.348,93.083,% | Adaptive Acc: 83.308% | clf_exit: 0.849 0.131 0.020
Batch: 60 | Loss: 1.232 | Acc: 77.882,89.498,92.994,% | Adaptive Acc: 83.568% | clf_exit: 0.848 0.133 0.019
Train all parameters

Epoch: 179
Batch: 0 | Loss: 0.721 | Acc: 79.688,96.875,99.219,% | Adaptive Acc: 86.719% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.691 | Acc: 81.399,97.693,99.777,% | Adaptive Acc: 91.034% | clf_exit: 0.801 0.187 0.012
Batch: 40 | Loss: 0.709 | Acc: 80.793,97.656,99.771,% | Adaptive Acc: 90.873% | clf_exit: 0.801 0.186 0.014
Batch: 60 | Loss: 0.693 | Acc: 81.096,97.618,99.834,% | Adaptive Acc: 91.137% | clf_exit: 0.803 0.184 0.013
Batch: 80 | Loss: 0.692 | Acc: 81.356,97.541,99.846,% | Adaptive Acc: 91.107% | clf_exit: 0.807 0.181 0.013
Batch: 100 | Loss: 0.685 | Acc: 81.528,97.656,99.838,% | Adaptive Acc: 91.190% | clf_exit: 0.809 0.179 0.012
Batch: 120 | Loss: 0.682 | Acc: 81.560,97.766,99.851,% | Adaptive Acc: 91.225% | clf_exit: 0.811 0.178 0.012
Batch: 140 | Loss: 0.680 | Acc: 81.599,97.784,99.867,% | Adaptive Acc: 91.157% | clf_exit: 0.811 0.178 0.012
Batch: 160 | Loss: 0.684 | Acc: 81.430,97.797,99.884,% | Adaptive Acc: 91.105% | clf_exit: 0.809 0.179 0.012
Batch: 180 | Loss: 0.684 | Acc: 81.526,97.790,99.862,% | Adaptive Acc: 91.139% | clf_exit: 0.809 0.179 0.012
Batch: 200 | Loss: 0.683 | Acc: 81.491,97.823,99.868,% | Adaptive Acc: 91.157% | clf_exit: 0.809 0.179 0.012
Batch: 220 | Loss: 0.686 | Acc: 81.349,97.776,99.852,% | Adaptive Acc: 91.042% | clf_exit: 0.809 0.179 0.012
Batch: 240 | Loss: 0.684 | Acc: 81.409,97.776,99.844,% | Adaptive Acc: 91.085% | clf_exit: 0.809 0.179 0.012
Batch: 260 | Loss: 0.684 | Acc: 81.445,97.755,99.838,% | Adaptive Acc: 91.071% | clf_exit: 0.810 0.177 0.013
Batch: 280 | Loss: 0.686 | Acc: 81.442,97.715,99.833,% | Adaptive Acc: 91.114% | clf_exit: 0.809 0.178 0.013
Batch: 300 | Loss: 0.688 | Acc: 81.341,97.732,99.829,% | Adaptive Acc: 91.066% | clf_exit: 0.808 0.179 0.013
Batch: 320 | Loss: 0.690 | Acc: 81.223,97.717,99.822,% | Adaptive Acc: 91.019% | clf_exit: 0.807 0.180 0.013
Batch: 340 | Loss: 0.691 | Acc: 81.227,97.720,99.821,% | Adaptive Acc: 91.001% | clf_exit: 0.807 0.179 0.013
Batch: 360 | Loss: 0.692 | Acc: 81.194,97.723,99.818,% | Adaptive Acc: 90.997% | clf_exit: 0.807 0.180 0.013
Batch: 380 | Loss: 0.696 | Acc: 81.033,97.710,99.815,% | Adaptive Acc: 90.908% | clf_exit: 0.806 0.181 0.013
Batch: 0 | Loss: 1.031 | Acc: 81.250,91.406,92.969,% | Adaptive Acc: 85.938% | clf_exit: 0.836 0.133 0.031
Batch: 20 | Loss: 1.219 | Acc: 78.385,89.360,93.118,% | Adaptive Acc: 84.301% | clf_exit: 0.833 0.145 0.022
Batch: 40 | Loss: 1.219 | Acc: 78.830,89.558,93.121,% | Adaptive Acc: 84.813% | clf_exit: 0.836 0.143 0.021
Batch: 60 | Loss: 1.204 | Acc: 78.932,89.613,93.135,% | Adaptive Acc: 84.874% | clf_exit: 0.834 0.144 0.022
Train all parameters

Epoch: 180
Batch: 0 | Loss: 0.677 | Acc: 83.594,98.438,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.758 0.219 0.023
Batch: 20 | Loss: 0.690 | Acc: 81.138,97.842,99.851,% | Adaptive Acc: 91.257% | clf_exit: 0.804 0.183 0.013
Batch: 40 | Loss: 0.665 | Acc: 82.069,98.171,99.867,% | Adaptive Acc: 91.406% | clf_exit: 0.812 0.175 0.012
Batch: 60 | Loss: 0.670 | Acc: 82.070,98.040,99.808,% | Adaptive Acc: 91.368% | clf_exit: 0.809 0.180 0.011
Batch: 80 | Loss: 0.672 | Acc: 82.051,97.984,99.817,% | Adaptive Acc: 91.348% | clf_exit: 0.808 0.181 0.011
Batch: 100 | Loss: 0.683 | Acc: 81.490,97.981,99.845,% | Adaptive Acc: 91.050% | clf_exit: 0.805 0.183 0.012
Batch: 120 | Loss: 0.680 | Acc: 81.450,97.998,99.826,% | Adaptive Acc: 91.142% | clf_exit: 0.803 0.186 0.011
Batch: 140 | Loss: 0.684 | Acc: 81.355,98.022,99.834,% | Adaptive Acc: 91.168% | clf_exit: 0.801 0.187 0.012
Batch: 160 | Loss: 0.686 | Acc: 81.303,97.899,99.811,% | Adaptive Acc: 91.139% | clf_exit: 0.801 0.186 0.013
Batch: 180 | Loss: 0.689 | Acc: 81.181,97.855,99.797,% | Adaptive Acc: 91.001% | clf_exit: 0.803 0.184 0.013
Batch: 200 | Loss: 0.692 | Acc: 81.091,97.788,99.798,% | Adaptive Acc: 90.986% | clf_exit: 0.802 0.186 0.013
Batch: 220 | Loss: 0.693 | Acc: 81.056,97.805,99.791,% | Adaptive Acc: 90.918% | clf_exit: 0.802 0.185 0.013
Batch: 240 | Loss: 0.694 | Acc: 81.030,97.822,99.799,% | Adaptive Acc: 90.901% | clf_exit: 0.802 0.185 0.013
Batch: 260 | Loss: 0.695 | Acc: 81.085,97.806,99.799,% | Adaptive Acc: 90.909% | clf_exit: 0.803 0.184 0.013
Batch: 280 | Loss: 0.696 | Acc: 81.028,97.759,99.791,% | Adaptive Acc: 90.859% | clf_exit: 0.803 0.184 0.013
Batch: 300 | Loss: 0.699 | Acc: 80.996,97.750,99.792,% | Adaptive Acc: 90.825% | clf_exit: 0.802 0.184 0.013
Batch: 320 | Loss: 0.703 | Acc: 80.941,97.710,99.783,% | Adaptive Acc: 90.730% | clf_exit: 0.802 0.185 0.014
Batch: 340 | Loss: 0.703 | Acc: 80.943,97.679,99.787,% | Adaptive Acc: 90.760% | clf_exit: 0.802 0.184 0.014
Batch: 360 | Loss: 0.703 | Acc: 80.921,97.656,99.792,% | Adaptive Acc: 90.783% | clf_exit: 0.802 0.184 0.014
Batch: 380 | Loss: 0.704 | Acc: 80.903,97.632,99.799,% | Adaptive Acc: 90.777% | clf_exit: 0.802 0.185 0.014
Batch: 0 | Loss: 1.042 | Acc: 79.688,93.750,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.805 0.164 0.031
Batch: 20 | Loss: 1.230 | Acc: 78.162,89.881,93.192,% | Adaptive Acc: 85.156% | clf_exit: 0.799 0.174 0.026
Batch: 40 | Loss: 1.246 | Acc: 78.316,89.615,93.064,% | Adaptive Acc: 85.194% | clf_exit: 0.805 0.170 0.025
Batch: 60 | Loss: 1.231 | Acc: 78.420,89.626,93.174,% | Adaptive Acc: 85.092% | clf_exit: 0.805 0.170 0.024
Train all parameters

Epoch: 181
Batch: 0 | Loss: 0.727 | Acc: 84.375,97.656,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.758 0.227 0.016
Batch: 20 | Loss: 0.667 | Acc: 82.515,98.140,99.777,% | Adaptive Acc: 92.262% | clf_exit: 0.804 0.183 0.013
Batch: 40 | Loss: 0.671 | Acc: 82.146,98.228,99.752,% | Adaptive Acc: 91.940% | clf_exit: 0.804 0.183 0.013
Batch: 60 | Loss: 0.661 | Acc: 81.942,98.207,99.769,% | Adaptive Acc: 91.701% | clf_exit: 0.806 0.182 0.012
Batch: 80 | Loss: 0.657 | Acc: 81.867,98.225,99.807,% | Adaptive Acc: 91.696% | clf_exit: 0.810 0.178 0.011
Batch: 100 | Loss: 0.667 | Acc: 81.675,98.113,99.830,% | Adaptive Acc: 91.484% | clf_exit: 0.809 0.179 0.012
Batch: 120 | Loss: 0.670 | Acc: 81.605,98.044,99.832,% | Adaptive Acc: 91.406% | clf_exit: 0.808 0.179 0.013
Batch: 140 | Loss: 0.677 | Acc: 81.477,97.989,99.834,% | Adaptive Acc: 91.234% | clf_exit: 0.808 0.179 0.013
Batch: 160 | Loss: 0.681 | Acc: 81.318,97.972,99.825,% | Adaptive Acc: 91.173% | clf_exit: 0.805 0.182 0.013
Batch: 180 | Loss: 0.682 | Acc: 81.418,97.958,99.814,% | Adaptive Acc: 91.255% | clf_exit: 0.805 0.182 0.013
Batch: 200 | Loss: 0.683 | Acc: 81.460,97.897,99.810,% | Adaptive Acc: 91.189% | clf_exit: 0.805 0.182 0.013
Batch: 220 | Loss: 0.684 | Acc: 81.448,97.914,99.813,% | Adaptive Acc: 91.198% | clf_exit: 0.804 0.183 0.013
Batch: 240 | Loss: 0.684 | Acc: 81.383,97.925,99.822,% | Adaptive Acc: 91.131% | clf_exit: 0.804 0.183 0.013
Batch: 260 | Loss: 0.687 | Acc: 81.244,97.899,99.817,% | Adaptive Acc: 91.098% | clf_exit: 0.803 0.184 0.013
Batch: 280 | Loss: 0.687 | Acc: 81.289,97.890,99.805,% | Adaptive Acc: 91.073% | clf_exit: 0.804 0.183 0.013
Batch: 300 | Loss: 0.689 | Acc: 81.224,97.879,99.805,% | Adaptive Acc: 90.991% | clf_exit: 0.805 0.182 0.013
Batch: 320 | Loss: 0.689 | Acc: 81.223,97.873,99.805,% | Adaptive Acc: 90.956% | clf_exit: 0.805 0.182 0.013
Batch: 340 | Loss: 0.690 | Acc: 81.181,97.856,99.808,% | Adaptive Acc: 90.948% | clf_exit: 0.805 0.182 0.013
Batch: 360 | Loss: 0.690 | Acc: 81.196,97.836,99.810,% | Adaptive Acc: 90.932% | clf_exit: 0.805 0.182 0.013
Batch: 380 | Loss: 0.690 | Acc: 81.166,97.820,99.813,% | Adaptive Acc: 90.926% | clf_exit: 0.805 0.182 0.013
Batch: 0 | Loss: 0.981 | Acc: 79.688,94.531,95.312,% | Adaptive Acc: 87.500% | clf_exit: 0.844 0.148 0.008
Batch: 20 | Loss: 1.171 | Acc: 78.720,90.327,93.266,% | Adaptive Acc: 85.156% | clf_exit: 0.827 0.154 0.019
Batch: 40 | Loss: 1.176 | Acc: 78.525,90.206,93.312,% | Adaptive Acc: 84.928% | clf_exit: 0.835 0.145 0.020
Batch: 60 | Loss: 1.169 | Acc: 78.701,90.420,93.186,% | Adaptive Acc: 84.900% | clf_exit: 0.839 0.141 0.020
Train all parameters

Epoch: 182
Batch: 0 | Loss: 0.588 | Acc: 82.812,98.438,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.805 0.188 0.008
Batch: 20 | Loss: 0.687 | Acc: 81.362,97.917,99.851,% | Adaptive Acc: 91.295% | clf_exit: 0.799 0.183 0.017
Batch: 40 | Loss: 0.687 | Acc: 81.059,97.980,99.809,% | Adaptive Acc: 91.120% | clf_exit: 0.799 0.186 0.015
Batch: 60 | Loss: 0.684 | Acc: 81.186,97.938,99.834,% | Adaptive Acc: 91.150% | clf_exit: 0.803 0.183 0.014
Batch: 80 | Loss: 0.687 | Acc: 81.144,97.936,99.817,% | Adaptive Acc: 91.107% | clf_exit: 0.802 0.184 0.014
Batch: 100 | Loss: 0.689 | Acc: 81.049,97.904,99.791,% | Adaptive Acc: 91.112% | clf_exit: 0.800 0.186 0.013
Batch: 120 | Loss: 0.690 | Acc: 81.024,97.889,99.800,% | Adaptive Acc: 91.032% | clf_exit: 0.801 0.186 0.013
Batch: 140 | Loss: 0.689 | Acc: 81.161,97.944,99.789,% | Adaptive Acc: 91.013% | clf_exit: 0.803 0.184 0.013
Batch: 160 | Loss: 0.690 | Acc: 81.119,97.967,99.796,% | Adaptive Acc: 90.999% | clf_exit: 0.803 0.184 0.013
Batch: 180 | Loss: 0.691 | Acc: 81.112,97.924,99.806,% | Adaptive Acc: 91.005% | clf_exit: 0.803 0.184 0.013
Batch: 200 | Loss: 0.692 | Acc: 81.122,97.897,99.802,% | Adaptive Acc: 91.018% | clf_exit: 0.801 0.186 0.013
Batch: 220 | Loss: 0.694 | Acc: 81.172,97.829,99.788,% | Adaptive Acc: 90.989% | clf_exit: 0.801 0.185 0.014
Batch: 240 | Loss: 0.693 | Acc: 81.198,97.844,99.786,% | Adaptive Acc: 91.037% | clf_exit: 0.801 0.185 0.014
Batch: 260 | Loss: 0.692 | Acc: 81.256,97.854,99.787,% | Adaptive Acc: 91.035% | clf_exit: 0.802 0.184 0.014
Batch: 280 | Loss: 0.693 | Acc: 81.167,97.826,99.791,% | Adaptive Acc: 91.009% | clf_exit: 0.802 0.184 0.014
Batch: 300 | Loss: 0.694 | Acc: 81.149,97.817,99.800,% | Adaptive Acc: 90.991% | clf_exit: 0.802 0.184 0.014
Batch: 320 | Loss: 0.695 | Acc: 81.109,97.805,99.800,% | Adaptive Acc: 90.978% | clf_exit: 0.802 0.184 0.014
Batch: 340 | Loss: 0.695 | Acc: 81.085,97.801,99.794,% | Adaptive Acc: 90.985% | clf_exit: 0.802 0.184 0.014
Batch: 360 | Loss: 0.698 | Acc: 81.060,97.732,99.788,% | Adaptive Acc: 90.941% | clf_exit: 0.803 0.183 0.014
Batch: 380 | Loss: 0.697 | Acc: 81.068,97.724,99.789,% | Adaptive Acc: 90.922% | clf_exit: 0.804 0.183 0.014
Batch: 0 | Loss: 0.995 | Acc: 80.469,93.750,95.312,% | Adaptive Acc: 89.062% | clf_exit: 0.859 0.133 0.008
Batch: 20 | Loss: 1.262 | Acc: 77.567,89.732,92.560,% | Adaptive Acc: 84.077% | clf_exit: 0.833 0.149 0.018
Batch: 40 | Loss: 1.260 | Acc: 78.258,89.310,92.702,% | Adaptive Acc: 84.108% | clf_exit: 0.839 0.142 0.019
Batch: 60 | Loss: 1.236 | Acc: 78.343,89.524,92.892,% | Adaptive Acc: 84.362% | clf_exit: 0.839 0.142 0.020
Train all parameters

Epoch: 183
Batch: 0 | Loss: 0.651 | Acc: 82.812,99.219,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.812 0.172 0.016
Batch: 20 | Loss: 0.661 | Acc: 82.180,97.805,99.777,% | Adaptive Acc: 90.960% | clf_exit: 0.820 0.166 0.013
Batch: 40 | Loss: 0.676 | Acc: 81.898,97.771,99.752,% | Adaptive Acc: 91.254% | clf_exit: 0.813 0.174 0.014
Batch: 60 | Loss: 0.679 | Acc: 81.801,97.784,99.782,% | Adaptive Acc: 91.048% | clf_exit: 0.814 0.174 0.012
Batch: 80 | Loss: 0.690 | Acc: 81.356,97.859,99.797,% | Adaptive Acc: 90.876% | clf_exit: 0.810 0.177 0.013
Batch: 100 | Loss: 0.695 | Acc: 81.057,97.765,99.776,% | Adaptive Acc: 90.757% | clf_exit: 0.808 0.179 0.013
Batch: 120 | Loss: 0.690 | Acc: 81.153,97.856,99.800,% | Adaptive Acc: 90.819% | clf_exit: 0.807 0.180 0.013
Batch: 140 | Loss: 0.690 | Acc: 81.117,97.856,99.789,% | Adaptive Acc: 90.841% | clf_exit: 0.806 0.181 0.013
Batch: 160 | Loss: 0.692 | Acc: 81.003,97.826,99.796,% | Adaptive Acc: 90.829% | clf_exit: 0.805 0.182 0.013
Batch: 180 | Loss: 0.691 | Acc: 81.155,97.730,99.806,% | Adaptive Acc: 90.901% | clf_exit: 0.806 0.181 0.013
Batch: 200 | Loss: 0.691 | Acc: 81.172,97.730,99.802,% | Adaptive Acc: 90.979% | clf_exit: 0.806 0.182 0.013
Batch: 220 | Loss: 0.691 | Acc: 81.151,97.727,99.806,% | Adaptive Acc: 90.933% | clf_exit: 0.806 0.181 0.013
Batch: 240 | Loss: 0.691 | Acc: 81.146,97.708,99.805,% | Adaptive Acc: 90.933% | clf_exit: 0.806 0.181 0.013
Batch: 260 | Loss: 0.693 | Acc: 81.145,97.707,99.796,% | Adaptive Acc: 90.924% | clf_exit: 0.806 0.181 0.013
Batch: 280 | Loss: 0.693 | Acc: 81.164,97.712,99.791,% | Adaptive Acc: 90.914% | clf_exit: 0.805 0.182 0.013
Batch: 300 | Loss: 0.694 | Acc: 81.081,97.674,99.782,% | Adaptive Acc: 90.846% | clf_exit: 0.805 0.182 0.013
Batch: 320 | Loss: 0.693 | Acc: 81.128,97.668,99.791,% | Adaptive Acc: 90.871% | clf_exit: 0.805 0.182 0.013
Batch: 340 | Loss: 0.693 | Acc: 81.135,97.695,99.785,% | Adaptive Acc: 90.859% | clf_exit: 0.805 0.182 0.013
Batch: 360 | Loss: 0.693 | Acc: 81.118,97.676,99.788,% | Adaptive Acc: 90.848% | clf_exit: 0.805 0.182 0.013
Batch: 380 | Loss: 0.694 | Acc: 81.098,97.689,99.785,% | Adaptive Acc: 90.851% | clf_exit: 0.805 0.182 0.013
Batch: 0 | Loss: 1.101 | Acc: 79.688,92.969,92.969,% | Adaptive Acc: 86.719% | clf_exit: 0.828 0.125 0.047
Batch: 20 | Loss: 1.269 | Acc: 76.600,89.807,92.894,% | Adaptive Acc: 83.854% | clf_exit: 0.833 0.143 0.024
Batch: 40 | Loss: 1.275 | Acc: 76.963,89.348,93.007,% | Adaptive Acc: 83.460% | clf_exit: 0.839 0.137 0.025
Batch: 60 | Loss: 1.263 | Acc: 77.126,89.434,92.969,% | Adaptive Acc: 83.453% | clf_exit: 0.839 0.138 0.023
Train all parameters

Epoch: 184
Batch: 0 | Loss: 0.831 | Acc: 73.438,95.312,99.219,% | Adaptive Acc: 87.500% | clf_exit: 0.734 0.242 0.023
Batch: 20 | Loss: 0.694 | Acc: 80.580,98.065,99.814,% | Adaptive Acc: 91.592% | clf_exit: 0.789 0.195 0.016
Batch: 40 | Loss: 0.701 | Acc: 80.583,97.828,99.829,% | Adaptive Acc: 91.082% | clf_exit: 0.796 0.188 0.016
Batch: 60 | Loss: 0.690 | Acc: 81.084,97.836,99.834,% | Adaptive Acc: 91.124% | clf_exit: 0.800 0.185 0.015
Batch: 80 | Loss: 0.689 | Acc: 81.134,97.936,99.836,% | Adaptive Acc: 91.059% | clf_exit: 0.801 0.184 0.015
Batch: 100 | Loss: 0.683 | Acc: 81.412,98.020,99.853,% | Adaptive Acc: 91.151% | clf_exit: 0.803 0.184 0.014
Batch: 120 | Loss: 0.684 | Acc: 81.302,97.992,99.858,% | Adaptive Acc: 91.154% | clf_exit: 0.803 0.183 0.014
Batch: 140 | Loss: 0.685 | Acc: 81.294,97.967,99.834,% | Adaptive Acc: 91.229% | clf_exit: 0.803 0.183 0.014
Batch: 160 | Loss: 0.687 | Acc: 81.269,97.909,99.825,% | Adaptive Acc: 91.178% | clf_exit: 0.804 0.181 0.015
Batch: 180 | Loss: 0.686 | Acc: 81.233,97.829,99.823,% | Adaptive Acc: 91.039% | clf_exit: 0.806 0.179 0.014
Batch: 200 | Loss: 0.685 | Acc: 81.266,97.792,99.821,% | Adaptive Acc: 91.049% | clf_exit: 0.806 0.179 0.014
Batch: 220 | Loss: 0.684 | Acc: 81.345,97.776,99.813,% | Adaptive Acc: 91.010% | clf_exit: 0.808 0.178 0.014
Batch: 240 | Loss: 0.687 | Acc: 81.276,97.786,99.805,% | Adaptive Acc: 90.936% | clf_exit: 0.807 0.178 0.014
Batch: 260 | Loss: 0.688 | Acc: 81.256,97.776,99.811,% | Adaptive Acc: 90.954% | clf_exit: 0.807 0.179 0.014
Batch: 280 | Loss: 0.690 | Acc: 81.147,97.754,99.811,% | Adaptive Acc: 90.922% | clf_exit: 0.806 0.180 0.014
Batch: 300 | Loss: 0.691 | Acc: 81.086,97.755,99.808,% | Adaptive Acc: 90.825% | clf_exit: 0.807 0.179 0.014
Batch: 320 | Loss: 0.691 | Acc: 81.043,97.734,99.810,% | Adaptive Acc: 90.803% | clf_exit: 0.807 0.179 0.014
Batch: 340 | Loss: 0.693 | Acc: 81.044,97.695,99.808,% | Adaptive Acc: 90.749% | clf_exit: 0.807 0.180 0.014
Batch: 360 | Loss: 0.690 | Acc: 81.111,97.697,99.801,% | Adaptive Acc: 90.824% | clf_exit: 0.807 0.179 0.014
Batch: 380 | Loss: 0.691 | Acc: 81.141,97.679,99.791,% | Adaptive Acc: 90.801% | clf_exit: 0.807 0.179 0.014
Batch: 0 | Loss: 1.062 | Acc: 79.688,92.188,92.969,% | Adaptive Acc: 85.938% | clf_exit: 0.797 0.180 0.023
Batch: 20 | Loss: 1.232 | Acc: 77.604,89.955,92.411,% | Adaptive Acc: 84.673% | clf_exit: 0.824 0.156 0.020
Batch: 40 | Loss: 1.218 | Acc: 77.954,89.748,92.816,% | Adaptive Acc: 84.470% | clf_exit: 0.836 0.143 0.021
Batch: 60 | Loss: 1.194 | Acc: 78.420,90.190,92.982,% | Adaptive Acc: 84.926% | clf_exit: 0.836 0.144 0.020
Train classifier parameters

Epoch: 185
Batch: 0 | Loss: 0.696 | Acc: 79.688,97.656,98.438,% | Adaptive Acc: 91.406% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 0.696 | Acc: 80.208,97.842,99.814,% | Adaptive Acc: 90.699% | clf_exit: 0.804 0.186 0.010
Batch: 40 | Loss: 0.711 | Acc: 80.069,97.637,99.771,% | Adaptive Acc: 90.396% | clf_exit: 0.798 0.190 0.012
Batch: 60 | Loss: 0.719 | Acc: 80.072,97.477,99.744,% | Adaptive Acc: 89.985% | clf_exit: 0.804 0.183 0.012
Batch: 80 | Loss: 0.723 | Acc: 80.131,97.492,99.701,% | Adaptive Acc: 89.882% | clf_exit: 0.806 0.182 0.013
Batch: 100 | Loss: 0.725 | Acc: 80.005,97.432,99.714,% | Adaptive Acc: 89.913% | clf_exit: 0.804 0.183 0.013
Batch: 120 | Loss: 0.730 | Acc: 79.894,97.346,99.748,% | Adaptive Acc: 89.831% | clf_exit: 0.802 0.185 0.013
Batch: 140 | Loss: 0.729 | Acc: 79.987,97.296,99.751,% | Adaptive Acc: 89.932% | clf_exit: 0.802 0.185 0.013
Batch: 160 | Loss: 0.734 | Acc: 79.940,97.224,99.743,% | Adaptive Acc: 89.917% | clf_exit: 0.802 0.185 0.013
Batch: 180 | Loss: 0.735 | Acc: 79.903,97.203,99.706,% | Adaptive Acc: 89.874% | clf_exit: 0.802 0.184 0.013
Batch: 200 | Loss: 0.737 | Acc: 79.928,97.155,99.693,% | Adaptive Acc: 89.879% | clf_exit: 0.802 0.184 0.013
Batch: 220 | Loss: 0.738 | Acc: 79.875,97.137,99.685,% | Adaptive Acc: 89.900% | clf_exit: 0.800 0.186 0.014
Batch: 240 | Loss: 0.739 | Acc: 79.856,97.118,99.689,% | Adaptive Acc: 89.863% | clf_exit: 0.800 0.186 0.014
Batch: 260 | Loss: 0.741 | Acc: 79.786,97.102,99.680,% | Adaptive Acc: 89.817% | clf_exit: 0.801 0.185 0.014
Batch: 280 | Loss: 0.742 | Acc: 79.751,97.100,99.677,% | Adaptive Acc: 89.766% | clf_exit: 0.801 0.186 0.014
Batch: 300 | Loss: 0.742 | Acc: 79.700,97.098,99.670,% | Adaptive Acc: 89.724% | clf_exit: 0.801 0.185 0.014
Batch: 320 | Loss: 0.744 | Acc: 79.666,97.070,99.674,% | Adaptive Acc: 89.724% | clf_exit: 0.801 0.185 0.014
Batch: 340 | Loss: 0.745 | Acc: 79.598,97.083,99.679,% | Adaptive Acc: 89.718% | clf_exit: 0.800 0.186 0.014
Batch: 360 | Loss: 0.743 | Acc: 79.636,97.091,99.675,% | Adaptive Acc: 89.738% | clf_exit: 0.801 0.185 0.014
Batch: 380 | Loss: 0.744 | Acc: 79.636,97.105,99.678,% | Adaptive Acc: 89.751% | clf_exit: 0.801 0.185 0.014
Batch: 0 | Loss: 1.094 | Acc: 78.906,92.188,92.969,% | Adaptive Acc: 83.594% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.292 | Acc: 77.530,89.621,92.522,% | Adaptive Acc: 84.003% | clf_exit: 0.830 0.154 0.016
Batch: 40 | Loss: 1.277 | Acc: 77.458,89.596,92.835,% | Adaptive Acc: 84.089% | clf_exit: 0.836 0.145 0.018
Batch: 60 | Loss: 1.257 | Acc: 77.344,89.754,92.982,% | Adaptive Acc: 83.901% | clf_exit: 0.834 0.147 0.018
Train classifier parameters

Epoch: 186
Batch: 0 | Loss: 0.664 | Acc: 82.031,97.656,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.773 0.195 0.031
Batch: 20 | Loss: 0.715 | Acc: 80.580,97.321,99.777,% | Adaptive Acc: 90.625% | clf_exit: 0.803 0.184 0.013
Batch: 40 | Loss: 0.747 | Acc: 79.688,96.913,99.695,% | Adaptive Acc: 90.053% | clf_exit: 0.798 0.188 0.014
Batch: 60 | Loss: 0.751 | Acc: 79.226,97.003,99.718,% | Adaptive Acc: 89.908% | clf_exit: 0.796 0.189 0.015
Batch: 80 | Loss: 0.749 | Acc: 79.282,97.106,99.730,% | Adaptive Acc: 89.824% | clf_exit: 0.799 0.186 0.014
Batch: 100 | Loss: 0.753 | Acc: 79.278,96.929,99.722,% | Adaptive Acc: 89.674% | clf_exit: 0.801 0.184 0.015
Batch: 120 | Loss: 0.755 | Acc: 79.462,96.972,99.716,% | Adaptive Acc: 89.876% | clf_exit: 0.800 0.185 0.015
Batch: 140 | Loss: 0.752 | Acc: 79.427,97.086,99.723,% | Adaptive Acc: 89.877% | clf_exit: 0.800 0.185 0.015
Batch: 160 | Loss: 0.747 | Acc: 79.459,97.132,99.719,% | Adaptive Acc: 89.936% | clf_exit: 0.801 0.184 0.015
Batch: 180 | Loss: 0.744 | Acc: 79.472,97.147,99.728,% | Adaptive Acc: 89.960% | clf_exit: 0.802 0.183 0.015
Batch: 200 | Loss: 0.741 | Acc: 79.571,97.205,99.740,% | Adaptive Acc: 90.054% | clf_exit: 0.801 0.184 0.015
Batch: 220 | Loss: 0.743 | Acc: 79.493,97.186,99.724,% | Adaptive Acc: 89.989% | clf_exit: 0.800 0.185 0.015
Batch: 240 | Loss: 0.745 | Acc: 79.422,97.206,99.718,% | Adaptive Acc: 89.970% | clf_exit: 0.800 0.185 0.015
Batch: 260 | Loss: 0.746 | Acc: 79.385,97.192,99.710,% | Adaptive Acc: 89.963% | clf_exit: 0.800 0.186 0.015
Batch: 280 | Loss: 0.746 | Acc: 79.318,97.209,99.716,% | Adaptive Acc: 89.919% | clf_exit: 0.800 0.186 0.015
Batch: 300 | Loss: 0.744 | Acc: 79.358,97.228,99.696,% | Adaptive Acc: 89.997% | clf_exit: 0.799 0.186 0.015
Batch: 320 | Loss: 0.742 | Acc: 79.408,97.247,99.703,% | Adaptive Acc: 90.017% | clf_exit: 0.800 0.185 0.015
Batch: 340 | Loss: 0.743 | Acc: 79.321,97.248,99.704,% | Adaptive Acc: 89.933% | clf_exit: 0.800 0.185 0.015
Batch: 360 | Loss: 0.745 | Acc: 79.285,97.232,99.704,% | Adaptive Acc: 89.935% | clf_exit: 0.799 0.186 0.015
Batch: 380 | Loss: 0.745 | Acc: 79.337,97.238,99.703,% | Adaptive Acc: 89.957% | clf_exit: 0.800 0.186 0.015
Batch: 0 | Loss: 1.070 | Acc: 78.906,92.188,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.292 | Acc: 77.753,89.472,92.485,% | Adaptive Acc: 84.077% | clf_exit: 0.830 0.154 0.016
Batch: 40 | Loss: 1.277 | Acc: 77.611,89.634,92.893,% | Adaptive Acc: 84.280% | clf_exit: 0.834 0.148 0.018
Batch: 60 | Loss: 1.259 | Acc: 77.485,89.741,92.956,% | Adaptive Acc: 84.144% | clf_exit: 0.831 0.150 0.019
Train classifier parameters

Epoch: 187
Batch: 0 | Loss: 0.909 | Acc: 75.781,92.188,100.000,% | Adaptive Acc: 85.938% | clf_exit: 0.750 0.234 0.016
Batch: 20 | Loss: 0.725 | Acc: 80.432,96.689,99.777,% | Adaptive Acc: 90.699% | clf_exit: 0.799 0.182 0.019
Batch: 40 | Loss: 0.711 | Acc: 80.945,96.856,99.790,% | Adaptive Acc: 90.339% | clf_exit: 0.806 0.180 0.014
Batch: 60 | Loss: 0.709 | Acc: 80.866,97.054,99.744,% | Adaptive Acc: 90.228% | clf_exit: 0.810 0.176 0.014
Batch: 80 | Loss: 0.714 | Acc: 80.623,97.106,99.749,% | Adaptive Acc: 90.191% | clf_exit: 0.809 0.177 0.014
Batch: 100 | Loss: 0.720 | Acc: 80.492,96.968,99.752,% | Adaptive Acc: 90.238% | clf_exit: 0.806 0.180 0.015
Batch: 120 | Loss: 0.719 | Acc: 80.746,96.927,99.768,% | Adaptive Acc: 90.373% | clf_exit: 0.804 0.181 0.015
Batch: 140 | Loss: 0.722 | Acc: 80.447,97.036,99.756,% | Adaptive Acc: 90.326% | clf_exit: 0.802 0.183 0.015
Batch: 160 | Loss: 0.727 | Acc: 80.289,97.006,99.728,% | Adaptive Acc: 90.164% | clf_exit: 0.804 0.181 0.015
Batch: 180 | Loss: 0.728 | Acc: 80.292,97.017,99.737,% | Adaptive Acc: 90.055% | clf_exit: 0.804 0.182 0.014
Batch: 200 | Loss: 0.734 | Acc: 80.119,96.984,99.701,% | Adaptive Acc: 90.026% | clf_exit: 0.803 0.182 0.015
Batch: 220 | Loss: 0.734 | Acc: 80.172,97.045,99.692,% | Adaptive Acc: 90.031% | clf_exit: 0.802 0.183 0.015
Batch: 240 | Loss: 0.735 | Acc: 80.122,97.053,99.689,% | Adaptive Acc: 90.032% | clf_exit: 0.802 0.183 0.015
Batch: 260 | Loss: 0.738 | Acc: 79.933,97.037,99.689,% | Adaptive Acc: 89.963% | clf_exit: 0.800 0.185 0.014
Batch: 280 | Loss: 0.737 | Acc: 79.910,97.058,99.697,% | Adaptive Acc: 90.016% | clf_exit: 0.800 0.185 0.015
Batch: 300 | Loss: 0.739 | Acc: 79.822,97.044,99.709,% | Adaptive Acc: 89.971% | clf_exit: 0.800 0.186 0.015
Batch: 320 | Loss: 0.738 | Acc: 79.909,97.109,99.713,% | Adaptive Acc: 90.034% | clf_exit: 0.799 0.186 0.015
Batch: 340 | Loss: 0.739 | Acc: 79.885,97.102,99.720,% | Adaptive Acc: 90.025% | clf_exit: 0.799 0.186 0.015
Batch: 360 | Loss: 0.740 | Acc: 79.895,97.098,99.721,% | Adaptive Acc: 89.995% | clf_exit: 0.799 0.186 0.015
Batch: 380 | Loss: 0.740 | Acc: 79.864,97.107,99.721,% | Adaptive Acc: 90.030% | clf_exit: 0.798 0.187 0.015
Batch: 0 | Loss: 1.078 | Acc: 79.688,92.188,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 1.283 | Acc: 77.567,89.658,92.634,% | Adaptive Acc: 84.263% | clf_exit: 0.827 0.156 0.018
Batch: 40 | Loss: 1.268 | Acc: 77.553,89.691,92.893,% | Adaptive Acc: 84.261% | clf_exit: 0.832 0.149 0.019
Batch: 60 | Loss: 1.249 | Acc: 77.485,89.805,93.020,% | Adaptive Acc: 84.234% | clf_exit: 0.830 0.152 0.019
Train classifier parameters

Epoch: 188
Batch: 0 | Loss: 0.812 | Acc: 76.562,95.312,100.000,% | Adaptive Acc: 83.594% | clf_exit: 0.812 0.172 0.016
Batch: 20 | Loss: 0.753 | Acc: 78.906,96.801,99.554,% | Adaptive Acc: 88.876% | clf_exit: 0.804 0.181 0.015
Batch: 40 | Loss: 0.739 | Acc: 79.249,96.818,99.619,% | Adaptive Acc: 89.615% | clf_exit: 0.798 0.183 0.018
Batch: 60 | Loss: 0.752 | Acc: 78.829,96.760,99.654,% | Adaptive Acc: 89.511% | clf_exit: 0.798 0.184 0.019
Batch: 80 | Loss: 0.749 | Acc: 78.964,96.740,99.672,% | Adaptive Acc: 89.824% | clf_exit: 0.797 0.185 0.018
Batch: 100 | Loss: 0.741 | Acc: 79.432,96.844,99.667,% | Adaptive Acc: 89.929% | clf_exit: 0.797 0.184 0.019
Batch: 120 | Loss: 0.741 | Acc: 79.384,96.888,99.651,% | Adaptive Acc: 89.734% | clf_exit: 0.799 0.183 0.018
Batch: 140 | Loss: 0.740 | Acc: 79.494,96.964,99.645,% | Adaptive Acc: 89.871% | clf_exit: 0.798 0.185 0.017
Batch: 160 | Loss: 0.740 | Acc: 79.435,96.991,99.646,% | Adaptive Acc: 89.849% | clf_exit: 0.797 0.185 0.017
Batch: 180 | Loss: 0.740 | Acc: 79.472,97.000,99.650,% | Adaptive Acc: 89.831% | clf_exit: 0.798 0.185 0.017
Batch: 200 | Loss: 0.741 | Acc: 79.516,97.011,99.670,% | Adaptive Acc: 89.859% | clf_exit: 0.798 0.186 0.016
Batch: 220 | Loss: 0.738 | Acc: 79.649,97.062,99.675,% | Adaptive Acc: 89.911% | clf_exit: 0.798 0.185 0.016
Batch: 240 | Loss: 0.738 | Acc: 79.561,97.118,99.682,% | Adaptive Acc: 89.899% | clf_exit: 0.799 0.186 0.016
Batch: 260 | Loss: 0.741 | Acc: 79.568,97.105,99.683,% | Adaptive Acc: 89.960% | clf_exit: 0.797 0.187 0.016
Batch: 280 | Loss: 0.742 | Acc: 79.523,97.097,99.680,% | Adaptive Acc: 89.941% | clf_exit: 0.797 0.187 0.016
Batch: 300 | Loss: 0.743 | Acc: 79.553,97.096,99.686,% | Adaptive Acc: 89.968% | clf_exit: 0.797 0.187 0.016
Batch: 320 | Loss: 0.742 | Acc: 79.522,97.104,99.688,% | Adaptive Acc: 89.924% | clf_exit: 0.798 0.187 0.016
Batch: 340 | Loss: 0.742 | Acc: 79.539,97.118,99.691,% | Adaptive Acc: 89.967% | clf_exit: 0.798 0.187 0.016
Batch: 360 | Loss: 0.742 | Acc: 79.575,97.128,99.688,% | Adaptive Acc: 89.995% | clf_exit: 0.798 0.186 0.015
Batch: 380 | Loss: 0.742 | Acc: 79.605,97.117,99.684,% | Adaptive Acc: 89.967% | clf_exit: 0.799 0.186 0.015
Batch: 0 | Loss: 1.067 | Acc: 80.469,92.188,92.969,% | Adaptive Acc: 85.938% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.284 | Acc: 77.827,89.583,92.299,% | Adaptive Acc: 84.375% | clf_exit: 0.827 0.154 0.020
Batch: 40 | Loss: 1.268 | Acc: 77.687,89.444,92.759,% | Adaptive Acc: 84.489% | clf_exit: 0.833 0.147 0.021
Batch: 60 | Loss: 1.248 | Acc: 77.561,89.652,92.905,% | Adaptive Acc: 84.503% | clf_exit: 0.829 0.151 0.020
Train classifier parameters

Epoch: 189
Batch: 0 | Loss: 0.771 | Acc: 78.906,96.094,98.438,% | Adaptive Acc: 89.844% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 0.744 | Acc: 79.167,96.912,99.628,% | Adaptive Acc: 89.769% | clf_exit: 0.805 0.177 0.017
Batch: 40 | Loss: 0.763 | Acc: 79.135,96.837,99.657,% | Adaptive Acc: 89.863% | clf_exit: 0.794 0.188 0.018
Batch: 60 | Loss: 0.757 | Acc: 79.470,96.926,99.590,% | Adaptive Acc: 89.882% | clf_exit: 0.795 0.189 0.016
Batch: 80 | Loss: 0.755 | Acc: 79.620,96.943,99.556,% | Adaptive Acc: 89.950% | clf_exit: 0.796 0.187 0.017
Batch: 100 | Loss: 0.750 | Acc: 79.749,96.960,99.575,% | Adaptive Acc: 89.890% | clf_exit: 0.800 0.185 0.016
Batch: 120 | Loss: 0.749 | Acc: 79.791,96.952,99.587,% | Adaptive Acc: 89.740% | clf_exit: 0.803 0.182 0.015
Batch: 140 | Loss: 0.748 | Acc: 79.804,96.991,99.596,% | Adaptive Acc: 89.816% | clf_exit: 0.802 0.183 0.016
Batch: 160 | Loss: 0.745 | Acc: 79.954,97.059,99.612,% | Adaptive Acc: 90.043% | clf_exit: 0.800 0.184 0.016
Batch: 180 | Loss: 0.745 | Acc: 79.981,97.082,99.629,% | Adaptive Acc: 90.038% | clf_exit: 0.801 0.184 0.015
Batch: 200 | Loss: 0.742 | Acc: 80.049,97.170,99.639,% | Adaptive Acc: 90.108% | clf_exit: 0.801 0.184 0.015
Batch: 220 | Loss: 0.743 | Acc: 79.984,97.126,99.639,% | Adaptive Acc: 90.028% | clf_exit: 0.801 0.184 0.015
Batch: 240 | Loss: 0.743 | Acc: 79.911,97.134,99.643,% | Adaptive Acc: 90.022% | clf_exit: 0.800 0.185 0.015
Batch: 260 | Loss: 0.743 | Acc: 79.867,97.129,99.650,% | Adaptive Acc: 90.017% | clf_exit: 0.800 0.185 0.015
Batch: 280 | Loss: 0.743 | Acc: 79.832,97.145,99.664,% | Adaptive Acc: 90.052% | clf_exit: 0.799 0.185 0.016
Batch: 300 | Loss: 0.744 | Acc: 79.771,97.171,99.670,% | Adaptive Acc: 90.041% | clf_exit: 0.798 0.187 0.016
Batch: 320 | Loss: 0.741 | Acc: 79.855,97.189,99.676,% | Adaptive Acc: 90.131% | clf_exit: 0.798 0.186 0.016
Batch: 340 | Loss: 0.739 | Acc: 79.985,97.212,99.679,% | Adaptive Acc: 90.194% | clf_exit: 0.798 0.186 0.016
Batch: 360 | Loss: 0.738 | Acc: 79.952,97.189,99.684,% | Adaptive Acc: 90.147% | clf_exit: 0.799 0.186 0.016
Batch: 380 | Loss: 0.736 | Acc: 79.995,97.191,99.686,% | Adaptive Acc: 90.164% | clf_exit: 0.799 0.185 0.016
Batch: 0 | Loss: 1.083 | Acc: 79.688,92.188,92.969,% | Adaptive Acc: 85.938% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.279 | Acc: 77.679,89.621,92.560,% | Adaptive Acc: 84.152% | clf_exit: 0.826 0.156 0.019
Batch: 40 | Loss: 1.266 | Acc: 77.591,89.634,92.835,% | Adaptive Acc: 84.108% | clf_exit: 0.832 0.149 0.019
Batch: 60 | Loss: 1.246 | Acc: 77.715,89.754,92.956,% | Adaptive Acc: 84.144% | clf_exit: 0.830 0.151 0.019
Train classifier parameters

Epoch: 190
Batch: 0 | Loss: 0.861 | Acc: 76.562,96.094,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.734 0.227 0.039
Batch: 20 | Loss: 0.770 | Acc: 78.385,96.540,99.442,% | Adaptive Acc: 89.211% | clf_exit: 0.801 0.183 0.016
Batch: 40 | Loss: 0.752 | Acc: 79.402,96.894,99.562,% | Adaptive Acc: 89.596% | clf_exit: 0.801 0.185 0.014
Batch: 60 | Loss: 0.746 | Acc: 79.636,97.041,99.552,% | Adaptive Acc: 89.549% | clf_exit: 0.804 0.182 0.014
Batch: 80 | Loss: 0.748 | Acc: 79.659,96.991,99.527,% | Adaptive Acc: 89.660% | clf_exit: 0.800 0.185 0.015
Batch: 100 | Loss: 0.749 | Acc: 79.834,96.929,99.567,% | Adaptive Acc: 89.790% | clf_exit: 0.801 0.184 0.015
Batch: 120 | Loss: 0.744 | Acc: 79.901,97.024,99.567,% | Adaptive Acc: 89.921% | clf_exit: 0.803 0.183 0.015
Batch: 140 | Loss: 0.744 | Acc: 79.926,97.080,99.573,% | Adaptive Acc: 89.938% | clf_exit: 0.803 0.183 0.015
Batch: 160 | Loss: 0.740 | Acc: 79.945,97.103,99.592,% | Adaptive Acc: 89.989% | clf_exit: 0.802 0.184 0.015
Batch: 180 | Loss: 0.738 | Acc: 80.041,97.117,99.612,% | Adaptive Acc: 90.055% | clf_exit: 0.802 0.184 0.014
Batch: 200 | Loss: 0.742 | Acc: 79.998,97.042,99.615,% | Adaptive Acc: 90.093% | clf_exit: 0.799 0.186 0.014
Batch: 220 | Loss: 0.742 | Acc: 80.006,97.105,99.639,% | Adaptive Acc: 90.190% | clf_exit: 0.797 0.188 0.015
Batch: 240 | Loss: 0.741 | Acc: 79.960,97.115,99.637,% | Adaptive Acc: 90.165% | clf_exit: 0.797 0.188 0.014
Batch: 260 | Loss: 0.740 | Acc: 79.975,97.111,99.650,% | Adaptive Acc: 90.125% | clf_exit: 0.799 0.187 0.014
Batch: 280 | Loss: 0.740 | Acc: 79.963,97.122,99.652,% | Adaptive Acc: 90.141% | clf_exit: 0.798 0.188 0.014
Batch: 300 | Loss: 0.741 | Acc: 79.911,97.137,99.660,% | Adaptive Acc: 90.093% | clf_exit: 0.798 0.187 0.015
Batch: 320 | Loss: 0.743 | Acc: 79.853,97.143,99.667,% | Adaptive Acc: 90.073% | clf_exit: 0.798 0.188 0.015
Batch: 340 | Loss: 0.743 | Acc: 79.919,97.143,99.663,% | Adaptive Acc: 90.103% | clf_exit: 0.798 0.188 0.015
Batch: 360 | Loss: 0.742 | Acc: 79.926,97.146,99.671,% | Adaptive Acc: 90.088% | clf_exit: 0.798 0.187 0.015
Batch: 380 | Loss: 0.744 | Acc: 79.858,97.127,99.660,% | Adaptive Acc: 90.047% | clf_exit: 0.798 0.188 0.015
Batch: 0 | Loss: 1.077 | Acc: 80.469,91.406,92.188,% | Adaptive Acc: 85.938% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 1.277 | Acc: 77.865,89.583,92.374,% | Adaptive Acc: 84.598% | clf_exit: 0.824 0.158 0.018
Batch: 40 | Loss: 1.262 | Acc: 77.763,89.520,92.759,% | Adaptive Acc: 84.356% | clf_exit: 0.831 0.150 0.019
Batch: 60 | Loss: 1.241 | Acc: 77.805,89.690,92.918,% | Adaptive Acc: 84.362% | clf_exit: 0.829 0.153 0.019
Train classifier parameters

Epoch: 191
Batch: 0 | Loss: 0.891 | Acc: 77.344,96.094,98.438,% | Adaptive Acc: 89.062% | clf_exit: 0.789 0.172 0.039
Batch: 20 | Loss: 0.715 | Acc: 80.804,97.842,99.740,% | Adaptive Acc: 89.993% | clf_exit: 0.810 0.179 0.011
Batch: 40 | Loss: 0.736 | Acc: 79.630,97.447,99.790,% | Adaptive Acc: 89.272% | clf_exit: 0.808 0.179 0.012
Batch: 60 | Loss: 0.749 | Acc: 78.996,97.272,99.757,% | Adaptive Acc: 89.331% | clf_exit: 0.801 0.185 0.014
Batch: 80 | Loss: 0.748 | Acc: 79.090,97.213,99.749,% | Adaptive Acc: 89.468% | clf_exit: 0.801 0.184 0.015
Batch: 100 | Loss: 0.741 | Acc: 79.564,97.161,99.745,% | Adaptive Acc: 89.650% | clf_exit: 0.801 0.184 0.015
Batch: 120 | Loss: 0.737 | Acc: 79.784,97.140,99.748,% | Adaptive Acc: 89.760% | clf_exit: 0.802 0.184 0.014
Batch: 140 | Loss: 0.735 | Acc: 79.942,97.113,99.740,% | Adaptive Acc: 89.816% | clf_exit: 0.802 0.184 0.014
Batch: 160 | Loss: 0.735 | Acc: 79.998,97.118,99.748,% | Adaptive Acc: 89.834% | clf_exit: 0.803 0.183 0.014
Batch: 180 | Loss: 0.736 | Acc: 79.934,97.138,99.741,% | Adaptive Acc: 89.848% | clf_exit: 0.802 0.184 0.014
Batch: 200 | Loss: 0.736 | Acc: 79.878,97.178,99.740,% | Adaptive Acc: 89.953% | clf_exit: 0.800 0.185 0.014
Batch: 220 | Loss: 0.735 | Acc: 79.938,97.183,99.749,% | Adaptive Acc: 90.024% | clf_exit: 0.800 0.186 0.014
Batch: 240 | Loss: 0.734 | Acc: 80.005,97.186,99.750,% | Adaptive Acc: 90.058% | clf_exit: 0.801 0.185 0.014
Batch: 260 | Loss: 0.736 | Acc: 79.978,97.129,99.728,% | Adaptive Acc: 90.008% | clf_exit: 0.801 0.185 0.014
Batch: 280 | Loss: 0.737 | Acc: 79.929,97.097,99.714,% | Adaptive Acc: 89.986% | clf_exit: 0.802 0.184 0.014
Batch: 300 | Loss: 0.737 | Acc: 79.903,97.114,99.714,% | Adaptive Acc: 90.036% | clf_exit: 0.801 0.184 0.014
Batch: 320 | Loss: 0.739 | Acc: 79.863,97.116,99.703,% | Adaptive Acc: 90.041% | clf_exit: 0.800 0.185 0.015
Batch: 340 | Loss: 0.738 | Acc: 79.910,97.132,99.704,% | Adaptive Acc: 90.091% | clf_exit: 0.800 0.186 0.015
Batch: 360 | Loss: 0.738 | Acc: 79.954,97.120,99.693,% | Adaptive Acc: 90.099% | clf_exit: 0.800 0.185 0.015
Batch: 380 | Loss: 0.737 | Acc: 79.966,97.140,99.697,% | Adaptive Acc: 90.151% | clf_exit: 0.800 0.186 0.015
Batch: 0 | Loss: 1.059 | Acc: 79.688,92.188,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 1.276 | Acc: 78.051,89.472,92.448,% | Adaptive Acc: 84.449% | clf_exit: 0.823 0.158 0.019
Batch: 40 | Loss: 1.261 | Acc: 77.992,89.539,92.759,% | Adaptive Acc: 84.375% | clf_exit: 0.830 0.150 0.020
Batch: 60 | Loss: 1.241 | Acc: 77.856,89.703,92.841,% | Adaptive Acc: 84.554% | clf_exit: 0.827 0.153 0.020
Train classifier parameters

Epoch: 192
Batch: 0 | Loss: 0.700 | Acc: 85.938,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.781 0.211 0.008
Batch: 20 | Loss: 0.732 | Acc: 80.060,97.173,99.702,% | Adaptive Acc: 90.811% | clf_exit: 0.796 0.186 0.018
Batch: 40 | Loss: 0.762 | Acc: 79.440,96.951,99.657,% | Adaptive Acc: 90.111% | clf_exit: 0.796 0.188 0.016
Batch: 60 | Loss: 0.756 | Acc: 79.406,96.965,99.654,% | Adaptive Acc: 90.023% | clf_exit: 0.799 0.185 0.016
Batch: 80 | Loss: 0.753 | Acc: 79.475,96.875,99.624,% | Adaptive Acc: 89.940% | clf_exit: 0.799 0.185 0.016
Batch: 100 | Loss: 0.746 | Acc: 79.649,97.006,99.660,% | Adaptive Acc: 90.091% | clf_exit: 0.801 0.184 0.015
Batch: 120 | Loss: 0.741 | Acc: 79.791,97.056,99.664,% | Adaptive Acc: 90.225% | clf_exit: 0.801 0.184 0.015
Batch: 140 | Loss: 0.743 | Acc: 79.787,97.036,99.623,% | Adaptive Acc: 90.160% | clf_exit: 0.800 0.184 0.016
Batch: 160 | Loss: 0.741 | Acc: 79.886,97.055,99.622,% | Adaptive Acc: 90.140% | clf_exit: 0.802 0.183 0.015
Batch: 180 | Loss: 0.742 | Acc: 79.895,97.082,99.603,% | Adaptive Acc: 90.137% | clf_exit: 0.801 0.184 0.015
Batch: 200 | Loss: 0.739 | Acc: 79.936,97.120,99.623,% | Adaptive Acc: 90.209% | clf_exit: 0.800 0.185 0.015
Batch: 220 | Loss: 0.740 | Acc: 79.840,97.133,99.636,% | Adaptive Acc: 90.176% | clf_exit: 0.800 0.185 0.015
Batch: 240 | Loss: 0.739 | Acc: 79.853,97.131,99.637,% | Adaptive Acc: 90.194% | clf_exit: 0.801 0.184 0.015
Batch: 260 | Loss: 0.736 | Acc: 79.972,97.144,99.641,% | Adaptive Acc: 90.185% | clf_exit: 0.802 0.183 0.015
Batch: 280 | Loss: 0.739 | Acc: 79.865,97.125,99.636,% | Adaptive Acc: 90.108% | clf_exit: 0.801 0.184 0.015
Batch: 300 | Loss: 0.739 | Acc: 79.874,97.098,99.629,% | Adaptive Acc: 90.080% | clf_exit: 0.801 0.184 0.015
Batch: 320 | Loss: 0.737 | Acc: 79.970,97.082,99.637,% | Adaptive Acc: 90.131% | clf_exit: 0.800 0.184 0.015
Batch: 340 | Loss: 0.736 | Acc: 79.978,97.106,99.643,% | Adaptive Acc: 90.192% | clf_exit: 0.799 0.186 0.015
Batch: 360 | Loss: 0.737 | Acc: 79.988,97.083,99.654,% | Adaptive Acc: 90.171% | clf_exit: 0.799 0.185 0.015
Batch: 380 | Loss: 0.737 | Acc: 80.016,97.107,99.658,% | Adaptive Acc: 90.205% | clf_exit: 0.799 0.186 0.015
Batch: 0 | Loss: 1.074 | Acc: 80.469,92.188,92.969,% | Adaptive Acc: 86.719% | clf_exit: 0.781 0.203 0.016
Batch: 20 | Loss: 1.271 | Acc: 78.125,89.807,92.560,% | Adaptive Acc: 84.821% | clf_exit: 0.823 0.159 0.018
Batch: 40 | Loss: 1.257 | Acc: 77.896,89.691,92.835,% | Adaptive Acc: 84.642% | clf_exit: 0.829 0.151 0.019
Batch: 60 | Loss: 1.239 | Acc: 77.920,89.741,92.918,% | Adaptive Acc: 84.618% | clf_exit: 0.826 0.154 0.019
Train classifier parameters

Epoch: 193
Batch: 0 | Loss: 0.758 | Acc: 76.562,98.438,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 0.734 | Acc: 80.283,97.284,99.442,% | Adaptive Acc: 90.551% | clf_exit: 0.786 0.200 0.014
Batch: 40 | Loss: 0.743 | Acc: 80.050,97.161,99.619,% | Adaptive Acc: 90.168% | clf_exit: 0.792 0.194 0.015
Batch: 60 | Loss: 0.741 | Acc: 79.918,97.106,99.603,% | Adaptive Acc: 90.305% | clf_exit: 0.792 0.192 0.016
Batch: 80 | Loss: 0.734 | Acc: 80.199,97.068,99.643,% | Adaptive Acc: 90.500% | clf_exit: 0.793 0.191 0.016
Batch: 100 | Loss: 0.731 | Acc: 80.299,97.061,99.660,% | Adaptive Acc: 90.354% | clf_exit: 0.796 0.188 0.016
Batch: 120 | Loss: 0.730 | Acc: 80.424,97.107,99.638,% | Adaptive Acc: 90.405% | clf_exit: 0.798 0.186 0.016
Batch: 140 | Loss: 0.731 | Acc: 80.269,97.163,99.651,% | Adaptive Acc: 90.293% | clf_exit: 0.800 0.185 0.015
Batch: 160 | Loss: 0.732 | Acc: 80.289,97.147,99.660,% | Adaptive Acc: 90.256% | clf_exit: 0.799 0.186 0.015
Batch: 180 | Loss: 0.734 | Acc: 80.262,97.134,99.663,% | Adaptive Acc: 90.150% | clf_exit: 0.800 0.185 0.015
Batch: 200 | Loss: 0.735 | Acc: 80.243,97.108,99.670,% | Adaptive Acc: 90.205% | clf_exit: 0.799 0.186 0.016
Batch: 220 | Loss: 0.736 | Acc: 80.133,97.119,99.671,% | Adaptive Acc: 90.187% | clf_exit: 0.798 0.186 0.016
Batch: 240 | Loss: 0.736 | Acc: 80.138,97.138,99.689,% | Adaptive Acc: 90.223% | clf_exit: 0.797 0.187 0.016
Batch: 260 | Loss: 0.738 | Acc: 80.098,97.117,99.680,% | Adaptive Acc: 90.182% | clf_exit: 0.797 0.187 0.016
Batch: 280 | Loss: 0.737 | Acc: 80.074,97.120,99.691,% | Adaptive Acc: 90.205% | clf_exit: 0.797 0.187 0.016
Batch: 300 | Loss: 0.736 | Acc: 80.121,97.148,99.691,% | Adaptive Acc: 90.256% | clf_exit: 0.798 0.186 0.015
Batch: 320 | Loss: 0.739 | Acc: 80.031,97.126,99.684,% | Adaptive Acc: 90.155% | clf_exit: 0.799 0.186 0.015
Batch: 340 | Loss: 0.739 | Acc: 80.045,97.125,99.679,% | Adaptive Acc: 90.171% | clf_exit: 0.799 0.186 0.015
Batch: 360 | Loss: 0.738 | Acc: 80.118,97.137,99.675,% | Adaptive Acc: 90.194% | clf_exit: 0.798 0.186 0.016
Batch: 380 | Loss: 0.739 | Acc: 80.139,97.117,99.658,% | Adaptive Acc: 90.213% | clf_exit: 0.799 0.186 0.015
Batch: 0 | Loss: 1.068 | Acc: 82.031,92.188,92.969,% | Adaptive Acc: 86.719% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 1.267 | Acc: 78.646,89.658,92.485,% | Adaptive Acc: 84.598% | clf_exit: 0.824 0.159 0.017
Batch: 40 | Loss: 1.253 | Acc: 78.296,89.558,92.873,% | Adaptive Acc: 84.432% | clf_exit: 0.832 0.148 0.020
Batch: 60 | Loss: 1.233 | Acc: 78.253,89.780,93.007,% | Adaptive Acc: 84.516% | clf_exit: 0.830 0.151 0.019
Train classifier parameters

Epoch: 194
Batch: 0 | Loss: 0.624 | Acc: 88.281,99.219,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.812 0.180 0.008
Batch: 20 | Loss: 0.764 | Acc: 79.836,96.763,99.628,% | Adaptive Acc: 89.546% | clf_exit: 0.796 0.186 0.018
Batch: 40 | Loss: 0.739 | Acc: 80.678,96.856,99.619,% | Adaptive Acc: 90.091% | clf_exit: 0.801 0.181 0.018
Batch: 60 | Loss: 0.754 | Acc: 79.867,97.016,99.552,% | Adaptive Acc: 89.857% | clf_exit: 0.797 0.185 0.017
Batch: 80 | Loss: 0.756 | Acc: 79.620,96.962,99.605,% | Adaptive Acc: 89.911% | clf_exit: 0.794 0.189 0.017
Batch: 100 | Loss: 0.754 | Acc: 79.711,96.960,99.613,% | Adaptive Acc: 89.975% | clf_exit: 0.794 0.189 0.017
Batch: 120 | Loss: 0.753 | Acc: 79.662,96.998,99.638,% | Adaptive Acc: 89.960% | clf_exit: 0.797 0.186 0.017
Batch: 140 | Loss: 0.748 | Acc: 79.765,97.069,99.645,% | Adaptive Acc: 89.960% | clf_exit: 0.798 0.186 0.016
Batch: 160 | Loss: 0.747 | Acc: 79.726,97.084,99.636,% | Adaptive Acc: 89.984% | clf_exit: 0.796 0.188 0.016
Batch: 180 | Loss: 0.745 | Acc: 79.752,97.095,99.646,% | Adaptive Acc: 90.060% | clf_exit: 0.796 0.188 0.016
Batch: 200 | Loss: 0.744 | Acc: 79.781,97.112,99.662,% | Adaptive Acc: 90.089% | clf_exit: 0.797 0.188 0.016
Batch: 220 | Loss: 0.742 | Acc: 79.825,97.137,99.661,% | Adaptive Acc: 90.084% | clf_exit: 0.796 0.188 0.016
Batch: 240 | Loss: 0.740 | Acc: 79.856,97.160,99.669,% | Adaptive Acc: 90.093% | clf_exit: 0.797 0.187 0.015
Batch: 260 | Loss: 0.737 | Acc: 79.969,97.213,99.680,% | Adaptive Acc: 90.227% | clf_exit: 0.797 0.188 0.015
Batch: 280 | Loss: 0.736 | Acc: 79.993,97.225,99.680,% | Adaptive Acc: 90.250% | clf_exit: 0.797 0.188 0.015
Batch: 300 | Loss: 0.736 | Acc: 79.991,97.246,99.686,% | Adaptive Acc: 90.267% | clf_exit: 0.796 0.189 0.015
Batch: 320 | Loss: 0.735 | Acc: 79.989,97.230,99.688,% | Adaptive Acc: 90.250% | clf_exit: 0.797 0.188 0.015
Batch: 340 | Loss: 0.734 | Acc: 80.070,97.216,99.693,% | Adaptive Acc: 90.316% | clf_exit: 0.797 0.188 0.015
Batch: 360 | Loss: 0.733 | Acc: 80.125,97.204,99.686,% | Adaptive Acc: 90.344% | clf_exit: 0.797 0.188 0.015
Batch: 380 | Loss: 0.735 | Acc: 80.075,97.222,99.678,% | Adaptive Acc: 90.289% | clf_exit: 0.797 0.188 0.015
Batch: 0 | Loss: 1.068 | Acc: 81.250,92.188,92.969,% | Adaptive Acc: 85.938% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 1.270 | Acc: 78.534,89.732,92.448,% | Adaptive Acc: 84.449% | clf_exit: 0.822 0.157 0.021
Batch: 40 | Loss: 1.256 | Acc: 78.316,89.729,92.797,% | Adaptive Acc: 84.413% | clf_exit: 0.830 0.148 0.021
Batch: 60 | Loss: 1.236 | Acc: 78.253,89.831,92.956,% | Adaptive Acc: 84.503% | clf_exit: 0.829 0.151 0.020
Train classifier parameters

Epoch: 195
Batch: 0 | Loss: 0.539 | Acc: 83.594,98.438,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.859 0.141 0.000
Batch: 20 | Loss: 0.757 | Acc: 79.799,96.689,99.554,% | Adaptive Acc: 90.402% | clf_exit: 0.797 0.188 0.016
Batch: 40 | Loss: 0.721 | Acc: 80.583,97.351,99.714,% | Adaptive Acc: 90.396% | clf_exit: 0.809 0.177 0.014
Batch: 60 | Loss: 0.725 | Acc: 80.584,97.157,99.667,% | Adaptive Acc: 90.382% | clf_exit: 0.804 0.180 0.016
Batch: 80 | Loss: 0.729 | Acc: 80.594,97.203,99.701,% | Adaptive Acc: 90.471% | clf_exit: 0.804 0.180 0.016
Batch: 100 | Loss: 0.731 | Acc: 80.376,97.146,99.667,% | Adaptive Acc: 90.130% | clf_exit: 0.805 0.179 0.016
Batch: 120 | Loss: 0.737 | Acc: 80.204,97.153,99.638,% | Adaptive Acc: 90.037% | clf_exit: 0.802 0.183 0.015
Batch: 140 | Loss: 0.739 | Acc: 80.136,97.108,99.651,% | Adaptive Acc: 90.065% | clf_exit: 0.800 0.184 0.015
Batch: 160 | Loss: 0.734 | Acc: 80.270,97.171,99.660,% | Adaptive Acc: 90.213% | clf_exit: 0.800 0.185 0.015
Batch: 180 | Loss: 0.732 | Acc: 80.326,97.242,99.672,% | Adaptive Acc: 90.258% | clf_exit: 0.801 0.184 0.016
Batch: 200 | Loss: 0.734 | Acc: 80.298,97.233,99.670,% | Adaptive Acc: 90.318% | clf_exit: 0.799 0.185 0.016
Batch: 220 | Loss: 0.732 | Acc: 80.423,97.211,99.678,% | Adaptive Acc: 90.402% | clf_exit: 0.800 0.185 0.015
Batch: 240 | Loss: 0.734 | Acc: 80.391,97.180,99.666,% | Adaptive Acc: 90.366% | clf_exit: 0.800 0.185 0.016
Batch: 260 | Loss: 0.735 | Acc: 80.358,97.225,99.665,% | Adaptive Acc: 90.365% | clf_exit: 0.798 0.186 0.016
Batch: 280 | Loss: 0.735 | Acc: 80.302,97.228,99.666,% | Adaptive Acc: 90.436% | clf_exit: 0.798 0.187 0.015
Batch: 300 | Loss: 0.736 | Acc: 80.251,97.218,99.660,% | Adaptive Acc: 90.410% | clf_exit: 0.797 0.188 0.015
Batch: 320 | Loss: 0.735 | Acc: 80.247,97.252,99.671,% | Adaptive Acc: 90.421% | clf_exit: 0.797 0.188 0.015
Batch: 340 | Loss: 0.733 | Acc: 80.269,97.283,99.684,% | Adaptive Acc: 90.423% | clf_exit: 0.798 0.187 0.015
Batch: 360 | Loss: 0.733 | Acc: 80.274,97.262,99.682,% | Adaptive Acc: 90.419% | clf_exit: 0.798 0.187 0.015
Batch: 380 | Loss: 0.730 | Acc: 80.370,97.279,99.676,% | Adaptive Acc: 90.484% | clf_exit: 0.798 0.187 0.015
Batch: 0 | Loss: 1.060 | Acc: 82.031,92.188,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.789 0.195 0.016
Batch: 20 | Loss: 1.268 | Acc: 78.311,89.844,92.448,% | Adaptive Acc: 84.598% | clf_exit: 0.821 0.161 0.019
Batch: 40 | Loss: 1.253 | Acc: 78.182,89.691,92.835,% | Adaptive Acc: 84.489% | clf_exit: 0.830 0.150 0.020
Batch: 60 | Loss: 1.234 | Acc: 78.151,89.869,92.930,% | Adaptive Acc: 84.606% | clf_exit: 0.827 0.153 0.019
Train classifier parameters

Epoch: 196
Batch: 0 | Loss: 0.604 | Acc: 82.031,96.875,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.820 0.180 0.000
Batch: 20 | Loss: 0.696 | Acc: 81.101,97.842,99.851,% | Adaptive Acc: 90.997% | clf_exit: 0.802 0.183 0.015
Batch: 40 | Loss: 0.712 | Acc: 80.469,97.580,99.848,% | Adaptive Acc: 90.473% | clf_exit: 0.798 0.189 0.013
Batch: 60 | Loss: 0.717 | Acc: 80.597,97.490,99.782,% | Adaptive Acc: 90.535% | clf_exit: 0.794 0.193 0.013
Batch: 80 | Loss: 0.727 | Acc: 80.488,97.348,99.797,% | Adaptive Acc: 90.432% | clf_exit: 0.794 0.192 0.013
Batch: 100 | Loss: 0.722 | Acc: 80.670,97.401,99.791,% | Adaptive Acc: 90.555% | clf_exit: 0.797 0.190 0.013
Batch: 120 | Loss: 0.730 | Acc: 80.378,97.301,99.748,% | Adaptive Acc: 90.444% | clf_exit: 0.796 0.191 0.014
Batch: 140 | Loss: 0.730 | Acc: 80.208,97.318,99.740,% | Adaptive Acc: 90.453% | clf_exit: 0.795 0.191 0.014
Batch: 160 | Loss: 0.729 | Acc: 80.270,97.321,99.728,% | Adaptive Acc: 90.606% | clf_exit: 0.794 0.192 0.014
Batch: 180 | Loss: 0.730 | Acc: 80.119,97.268,99.732,% | Adaptive Acc: 90.534% | clf_exit: 0.794 0.192 0.014
Batch: 200 | Loss: 0.730 | Acc: 80.080,97.252,99.740,% | Adaptive Acc: 90.586% | clf_exit: 0.793 0.193 0.014
Batch: 220 | Loss: 0.729 | Acc: 80.133,97.264,99.742,% | Adaptive Acc: 90.636% | clf_exit: 0.793 0.192 0.014
Batch: 240 | Loss: 0.728 | Acc: 80.158,97.287,99.734,% | Adaptive Acc: 90.583% | clf_exit: 0.795 0.191 0.014
Batch: 260 | Loss: 0.730 | Acc: 80.166,97.291,99.725,% | Adaptive Acc: 90.589% | clf_exit: 0.794 0.191 0.015
Batch: 280 | Loss: 0.728 | Acc: 80.269,97.295,99.725,% | Adaptive Acc: 90.633% | clf_exit: 0.795 0.190 0.015
Batch: 300 | Loss: 0.728 | Acc: 80.225,97.303,99.735,% | Adaptive Acc: 90.594% | clf_exit: 0.795 0.190 0.015
Batch: 320 | Loss: 0.732 | Acc: 80.087,97.262,99.730,% | Adaptive Acc: 90.528% | clf_exit: 0.794 0.191 0.015
Batch: 340 | Loss: 0.733 | Acc: 80.098,97.244,99.734,% | Adaptive Acc: 90.554% | clf_exit: 0.794 0.191 0.015
Batch: 360 | Loss: 0.732 | Acc: 80.138,97.262,99.736,% | Adaptive Acc: 90.584% | clf_exit: 0.794 0.191 0.015
Batch: 380 | Loss: 0.730 | Acc: 80.163,97.263,99.735,% | Adaptive Acc: 90.580% | clf_exit: 0.794 0.191 0.015
Batch: 0 | Loss: 1.046 | Acc: 81.250,91.406,92.188,% | Adaptive Acc: 85.938% | clf_exit: 0.812 0.164 0.023
Batch: 20 | Loss: 1.269 | Acc: 78.088,89.509,92.336,% | Adaptive Acc: 84.375% | clf_exit: 0.824 0.157 0.019
Batch: 40 | Loss: 1.253 | Acc: 77.992,89.539,92.778,% | Adaptive Acc: 84.356% | clf_exit: 0.832 0.147 0.021
Batch: 60 | Loss: 1.233 | Acc: 78.125,89.690,92.943,% | Adaptive Acc: 84.452% | clf_exit: 0.828 0.151 0.021
Train classifier parameters

Epoch: 197
Batch: 0 | Loss: 0.769 | Acc: 78.125,98.438,99.219,% | Adaptive Acc: 89.844% | clf_exit: 0.781 0.219 0.000
Batch: 20 | Loss: 0.729 | Acc: 79.948,97.656,99.740,% | Adaptive Acc: 90.923% | clf_exit: 0.786 0.195 0.020
Batch: 40 | Loss: 0.718 | Acc: 80.431,97.580,99.733,% | Adaptive Acc: 90.835% | clf_exit: 0.794 0.187 0.018
Batch: 60 | Loss: 0.728 | Acc: 80.302,97.439,99.744,% | Adaptive Acc: 90.548% | clf_exit: 0.795 0.188 0.017
Batch: 80 | Loss: 0.726 | Acc: 80.276,97.386,99.769,% | Adaptive Acc: 90.461% | clf_exit: 0.797 0.187 0.016
Batch: 100 | Loss: 0.724 | Acc: 80.345,97.347,99.752,% | Adaptive Acc: 90.633% | clf_exit: 0.795 0.189 0.016
Batch: 120 | Loss: 0.722 | Acc: 80.469,97.333,99.735,% | Adaptive Acc: 90.599% | clf_exit: 0.797 0.187 0.016
Batch: 140 | Loss: 0.721 | Acc: 80.568,97.363,99.723,% | Adaptive Acc: 90.575% | clf_exit: 0.798 0.185 0.016
Batch: 160 | Loss: 0.715 | Acc: 80.707,97.462,99.748,% | Adaptive Acc: 90.683% | clf_exit: 0.799 0.185 0.016
Batch: 180 | Loss: 0.713 | Acc: 80.879,97.488,99.745,% | Adaptive Acc: 90.793% | clf_exit: 0.799 0.185 0.015
Batch: 200 | Loss: 0.717 | Acc: 80.752,97.439,99.755,% | Adaptive Acc: 90.707% | clf_exit: 0.799 0.186 0.015
Batch: 220 | Loss: 0.717 | Acc: 80.706,97.430,99.753,% | Adaptive Acc: 90.706% | clf_exit: 0.798 0.187 0.015
Batch: 240 | Loss: 0.717 | Acc: 80.718,97.420,99.744,% | Adaptive Acc: 90.696% | clf_exit: 0.799 0.186 0.015
Batch: 260 | Loss: 0.724 | Acc: 80.526,97.336,99.722,% | Adaptive Acc: 90.529% | clf_exit: 0.798 0.187 0.015
Batch: 280 | Loss: 0.726 | Acc: 80.472,97.317,99.711,% | Adaptive Acc: 90.508% | clf_exit: 0.798 0.186 0.015
Batch: 300 | Loss: 0.726 | Acc: 80.443,97.280,99.694,% | Adaptive Acc: 90.547% | clf_exit: 0.797 0.188 0.015
Batch: 320 | Loss: 0.727 | Acc: 80.457,97.296,99.688,% | Adaptive Acc: 90.508% | clf_exit: 0.797 0.188 0.015
Batch: 340 | Loss: 0.728 | Acc: 80.414,97.269,99.686,% | Adaptive Acc: 90.513% | clf_exit: 0.797 0.188 0.015
Batch: 360 | Loss: 0.730 | Acc: 80.326,97.243,99.688,% | Adaptive Acc: 90.497% | clf_exit: 0.797 0.188 0.015
Batch: 380 | Loss: 0.730 | Acc: 80.303,97.222,99.690,% | Adaptive Acc: 90.502% | clf_exit: 0.797 0.188 0.015
Batch: 0 | Loss: 1.062 | Acc: 81.250,92.188,92.188,% | Adaptive Acc: 85.156% | clf_exit: 0.805 0.172 0.023
Batch: 20 | Loss: 1.258 | Acc: 78.423,89.621,92.560,% | Adaptive Acc: 84.487% | clf_exit: 0.823 0.157 0.019
Batch: 40 | Loss: 1.244 | Acc: 78.220,89.539,92.969,% | Adaptive Acc: 84.489% | clf_exit: 0.831 0.148 0.021
Batch: 60 | Loss: 1.226 | Acc: 78.253,89.741,93.071,% | Adaptive Acc: 84.529% | clf_exit: 0.828 0.151 0.021
Train classifier parameters

Epoch: 198
Batch: 0 | Loss: 0.698 | Acc: 80.469,96.875,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 0.730 | Acc: 79.427,97.284,99.591,% | Adaptive Acc: 90.216% | clf_exit: 0.788 0.197 0.015
Batch: 40 | Loss: 0.734 | Acc: 79.535,97.161,99.562,% | Adaptive Acc: 90.434% | clf_exit: 0.795 0.190 0.015
Batch: 60 | Loss: 0.722 | Acc: 80.213,97.323,99.552,% | Adaptive Acc: 90.676% | clf_exit: 0.798 0.187 0.015
Batch: 80 | Loss: 0.723 | Acc: 80.160,97.463,99.595,% | Adaptive Acc: 90.529% | clf_exit: 0.798 0.187 0.015
Batch: 100 | Loss: 0.727 | Acc: 80.260,97.416,99.590,% | Adaptive Acc: 90.470% | clf_exit: 0.797 0.187 0.016
Batch: 120 | Loss: 0.727 | Acc: 80.294,97.424,99.574,% | Adaptive Acc: 90.612% | clf_exit: 0.797 0.187 0.016
Batch: 140 | Loss: 0.727 | Acc: 80.380,97.352,99.596,% | Adaptive Acc: 90.675% | clf_exit: 0.796 0.188 0.016
Batch: 160 | Loss: 0.726 | Acc: 80.415,97.346,99.597,% | Adaptive Acc: 90.683% | clf_exit: 0.797 0.187 0.016
Batch: 180 | Loss: 0.727 | Acc: 80.404,97.311,99.612,% | Adaptive Acc: 90.707% | clf_exit: 0.796 0.188 0.016
Batch: 200 | Loss: 0.725 | Acc: 80.550,97.326,99.604,% | Adaptive Acc: 90.730% | clf_exit: 0.797 0.188 0.016
Batch: 220 | Loss: 0.725 | Acc: 80.592,97.313,99.604,% | Adaptive Acc: 90.667% | clf_exit: 0.798 0.187 0.016
Batch: 240 | Loss: 0.724 | Acc: 80.563,97.293,99.611,% | Adaptive Acc: 90.693% | clf_exit: 0.798 0.186 0.016
Batch: 260 | Loss: 0.725 | Acc: 80.541,97.285,99.617,% | Adaptive Acc: 90.679% | clf_exit: 0.797 0.187 0.016
Batch: 280 | Loss: 0.727 | Acc: 80.535,97.264,99.614,% | Adaptive Acc: 90.650% | clf_exit: 0.797 0.187 0.016
Batch: 300 | Loss: 0.725 | Acc: 80.547,97.272,99.621,% | Adaptive Acc: 90.674% | clf_exit: 0.797 0.187 0.016
Batch: 320 | Loss: 0.726 | Acc: 80.437,97.272,99.628,% | Adaptive Acc: 90.644% | clf_exit: 0.796 0.188 0.015
Batch: 340 | Loss: 0.727 | Acc: 80.375,97.255,99.631,% | Adaptive Acc: 90.634% | clf_exit: 0.795 0.189 0.015
Batch: 360 | Loss: 0.728 | Acc: 80.365,97.258,99.634,% | Adaptive Acc: 90.649% | clf_exit: 0.795 0.189 0.016
Batch: 380 | Loss: 0.729 | Acc: 80.360,97.254,99.641,% | Adaptive Acc: 90.621% | clf_exit: 0.796 0.189 0.016
Batch: 0 | Loss: 1.062 | Acc: 79.688,91.406,91.406,% | Adaptive Acc: 85.938% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 1.258 | Acc: 78.199,89.658,92.560,% | Adaptive Acc: 85.007% | clf_exit: 0.818 0.161 0.020
Batch: 40 | Loss: 1.245 | Acc: 78.030,89.482,92.931,% | Adaptive Acc: 84.794% | clf_exit: 0.826 0.153 0.021
Batch: 60 | Loss: 1.228 | Acc: 78.074,89.613,92.943,% | Adaptive Acc: 84.785% | clf_exit: 0.825 0.154 0.021
Train classifier parameters

Epoch: 199
Batch: 0 | Loss: 0.766 | Acc: 80.469,97.656,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.805 0.180 0.016
Batch: 20 | Loss: 0.713 | Acc: 81.287,97.470,99.777,% | Adaptive Acc: 90.625% | clf_exit: 0.807 0.178 0.015
Batch: 40 | Loss: 0.726 | Acc: 80.183,97.504,99.790,% | Adaptive Acc: 90.225% | clf_exit: 0.799 0.187 0.014
Batch: 60 | Loss: 0.726 | Acc: 80.174,97.490,99.808,% | Adaptive Acc: 90.548% | clf_exit: 0.793 0.192 0.015
Batch: 80 | Loss: 0.731 | Acc: 79.977,97.377,99.759,% | Adaptive Acc: 90.326% | clf_exit: 0.796 0.190 0.015
Batch: 100 | Loss: 0.728 | Acc: 80.128,97.393,99.783,% | Adaptive Acc: 90.501% | clf_exit: 0.795 0.190 0.015
Batch: 120 | Loss: 0.728 | Acc: 80.114,97.443,99.806,% | Adaptive Acc: 90.586% | clf_exit: 0.794 0.191 0.015
Batch: 140 | Loss: 0.725 | Acc: 80.330,97.451,99.778,% | Adaptive Acc: 90.631% | clf_exit: 0.795 0.190 0.015
Batch: 160 | Loss: 0.729 | Acc: 80.260,97.336,99.777,% | Adaptive Acc: 90.610% | clf_exit: 0.795 0.190 0.016
Batch: 180 | Loss: 0.731 | Acc: 80.223,97.242,99.763,% | Adaptive Acc: 90.543% | clf_exit: 0.795 0.189 0.016
Batch: 200 | Loss: 0.730 | Acc: 80.177,97.248,99.767,% | Adaptive Acc: 90.539% | clf_exit: 0.795 0.190 0.016
Batch: 220 | Loss: 0.729 | Acc: 80.214,97.200,99.753,% | Adaptive Acc: 90.508% | clf_exit: 0.796 0.189 0.016
Batch: 240 | Loss: 0.731 | Acc: 80.122,97.206,99.744,% | Adaptive Acc: 90.502% | clf_exit: 0.796 0.189 0.016
Batch: 260 | Loss: 0.730 | Acc: 80.166,97.192,99.743,% | Adaptive Acc: 90.499% | clf_exit: 0.795 0.189 0.016
Batch: 280 | Loss: 0.730 | Acc: 80.180,97.186,99.733,% | Adaptive Acc: 90.480% | clf_exit: 0.796 0.189 0.015
Batch: 300 | Loss: 0.731 | Acc: 80.168,97.168,99.720,% | Adaptive Acc: 90.487% | clf_exit: 0.796 0.189 0.016
Batch: 320 | Loss: 0.728 | Acc: 80.167,97.179,99.723,% | Adaptive Acc: 90.472% | clf_exit: 0.796 0.188 0.016
Batch: 340 | Loss: 0.727 | Acc: 80.221,97.203,99.720,% | Adaptive Acc: 90.515% | clf_exit: 0.797 0.187 0.015
Batch: 360 | Loss: 0.727 | Acc: 80.205,97.232,99.723,% | Adaptive Acc: 90.545% | clf_exit: 0.797 0.188 0.015
Batch: 380 | Loss: 0.730 | Acc: 80.169,97.213,99.719,% | Adaptive Acc: 90.522% | clf_exit: 0.796 0.188 0.015
Batch: 0 | Loss: 1.049 | Acc: 79.688,92.188,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 1.262 | Acc: 78.385,89.732,92.336,% | Adaptive Acc: 84.859% | clf_exit: 0.821 0.160 0.018
Batch: 40 | Loss: 1.248 | Acc: 78.239,89.691,92.797,% | Adaptive Acc: 84.813% | clf_exit: 0.830 0.150 0.020
Batch: 60 | Loss: 1.227 | Acc: 78.266,89.908,92.905,% | Adaptive Acc: 84.810% | clf_exit: 0.826 0.155 0.019
Train all parameters

Epoch: 200
Batch: 0 | Loss: 0.694 | Acc: 82.812,98.438,98.438,% | Adaptive Acc: 90.625% | clf_exit: 0.812 0.188 0.000
Batch: 20 | Loss: 0.702 | Acc: 81.585,97.545,99.740,% | Adaptive Acc: 90.290% | clf_exit: 0.816 0.172 0.012
Batch: 40 | Loss: 0.705 | Acc: 81.555,97.523,99.695,% | Adaptive Acc: 90.720% | clf_exit: 0.805 0.182 0.013
Batch: 60 | Loss: 0.712 | Acc: 81.301,97.400,99.705,% | Adaptive Acc: 90.791% | clf_exit: 0.803 0.182 0.014
Batch: 80 | Loss: 0.709 | Acc: 81.221,97.473,99.749,% | Adaptive Acc: 90.750% | clf_exit: 0.804 0.181 0.015
Batch: 100 | Loss: 0.709 | Acc: 81.211,97.424,99.729,% | Adaptive Acc: 90.702% | clf_exit: 0.804 0.181 0.015
Batch: 120 | Loss: 0.706 | Acc: 81.224,97.456,99.729,% | Adaptive Acc: 90.741% | clf_exit: 0.805 0.180 0.015
Batch: 140 | Loss: 0.704 | Acc: 81.228,97.507,99.740,% | Adaptive Acc: 90.708% | clf_exit: 0.806 0.179 0.015
Batch: 160 | Loss: 0.703 | Acc: 81.235,97.549,99.723,% | Adaptive Acc: 90.766% | clf_exit: 0.805 0.180 0.015
Batch: 180 | Loss: 0.701 | Acc: 81.332,97.531,99.715,% | Adaptive Acc: 90.953% | clf_exit: 0.804 0.181 0.015
Batch: 200 | Loss: 0.704 | Acc: 81.285,97.544,99.720,% | Adaptive Acc: 90.874% | clf_exit: 0.804 0.181 0.015
Batch: 220 | Loss: 0.706 | Acc: 81.137,97.518,99.735,% | Adaptive Acc: 90.872% | clf_exit: 0.802 0.182 0.015
Batch: 240 | Loss: 0.708 | Acc: 81.169,97.488,99.737,% | Adaptive Acc: 90.910% | clf_exit: 0.802 0.183 0.015
Batch: 260 | Loss: 0.708 | Acc: 81.214,97.453,99.725,% | Adaptive Acc: 90.891% | clf_exit: 0.802 0.182 0.015
Batch: 280 | Loss: 0.708 | Acc: 81.158,97.459,99.730,% | Adaptive Acc: 90.806% | clf_exit: 0.803 0.182 0.015
Batch: 300 | Loss: 0.708 | Acc: 81.138,97.454,99.725,% | Adaptive Acc: 90.814% | clf_exit: 0.802 0.183 0.015
Batch: 320 | Loss: 0.709 | Acc: 81.187,97.430,99.723,% | Adaptive Acc: 90.832% | clf_exit: 0.802 0.182 0.016
Batch: 340 | Loss: 0.711 | Acc: 81.113,97.434,99.727,% | Adaptive Acc: 90.788% | clf_exit: 0.802 0.182 0.016
Batch: 360 | Loss: 0.714 | Acc: 81.014,97.410,99.725,% | Adaptive Acc: 90.772% | clf_exit: 0.801 0.183 0.016
Batch: 380 | Loss: 0.714 | Acc: 80.957,97.414,99.719,% | Adaptive Acc: 90.756% | clf_exit: 0.801 0.183 0.016
Batch: 0 | Loss: 1.098 | Acc: 82.031,91.406,90.625,% | Adaptive Acc: 89.062% | clf_exit: 0.812 0.141 0.047
Batch: 20 | Loss: 1.235 | Acc: 77.567,89.993,92.634,% | Adaptive Acc: 84.598% | clf_exit: 0.826 0.151 0.023
Batch: 40 | Loss: 1.233 | Acc: 78.163,89.768,92.931,% | Adaptive Acc: 84.470% | clf_exit: 0.833 0.146 0.022
Batch: 60 | Loss: 1.215 | Acc: 78.048,89.921,93.007,% | Adaptive Acc: 84.554% | clf_exit: 0.829 0.149 0.022
Train all parameters

Epoch: 201
Batch: 0 | Loss: 0.626 | Acc: 83.594,98.438,99.219,% | Adaptive Acc: 90.625% | clf_exit: 0.836 0.164 0.000
Batch: 20 | Loss: 0.669 | Acc: 82.664,98.140,99.702,% | Adaptive Acc: 91.964% | clf_exit: 0.806 0.180 0.013
Batch: 40 | Loss: 0.668 | Acc: 82.336,98.133,99.809,% | Adaptive Acc: 91.711% | clf_exit: 0.809 0.177 0.014
Batch: 60 | Loss: 0.672 | Acc: 81.865,98.194,99.859,% | Adaptive Acc: 91.470% | clf_exit: 0.809 0.178 0.013
Batch: 80 | Loss: 0.684 | Acc: 81.539,98.187,99.875,% | Adaptive Acc: 91.300% | clf_exit: 0.805 0.182 0.013
Batch: 100 | Loss: 0.686 | Acc: 81.451,98.074,99.861,% | Adaptive Acc: 91.344% | clf_exit: 0.802 0.185 0.014
Batch: 120 | Loss: 0.684 | Acc: 81.541,98.108,99.858,% | Adaptive Acc: 91.309% | clf_exit: 0.803 0.183 0.014
Batch: 140 | Loss: 0.685 | Acc: 81.555,98.050,99.839,% | Adaptive Acc: 91.323% | clf_exit: 0.804 0.183 0.014
Batch: 160 | Loss: 0.686 | Acc: 81.488,97.972,99.830,% | Adaptive Acc: 91.266% | clf_exit: 0.806 0.181 0.014
Batch: 180 | Loss: 0.691 | Acc: 81.397,97.902,99.823,% | Adaptive Acc: 91.212% | clf_exit: 0.805 0.181 0.014
Batch: 200 | Loss: 0.692 | Acc: 81.370,97.858,99.825,% | Adaptive Acc: 91.220% | clf_exit: 0.804 0.182 0.014
Batch: 220 | Loss: 0.693 | Acc: 81.278,97.840,99.820,% | Adaptive Acc: 91.131% | clf_exit: 0.804 0.182 0.014
Batch: 240 | Loss: 0.694 | Acc: 81.250,97.809,99.805,% | Adaptive Acc: 91.108% | clf_exit: 0.804 0.183 0.014
Batch: 260 | Loss: 0.695 | Acc: 81.292,97.803,99.802,% | Adaptive Acc: 91.044% | clf_exit: 0.805 0.182 0.014
Batch: 280 | Loss: 0.696 | Acc: 81.211,97.762,99.783,% | Adaptive Acc: 90.967% | clf_exit: 0.804 0.182 0.014
Batch: 300 | Loss: 0.698 | Acc: 81.167,97.739,99.779,% | Adaptive Acc: 90.926% | clf_exit: 0.805 0.182 0.014
Batch: 320 | Loss: 0.699 | Acc: 81.150,97.715,99.788,% | Adaptive Acc: 90.939% | clf_exit: 0.804 0.182 0.014
Batch: 340 | Loss: 0.700 | Acc: 81.117,97.684,99.785,% | Adaptive Acc: 90.937% | clf_exit: 0.804 0.182 0.014
Batch: 360 | Loss: 0.700 | Acc: 81.118,97.667,99.786,% | Adaptive Acc: 90.967% | clf_exit: 0.804 0.183 0.014
Batch: 380 | Loss: 0.701 | Acc: 81.115,97.664,99.785,% | Adaptive Acc: 90.943% | clf_exit: 0.804 0.182 0.014
Batch: 0 | Loss: 1.027 | Acc: 85.938,91.406,91.406,% | Adaptive Acc: 89.062% | clf_exit: 0.828 0.141 0.031
Batch: 20 | Loss: 1.202 | Acc: 78.646,90.327,92.932,% | Adaptive Acc: 85.268% | clf_exit: 0.826 0.154 0.020
Batch: 40 | Loss: 1.207 | Acc: 78.887,89.977,92.912,% | Adaptive Acc: 85.156% | clf_exit: 0.833 0.145 0.022
Batch: 60 | Loss: 1.199 | Acc: 78.932,89.895,92.930,% | Adaptive Acc: 85.105% | clf_exit: 0.833 0.147 0.020
Train all parameters

Epoch: 202
Batch: 0 | Loss: 0.593 | Acc: 85.938,99.219,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.812 0.164 0.023
Batch: 20 | Loss: 0.682 | Acc: 80.990,97.842,99.665,% | Adaptive Acc: 90.923% | clf_exit: 0.795 0.193 0.012
Batch: 40 | Loss: 0.702 | Acc: 80.831,97.542,99.619,% | Adaptive Acc: 90.587% | clf_exit: 0.801 0.188 0.011
Batch: 60 | Loss: 0.706 | Acc: 80.610,97.515,99.705,% | Adaptive Acc: 90.484% | clf_exit: 0.799 0.189 0.012
Batch: 80 | Loss: 0.699 | Acc: 80.932,97.598,99.720,% | Adaptive Acc: 90.490% | clf_exit: 0.802 0.186 0.012
Batch: 100 | Loss: 0.695 | Acc: 81.211,97.594,99.691,% | Adaptive Acc: 90.640% | clf_exit: 0.803 0.185 0.012
Batch: 120 | Loss: 0.694 | Acc: 81.153,97.676,99.709,% | Adaptive Acc: 90.806% | clf_exit: 0.803 0.185 0.012
Batch: 140 | Loss: 0.695 | Acc: 81.217,97.617,99.723,% | Adaptive Acc: 90.880% | clf_exit: 0.802 0.186 0.012
Batch: 160 | Loss: 0.695 | Acc: 81.308,97.608,99.719,% | Adaptive Acc: 90.839% | clf_exit: 0.805 0.183 0.012
Batch: 180 | Loss: 0.700 | Acc: 81.211,97.566,99.711,% | Adaptive Acc: 90.763% | clf_exit: 0.804 0.184 0.012
Batch: 200 | Loss: 0.701 | Acc: 81.258,97.555,99.697,% | Adaptive Acc: 90.823% | clf_exit: 0.804 0.184 0.012
Batch: 220 | Loss: 0.700 | Acc: 81.250,97.550,99.710,% | Adaptive Acc: 90.805% | clf_exit: 0.803 0.184 0.012
Batch: 240 | Loss: 0.701 | Acc: 81.201,97.575,99.731,% | Adaptive Acc: 90.868% | clf_exit: 0.802 0.185 0.013
Batch: 260 | Loss: 0.701 | Acc: 81.199,97.593,99.725,% | Adaptive Acc: 90.778% | clf_exit: 0.803 0.184 0.012
Batch: 280 | Loss: 0.702 | Acc: 81.153,97.576,99.719,% | Adaptive Acc: 90.758% | clf_exit: 0.803 0.184 0.013
Batch: 300 | Loss: 0.703 | Acc: 81.105,97.584,99.717,% | Adaptive Acc: 90.744% | clf_exit: 0.803 0.184 0.013
Batch: 320 | Loss: 0.705 | Acc: 80.987,97.544,99.715,% | Adaptive Acc: 90.649% | clf_exit: 0.804 0.183 0.013
Batch: 340 | Loss: 0.705 | Acc: 80.918,97.544,99.725,% | Adaptive Acc: 90.581% | clf_exit: 0.804 0.183 0.013
Batch: 360 | Loss: 0.705 | Acc: 80.904,97.520,99.725,% | Adaptive Acc: 90.567% | clf_exit: 0.804 0.182 0.013
Batch: 380 | Loss: 0.707 | Acc: 80.885,97.511,99.727,% | Adaptive Acc: 90.574% | clf_exit: 0.804 0.183 0.014
Batch: 0 | Loss: 1.068 | Acc: 77.344,90.625,91.406,% | Adaptive Acc: 87.500% | clf_exit: 0.797 0.164 0.039
Batch: 20 | Loss: 1.191 | Acc: 77.753,90.365,93.452,% | Adaptive Acc: 84.487% | clf_exit: 0.829 0.149 0.022
Batch: 40 | Loss: 1.215 | Acc: 78.087,90.149,93.045,% | Adaptive Acc: 84.851% | clf_exit: 0.831 0.144 0.024
Batch: 60 | Loss: 1.210 | Acc: 78.509,90.100,92.892,% | Adaptive Acc: 84.977% | clf_exit: 0.832 0.145 0.024
Train all parameters

Epoch: 203
Batch: 0 | Loss: 0.702 | Acc: 82.031,97.656,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.750 0.234 0.016
Batch: 20 | Loss: 0.696 | Acc: 80.357,97.507,99.926,% | Adaptive Acc: 91.406% | clf_exit: 0.800 0.184 0.016
Batch: 40 | Loss: 0.696 | Acc: 80.793,97.732,99.886,% | Adaptive Acc: 91.139% | clf_exit: 0.803 0.183 0.014
Batch: 60 | Loss: 0.685 | Acc: 81.340,97.784,99.859,% | Adaptive Acc: 91.253% | clf_exit: 0.803 0.183 0.014
Batch: 80 | Loss: 0.681 | Acc: 81.568,97.801,99.865,% | Adaptive Acc: 91.387% | clf_exit: 0.804 0.182 0.014
Batch: 100 | Loss: 0.691 | Acc: 81.258,97.672,99.822,% | Adaptive Acc: 91.159% | clf_exit: 0.804 0.181 0.014
Batch: 120 | Loss: 0.694 | Acc: 81.173,97.611,99.813,% | Adaptive Acc: 91.103% | clf_exit: 0.805 0.181 0.014
Batch: 140 | Loss: 0.697 | Acc: 81.095,97.595,99.812,% | Adaptive Acc: 91.063% | clf_exit: 0.805 0.181 0.014
Batch: 160 | Loss: 0.694 | Acc: 81.168,97.637,99.811,% | Adaptive Acc: 91.008% | clf_exit: 0.805 0.181 0.014
Batch: 180 | Loss: 0.693 | Acc: 81.267,97.635,99.806,% | Adaptive Acc: 91.078% | clf_exit: 0.806 0.181 0.014
Batch: 200 | Loss: 0.696 | Acc: 81.234,97.625,99.790,% | Adaptive Acc: 91.064% | clf_exit: 0.806 0.181 0.014
Batch: 220 | Loss: 0.697 | Acc: 81.211,97.610,99.774,% | Adaptive Acc: 91.099% | clf_exit: 0.804 0.182 0.014
Batch: 240 | Loss: 0.696 | Acc: 81.234,97.608,99.754,% | Adaptive Acc: 91.089% | clf_exit: 0.804 0.182 0.014
Batch: 260 | Loss: 0.699 | Acc: 81.196,97.578,99.752,% | Adaptive Acc: 90.987% | clf_exit: 0.805 0.181 0.014
Batch: 280 | Loss: 0.700 | Acc: 81.172,97.542,99.747,% | Adaptive Acc: 90.961% | clf_exit: 0.805 0.181 0.014
Batch: 300 | Loss: 0.700 | Acc: 81.128,97.532,99.746,% | Adaptive Acc: 90.939% | clf_exit: 0.805 0.181 0.014
Batch: 320 | Loss: 0.702 | Acc: 81.114,97.500,99.735,% | Adaptive Acc: 90.873% | clf_exit: 0.805 0.181 0.014
Batch: 340 | Loss: 0.705 | Acc: 81.019,97.484,99.727,% | Adaptive Acc: 90.827% | clf_exit: 0.804 0.182 0.014
Batch: 360 | Loss: 0.705 | Acc: 80.932,97.492,99.736,% | Adaptive Acc: 90.768% | clf_exit: 0.804 0.182 0.014
Batch: 380 | Loss: 0.708 | Acc: 80.879,97.476,99.738,% | Adaptive Acc: 90.725% | clf_exit: 0.804 0.182 0.014
Batch: 0 | Loss: 1.205 | Acc: 80.469,89.844,92.188,% | Adaptive Acc: 85.938% | clf_exit: 0.797 0.164 0.039
Batch: 20 | Loss: 1.217 | Acc: 78.943,89.211,92.969,% | Adaptive Acc: 84.115% | clf_exit: 0.817 0.162 0.022
Batch: 40 | Loss: 1.232 | Acc: 78.849,89.196,92.854,% | Adaptive Acc: 84.356% | clf_exit: 0.824 0.152 0.025
Batch: 60 | Loss: 1.210 | Acc: 79.124,89.383,92.930,% | Adaptive Acc: 84.746% | clf_exit: 0.826 0.149 0.024
Train all parameters

Epoch: 204
Batch: 0 | Loss: 0.726 | Acc: 77.344,95.312,100.000,% | Adaptive Acc: 87.500% | clf_exit: 0.812 0.180 0.008
Batch: 20 | Loss: 0.711 | Acc: 80.246,97.507,99.628,% | Adaptive Acc: 89.881% | clf_exit: 0.811 0.180 0.009
Batch: 40 | Loss: 0.710 | Acc: 80.431,97.675,99.619,% | Adaptive Acc: 90.072% | clf_exit: 0.816 0.172 0.012
Batch: 60 | Loss: 0.697 | Acc: 80.930,97.643,99.705,% | Adaptive Acc: 90.651% | clf_exit: 0.812 0.176 0.012
Batch: 80 | Loss: 0.694 | Acc: 81.028,97.608,99.730,% | Adaptive Acc: 90.673% | clf_exit: 0.812 0.177 0.012
Batch: 100 | Loss: 0.697 | Acc: 81.026,97.594,99.752,% | Adaptive Acc: 90.702% | clf_exit: 0.809 0.179 0.012
Batch: 120 | Loss: 0.698 | Acc: 80.940,97.637,99.742,% | Adaptive Acc: 90.903% | clf_exit: 0.807 0.180 0.013
Batch: 140 | Loss: 0.693 | Acc: 80.990,97.667,99.740,% | Adaptive Acc: 90.963% | clf_exit: 0.806 0.181 0.013
Batch: 160 | Loss: 0.700 | Acc: 80.915,97.549,99.699,% | Adaptive Acc: 90.863% | clf_exit: 0.805 0.182 0.013
Batch: 180 | Loss: 0.703 | Acc: 80.961,97.484,99.685,% | Adaptive Acc: 90.841% | clf_exit: 0.804 0.182 0.013
Batch: 200 | Loss: 0.704 | Acc: 80.982,97.497,99.689,% | Adaptive Acc: 90.847% | clf_exit: 0.804 0.182 0.013
Batch: 220 | Loss: 0.704 | Acc: 80.988,97.494,99.678,% | Adaptive Acc: 90.872% | clf_exit: 0.804 0.182 0.013
Batch: 240 | Loss: 0.705 | Acc: 80.984,97.501,99.689,% | Adaptive Acc: 90.878% | clf_exit: 0.803 0.183 0.014
Batch: 260 | Loss: 0.708 | Acc: 80.957,97.465,99.698,% | Adaptive Acc: 90.814% | clf_exit: 0.803 0.182 0.014
Batch: 280 | Loss: 0.707 | Acc: 80.969,97.489,99.714,% | Adaptive Acc: 90.831% | clf_exit: 0.803 0.183 0.014
Batch: 300 | Loss: 0.707 | Acc: 80.980,97.488,99.717,% | Adaptive Acc: 90.843% | clf_exit: 0.803 0.182 0.014
Batch: 320 | Loss: 0.707 | Acc: 80.924,97.503,99.720,% | Adaptive Acc: 90.812% | clf_exit: 0.803 0.183 0.014
Batch: 340 | Loss: 0.706 | Acc: 80.975,97.496,99.718,% | Adaptive Acc: 90.779% | clf_exit: 0.804 0.182 0.014
Batch: 360 | Loss: 0.709 | Acc: 80.962,97.472,99.716,% | Adaptive Acc: 90.718% | clf_exit: 0.804 0.182 0.014
Batch: 380 | Loss: 0.707 | Acc: 80.994,97.470,99.711,% | Adaptive Acc: 90.773% | clf_exit: 0.804 0.182 0.014
Batch: 0 | Loss: 1.028 | Acc: 76.562,92.969,94.531,% | Adaptive Acc: 86.719% | clf_exit: 0.797 0.180 0.023
Batch: 20 | Loss: 1.243 | Acc: 77.344,89.732,92.820,% | Adaptive Acc: 83.557% | clf_exit: 0.838 0.141 0.020
Batch: 40 | Loss: 1.229 | Acc: 77.839,89.882,92.988,% | Adaptive Acc: 83.822% | clf_exit: 0.843 0.135 0.022
Batch: 60 | Loss: 1.212 | Acc: 77.613,90.023,93.148,% | Adaptive Acc: 83.760% | clf_exit: 0.842 0.136 0.022
Train all parameters

Epoch: 205
Batch: 0 | Loss: 0.621 | Acc: 81.250,98.438,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.711 | Acc: 80.952,97.507,99.740,% | Adaptive Acc: 91.034% | clf_exit: 0.801 0.185 0.014
Batch: 40 | Loss: 0.696 | Acc: 81.517,97.771,99.714,% | Adaptive Acc: 90.949% | clf_exit: 0.813 0.173 0.014
Batch: 60 | Loss: 0.687 | Acc: 81.673,97.759,99.718,% | Adaptive Acc: 90.894% | clf_exit: 0.813 0.174 0.013
Batch: 80 | Loss: 0.686 | Acc: 81.395,97.811,99.740,% | Adaptive Acc: 90.847% | clf_exit: 0.813 0.176 0.012
Batch: 100 | Loss: 0.681 | Acc: 81.598,97.788,99.737,% | Adaptive Acc: 90.919% | clf_exit: 0.816 0.173 0.012
Batch: 120 | Loss: 0.681 | Acc: 81.586,97.779,99.748,% | Adaptive Acc: 90.980% | clf_exit: 0.814 0.175 0.012
Batch: 140 | Loss: 0.691 | Acc: 81.222,97.645,99.740,% | Adaptive Acc: 90.786% | clf_exit: 0.813 0.175 0.012
Batch: 160 | Loss: 0.694 | Acc: 81.085,97.705,99.719,% | Adaptive Acc: 90.722% | clf_exit: 0.812 0.176 0.012
Batch: 180 | Loss: 0.700 | Acc: 80.896,97.652,99.711,% | Adaptive Acc: 90.694% | clf_exit: 0.809 0.178 0.012
Batch: 200 | Loss: 0.701 | Acc: 80.978,97.645,99.705,% | Adaptive Acc: 90.831% | clf_exit: 0.807 0.180 0.013
Batch: 220 | Loss: 0.700 | Acc: 81.031,97.624,99.692,% | Adaptive Acc: 90.844% | clf_exit: 0.806 0.181 0.013
Batch: 240 | Loss: 0.701 | Acc: 81.062,97.582,99.692,% | Adaptive Acc: 90.800% | clf_exit: 0.807 0.180 0.013
Batch: 260 | Loss: 0.702 | Acc: 80.990,97.531,99.692,% | Adaptive Acc: 90.745% | clf_exit: 0.807 0.180 0.013
Batch: 280 | Loss: 0.704 | Acc: 80.975,97.501,99.680,% | Adaptive Acc: 90.711% | clf_exit: 0.807 0.180 0.013
Batch: 300 | Loss: 0.705 | Acc: 80.939,97.501,99.678,% | Adaptive Acc: 90.667% | clf_exit: 0.807 0.179 0.013
Batch: 320 | Loss: 0.706 | Acc: 80.938,97.491,99.679,% | Adaptive Acc: 90.647% | clf_exit: 0.807 0.179 0.014
Batch: 340 | Loss: 0.707 | Acc: 80.916,97.468,99.675,% | Adaptive Acc: 90.673% | clf_exit: 0.806 0.180 0.014
Batch: 360 | Loss: 0.707 | Acc: 80.951,97.468,99.662,% | Adaptive Acc: 90.692% | clf_exit: 0.806 0.180 0.014
Batch: 380 | Loss: 0.709 | Acc: 80.899,97.480,99.656,% | Adaptive Acc: 90.641% | clf_exit: 0.806 0.180 0.014
Batch: 0 | Loss: 1.028 | Acc: 79.688,94.531,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 1.292 | Acc: 76.414,89.732,92.299,% | Adaptive Acc: 83.073% | clf_exit: 0.819 0.161 0.020
Batch: 40 | Loss: 1.289 | Acc: 77.134,89.425,92.778,% | Adaptive Acc: 83.365% | clf_exit: 0.825 0.153 0.021
Batch: 60 | Loss: 1.280 | Acc: 77.344,89.357,92.802,% | Adaptive Acc: 83.517% | clf_exit: 0.827 0.151 0.021
Train all parameters

Epoch: 206
Batch: 0 | Loss: 0.674 | Acc: 78.906,98.438,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.805 0.188 0.008
Batch: 20 | Loss: 0.678 | Acc: 81.882,97.991,99.702,% | Adaptive Acc: 91.257% | clf_exit: 0.812 0.175 0.013
Batch: 40 | Loss: 0.683 | Acc: 81.993,97.618,99.752,% | Adaptive Acc: 91.044% | clf_exit: 0.820 0.168 0.012
Batch: 60 | Loss: 0.688 | Acc: 81.673,97.759,99.782,% | Adaptive Acc: 91.150% | clf_exit: 0.813 0.174 0.013
Batch: 80 | Loss: 0.697 | Acc: 81.298,97.695,99.788,% | Adaptive Acc: 90.924% | clf_exit: 0.809 0.178 0.013
Batch: 100 | Loss: 0.699 | Acc: 81.142,97.703,99.760,% | Adaptive Acc: 90.811% | clf_exit: 0.809 0.177 0.014
Batch: 120 | Loss: 0.694 | Acc: 81.308,97.779,99.787,% | Adaptive Acc: 90.916% | clf_exit: 0.810 0.177 0.013
Batch: 140 | Loss: 0.694 | Acc: 81.300,97.750,99.795,% | Adaptive Acc: 90.896% | clf_exit: 0.809 0.178 0.013
Batch: 160 | Loss: 0.694 | Acc: 81.221,97.753,99.801,% | Adaptive Acc: 90.979% | clf_exit: 0.808 0.179 0.013
Batch: 180 | Loss: 0.693 | Acc: 81.323,97.751,99.810,% | Adaptive Acc: 91.065% | clf_exit: 0.807 0.180 0.014
Batch: 200 | Loss: 0.693 | Acc: 81.316,97.703,99.798,% | Adaptive Acc: 91.049% | clf_exit: 0.806 0.180 0.014
Batch: 220 | Loss: 0.694 | Acc: 81.261,97.681,99.799,% | Adaptive Acc: 91.024% | clf_exit: 0.807 0.180 0.014
Batch: 240 | Loss: 0.692 | Acc: 81.308,97.698,99.789,% | Adaptive Acc: 91.014% | clf_exit: 0.807 0.180 0.014
Batch: 260 | Loss: 0.694 | Acc: 81.304,97.707,99.781,% | Adaptive Acc: 91.002% | clf_exit: 0.806 0.180 0.014
Batch: 280 | Loss: 0.693 | Acc: 81.322,97.717,99.778,% | Adaptive Acc: 91.062% | clf_exit: 0.806 0.180 0.014
Batch: 300 | Loss: 0.695 | Acc: 81.247,97.687,99.782,% | Adaptive Acc: 91.030% | clf_exit: 0.806 0.181 0.014
Batch: 320 | Loss: 0.696 | Acc: 81.172,97.678,99.781,% | Adaptive Acc: 90.975% | clf_exit: 0.806 0.180 0.014
Batch: 340 | Loss: 0.696 | Acc: 81.181,97.672,99.782,% | Adaptive Acc: 90.992% | clf_exit: 0.806 0.180 0.014
Batch: 360 | Loss: 0.696 | Acc: 81.179,97.682,99.786,% | Adaptive Acc: 90.976% | clf_exit: 0.806 0.180 0.014
Batch: 380 | Loss: 0.695 | Acc: 81.215,97.671,99.783,% | Adaptive Acc: 91.006% | clf_exit: 0.806 0.180 0.014
Batch: 0 | Loss: 1.015 | Acc: 81.250,92.188,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.102 0.016
Batch: 20 | Loss: 1.164 | Acc: 78.646,90.737,93.304,% | Adaptive Acc: 85.193% | clf_exit: 0.838 0.143 0.019
Batch: 40 | Loss: 1.177 | Acc: 78.811,89.996,93.445,% | Adaptive Acc: 84.661% | clf_exit: 0.844 0.136 0.020
Batch: 60 | Loss: 1.172 | Acc: 78.906,89.946,93.430,% | Adaptive Acc: 84.849% | clf_exit: 0.845 0.136 0.020
Train all parameters

Epoch: 207
Batch: 0 | Loss: 0.692 | Acc: 79.688,96.875,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.781 0.211 0.008
Batch: 20 | Loss: 0.674 | Acc: 81.882,98.289,99.740,% | Adaptive Acc: 91.443% | clf_exit: 0.799 0.189 0.012
Batch: 40 | Loss: 0.683 | Acc: 81.974,98.018,99.695,% | Adaptive Acc: 91.711% | clf_exit: 0.800 0.189 0.011
Batch: 60 | Loss: 0.694 | Acc: 81.609,97.874,99.705,% | Adaptive Acc: 91.534% | clf_exit: 0.797 0.191 0.013
Batch: 80 | Loss: 0.692 | Acc: 81.771,97.926,99.711,% | Adaptive Acc: 91.609% | clf_exit: 0.799 0.188 0.013
Batch: 100 | Loss: 0.691 | Acc: 81.614,97.950,99.745,% | Adaptive Acc: 91.399% | clf_exit: 0.801 0.186 0.013
Batch: 120 | Loss: 0.694 | Acc: 81.502,97.831,99.761,% | Adaptive Acc: 91.264% | clf_exit: 0.803 0.183 0.013
Batch: 140 | Loss: 0.694 | Acc: 81.499,97.817,99.756,% | Adaptive Acc: 91.279% | clf_exit: 0.804 0.183 0.013
Batch: 160 | Loss: 0.692 | Acc: 81.527,97.816,99.723,% | Adaptive Acc: 91.266% | clf_exit: 0.804 0.183 0.013
Batch: 180 | Loss: 0.689 | Acc: 81.591,97.777,99.737,% | Adaptive Acc: 91.294% | clf_exit: 0.804 0.183 0.013
Batch: 200 | Loss: 0.689 | Acc: 81.588,97.765,99.743,% | Adaptive Acc: 91.266% | clf_exit: 0.804 0.183 0.013
Batch: 220 | Loss: 0.690 | Acc: 81.476,97.773,99.756,% | Adaptive Acc: 91.208% | clf_exit: 0.803 0.184 0.013
Batch: 240 | Loss: 0.690 | Acc: 81.445,97.802,99.757,% | Adaptive Acc: 91.218% | clf_exit: 0.803 0.185 0.013
Batch: 260 | Loss: 0.694 | Acc: 81.355,97.728,99.749,% | Adaptive Acc: 91.179% | clf_exit: 0.802 0.185 0.013
Batch: 280 | Loss: 0.695 | Acc: 81.325,97.737,99.725,% | Adaptive Acc: 91.153% | clf_exit: 0.802 0.185 0.013
Batch: 300 | Loss: 0.698 | Acc: 81.232,97.661,99.720,% | Adaptive Acc: 91.066% | clf_exit: 0.802 0.185 0.013
Batch: 320 | Loss: 0.698 | Acc: 81.216,97.651,99.713,% | Adaptive Acc: 91.002% | clf_exit: 0.803 0.184 0.013
Batch: 340 | Loss: 0.698 | Acc: 81.248,97.636,99.720,% | Adaptive Acc: 90.985% | clf_exit: 0.804 0.182 0.013
Batch: 360 | Loss: 0.697 | Acc: 81.269,97.619,99.708,% | Adaptive Acc: 90.984% | clf_exit: 0.805 0.182 0.013
Batch: 380 | Loss: 0.697 | Acc: 81.258,97.617,99.707,% | Adaptive Acc: 90.955% | clf_exit: 0.806 0.181 0.013
Batch: 0 | Loss: 1.063 | Acc: 79.688,92.188,92.969,% | Adaptive Acc: 82.031% | clf_exit: 0.859 0.102 0.039
Batch: 20 | Loss: 1.188 | Acc: 78.274,89.993,92.969,% | Adaptive Acc: 84.375% | clf_exit: 0.821 0.156 0.023
Batch: 40 | Loss: 1.204 | Acc: 78.430,90.015,93.083,% | Adaptive Acc: 84.604% | clf_exit: 0.828 0.146 0.026
Batch: 60 | Loss: 1.200 | Acc: 78.445,89.767,93.110,% | Adaptive Acc: 84.695% | clf_exit: 0.827 0.148 0.025
Train all parameters

Epoch: 208
Batch: 0 | Loss: 0.538 | Acc: 82.031,100.000,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.859 0.133 0.008
Batch: 20 | Loss: 0.650 | Acc: 82.626,97.917,99.888,% | Adaptive Acc: 91.741% | clf_exit: 0.808 0.183 0.010
Batch: 40 | Loss: 0.674 | Acc: 81.307,97.694,99.829,% | Adaptive Acc: 90.911% | clf_exit: 0.812 0.175 0.012
Batch: 60 | Loss: 0.685 | Acc: 81.160,97.797,99.846,% | Adaptive Acc: 91.048% | clf_exit: 0.806 0.181 0.013
Batch: 80 | Loss: 0.681 | Acc: 81.414,97.801,99.846,% | Adaptive Acc: 91.155% | clf_exit: 0.808 0.178 0.014
Batch: 100 | Loss: 0.680 | Acc: 81.621,97.788,99.845,% | Adaptive Acc: 91.174% | clf_exit: 0.809 0.178 0.013
Batch: 120 | Loss: 0.679 | Acc: 81.592,97.772,99.839,% | Adaptive Acc: 91.109% | clf_exit: 0.810 0.177 0.013
Batch: 140 | Loss: 0.682 | Acc: 81.510,97.756,99.817,% | Adaptive Acc: 91.096% | clf_exit: 0.808 0.178 0.014
Batch: 160 | Loss: 0.685 | Acc: 81.410,97.729,99.801,% | Adaptive Acc: 91.033% | clf_exit: 0.808 0.179 0.013
Batch: 180 | Loss: 0.685 | Acc: 81.457,97.699,99.797,% | Adaptive Acc: 91.074% | clf_exit: 0.808 0.178 0.013
Batch: 200 | Loss: 0.690 | Acc: 81.293,97.703,99.794,% | Adaptive Acc: 91.021% | clf_exit: 0.807 0.179 0.013
Batch: 220 | Loss: 0.687 | Acc: 81.391,97.734,99.791,% | Adaptive Acc: 91.095% | clf_exit: 0.808 0.179 0.013
Batch: 240 | Loss: 0.686 | Acc: 81.448,97.715,99.799,% | Adaptive Acc: 91.105% | clf_exit: 0.808 0.179 0.013
Batch: 260 | Loss: 0.686 | Acc: 81.483,97.731,99.793,% | Adaptive Acc: 91.107% | clf_exit: 0.808 0.179 0.013
Batch: 280 | Loss: 0.689 | Acc: 81.453,97.678,99.783,% | Adaptive Acc: 91.089% | clf_exit: 0.807 0.180 0.013
Batch: 300 | Loss: 0.693 | Acc: 81.377,97.635,99.774,% | Adaptive Acc: 91.048% | clf_exit: 0.806 0.180 0.013
Batch: 320 | Loss: 0.691 | Acc: 81.440,97.642,99.776,% | Adaptive Acc: 91.053% | clf_exit: 0.806 0.181 0.013
Batch: 340 | Loss: 0.693 | Acc: 81.355,97.597,99.769,% | Adaptive Acc: 90.962% | clf_exit: 0.806 0.181 0.013
Batch: 360 | Loss: 0.696 | Acc: 81.278,97.576,99.747,% | Adaptive Acc: 90.909% | clf_exit: 0.806 0.180 0.013
Batch: 380 | Loss: 0.696 | Acc: 81.260,97.570,99.729,% | Adaptive Acc: 90.894% | clf_exit: 0.806 0.181 0.013
Batch: 0 | Loss: 1.077 | Acc: 80.469,91.406,92.188,% | Adaptive Acc: 82.812% | clf_exit: 0.875 0.117 0.008
Batch: 20 | Loss: 1.256 | Acc: 77.939,89.583,92.225,% | Adaptive Acc: 84.635% | clf_exit: 0.812 0.166 0.022
Batch: 40 | Loss: 1.248 | Acc: 78.335,89.444,92.626,% | Adaptive Acc: 84.604% | clf_exit: 0.823 0.155 0.022
Batch: 60 | Loss: 1.238 | Acc: 78.407,89.498,92.828,% | Adaptive Acc: 84.618% | clf_exit: 0.826 0.152 0.022
Train all parameters

Epoch: 209
Batch: 0 | Loss: 0.571 | Acc: 81.250,99.219,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.789 0.211 0.000
Batch: 20 | Loss: 0.691 | Acc: 81.250,97.693,99.479,% | Adaptive Acc: 90.960% | clf_exit: 0.803 0.187 0.010
Batch: 40 | Loss: 0.690 | Acc: 81.612,97.580,99.638,% | Adaptive Acc: 90.949% | clf_exit: 0.812 0.176 0.013
Batch: 60 | Loss: 0.683 | Acc: 81.609,97.656,99.705,% | Adaptive Acc: 91.073% | clf_exit: 0.811 0.177 0.012
Batch: 80 | Loss: 0.683 | Acc: 81.539,97.840,99.682,% | Adaptive Acc: 91.011% | clf_exit: 0.808 0.181 0.011
Batch: 100 | Loss: 0.685 | Acc: 81.397,97.881,99.675,% | Adaptive Acc: 91.066% | clf_exit: 0.806 0.182 0.012
Batch: 120 | Loss: 0.695 | Acc: 81.082,97.714,99.671,% | Adaptive Acc: 90.883% | clf_exit: 0.804 0.184 0.012
Batch: 140 | Loss: 0.695 | Acc: 81.134,97.739,99.706,% | Adaptive Acc: 90.874% | clf_exit: 0.806 0.182 0.013
Batch: 160 | Loss: 0.700 | Acc: 81.085,97.724,99.709,% | Adaptive Acc: 90.771% | clf_exit: 0.805 0.183 0.013
Batch: 180 | Loss: 0.698 | Acc: 81.181,97.743,99.719,% | Adaptive Acc: 90.845% | clf_exit: 0.805 0.183 0.013
Batch: 200 | Loss: 0.702 | Acc: 81.118,97.652,99.697,% | Adaptive Acc: 90.796% | clf_exit: 0.804 0.183 0.013
Batch: 220 | Loss: 0.703 | Acc: 81.123,97.614,99.678,% | Adaptive Acc: 90.713% | clf_exit: 0.806 0.182 0.012
Batch: 240 | Loss: 0.700 | Acc: 81.256,97.643,99.692,% | Adaptive Acc: 90.748% | clf_exit: 0.807 0.181 0.012
Batch: 260 | Loss: 0.701 | Acc: 81.268,97.635,99.704,% | Adaptive Acc: 90.793% | clf_exit: 0.806 0.181 0.012
Batch: 280 | Loss: 0.701 | Acc: 81.231,97.620,99.708,% | Adaptive Acc: 90.792% | clf_exit: 0.806 0.181 0.013
Batch: 300 | Loss: 0.701 | Acc: 81.245,97.617,99.704,% | Adaptive Acc: 90.786% | clf_exit: 0.807 0.181 0.013
Batch: 320 | Loss: 0.700 | Acc: 81.279,97.637,99.706,% | Adaptive Acc: 90.783% | clf_exit: 0.808 0.180 0.012
Batch: 340 | Loss: 0.699 | Acc: 81.280,97.599,99.709,% | Adaptive Acc: 90.797% | clf_exit: 0.808 0.180 0.013
Batch: 360 | Loss: 0.699 | Acc: 81.235,97.587,99.708,% | Adaptive Acc: 90.794% | clf_exit: 0.808 0.180 0.013
Batch: 380 | Loss: 0.699 | Acc: 81.291,97.597,99.707,% | Adaptive Acc: 90.818% | clf_exit: 0.808 0.180 0.013
Batch: 0 | Loss: 0.896 | Acc: 81.250,90.625,93.750,% | Adaptive Acc: 90.625% | clf_exit: 0.820 0.156 0.023
Batch: 20 | Loss: 1.246 | Acc: 77.827,89.993,92.448,% | Adaptive Acc: 84.115% | clf_exit: 0.831 0.148 0.020
Batch: 40 | Loss: 1.235 | Acc: 78.335,90.015,92.626,% | Adaptive Acc: 84.318% | clf_exit: 0.839 0.139 0.023
Batch: 60 | Loss: 1.222 | Acc: 78.548,89.908,92.636,% | Adaptive Acc: 84.580% | clf_exit: 0.838 0.140 0.022
Train all parameters

Epoch: 210
Batch: 0 | Loss: 0.584 | Acc: 86.719,99.219,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.852 0.148 0.000
Batch: 20 | Loss: 0.680 | Acc: 81.176,98.400,99.814,% | Adaptive Acc: 90.960% | clf_exit: 0.811 0.177 0.012
Batch: 40 | Loss: 0.665 | Acc: 81.936,98.342,99.790,% | Adaptive Acc: 91.559% | clf_exit: 0.813 0.175 0.012
Batch: 60 | Loss: 0.675 | Acc: 81.814,98.066,99.782,% | Adaptive Acc: 91.624% | clf_exit: 0.806 0.180 0.013
Batch: 80 | Loss: 0.677 | Acc: 81.983,98.052,99.807,% | Adaptive Acc: 91.628% | clf_exit: 0.806 0.179 0.015
Batch: 100 | Loss: 0.684 | Acc: 81.652,97.896,99.745,% | Adaptive Acc: 91.368% | clf_exit: 0.807 0.178 0.015
Batch: 120 | Loss: 0.685 | Acc: 81.579,97.876,99.742,% | Adaptive Acc: 91.419% | clf_exit: 0.805 0.180 0.015
Batch: 140 | Loss: 0.687 | Acc: 81.400,97.872,99.740,% | Adaptive Acc: 91.401% | clf_exit: 0.803 0.182 0.014
Batch: 160 | Loss: 0.684 | Acc: 81.347,97.879,99.753,% | Adaptive Acc: 91.338% | clf_exit: 0.803 0.183 0.014
Batch: 180 | Loss: 0.683 | Acc: 81.397,97.876,99.763,% | Adaptive Acc: 91.277% | clf_exit: 0.804 0.182 0.014
Batch: 200 | Loss: 0.683 | Acc: 81.405,97.862,99.751,% | Adaptive Acc: 91.169% | clf_exit: 0.806 0.180 0.014
Batch: 220 | Loss: 0.683 | Acc: 81.406,97.875,99.756,% | Adaptive Acc: 91.233% | clf_exit: 0.805 0.181 0.014
Batch: 240 | Loss: 0.682 | Acc: 81.445,97.857,99.750,% | Adaptive Acc: 91.221% | clf_exit: 0.806 0.180 0.014
Batch: 260 | Loss: 0.684 | Acc: 81.421,97.836,99.752,% | Adaptive Acc: 91.197% | clf_exit: 0.805 0.182 0.014
Batch: 280 | Loss: 0.687 | Acc: 81.347,97.779,99.733,% | Adaptive Acc: 91.089% | clf_exit: 0.805 0.181 0.014
Batch: 300 | Loss: 0.693 | Acc: 81.240,97.703,99.730,% | Adaptive Acc: 91.069% | clf_exit: 0.804 0.182 0.014
Batch: 320 | Loss: 0.696 | Acc: 81.182,97.656,99.715,% | Adaptive Acc: 91.019% | clf_exit: 0.803 0.183 0.014
Batch: 340 | Loss: 0.696 | Acc: 81.193,97.663,99.707,% | Adaptive Acc: 90.996% | clf_exit: 0.804 0.182 0.014
Batch: 360 | Loss: 0.697 | Acc: 81.179,97.641,99.704,% | Adaptive Acc: 91.006% | clf_exit: 0.803 0.183 0.014
Batch: 380 | Loss: 0.699 | Acc: 81.164,97.607,99.692,% | Adaptive Acc: 90.943% | clf_exit: 0.803 0.183 0.014
Batch: 0 | Loss: 1.069 | Acc: 78.906,89.062,94.531,% | Adaptive Acc: 83.594% | clf_exit: 0.891 0.109 0.000
Batch: 20 | Loss: 1.247 | Acc: 77.530,90.104,92.522,% | Adaptive Acc: 84.152% | clf_exit: 0.825 0.158 0.017
Batch: 40 | Loss: 1.239 | Acc: 78.049,90.130,92.778,% | Adaptive Acc: 84.508% | clf_exit: 0.832 0.148 0.020
Batch: 60 | Loss: 1.217 | Acc: 78.291,90.100,92.802,% | Adaptive Acc: 84.721% | clf_exit: 0.833 0.147 0.020
Train all parameters

Epoch: 211
Batch: 0 | Loss: 0.600 | Acc: 82.812,97.656,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.686 | Acc: 81.101,97.656,99.740,% | Adaptive Acc: 90.737% | clf_exit: 0.809 0.179 0.012
Batch: 40 | Loss: 0.702 | Acc: 80.926,97.637,99.714,% | Adaptive Acc: 90.663% | clf_exit: 0.803 0.184 0.013
Batch: 60 | Loss: 0.695 | Acc: 81.058,97.707,99.744,% | Adaptive Acc: 90.779% | clf_exit: 0.806 0.181 0.013
Batch: 80 | Loss: 0.697 | Acc: 80.980,97.676,99.749,% | Adaptive Acc: 90.828% | clf_exit: 0.804 0.184 0.013
Batch: 100 | Loss: 0.693 | Acc: 81.149,97.703,99.799,% | Adaptive Acc: 90.818% | clf_exit: 0.805 0.182 0.013
Batch: 120 | Loss: 0.697 | Acc: 81.095,97.624,99.800,% | Adaptive Acc: 90.619% | clf_exit: 0.807 0.180 0.013
Batch: 140 | Loss: 0.695 | Acc: 81.161,97.617,99.801,% | Adaptive Acc: 90.775% | clf_exit: 0.806 0.181 0.014
Batch: 160 | Loss: 0.691 | Acc: 81.211,97.676,99.772,% | Adaptive Acc: 90.809% | clf_exit: 0.806 0.181 0.014
Batch: 180 | Loss: 0.690 | Acc: 81.306,97.695,99.763,% | Adaptive Acc: 90.884% | clf_exit: 0.807 0.180 0.013
Batch: 200 | Loss: 0.688 | Acc: 81.472,97.660,99.759,% | Adaptive Acc: 90.905% | clf_exit: 0.808 0.179 0.013
Batch: 220 | Loss: 0.690 | Acc: 81.434,97.642,99.738,% | Adaptive Acc: 90.809% | clf_exit: 0.809 0.177 0.014
Batch: 240 | Loss: 0.690 | Acc: 81.451,97.643,99.734,% | Adaptive Acc: 90.852% | clf_exit: 0.809 0.178 0.013
Batch: 260 | Loss: 0.694 | Acc: 81.328,97.629,99.725,% | Adaptive Acc: 90.784% | clf_exit: 0.808 0.179 0.013
Batch: 280 | Loss: 0.692 | Acc: 81.484,97.642,99.714,% | Adaptive Acc: 90.859% | clf_exit: 0.808 0.178 0.013
Batch: 300 | Loss: 0.694 | Acc: 81.408,97.615,99.704,% | Adaptive Acc: 90.799% | clf_exit: 0.808 0.179 0.014
Batch: 320 | Loss: 0.694 | Acc: 81.437,97.617,99.701,% | Adaptive Acc: 90.854% | clf_exit: 0.808 0.179 0.014
Batch: 340 | Loss: 0.694 | Acc: 81.470,97.599,99.702,% | Adaptive Acc: 90.861% | clf_exit: 0.808 0.178 0.014
Batch: 360 | Loss: 0.695 | Acc: 81.427,97.613,99.706,% | Adaptive Acc: 90.837% | clf_exit: 0.808 0.178 0.014
Batch: 380 | Loss: 0.698 | Acc: 81.365,97.587,99.699,% | Adaptive Acc: 90.816% | clf_exit: 0.807 0.179 0.014
Batch: 0 | Loss: 1.142 | Acc: 78.906,95.312,91.406,% | Adaptive Acc: 84.375% | clf_exit: 0.828 0.156 0.016
Batch: 20 | Loss: 1.258 | Acc: 76.562,89.918,92.746,% | Adaptive Acc: 83.445% | clf_exit: 0.839 0.138 0.022
Batch: 40 | Loss: 1.249 | Acc: 77.134,89.748,92.893,% | Adaptive Acc: 83.556% | clf_exit: 0.844 0.133 0.022
Batch: 60 | Loss: 1.242 | Acc: 77.433,89.677,92.815,% | Adaptive Acc: 83.760% | clf_exit: 0.842 0.136 0.022
Train all parameters

Epoch: 212
Batch: 0 | Loss: 0.729 | Acc: 78.906,96.094,99.219,% | Adaptive Acc: 89.844% | clf_exit: 0.820 0.164 0.016
Batch: 20 | Loss: 0.692 | Acc: 80.990,98.363,99.665,% | Adaptive Acc: 90.737% | clf_exit: 0.797 0.192 0.012
Batch: 40 | Loss: 0.689 | Acc: 81.040,98.228,99.733,% | Adaptive Acc: 90.892% | clf_exit: 0.799 0.190 0.011
Batch: 60 | Loss: 0.685 | Acc: 81.314,98.028,99.769,% | Adaptive Acc: 91.214% | clf_exit: 0.801 0.186 0.012
Batch: 80 | Loss: 0.689 | Acc: 81.250,97.801,99.759,% | Adaptive Acc: 91.117% | clf_exit: 0.803 0.183 0.014
Batch: 100 | Loss: 0.689 | Acc: 81.242,97.826,99.752,% | Adaptive Acc: 91.074% | clf_exit: 0.804 0.183 0.013
Batch: 120 | Loss: 0.692 | Acc: 81.114,97.785,99.722,% | Adaptive Acc: 90.948% | clf_exit: 0.805 0.182 0.013
Batch: 140 | Loss: 0.689 | Acc: 81.261,97.739,99.723,% | Adaptive Acc: 91.085% | clf_exit: 0.806 0.180 0.013
Batch: 160 | Loss: 0.685 | Acc: 81.371,97.778,99.719,% | Adaptive Acc: 91.232% | clf_exit: 0.806 0.181 0.013
Batch: 180 | Loss: 0.687 | Acc: 81.375,97.734,99.711,% | Adaptive Acc: 91.087% | clf_exit: 0.808 0.179 0.013
Batch: 200 | Loss: 0.689 | Acc: 81.277,97.757,99.736,% | Adaptive Acc: 91.021% | clf_exit: 0.806 0.181 0.013
Batch: 220 | Loss: 0.688 | Acc: 81.299,97.752,99.745,% | Adaptive Acc: 91.095% | clf_exit: 0.805 0.181 0.013
Batch: 240 | Loss: 0.690 | Acc: 81.175,97.760,99.728,% | Adaptive Acc: 91.001% | clf_exit: 0.805 0.182 0.013
Batch: 260 | Loss: 0.694 | Acc: 81.070,97.704,99.704,% | Adaptive Acc: 90.993% | clf_exit: 0.804 0.183 0.013
Batch: 280 | Loss: 0.695 | Acc: 81.092,97.670,99.700,% | Adaptive Acc: 90.989% | clf_exit: 0.804 0.183 0.013
Batch: 300 | Loss: 0.696 | Acc: 81.097,97.651,99.696,% | Adaptive Acc: 90.962% | clf_exit: 0.804 0.182 0.013
Batch: 320 | Loss: 0.695 | Acc: 81.155,97.654,99.701,% | Adaptive Acc: 90.995% | clf_exit: 0.805 0.182 0.013
Batch: 340 | Loss: 0.695 | Acc: 81.200,97.629,99.695,% | Adaptive Acc: 91.005% | clf_exit: 0.805 0.181 0.013
Batch: 360 | Loss: 0.693 | Acc: 81.241,97.628,99.693,% | Adaptive Acc: 90.973% | clf_exit: 0.806 0.181 0.013
Batch: 380 | Loss: 0.694 | Acc: 81.223,97.617,99.699,% | Adaptive Acc: 90.988% | clf_exit: 0.806 0.181 0.013
Batch: 0 | Loss: 1.089 | Acc: 80.469,91.406,94.531,% | Adaptive Acc: 85.938% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 1.259 | Acc: 77.232,88.839,93.006,% | Adaptive Acc: 83.371% | clf_exit: 0.829 0.147 0.024
Batch: 40 | Loss: 1.266 | Acc: 77.611,88.891,92.759,% | Adaptive Acc: 83.575% | clf_exit: 0.835 0.141 0.023
Batch: 60 | Loss: 1.242 | Acc: 77.766,89.255,93.046,% | Adaptive Acc: 83.747% | clf_exit: 0.837 0.141 0.022
Train all parameters

Epoch: 213
Batch: 0 | Loss: 0.512 | Acc: 86.719,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.663 | Acc: 81.845,98.028,99.888,% | Adaptive Acc: 91.034% | clf_exit: 0.816 0.171 0.013
Batch: 40 | Loss: 0.688 | Acc: 81.231,97.961,99.809,% | Adaptive Acc: 90.606% | clf_exit: 0.810 0.177 0.013
Batch: 60 | Loss: 0.691 | Acc: 81.314,97.823,99.782,% | Adaptive Acc: 90.625% | clf_exit: 0.810 0.177 0.013
Batch: 80 | Loss: 0.683 | Acc: 81.588,97.878,99.769,% | Adaptive Acc: 90.963% | clf_exit: 0.808 0.179 0.013
Batch: 100 | Loss: 0.680 | Acc: 81.714,97.857,99.760,% | Adaptive Acc: 91.089% | clf_exit: 0.807 0.181 0.012
Batch: 120 | Loss: 0.686 | Acc: 81.579,97.824,99.716,% | Adaptive Acc: 91.154% | clf_exit: 0.806 0.182 0.012
Batch: 140 | Loss: 0.691 | Acc: 81.389,97.795,99.740,% | Adaptive Acc: 91.079% | clf_exit: 0.803 0.185 0.012
Batch: 160 | Loss: 0.692 | Acc: 81.386,97.778,99.748,% | Adaptive Acc: 91.062% | clf_exit: 0.804 0.184 0.012
Batch: 180 | Loss: 0.693 | Acc: 81.392,97.730,99.750,% | Adaptive Acc: 90.996% | clf_exit: 0.804 0.183 0.012
Batch: 200 | Loss: 0.695 | Acc: 81.402,97.683,99.751,% | Adaptive Acc: 90.951% | clf_exit: 0.805 0.182 0.013
Batch: 220 | Loss: 0.697 | Acc: 81.391,97.663,99.753,% | Adaptive Acc: 90.961% | clf_exit: 0.805 0.182 0.013
Batch: 240 | Loss: 0.694 | Acc: 81.457,97.685,99.737,% | Adaptive Acc: 91.014% | clf_exit: 0.806 0.182 0.013
Batch: 260 | Loss: 0.696 | Acc: 81.385,97.689,99.743,% | Adaptive Acc: 90.954% | clf_exit: 0.806 0.182 0.013
Batch: 280 | Loss: 0.696 | Acc: 81.375,97.704,99.747,% | Adaptive Acc: 90.942% | clf_exit: 0.805 0.182 0.013
Batch: 300 | Loss: 0.697 | Acc: 81.315,97.726,99.753,% | Adaptive Acc: 90.936% | clf_exit: 0.805 0.183 0.013
Batch: 320 | Loss: 0.698 | Acc: 81.267,97.724,99.757,% | Adaptive Acc: 90.900% | clf_exit: 0.805 0.183 0.013
Batch: 340 | Loss: 0.699 | Acc: 81.209,97.714,99.753,% | Adaptive Acc: 90.900% | clf_exit: 0.805 0.183 0.013
Batch: 360 | Loss: 0.699 | Acc: 81.144,97.693,99.751,% | Adaptive Acc: 90.844% | clf_exit: 0.805 0.182 0.013
Batch: 380 | Loss: 0.700 | Acc: 81.115,97.664,99.756,% | Adaptive Acc: 90.789% | clf_exit: 0.806 0.181 0.013
Batch: 0 | Loss: 0.935 | Acc: 82.031,93.750,94.531,% | Adaptive Acc: 87.500% | clf_exit: 0.828 0.133 0.039
Batch: 20 | Loss: 1.222 | Acc: 78.423,89.621,92.634,% | Adaptive Acc: 84.896% | clf_exit: 0.805 0.166 0.029
Batch: 40 | Loss: 1.230 | Acc: 78.639,89.329,92.588,% | Adaptive Acc: 85.232% | clf_exit: 0.814 0.158 0.028
Batch: 60 | Loss: 1.211 | Acc: 78.829,89.549,92.853,% | Adaptive Acc: 85.143% | clf_exit: 0.816 0.159 0.025
Train all parameters

Epoch: 214
Batch: 0 | Loss: 0.667 | Acc: 82.031,97.656,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.828 0.164 0.008
Batch: 20 | Loss: 0.677 | Acc: 80.766,98.140,99.851,% | Adaptive Acc: 91.369% | clf_exit: 0.801 0.187 0.012
Batch: 40 | Loss: 0.672 | Acc: 81.383,98.133,99.809,% | Adaptive Acc: 91.425% | clf_exit: 0.808 0.181 0.011
Batch: 60 | Loss: 0.670 | Acc: 81.237,98.156,99.821,% | Adaptive Acc: 91.060% | clf_exit: 0.811 0.179 0.010
Batch: 80 | Loss: 0.677 | Acc: 81.211,98.090,99.807,% | Adaptive Acc: 90.943% | clf_exit: 0.810 0.179 0.011
Batch: 100 | Loss: 0.687 | Acc: 80.972,98.043,99.791,% | Adaptive Acc: 90.640% | clf_exit: 0.811 0.178 0.011
Batch: 120 | Loss: 0.695 | Acc: 80.779,97.960,99.716,% | Adaptive Acc: 90.683% | clf_exit: 0.807 0.181 0.012
Batch: 140 | Loss: 0.695 | Acc: 80.890,97.878,99.729,% | Adaptive Acc: 90.669% | clf_exit: 0.807 0.180 0.012
Batch: 160 | Loss: 0.693 | Acc: 80.998,97.879,99.733,% | Adaptive Acc: 90.805% | clf_exit: 0.808 0.180 0.013
Batch: 180 | Loss: 0.692 | Acc: 81.090,97.855,99.732,% | Adaptive Acc: 90.785% | clf_exit: 0.808 0.179 0.013
Batch: 200 | Loss: 0.690 | Acc: 81.141,97.866,99.747,% | Adaptive Acc: 90.885% | clf_exit: 0.808 0.179 0.013
Batch: 220 | Loss: 0.690 | Acc: 81.130,97.868,99.742,% | Adaptive Acc: 90.816% | clf_exit: 0.809 0.178 0.013
Batch: 240 | Loss: 0.689 | Acc: 81.185,97.880,99.744,% | Adaptive Acc: 90.894% | clf_exit: 0.810 0.178 0.013
Batch: 260 | Loss: 0.689 | Acc: 81.184,97.863,99.755,% | Adaptive Acc: 90.882% | clf_exit: 0.810 0.178 0.013
Batch: 280 | Loss: 0.692 | Acc: 81.130,97.829,99.750,% | Adaptive Acc: 90.831% | clf_exit: 0.810 0.178 0.013
Batch: 300 | Loss: 0.693 | Acc: 81.131,97.830,99.748,% | Adaptive Acc: 90.791% | clf_exit: 0.809 0.178 0.013
Batch: 320 | Loss: 0.693 | Acc: 81.153,97.788,99.749,% | Adaptive Acc: 90.771% | clf_exit: 0.809 0.178 0.013
Batch: 340 | Loss: 0.694 | Acc: 81.163,97.741,99.741,% | Adaptive Acc: 90.783% | clf_exit: 0.809 0.178 0.013
Batch: 360 | Loss: 0.694 | Acc: 81.185,97.743,99.738,% | Adaptive Acc: 90.781% | clf_exit: 0.809 0.178 0.013
Batch: 380 | Loss: 0.694 | Acc: 81.248,97.742,99.731,% | Adaptive Acc: 90.824% | clf_exit: 0.809 0.178 0.013
Batch: 0 | Loss: 1.049 | Acc: 77.344,93.750,93.750,% | Adaptive Acc: 82.812% | clf_exit: 0.820 0.156 0.023
Batch: 20 | Loss: 1.238 | Acc: 76.897,90.030,92.857,% | Adaptive Acc: 83.705% | clf_exit: 0.825 0.154 0.021
Batch: 40 | Loss: 1.239 | Acc: 77.229,89.996,92.931,% | Adaptive Acc: 83.365% | clf_exit: 0.839 0.142 0.019
Batch: 60 | Loss: 1.216 | Acc: 77.613,90.190,92.969,% | Adaptive Acc: 83.555% | clf_exit: 0.841 0.138 0.020
Train all parameters

Epoch: 215
Batch: 0 | Loss: 0.563 | Acc: 86.719,100.000,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.875 0.125 0.000
Batch: 20 | Loss: 0.655 | Acc: 82.552,98.363,99.740,% | Adaptive Acc: 91.964% | clf_exit: 0.804 0.183 0.013
Batch: 40 | Loss: 0.679 | Acc: 81.174,98.190,99.771,% | Adaptive Acc: 90.796% | clf_exit: 0.810 0.179 0.012
Batch: 60 | Loss: 0.669 | Acc: 81.801,98.015,99.808,% | Adaptive Acc: 90.984% | clf_exit: 0.812 0.176 0.011
Batch: 80 | Loss: 0.674 | Acc: 81.568,98.032,99.807,% | Adaptive Acc: 90.992% | clf_exit: 0.808 0.180 0.011
Batch: 100 | Loss: 0.683 | Acc: 81.142,97.935,99.791,% | Adaptive Acc: 90.757% | clf_exit: 0.808 0.181 0.011
Batch: 120 | Loss: 0.682 | Acc: 81.295,97.927,99.787,% | Adaptive Acc: 91.038% | clf_exit: 0.806 0.182 0.012
Batch: 140 | Loss: 0.684 | Acc: 81.195,97.917,99.784,% | Adaptive Acc: 90.980% | clf_exit: 0.807 0.181 0.012
Batch: 160 | Loss: 0.689 | Acc: 81.036,97.879,99.791,% | Adaptive Acc: 90.839% | clf_exit: 0.807 0.181 0.012
Batch: 180 | Loss: 0.690 | Acc: 81.077,97.894,99.780,% | Adaptive Acc: 90.893% | clf_exit: 0.805 0.182 0.013
Batch: 200 | Loss: 0.689 | Acc: 81.075,97.847,99.759,% | Adaptive Acc: 91.010% | clf_exit: 0.805 0.182 0.013
Batch: 220 | Loss: 0.687 | Acc: 81.151,97.854,99.760,% | Adaptive Acc: 91.123% | clf_exit: 0.805 0.182 0.013
Batch: 240 | Loss: 0.688 | Acc: 81.150,97.809,99.760,% | Adaptive Acc: 91.069% | clf_exit: 0.806 0.181 0.013
Batch: 260 | Loss: 0.687 | Acc: 81.190,97.791,99.740,% | Adaptive Acc: 91.059% | clf_exit: 0.806 0.180 0.013
Batch: 280 | Loss: 0.688 | Acc: 81.186,97.742,99.730,% | Adaptive Acc: 91.025% | clf_exit: 0.806 0.180 0.014
Batch: 300 | Loss: 0.689 | Acc: 81.185,97.726,99.725,% | Adaptive Acc: 90.957% | clf_exit: 0.806 0.180 0.013
Batch: 320 | Loss: 0.692 | Acc: 81.162,97.688,99.710,% | Adaptive Acc: 90.866% | clf_exit: 0.806 0.181 0.013
Batch: 340 | Loss: 0.692 | Acc: 81.168,97.672,99.711,% | Adaptive Acc: 90.888% | clf_exit: 0.806 0.181 0.013
Batch: 360 | Loss: 0.692 | Acc: 81.213,97.645,99.719,% | Adaptive Acc: 90.919% | clf_exit: 0.806 0.181 0.013
Batch: 380 | Loss: 0.692 | Acc: 81.217,97.621,99.707,% | Adaptive Acc: 90.926% | clf_exit: 0.805 0.182 0.013
Batch: 0 | Loss: 1.010 | Acc: 78.125,92.969,95.312,% | Adaptive Acc: 85.156% | clf_exit: 0.828 0.141 0.031
Batch: 20 | Loss: 1.220 | Acc: 78.423,90.030,92.969,% | Adaptive Acc: 84.338% | clf_exit: 0.837 0.143 0.020
Batch: 40 | Loss: 1.219 | Acc: 78.335,89.977,93.178,% | Adaptive Acc: 84.585% | clf_exit: 0.839 0.141 0.020
Batch: 60 | Loss: 1.208 | Acc: 78.573,89.754,93.161,% | Adaptive Acc: 84.618% | clf_exit: 0.838 0.142 0.020
Train all parameters

Epoch: 216
Batch: 0 | Loss: 0.721 | Acc: 76.562,98.438,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.766 0.219 0.016
Batch: 20 | Loss: 0.690 | Acc: 80.283,98.065,99.814,% | Adaptive Acc: 90.216% | clf_exit: 0.812 0.176 0.013
Batch: 40 | Loss: 0.682 | Acc: 80.983,98.037,99.809,% | Adaptive Acc: 90.930% | clf_exit: 0.807 0.179 0.014
Batch: 60 | Loss: 0.677 | Acc: 81.250,98.028,99.821,% | Adaptive Acc: 90.868% | clf_exit: 0.809 0.178 0.013
Batch: 80 | Loss: 0.677 | Acc: 81.289,97.994,99.817,% | Adaptive Acc: 91.049% | clf_exit: 0.808 0.179 0.012
Batch: 100 | Loss: 0.679 | Acc: 81.196,97.942,99.814,% | Adaptive Acc: 91.066% | clf_exit: 0.806 0.182 0.012
Batch: 120 | Loss: 0.683 | Acc: 81.218,97.876,99.787,% | Adaptive Acc: 91.058% | clf_exit: 0.808 0.180 0.012
Batch: 140 | Loss: 0.687 | Acc: 81.195,97.800,99.784,% | Adaptive Acc: 90.991% | clf_exit: 0.807 0.180 0.013
Batch: 160 | Loss: 0.686 | Acc: 81.231,97.744,99.753,% | Adaptive Acc: 90.950% | clf_exit: 0.808 0.179 0.013
Batch: 180 | Loss: 0.690 | Acc: 81.138,97.721,99.728,% | Adaptive Acc: 90.793% | clf_exit: 0.809 0.178 0.013
Batch: 200 | Loss: 0.694 | Acc: 80.962,97.676,99.732,% | Adaptive Acc: 90.738% | clf_exit: 0.807 0.180 0.013
Batch: 220 | Loss: 0.693 | Acc: 80.988,97.667,99.721,% | Adaptive Acc: 90.756% | clf_exit: 0.808 0.179 0.013
Batch: 240 | Loss: 0.693 | Acc: 81.004,97.653,99.731,% | Adaptive Acc: 90.771% | clf_exit: 0.808 0.179 0.013
Batch: 260 | Loss: 0.693 | Acc: 81.043,97.650,99.725,% | Adaptive Acc: 90.811% | clf_exit: 0.807 0.180 0.013
Batch: 280 | Loss: 0.694 | Acc: 81.092,97.626,99.733,% | Adaptive Acc: 90.834% | clf_exit: 0.807 0.179 0.013
Batch: 300 | Loss: 0.692 | Acc: 81.159,97.646,99.746,% | Adaptive Acc: 90.864% | clf_exit: 0.808 0.179 0.013
Batch: 320 | Loss: 0.691 | Acc: 81.206,97.678,99.752,% | Adaptive Acc: 90.888% | clf_exit: 0.808 0.180 0.013
Batch: 340 | Loss: 0.691 | Acc: 81.223,97.679,99.741,% | Adaptive Acc: 90.859% | clf_exit: 0.808 0.179 0.012
Batch: 360 | Loss: 0.693 | Acc: 81.200,97.656,99.729,% | Adaptive Acc: 90.828% | clf_exit: 0.808 0.180 0.013
Batch: 380 | Loss: 0.692 | Acc: 81.262,97.640,99.721,% | Adaptive Acc: 90.859% | clf_exit: 0.808 0.180 0.013
Batch: 0 | Loss: 1.030 | Acc: 80.469,90.625,95.312,% | Adaptive Acc: 85.156% | clf_exit: 0.812 0.141 0.047
Batch: 20 | Loss: 1.220 | Acc: 78.795,89.621,92.560,% | Adaptive Acc: 84.784% | clf_exit: 0.829 0.148 0.022
Batch: 40 | Loss: 1.232 | Acc: 78.639,89.482,92.645,% | Adaptive Acc: 84.508% | clf_exit: 0.836 0.140 0.024
Batch: 60 | Loss: 1.217 | Acc: 78.906,89.562,92.687,% | Adaptive Acc: 84.631% | clf_exit: 0.836 0.142 0.022
Train all parameters

Epoch: 217
Batch: 0 | Loss: 0.630 | Acc: 81.250,97.656,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.859 0.125 0.016
Batch: 20 | Loss: 0.676 | Acc: 81.250,97.768,99.814,% | Adaptive Acc: 91.295% | clf_exit: 0.814 0.172 0.014
Batch: 40 | Loss: 0.664 | Acc: 81.460,97.923,99.771,% | Adaptive Acc: 91.120% | clf_exit: 0.814 0.172 0.014
Batch: 60 | Loss: 0.674 | Acc: 81.391,97.912,99.744,% | Adaptive Acc: 91.073% | clf_exit: 0.811 0.176 0.013
Batch: 80 | Loss: 0.685 | Acc: 81.298,97.811,99.759,% | Adaptive Acc: 91.088% | clf_exit: 0.805 0.182 0.013
Batch: 100 | Loss: 0.686 | Acc: 81.405,97.749,99.729,% | Adaptive Acc: 90.989% | clf_exit: 0.808 0.179 0.013
Batch: 120 | Loss: 0.690 | Acc: 81.185,97.785,99.729,% | Adaptive Acc: 90.793% | clf_exit: 0.809 0.179 0.013
Batch: 140 | Loss: 0.689 | Acc: 81.211,97.811,99.729,% | Adaptive Acc: 90.858% | clf_exit: 0.810 0.178 0.013
Batch: 160 | Loss: 0.686 | Acc: 81.250,97.836,99.733,% | Adaptive Acc: 90.853% | clf_exit: 0.810 0.178 0.012
Batch: 180 | Loss: 0.687 | Acc: 81.272,97.833,99.732,% | Adaptive Acc: 90.841% | clf_exit: 0.810 0.178 0.012
Batch: 200 | Loss: 0.686 | Acc: 81.285,97.874,99.716,% | Adaptive Acc: 90.897% | clf_exit: 0.808 0.180 0.012
Batch: 220 | Loss: 0.689 | Acc: 81.201,97.805,99.710,% | Adaptive Acc: 90.823% | clf_exit: 0.808 0.180 0.012
Batch: 240 | Loss: 0.688 | Acc: 81.214,97.828,99.708,% | Adaptive Acc: 90.829% | clf_exit: 0.808 0.180 0.012
Batch: 260 | Loss: 0.686 | Acc: 81.301,97.827,99.710,% | Adaptive Acc: 90.876% | clf_exit: 0.808 0.180 0.012
Batch: 280 | Loss: 0.686 | Acc: 81.356,97.784,99.711,% | Adaptive Acc: 90.892% | clf_exit: 0.808 0.180 0.012
Batch: 300 | Loss: 0.689 | Acc: 81.245,97.783,99.712,% | Adaptive Acc: 90.874% | clf_exit: 0.807 0.181 0.013
Batch: 320 | Loss: 0.686 | Acc: 81.352,97.785,99.713,% | Adaptive Acc: 90.941% | clf_exit: 0.807 0.180 0.012
Batch: 340 | Loss: 0.688 | Acc: 81.387,97.720,99.711,% | Adaptive Acc: 90.923% | clf_exit: 0.808 0.180 0.013
Batch: 360 | Loss: 0.689 | Acc: 81.354,97.728,99.701,% | Adaptive Acc: 90.880% | clf_exit: 0.808 0.180 0.012
Batch: 380 | Loss: 0.689 | Acc: 81.336,97.708,99.690,% | Adaptive Acc: 90.873% | clf_exit: 0.808 0.179 0.012
Batch: 0 | Loss: 1.069 | Acc: 78.906,91.406,93.750,% | Adaptive Acc: 85.156% | clf_exit: 0.828 0.148 0.023
Batch: 20 | Loss: 1.227 | Acc: 77.865,89.658,92.894,% | Adaptive Acc: 83.705% | clf_exit: 0.824 0.154 0.021
Batch: 40 | Loss: 1.233 | Acc: 78.068,89.539,92.778,% | Adaptive Acc: 83.975% | clf_exit: 0.834 0.144 0.022
Batch: 60 | Loss: 1.214 | Acc: 78.215,89.652,92.930,% | Adaptive Acc: 84.132% | clf_exit: 0.837 0.142 0.022
Train all parameters

Epoch: 218
Batch: 0 | Loss: 0.768 | Acc: 78.906,98.438,97.656,% | Adaptive Acc: 86.719% | clf_exit: 0.828 0.156 0.016
Batch: 20 | Loss: 0.675 | Acc: 81.585,97.917,99.665,% | Adaptive Acc: 90.662% | clf_exit: 0.818 0.170 0.012
Batch: 40 | Loss: 0.688 | Acc: 81.269,97.847,99.733,% | Adaptive Acc: 90.739% | clf_exit: 0.816 0.173 0.011
Batch: 60 | Loss: 0.693 | Acc: 81.404,97.772,99.731,% | Adaptive Acc: 90.651% | clf_exit: 0.813 0.175 0.012
Batch: 80 | Loss: 0.689 | Acc: 81.424,97.888,99.711,% | Adaptive Acc: 90.837% | clf_exit: 0.809 0.179 0.012
Batch: 100 | Loss: 0.688 | Acc: 81.451,97.857,99.714,% | Adaptive Acc: 90.927% | clf_exit: 0.809 0.178 0.013
Batch: 120 | Loss: 0.685 | Acc: 81.515,97.947,99.735,% | Adaptive Acc: 91.129% | clf_exit: 0.808 0.179 0.013
Batch: 140 | Loss: 0.684 | Acc: 81.483,97.950,99.729,% | Adaptive Acc: 91.085% | clf_exit: 0.809 0.179 0.012
Batch: 160 | Loss: 0.687 | Acc: 81.328,97.899,99.719,% | Adaptive Acc: 90.984% | clf_exit: 0.808 0.180 0.012
Batch: 180 | Loss: 0.689 | Acc: 81.392,97.859,99.719,% | Adaptive Acc: 90.949% | clf_exit: 0.808 0.180 0.012
Batch: 200 | Loss: 0.688 | Acc: 81.448,97.854,99.720,% | Adaptive Acc: 91.014% | clf_exit: 0.809 0.179 0.012
Batch: 220 | Loss: 0.686 | Acc: 81.476,97.833,99.728,% | Adaptive Acc: 91.081% | clf_exit: 0.808 0.180 0.012
Batch: 240 | Loss: 0.687 | Acc: 81.480,97.818,99.718,% | Adaptive Acc: 91.105% | clf_exit: 0.807 0.181 0.012
Batch: 260 | Loss: 0.687 | Acc: 81.474,97.791,99.719,% | Adaptive Acc: 91.137% | clf_exit: 0.806 0.181 0.012
Batch: 280 | Loss: 0.687 | Acc: 81.536,97.773,99.722,% | Adaptive Acc: 91.142% | clf_exit: 0.807 0.181 0.012
Batch: 300 | Loss: 0.687 | Acc: 81.561,97.757,99.720,% | Adaptive Acc: 91.162% | clf_exit: 0.808 0.180 0.012
Batch: 320 | Loss: 0.687 | Acc: 81.593,97.746,99.720,% | Adaptive Acc: 91.165% | clf_exit: 0.808 0.180 0.012
Batch: 340 | Loss: 0.686 | Acc: 81.621,97.750,99.702,% | Adaptive Acc: 91.173% | clf_exit: 0.808 0.179 0.012
Batch: 360 | Loss: 0.686 | Acc: 81.581,97.758,99.706,% | Adaptive Acc: 91.175% | clf_exit: 0.808 0.179 0.013
Batch: 380 | Loss: 0.687 | Acc: 81.537,97.763,99.709,% | Adaptive Acc: 91.164% | clf_exit: 0.808 0.179 0.013
Batch: 0 | Loss: 1.181 | Acc: 83.594,88.281,88.281,% | Adaptive Acc: 88.281% | clf_exit: 0.836 0.133 0.031
Batch: 20 | Loss: 1.217 | Acc: 78.906,90.327,92.894,% | Adaptive Acc: 84.263% | clf_exit: 0.845 0.134 0.021
Batch: 40 | Loss: 1.210 | Acc: 79.230,90.091,92.931,% | Adaptive Acc: 84.585% | clf_exit: 0.849 0.129 0.022
Batch: 60 | Loss: 1.198 | Acc: 79.290,90.061,92.982,% | Adaptive Acc: 84.516% | clf_exit: 0.849 0.129 0.022
Train all parameters

Epoch: 219
Batch: 0 | Loss: 0.821 | Acc: 80.469,94.531,98.438,% | Adaptive Acc: 89.062% | clf_exit: 0.820 0.148 0.031
Batch: 20 | Loss: 0.671 | Acc: 81.585,97.768,99.591,% | Adaptive Acc: 91.778% | clf_exit: 0.799 0.189 0.013
Batch: 40 | Loss: 0.670 | Acc: 81.803,97.809,99.638,% | Adaptive Acc: 91.521% | clf_exit: 0.803 0.184 0.013
Batch: 60 | Loss: 0.664 | Acc: 81.993,97.925,99.693,% | Adaptive Acc: 91.739% | clf_exit: 0.803 0.184 0.013
Batch: 80 | Loss: 0.662 | Acc: 82.002,98.023,99.701,% | Adaptive Acc: 91.628% | clf_exit: 0.807 0.181 0.012
Batch: 100 | Loss: 0.662 | Acc: 82.093,98.051,99.698,% | Adaptive Acc: 91.538% | clf_exit: 0.808 0.179 0.013
Batch: 120 | Loss: 0.666 | Acc: 82.064,98.037,99.716,% | Adaptive Acc: 91.484% | clf_exit: 0.806 0.181 0.013
Batch: 140 | Loss: 0.669 | Acc: 82.009,97.972,99.729,% | Adaptive Acc: 91.334% | clf_exit: 0.808 0.179 0.013
Batch: 160 | Loss: 0.671 | Acc: 82.031,97.904,99.738,% | Adaptive Acc: 91.285% | clf_exit: 0.810 0.177 0.013
Batch: 180 | Loss: 0.672 | Acc: 81.967,97.872,99.741,% | Adaptive Acc: 91.208% | clf_exit: 0.810 0.177 0.013
Batch: 200 | Loss: 0.677 | Acc: 81.794,97.831,99.728,% | Adaptive Acc: 91.095% | clf_exit: 0.809 0.178 0.013
Batch: 220 | Loss: 0.678 | Acc: 81.748,97.812,99.717,% | Adaptive Acc: 91.095% | clf_exit: 0.809 0.178 0.013
Batch: 240 | Loss: 0.681 | Acc: 81.665,97.770,99.724,% | Adaptive Acc: 91.011% | clf_exit: 0.809 0.178 0.013
Batch: 260 | Loss: 0.679 | Acc: 81.681,97.764,99.740,% | Adaptive Acc: 91.020% | clf_exit: 0.809 0.178 0.013
Batch: 280 | Loss: 0.681 | Acc: 81.564,97.770,99.722,% | Adaptive Acc: 90.945% | clf_exit: 0.808 0.179 0.013
Batch: 300 | Loss: 0.681 | Acc: 81.582,97.763,99.733,% | Adaptive Acc: 90.944% | clf_exit: 0.809 0.178 0.013
Batch: 320 | Loss: 0.681 | Acc: 81.615,97.739,99.723,% | Adaptive Acc: 91.010% | clf_exit: 0.809 0.178 0.013
Batch: 340 | Loss: 0.683 | Acc: 81.557,97.695,99.707,% | Adaptive Acc: 90.992% | clf_exit: 0.809 0.178 0.013
Batch: 360 | Loss: 0.684 | Acc: 81.568,97.667,99.682,% | Adaptive Acc: 90.984% | clf_exit: 0.809 0.178 0.013
Batch: 380 | Loss: 0.687 | Acc: 81.560,97.662,99.664,% | Adaptive Acc: 91.013% | clf_exit: 0.808 0.179 0.014
Batch: 0 | Loss: 1.090 | Acc: 81.250,90.625,92.188,% | Adaptive Acc: 89.062% | clf_exit: 0.812 0.172 0.016
Batch: 20 | Loss: 1.250 | Acc: 77.865,90.662,92.262,% | Adaptive Acc: 84.821% | clf_exit: 0.814 0.170 0.016
Batch: 40 | Loss: 1.254 | Acc: 78.506,90.111,92.454,% | Adaptive Acc: 84.966% | clf_exit: 0.822 0.161 0.017
Batch: 60 | Loss: 1.228 | Acc: 78.689,90.164,92.623,% | Adaptive Acc: 85.041% | clf_exit: 0.822 0.159 0.018
Train all parameters

Epoch: 220
Batch: 0 | Loss: 0.547 | Acc: 84.375,100.000,99.219,% | Adaptive Acc: 89.062% | clf_exit: 0.875 0.117 0.008
Batch: 20 | Loss: 0.699 | Acc: 81.287,97.954,99.442,% | Adaptive Acc: 90.365% | clf_exit: 0.820 0.168 0.012
Batch: 40 | Loss: 0.681 | Acc: 81.479,97.980,99.638,% | Adaptive Acc: 91.216% | clf_exit: 0.810 0.178 0.012
Batch: 60 | Loss: 0.670 | Acc: 81.737,98.105,99.757,% | Adaptive Acc: 91.406% | clf_exit: 0.809 0.179 0.012
Batch: 80 | Loss: 0.667 | Acc: 81.780,97.946,99.797,% | Adaptive Acc: 91.300% | clf_exit: 0.811 0.177 0.012
Batch: 100 | Loss: 0.670 | Acc: 81.714,98.004,99.768,% | Adaptive Acc: 91.406% | clf_exit: 0.810 0.178 0.012
Batch: 120 | Loss: 0.674 | Acc: 81.579,97.966,99.755,% | Adaptive Acc: 91.180% | clf_exit: 0.811 0.178 0.011
Batch: 140 | Loss: 0.673 | Acc: 81.610,97.972,99.756,% | Adaptive Acc: 91.196% | clf_exit: 0.809 0.180 0.011
Batch: 160 | Loss: 0.671 | Acc: 81.764,97.972,99.753,% | Adaptive Acc: 91.256% | clf_exit: 0.810 0.179 0.011
Batch: 180 | Loss: 0.669 | Acc: 81.794,97.963,99.758,% | Adaptive Acc: 91.216% | clf_exit: 0.811 0.178 0.011
Batch: 200 | Loss: 0.671 | Acc: 81.751,97.905,99.763,% | Adaptive Acc: 91.126% | clf_exit: 0.812 0.176 0.011
Batch: 220 | Loss: 0.674 | Acc: 81.685,97.900,99.742,% | Adaptive Acc: 91.074% | clf_exit: 0.813 0.176 0.011
Batch: 240 | Loss: 0.674 | Acc: 81.714,97.896,99.721,% | Adaptive Acc: 91.079% | clf_exit: 0.813 0.176 0.011
Batch: 260 | Loss: 0.678 | Acc: 81.534,97.863,99.716,% | Adaptive Acc: 90.975% | clf_exit: 0.812 0.176 0.011
Batch: 280 | Loss: 0.680 | Acc: 81.522,97.843,99.714,% | Adaptive Acc: 90.998% | clf_exit: 0.812 0.177 0.012
Batch: 300 | Loss: 0.678 | Acc: 81.551,97.846,99.727,% | Adaptive Acc: 91.004% | clf_exit: 0.812 0.176 0.011
Batch: 320 | Loss: 0.681 | Acc: 81.481,97.824,99.725,% | Adaptive Acc: 90.983% | clf_exit: 0.811 0.177 0.012
Batch: 340 | Loss: 0.684 | Acc: 81.362,97.821,99.723,% | Adaptive Acc: 90.927% | clf_exit: 0.810 0.178 0.012
Batch: 360 | Loss: 0.686 | Acc: 81.330,97.812,99.725,% | Adaptive Acc: 90.937% | clf_exit: 0.810 0.178 0.012
Batch: 380 | Loss: 0.686 | Acc: 81.330,97.798,99.723,% | Adaptive Acc: 90.937% | clf_exit: 0.810 0.179 0.012
Batch: 0 | Loss: 1.051 | Acc: 78.906,89.844,92.188,% | Adaptive Acc: 86.719% | clf_exit: 0.844 0.125 0.031
Batch: 20 | Loss: 1.210 | Acc: 78.497,90.179,92.522,% | Adaptive Acc: 84.449% | clf_exit: 0.842 0.135 0.022
Batch: 40 | Loss: 1.222 | Acc: 79.078,89.596,92.511,% | Adaptive Acc: 84.623% | clf_exit: 0.853 0.127 0.020
Batch: 60 | Loss: 1.206 | Acc: 79.431,89.690,92.674,% | Adaptive Acc: 84.798% | clf_exit: 0.853 0.127 0.019
Train all parameters

Epoch: 221
Batch: 0 | Loss: 0.482 | Acc: 88.281,99.219,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.867 0.133 0.000
Batch: 20 | Loss: 0.640 | Acc: 82.775,98.400,99.740,% | Adaptive Acc: 92.894% | clf_exit: 0.806 0.183 0.011
Batch: 40 | Loss: 0.662 | Acc: 82.146,98.285,99.771,% | Adaptive Acc: 92.188% | clf_exit: 0.811 0.177 0.011
Batch: 60 | Loss: 0.661 | Acc: 82.172,98.220,99.744,% | Adaptive Acc: 91.752% | clf_exit: 0.813 0.176 0.011
Batch: 80 | Loss: 0.666 | Acc: 81.944,98.061,99.749,% | Adaptive Acc: 91.503% | clf_exit: 0.812 0.177 0.011
Batch: 100 | Loss: 0.664 | Acc: 81.822,98.167,99.791,% | Adaptive Acc: 91.406% | clf_exit: 0.812 0.177 0.012
Batch: 120 | Loss: 0.667 | Acc: 81.741,98.108,99.768,% | Adaptive Acc: 91.225% | clf_exit: 0.812 0.177 0.012
Batch: 140 | Loss: 0.667 | Acc: 81.821,98.072,99.767,% | Adaptive Acc: 91.290% | clf_exit: 0.812 0.177 0.012
Batch: 160 | Loss: 0.670 | Acc: 81.711,98.059,99.782,% | Adaptive Acc: 91.125% | clf_exit: 0.813 0.176 0.012
Batch: 180 | Loss: 0.669 | Acc: 81.841,98.084,99.776,% | Adaptive Acc: 91.190% | clf_exit: 0.813 0.175 0.012
Batch: 200 | Loss: 0.667 | Acc: 81.981,98.076,99.763,% | Adaptive Acc: 91.224% | clf_exit: 0.813 0.175 0.012
Batch: 220 | Loss: 0.671 | Acc: 81.869,98.010,99.756,% | Adaptive Acc: 91.138% | clf_exit: 0.812 0.176 0.012
Batch: 240 | Loss: 0.674 | Acc: 81.785,97.958,99.734,% | Adaptive Acc: 91.134% | clf_exit: 0.811 0.177 0.012
Batch: 260 | Loss: 0.675 | Acc: 81.753,97.908,99.743,% | Adaptive Acc: 91.107% | clf_exit: 0.811 0.177 0.012
Batch: 280 | Loss: 0.677 | Acc: 81.670,97.868,99.736,% | Adaptive Acc: 91.053% | clf_exit: 0.811 0.177 0.012
Batch: 300 | Loss: 0.681 | Acc: 81.543,97.815,99.720,% | Adaptive Acc: 90.978% | clf_exit: 0.810 0.178 0.012
Batch: 320 | Loss: 0.680 | Acc: 81.549,97.800,99.723,% | Adaptive Acc: 90.951% | clf_exit: 0.811 0.177 0.012
Batch: 340 | Loss: 0.682 | Acc: 81.511,97.769,99.720,% | Adaptive Acc: 90.921% | clf_exit: 0.811 0.177 0.012
Batch: 360 | Loss: 0.684 | Acc: 81.462,97.754,99.721,% | Adaptive Acc: 90.887% | clf_exit: 0.810 0.178 0.012
Batch: 380 | Loss: 0.686 | Acc: 81.430,97.734,99.705,% | Adaptive Acc: 90.859% | clf_exit: 0.810 0.178 0.012
Batch: 0 | Loss: 1.113 | Acc: 82.031,91.406,91.406,% | Adaptive Acc: 87.500% | clf_exit: 0.836 0.148 0.016
Batch: 20 | Loss: 1.308 | Acc: 77.269,89.435,92.634,% | Adaptive Acc: 84.635% | clf_exit: 0.823 0.156 0.020
Batch: 40 | Loss: 1.303 | Acc: 77.458,89.291,92.569,% | Adaptive Acc: 84.165% | clf_exit: 0.833 0.147 0.020
Batch: 60 | Loss: 1.281 | Acc: 77.497,89.664,92.802,% | Adaptive Acc: 84.183% | clf_exit: 0.832 0.148 0.020
Train all parameters

Epoch: 222
Batch: 0 | Loss: 0.686 | Acc: 81.250,99.219,99.219,% | Adaptive Acc: 92.188% | clf_exit: 0.781 0.203 0.016
Batch: 20 | Loss: 0.663 | Acc: 81.882,98.289,99.777,% | Adaptive Acc: 91.369% | clf_exit: 0.809 0.179 0.012
Batch: 40 | Loss: 0.680 | Acc: 81.784,97.847,99.676,% | Adaptive Acc: 91.235% | clf_exit: 0.809 0.179 0.012
Batch: 60 | Loss: 0.686 | Acc: 81.288,97.951,99.705,% | Adaptive Acc: 90.856% | clf_exit: 0.813 0.175 0.011
Batch: 80 | Loss: 0.682 | Acc: 81.279,97.955,99.740,% | Adaptive Acc: 90.712% | clf_exit: 0.815 0.174 0.011
Batch: 100 | Loss: 0.683 | Acc: 81.436,97.950,99.706,% | Adaptive Acc: 90.702% | clf_exit: 0.818 0.170 0.012
Batch: 120 | Loss: 0.683 | Acc: 81.444,97.882,99.690,% | Adaptive Acc: 90.793% | clf_exit: 0.815 0.173 0.012
Batch: 140 | Loss: 0.681 | Acc: 81.438,97.939,99.717,% | Adaptive Acc: 90.780% | clf_exit: 0.815 0.173 0.012
Batch: 160 | Loss: 0.679 | Acc: 81.536,97.923,99.733,% | Adaptive Acc: 90.906% | clf_exit: 0.814 0.174 0.012
Batch: 180 | Loss: 0.678 | Acc: 81.535,97.876,99.741,% | Adaptive Acc: 90.884% | clf_exit: 0.814 0.173 0.013
Batch: 200 | Loss: 0.679 | Acc: 81.522,97.897,99.755,% | Adaptive Acc: 90.951% | clf_exit: 0.813 0.174 0.013
Batch: 220 | Loss: 0.681 | Acc: 81.497,97.900,99.745,% | Adaptive Acc: 90.922% | clf_exit: 0.813 0.175 0.013
Batch: 240 | Loss: 0.684 | Acc: 81.419,97.893,99.731,% | Adaptive Acc: 90.888% | clf_exit: 0.811 0.176 0.013
Batch: 260 | Loss: 0.686 | Acc: 81.385,97.854,99.719,% | Adaptive Acc: 90.817% | clf_exit: 0.811 0.176 0.013
Batch: 280 | Loss: 0.686 | Acc: 81.375,97.843,99.722,% | Adaptive Acc: 90.842% | clf_exit: 0.811 0.176 0.013
Batch: 300 | Loss: 0.685 | Acc: 81.382,97.843,99.720,% | Adaptive Acc: 90.843% | clf_exit: 0.812 0.176 0.013
Batch: 320 | Loss: 0.686 | Acc: 81.367,97.807,99.708,% | Adaptive Acc: 90.859% | clf_exit: 0.811 0.177 0.013
Batch: 340 | Loss: 0.684 | Acc: 81.433,97.803,99.709,% | Adaptive Acc: 90.914% | clf_exit: 0.810 0.177 0.012
Batch: 360 | Loss: 0.684 | Acc: 81.425,97.788,99.710,% | Adaptive Acc: 90.865% | clf_exit: 0.811 0.176 0.012
Batch: 380 | Loss: 0.686 | Acc: 81.391,97.751,99.701,% | Adaptive Acc: 90.875% | clf_exit: 0.810 0.177 0.013
Batch: 0 | Loss: 1.074 | Acc: 80.469,91.406,92.188,% | Adaptive Acc: 85.938% | clf_exit: 0.852 0.133 0.016
Batch: 20 | Loss: 1.300 | Acc: 77.716,89.286,92.336,% | Adaptive Acc: 83.519% | clf_exit: 0.836 0.143 0.021
Batch: 40 | Loss: 1.281 | Acc: 78.373,89.139,92.530,% | Adaptive Acc: 83.594% | clf_exit: 0.843 0.137 0.020
Batch: 60 | Loss: 1.243 | Acc: 78.612,89.216,92.713,% | Adaptive Acc: 84.080% | clf_exit: 0.842 0.137 0.021
Train all parameters

Epoch: 223
Batch: 0 | Loss: 0.694 | Acc: 82.812,97.656,99.219,% | Adaptive Acc: 89.844% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 0.690 | Acc: 81.510,97.545,99.516,% | Adaptive Acc: 90.774% | clf_exit: 0.812 0.176 0.012
Batch: 40 | Loss: 0.672 | Acc: 82.336,97.732,99.600,% | Adaptive Acc: 90.987% | clf_exit: 0.814 0.177 0.009
Batch: 60 | Loss: 0.673 | Acc: 82.095,97.759,99.654,% | Adaptive Acc: 90.971% | clf_exit: 0.811 0.180 0.009
Batch: 80 | Loss: 0.670 | Acc: 82.215,97.830,99.701,% | Adaptive Acc: 91.213% | clf_exit: 0.811 0.179 0.010
Batch: 100 | Loss: 0.671 | Acc: 82.101,97.873,99.722,% | Adaptive Acc: 91.267% | clf_exit: 0.809 0.180 0.011
Batch: 120 | Loss: 0.671 | Acc: 82.051,97.831,99.697,% | Adaptive Acc: 91.251% | clf_exit: 0.808 0.180 0.011
Batch: 140 | Loss: 0.674 | Acc: 81.932,97.773,99.679,% | Adaptive Acc: 91.151% | clf_exit: 0.809 0.179 0.012
Batch: 160 | Loss: 0.678 | Acc: 81.779,97.734,99.680,% | Adaptive Acc: 91.023% | clf_exit: 0.810 0.178 0.012
Batch: 180 | Loss: 0.679 | Acc: 81.712,97.743,99.689,% | Adaptive Acc: 90.970% | clf_exit: 0.810 0.177 0.012
Batch: 200 | Loss: 0.678 | Acc: 81.775,97.753,99.689,% | Adaptive Acc: 91.014% | clf_exit: 0.810 0.178 0.012
Batch: 220 | Loss: 0.678 | Acc: 81.763,97.730,99.682,% | Adaptive Acc: 90.929% | clf_exit: 0.810 0.178 0.012
Batch: 240 | Loss: 0.679 | Acc: 81.688,97.757,99.692,% | Adaptive Acc: 90.897% | clf_exit: 0.810 0.178 0.012
Batch: 260 | Loss: 0.680 | Acc: 81.729,97.749,99.698,% | Adaptive Acc: 90.942% | clf_exit: 0.810 0.178 0.012
Batch: 280 | Loss: 0.682 | Acc: 81.661,97.717,99.703,% | Adaptive Acc: 90.911% | clf_exit: 0.810 0.178 0.012
Batch: 300 | Loss: 0.683 | Acc: 81.642,97.708,99.712,% | Adaptive Acc: 90.939% | clf_exit: 0.810 0.178 0.012
Batch: 320 | Loss: 0.682 | Acc: 81.703,97.710,99.701,% | Adaptive Acc: 90.961% | clf_exit: 0.810 0.177 0.013
Batch: 340 | Loss: 0.685 | Acc: 81.580,97.681,99.709,% | Adaptive Acc: 90.877% | clf_exit: 0.810 0.177 0.013
Batch: 360 | Loss: 0.685 | Acc: 81.568,97.693,99.716,% | Adaptive Acc: 90.870% | clf_exit: 0.809 0.178 0.013
Batch: 380 | Loss: 0.685 | Acc: 81.541,97.703,99.707,% | Adaptive Acc: 90.822% | clf_exit: 0.810 0.178 0.013
Batch: 0 | Loss: 1.162 | Acc: 77.344,92.969,92.188,% | Adaptive Acc: 79.688% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.287 | Acc: 77.009,89.509,92.448,% | Adaptive Acc: 82.440% | clf_exit: 0.847 0.138 0.015
Batch: 40 | Loss: 1.283 | Acc: 77.496,89.558,92.569,% | Adaptive Acc: 83.022% | clf_exit: 0.852 0.131 0.018
Batch: 60 | Loss: 1.248 | Acc: 77.715,89.690,92.751,% | Adaptive Acc: 83.363% | clf_exit: 0.852 0.130 0.018
Train all parameters

Epoch: 224
Batch: 0 | Loss: 0.593 | Acc: 83.594,99.219,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.797 0.195 0.008
Batch: 20 | Loss: 0.676 | Acc: 81.696,97.693,99.702,% | Adaptive Acc: 90.923% | clf_exit: 0.812 0.177 0.012
Batch: 40 | Loss: 0.675 | Acc: 81.841,97.942,99.771,% | Adaptive Acc: 91.235% | clf_exit: 0.809 0.179 0.012
Batch: 60 | Loss: 0.677 | Acc: 81.916,97.887,99.654,% | Adaptive Acc: 91.278% | clf_exit: 0.806 0.181 0.012
Batch: 80 | Loss: 0.675 | Acc: 81.887,97.897,99.653,% | Adaptive Acc: 91.348% | clf_exit: 0.807 0.179 0.013
Batch: 100 | Loss: 0.674 | Acc: 81.884,97.919,99.698,% | Adaptive Acc: 91.368% | clf_exit: 0.810 0.177 0.013
Batch: 120 | Loss: 0.680 | Acc: 81.644,97.895,99.709,% | Adaptive Acc: 91.355% | clf_exit: 0.807 0.180 0.013
Batch: 140 | Loss: 0.679 | Acc: 81.621,97.906,99.729,% | Adaptive Acc: 91.390% | clf_exit: 0.806 0.181 0.013
Batch: 160 | Loss: 0.678 | Acc: 81.721,97.928,99.719,% | Adaptive Acc: 91.309% | clf_exit: 0.809 0.178 0.013
Batch: 180 | Loss: 0.683 | Acc: 81.591,97.863,99.715,% | Adaptive Acc: 91.216% | clf_exit: 0.808 0.179 0.013
Batch: 200 | Loss: 0.683 | Acc: 81.584,97.874,99.705,% | Adaptive Acc: 91.204% | clf_exit: 0.809 0.179 0.013
Batch: 220 | Loss: 0.683 | Acc: 81.568,97.829,99.671,% | Adaptive Acc: 91.169% | clf_exit: 0.808 0.179 0.013
Batch: 240 | Loss: 0.681 | Acc: 81.600,97.825,99.669,% | Adaptive Acc: 91.111% | clf_exit: 0.810 0.178 0.012
Batch: 260 | Loss: 0.679 | Acc: 81.585,97.848,99.668,% | Adaptive Acc: 91.122% | clf_exit: 0.809 0.179 0.012
Batch: 280 | Loss: 0.681 | Acc: 81.581,97.848,99.672,% | Adaptive Acc: 91.139% | clf_exit: 0.809 0.179 0.012
Batch: 300 | Loss: 0.683 | Acc: 81.520,97.825,99.683,% | Adaptive Acc: 91.144% | clf_exit: 0.808 0.180 0.012
Batch: 320 | Loss: 0.682 | Acc: 81.564,97.819,99.681,% | Adaptive Acc: 91.160% | clf_exit: 0.808 0.180 0.012
Batch: 340 | Loss: 0.682 | Acc: 81.587,97.810,99.684,% | Adaptive Acc: 91.147% | clf_exit: 0.808 0.179 0.013
Batch: 360 | Loss: 0.683 | Acc: 81.581,97.803,99.686,% | Adaptive Acc: 91.129% | clf_exit: 0.808 0.179 0.013
Batch: 380 | Loss: 0.681 | Acc: 81.631,97.814,99.688,% | Adaptive Acc: 91.168% | clf_exit: 0.809 0.179 0.013
Batch: 0 | Loss: 1.050 | Acc: 77.344,91.406,90.625,% | Adaptive Acc: 86.719% | clf_exit: 0.812 0.164 0.023
Batch: 20 | Loss: 1.195 | Acc: 78.348,90.179,92.746,% | Adaptive Acc: 84.412% | clf_exit: 0.831 0.148 0.021
Batch: 40 | Loss: 1.208 | Acc: 78.773,90.358,93.178,% | Adaptive Acc: 84.699% | clf_exit: 0.838 0.141 0.021
Batch: 60 | Loss: 1.191 | Acc: 78.970,90.369,93.199,% | Adaptive Acc: 84.964% | clf_exit: 0.841 0.139 0.021
Train all parameters

Epoch: 225
Batch: 0 | Loss: 0.561 | Acc: 89.062,97.656,99.219,% | Adaptive Acc: 96.875% | clf_exit: 0.789 0.203 0.008
Batch: 20 | Loss: 0.664 | Acc: 80.543,98.028,99.740,% | Adaptive Acc: 91.220% | clf_exit: 0.806 0.183 0.011
Batch: 40 | Loss: 0.660 | Acc: 81.364,98.075,99.790,% | Adaptive Acc: 91.406% | clf_exit: 0.809 0.181 0.010
Batch: 60 | Loss: 0.647 | Acc: 82.223,98.245,99.821,% | Adaptive Acc: 91.739% | clf_exit: 0.809 0.181 0.010
Batch: 80 | Loss: 0.645 | Acc: 82.311,98.302,99.788,% | Adaptive Acc: 91.840% | clf_exit: 0.814 0.175 0.011
Batch: 100 | Loss: 0.638 | Acc: 82.712,98.368,99.807,% | Adaptive Acc: 92.048% | clf_exit: 0.814 0.175 0.011
Batch: 120 | Loss: 0.638 | Acc: 82.793,98.366,99.832,% | Adaptive Acc: 92.091% | clf_exit: 0.813 0.176 0.011
Batch: 140 | Loss: 0.633 | Acc: 83.001,98.449,99.850,% | Adaptive Acc: 92.121% | clf_exit: 0.815 0.175 0.010
Batch: 160 | Loss: 0.631 | Acc: 82.982,98.501,99.859,% | Adaptive Acc: 92.061% | clf_exit: 0.816 0.174 0.010
Batch: 180 | Loss: 0.626 | Acc: 83.106,98.528,99.875,% | Adaptive Acc: 92.075% | clf_exit: 0.817 0.173 0.010
Batch: 200 | Loss: 0.626 | Acc: 83.022,98.546,99.883,% | Adaptive Acc: 92.044% | clf_exit: 0.818 0.173 0.010
Batch: 220 | Loss: 0.626 | Acc: 82.915,98.561,99.894,% | Adaptive Acc: 92.007% | clf_exit: 0.817 0.173 0.009
Batch: 240 | Loss: 0.623 | Acc: 82.903,98.616,99.900,% | Adaptive Acc: 91.996% | clf_exit: 0.818 0.173 0.009
Batch: 260 | Loss: 0.621 | Acc: 82.965,98.632,99.898,% | Adaptive Acc: 91.996% | clf_exit: 0.819 0.172 0.009
Batch: 280 | Loss: 0.619 | Acc: 82.982,98.652,99.894,% | Adaptive Acc: 92.046% | clf_exit: 0.819 0.173 0.009
Batch: 300 | Loss: 0.618 | Acc: 83.010,98.676,99.901,% | Adaptive Acc: 92.045% | clf_exit: 0.819 0.172 0.009
Batch: 320 | Loss: 0.618 | Acc: 83.070,98.693,99.900,% | Adaptive Acc: 92.102% | clf_exit: 0.819 0.173 0.009
Batch: 340 | Loss: 0.619 | Acc: 83.044,98.685,99.895,% | Adaptive Acc: 92.098% | clf_exit: 0.819 0.173 0.009
Batch: 360 | Loss: 0.619 | Acc: 83.016,98.686,99.900,% | Adaptive Acc: 92.103% | clf_exit: 0.818 0.173 0.009
Batch: 380 | Loss: 0.618 | Acc: 83.030,98.704,99.904,% | Adaptive Acc: 92.116% | clf_exit: 0.818 0.173 0.009
Batch: 0 | Loss: 0.928 | Acc: 81.250,91.406,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.836 0.133 0.031
Batch: 20 | Loss: 1.117 | Acc: 79.985,90.960,93.601,% | Adaptive Acc: 85.826% | clf_exit: 0.837 0.144 0.019
Batch: 40 | Loss: 1.126 | Acc: 80.316,90.739,93.579,% | Adaptive Acc: 85.671% | clf_exit: 0.847 0.132 0.021
Batch: 60 | Loss: 1.108 | Acc: 80.469,90.856,93.545,% | Adaptive Acc: 85.925% | clf_exit: 0.847 0.131 0.022
Train all parameters

Epoch: 226
Batch: 0 | Loss: 0.530 | Acc: 87.500,100.000,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.820 0.164 0.016
Batch: 20 | Loss: 0.602 | Acc: 83.631,99.070,99.963,% | Adaptive Acc: 92.969% | clf_exit: 0.818 0.174 0.008
Batch: 40 | Loss: 0.611 | Acc: 83.022,99.009,99.924,% | Adaptive Acc: 92.130% | clf_exit: 0.820 0.173 0.007
Batch: 60 | Loss: 0.603 | Acc: 83.325,99.001,99.910,% | Adaptive Acc: 92.559% | clf_exit: 0.817 0.175 0.008
Batch: 80 | Loss: 0.603 | Acc: 83.324,98.949,99.904,% | Adaptive Acc: 92.467% | clf_exit: 0.817 0.175 0.008
Batch: 100 | Loss: 0.606 | Acc: 83.145,98.971,99.915,% | Adaptive Acc: 92.342% | clf_exit: 0.816 0.175 0.008
Batch: 120 | Loss: 0.606 | Acc: 83.168,99.006,99.916,% | Adaptive Acc: 92.375% | clf_exit: 0.818 0.174 0.008
Batch: 140 | Loss: 0.608 | Acc: 83.051,99.008,99.922,% | Adaptive Acc: 92.298% | clf_exit: 0.818 0.174 0.008
Batch: 160 | Loss: 0.610 | Acc: 83.016,98.991,99.918,% | Adaptive Acc: 92.255% | clf_exit: 0.817 0.174 0.008
Batch: 180 | Loss: 0.614 | Acc: 82.869,98.968,99.901,% | Adaptive Acc: 92.205% | clf_exit: 0.815 0.176 0.008
Batch: 200 | Loss: 0.614 | Acc: 82.875,98.986,99.899,% | Adaptive Acc: 92.090% | clf_exit: 0.817 0.175 0.008
Batch: 220 | Loss: 0.612 | Acc: 82.989,98.975,99.901,% | Adaptive Acc: 92.134% | clf_exit: 0.817 0.175 0.008
Batch: 240 | Loss: 0.613 | Acc: 82.988,98.972,99.909,% | Adaptive Acc: 92.132% | clf_exit: 0.817 0.175 0.008
Batch: 260 | Loss: 0.611 | Acc: 83.055,98.988,99.910,% | Adaptive Acc: 92.161% | clf_exit: 0.818 0.175 0.008
Batch: 280 | Loss: 0.610 | Acc: 83.085,98.980,99.905,% | Adaptive Acc: 92.165% | clf_exit: 0.818 0.174 0.008
Batch: 300 | Loss: 0.609 | Acc: 83.121,98.970,99.899,% | Adaptive Acc: 92.175% | clf_exit: 0.818 0.174 0.008
Batch: 320 | Loss: 0.608 | Acc: 83.141,98.956,99.903,% | Adaptive Acc: 92.234% | clf_exit: 0.819 0.173 0.008
Batch: 340 | Loss: 0.607 | Acc: 83.142,98.955,99.906,% | Adaptive Acc: 92.226% | clf_exit: 0.819 0.173 0.008
Batch: 360 | Loss: 0.607 | Acc: 83.163,98.961,99.905,% | Adaptive Acc: 92.233% | clf_exit: 0.819 0.174 0.008
Batch: 380 | Loss: 0.606 | Acc: 83.257,98.954,99.908,% | Adaptive Acc: 92.284% | clf_exit: 0.819 0.173 0.008
Batch: 0 | Loss: 0.904 | Acc: 81.250,91.406,92.188,% | Adaptive Acc: 89.062% | clf_exit: 0.852 0.109 0.039
Batch: 20 | Loss: 1.107 | Acc: 80.432,90.885,93.527,% | Adaptive Acc: 85.714% | clf_exit: 0.840 0.144 0.017
Batch: 40 | Loss: 1.119 | Acc: 80.545,90.663,93.617,% | Adaptive Acc: 85.747% | clf_exit: 0.848 0.134 0.019
Batch: 60 | Loss: 1.101 | Acc: 80.674,90.804,93.532,% | Adaptive Acc: 86.078% | clf_exit: 0.849 0.132 0.019
Train all parameters

Epoch: 227
Batch: 0 | Loss: 0.496 | Acc: 89.844,100.000,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.581 | Acc: 83.854,99.107,99.926,% | Adaptive Acc: 93.229% | clf_exit: 0.826 0.169 0.005
Batch: 40 | Loss: 0.577 | Acc: 83.899,99.238,99.924,% | Adaptive Acc: 93.026% | clf_exit: 0.825 0.170 0.006
Batch: 60 | Loss: 0.585 | Acc: 83.773,99.244,99.936,% | Adaptive Acc: 92.930% | clf_exit: 0.821 0.173 0.006
Batch: 80 | Loss: 0.587 | Acc: 83.748,99.238,99.913,% | Adaptive Acc: 92.805% | clf_exit: 0.822 0.172 0.006
Batch: 100 | Loss: 0.584 | Acc: 83.803,99.219,99.915,% | Adaptive Acc: 92.768% | clf_exit: 0.823 0.171 0.006
Batch: 120 | Loss: 0.589 | Acc: 83.710,99.206,99.923,% | Adaptive Acc: 92.730% | clf_exit: 0.821 0.172 0.007
Batch: 140 | Loss: 0.594 | Acc: 83.500,99.158,99.917,% | Adaptive Acc: 92.586% | clf_exit: 0.819 0.174 0.007
Batch: 160 | Loss: 0.592 | Acc: 83.545,99.141,99.922,% | Adaptive Acc: 92.639% | clf_exit: 0.819 0.174 0.007
Batch: 180 | Loss: 0.591 | Acc: 83.576,99.119,99.922,% | Adaptive Acc: 92.615% | clf_exit: 0.820 0.173 0.006
Batch: 200 | Loss: 0.592 | Acc: 83.574,99.125,99.926,% | Adaptive Acc: 92.619% | clf_exit: 0.821 0.173 0.006
Batch: 220 | Loss: 0.592 | Acc: 83.622,99.109,99.926,% | Adaptive Acc: 92.583% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.594 | Acc: 83.509,99.099,99.929,% | Adaptive Acc: 92.508% | clf_exit: 0.822 0.171 0.007
Batch: 260 | Loss: 0.595 | Acc: 83.504,99.072,99.928,% | Adaptive Acc: 92.439% | clf_exit: 0.823 0.171 0.006
Batch: 280 | Loss: 0.596 | Acc: 83.521,99.057,99.933,% | Adaptive Acc: 92.466% | clf_exit: 0.822 0.171 0.006
Batch: 300 | Loss: 0.595 | Acc: 83.542,99.060,99.935,% | Adaptive Acc: 92.522% | clf_exit: 0.822 0.171 0.007
Batch: 320 | Loss: 0.595 | Acc: 83.506,99.061,99.932,% | Adaptive Acc: 92.497% | clf_exit: 0.822 0.171 0.007
Batch: 340 | Loss: 0.595 | Acc: 83.498,99.045,99.934,% | Adaptive Acc: 92.456% | clf_exit: 0.822 0.171 0.007
Batch: 360 | Loss: 0.598 | Acc: 83.408,99.041,99.931,% | Adaptive Acc: 92.449% | clf_exit: 0.821 0.172 0.007
Batch: 380 | Loss: 0.599 | Acc: 83.389,99.038,99.932,% | Adaptive Acc: 92.434% | clf_exit: 0.820 0.173 0.007
Batch: 0 | Loss: 0.915 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.844 0.102 0.055
Batch: 20 | Loss: 1.114 | Acc: 80.246,90.923,93.490,% | Adaptive Acc: 85.938% | clf_exit: 0.833 0.146 0.021
Batch: 40 | Loss: 1.118 | Acc: 80.412,90.587,93.636,% | Adaptive Acc: 85.842% | clf_exit: 0.844 0.134 0.022
Batch: 60 | Loss: 1.097 | Acc: 80.686,90.843,93.584,% | Adaptive Acc: 86.091% | clf_exit: 0.846 0.133 0.022
Train all parameters

Epoch: 228
Batch: 0 | Loss: 0.691 | Acc: 83.594,99.219,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.758 0.227 0.016
Batch: 20 | Loss: 0.606 | Acc: 83.110,99.219,99.963,% | Adaptive Acc: 92.262% | clf_exit: 0.815 0.178 0.007
Batch: 40 | Loss: 0.596 | Acc: 83.613,99.066,99.962,% | Adaptive Acc: 92.340% | clf_exit: 0.818 0.174 0.007
Batch: 60 | Loss: 0.590 | Acc: 83.517,99.168,99.974,% | Adaptive Acc: 92.572% | clf_exit: 0.816 0.177 0.007
Batch: 80 | Loss: 0.588 | Acc: 83.642,99.113,99.981,% | Adaptive Acc: 92.757% | clf_exit: 0.815 0.178 0.007
Batch: 100 | Loss: 0.587 | Acc: 83.547,99.118,99.985,% | Adaptive Acc: 92.644% | clf_exit: 0.817 0.176 0.007
Batch: 120 | Loss: 0.588 | Acc: 83.529,99.096,99.981,% | Adaptive Acc: 92.633% | clf_exit: 0.819 0.175 0.007
Batch: 140 | Loss: 0.591 | Acc: 83.433,99.058,99.983,% | Adaptive Acc: 92.514% | clf_exit: 0.819 0.175 0.007
Batch: 160 | Loss: 0.589 | Acc: 83.516,99.049,99.981,% | Adaptive Acc: 92.493% | clf_exit: 0.821 0.172 0.007
Batch: 180 | Loss: 0.593 | Acc: 83.408,98.990,99.974,% | Adaptive Acc: 92.446% | clf_exit: 0.821 0.173 0.007
Batch: 200 | Loss: 0.595 | Acc: 83.322,99.021,99.977,% | Adaptive Acc: 92.374% | clf_exit: 0.821 0.172 0.007
Batch: 220 | Loss: 0.597 | Acc: 83.219,99.024,99.972,% | Adaptive Acc: 92.315% | clf_exit: 0.820 0.173 0.007
Batch: 240 | Loss: 0.598 | Acc: 83.124,99.027,99.971,% | Adaptive Acc: 92.311% | clf_exit: 0.819 0.174 0.007
Batch: 260 | Loss: 0.598 | Acc: 83.118,99.045,99.973,% | Adaptive Acc: 92.313% | clf_exit: 0.819 0.174 0.007
Batch: 280 | Loss: 0.600 | Acc: 83.079,99.041,99.972,% | Adaptive Acc: 92.296% | clf_exit: 0.819 0.174 0.007
Batch: 300 | Loss: 0.601 | Acc: 83.101,99.040,99.969,% | Adaptive Acc: 92.281% | clf_exit: 0.819 0.174 0.007
Batch: 320 | Loss: 0.600 | Acc: 83.112,99.036,99.966,% | Adaptive Acc: 92.243% | clf_exit: 0.820 0.174 0.007
Batch: 340 | Loss: 0.601 | Acc: 83.099,99.049,99.968,% | Adaptive Acc: 92.233% | clf_exit: 0.820 0.174 0.006
Batch: 360 | Loss: 0.600 | Acc: 83.128,99.039,99.965,% | Adaptive Acc: 92.222% | clf_exit: 0.820 0.174 0.006
Batch: 380 | Loss: 0.598 | Acc: 83.241,99.032,99.965,% | Adaptive Acc: 92.288% | clf_exit: 0.821 0.172 0.007
Batch: 0 | Loss: 0.925 | Acc: 80.469,91.406,92.969,% | Adaptive Acc: 86.719% | clf_exit: 0.875 0.094 0.031
Batch: 20 | Loss: 1.110 | Acc: 80.097,90.551,93.452,% | Adaptive Acc: 85.193% | clf_exit: 0.846 0.135 0.019
Batch: 40 | Loss: 1.114 | Acc: 80.412,90.625,93.502,% | Adaptive Acc: 85.423% | clf_exit: 0.852 0.126 0.022
Batch: 60 | Loss: 1.094 | Acc: 80.546,90.856,93.609,% | Adaptive Acc: 85.656% | clf_exit: 0.853 0.126 0.021
Train all parameters

Epoch: 229
Batch: 0 | Loss: 0.635 | Acc: 81.250,97.656,100.000,% | Adaptive Acc: 85.938% | clf_exit: 0.883 0.109 0.008
Batch: 20 | Loss: 0.559 | Acc: 84.933,99.330,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.826 0.168 0.006
Batch: 40 | Loss: 0.583 | Acc: 84.318,99.143,100.000,% | Adaptive Acc: 92.473% | clf_exit: 0.830 0.163 0.007
Batch: 60 | Loss: 0.578 | Acc: 84.285,99.155,100.000,% | Adaptive Acc: 92.354% | clf_exit: 0.830 0.163 0.006
Batch: 80 | Loss: 0.585 | Acc: 83.845,99.103,99.990,% | Adaptive Acc: 92.371% | clf_exit: 0.827 0.167 0.006
Batch: 100 | Loss: 0.586 | Acc: 83.741,99.126,99.992,% | Adaptive Acc: 92.443% | clf_exit: 0.826 0.167 0.007
Batch: 120 | Loss: 0.583 | Acc: 83.794,99.096,99.981,% | Adaptive Acc: 92.485% | clf_exit: 0.827 0.167 0.006
Batch: 140 | Loss: 0.583 | Acc: 83.860,99.108,99.967,% | Adaptive Acc: 92.559% | clf_exit: 0.826 0.168 0.006
Batch: 160 | Loss: 0.586 | Acc: 83.768,99.122,99.961,% | Adaptive Acc: 92.445% | clf_exit: 0.826 0.168 0.006
Batch: 180 | Loss: 0.589 | Acc: 83.663,99.102,99.961,% | Adaptive Acc: 92.498% | clf_exit: 0.823 0.170 0.006
Batch: 200 | Loss: 0.588 | Acc: 83.640,99.114,99.957,% | Adaptive Acc: 92.561% | clf_exit: 0.822 0.171 0.006
Batch: 220 | Loss: 0.589 | Acc: 83.562,99.120,99.954,% | Adaptive Acc: 92.555% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.587 | Acc: 83.629,99.134,99.955,% | Adaptive Acc: 92.599% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.588 | Acc: 83.591,99.141,99.958,% | Adaptive Acc: 92.559% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.591 | Acc: 83.524,99.130,99.956,% | Adaptive Acc: 92.513% | clf_exit: 0.821 0.172 0.006
Batch: 300 | Loss: 0.591 | Acc: 83.534,99.118,99.956,% | Adaptive Acc: 92.530% | clf_exit: 0.822 0.172 0.006
Batch: 320 | Loss: 0.593 | Acc: 83.431,99.126,99.956,% | Adaptive Acc: 92.472% | clf_exit: 0.821 0.173 0.006
Batch: 340 | Loss: 0.592 | Acc: 83.424,99.120,99.956,% | Adaptive Acc: 92.460% | clf_exit: 0.821 0.173 0.006
Batch: 360 | Loss: 0.592 | Acc: 83.416,99.130,99.959,% | Adaptive Acc: 92.430% | clf_exit: 0.821 0.173 0.006
Batch: 380 | Loss: 0.593 | Acc: 83.378,99.120,99.961,% | Adaptive Acc: 92.429% | clf_exit: 0.821 0.172 0.006
Batch: 0 | Loss: 0.897 | Acc: 82.812,93.750,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.107 | Acc: 80.283,91.146,93.564,% | Adaptive Acc: 85.900% | clf_exit: 0.840 0.143 0.017
Batch: 40 | Loss: 1.114 | Acc: 80.469,90.873,93.617,% | Adaptive Acc: 85.804% | clf_exit: 0.849 0.131 0.020
Batch: 60 | Loss: 1.096 | Acc: 80.610,90.984,93.648,% | Adaptive Acc: 86.091% | clf_exit: 0.848 0.132 0.021
Train all parameters

Epoch: 230
Batch: 0 | Loss: 0.575 | Acc: 80.469,99.219,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.805 0.188 0.008
Batch: 20 | Loss: 0.583 | Acc: 83.371,98.958,99.963,% | Adaptive Acc: 92.188% | clf_exit: 0.827 0.168 0.005
Batch: 40 | Loss: 0.598 | Acc: 83.155,99.066,99.943,% | Adaptive Acc: 91.902% | clf_exit: 0.822 0.172 0.006
Batch: 60 | Loss: 0.603 | Acc: 83.017,99.129,99.936,% | Adaptive Acc: 92.200% | clf_exit: 0.816 0.177 0.006
Batch: 80 | Loss: 0.595 | Acc: 83.256,99.151,99.942,% | Adaptive Acc: 92.226% | clf_exit: 0.821 0.172 0.007
Batch: 100 | Loss: 0.592 | Acc: 83.393,99.172,99.946,% | Adaptive Acc: 92.389% | clf_exit: 0.820 0.173 0.007
Batch: 120 | Loss: 0.590 | Acc: 83.445,99.219,99.955,% | Adaptive Acc: 92.420% | clf_exit: 0.820 0.173 0.007
Batch: 140 | Loss: 0.590 | Acc: 83.350,99.197,99.961,% | Adaptive Acc: 92.343% | clf_exit: 0.820 0.174 0.006
Batch: 160 | Loss: 0.591 | Acc: 83.429,99.151,99.961,% | Adaptive Acc: 92.323% | clf_exit: 0.820 0.174 0.006
Batch: 180 | Loss: 0.590 | Acc: 83.464,99.171,99.965,% | Adaptive Acc: 92.364% | clf_exit: 0.820 0.174 0.006
Batch: 200 | Loss: 0.591 | Acc: 83.427,99.180,99.961,% | Adaptive Acc: 92.316% | clf_exit: 0.821 0.173 0.006
Batch: 220 | Loss: 0.587 | Acc: 83.516,99.215,99.965,% | Adaptive Acc: 92.414% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.589 | Acc: 83.509,99.183,99.961,% | Adaptive Acc: 92.408% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.591 | Acc: 83.498,99.162,99.958,% | Adaptive Acc: 92.391% | clf_exit: 0.823 0.171 0.006
Batch: 280 | Loss: 0.588 | Acc: 83.577,99.169,99.956,% | Adaptive Acc: 92.435% | clf_exit: 0.823 0.170 0.006
Batch: 300 | Loss: 0.588 | Acc: 83.609,99.172,99.958,% | Adaptive Acc: 92.457% | clf_exit: 0.824 0.170 0.006
Batch: 320 | Loss: 0.589 | Acc: 83.579,99.173,99.961,% | Adaptive Acc: 92.428% | clf_exit: 0.823 0.170 0.006
Batch: 340 | Loss: 0.590 | Acc: 83.527,99.155,99.961,% | Adaptive Acc: 92.359% | clf_exit: 0.823 0.170 0.006
Batch: 360 | Loss: 0.592 | Acc: 83.505,99.134,99.955,% | Adaptive Acc: 92.363% | clf_exit: 0.823 0.171 0.007
Batch: 380 | Loss: 0.593 | Acc: 83.467,99.116,99.955,% | Adaptive Acc: 92.343% | clf_exit: 0.823 0.171 0.007
Batch: 0 | Loss: 0.908 | Acc: 82.031,92.969,92.969,% | Adaptive Acc: 89.062% | clf_exit: 0.844 0.125 0.031
Batch: 20 | Loss: 1.110 | Acc: 80.432,91.146,93.490,% | Adaptive Acc: 86.124% | clf_exit: 0.835 0.145 0.019
Batch: 40 | Loss: 1.120 | Acc: 80.545,90.987,93.464,% | Adaptive Acc: 85.804% | clf_exit: 0.846 0.132 0.022
Batch: 60 | Loss: 1.097 | Acc: 80.610,91.124,93.648,% | Adaptive Acc: 86.078% | clf_exit: 0.847 0.130 0.022
Train all parameters

Epoch: 231
Batch: 0 | Loss: 0.458 | Acc: 88.281,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.570 | Acc: 84.077,99.219,99.963,% | Adaptive Acc: 92.820% | clf_exit: 0.828 0.166 0.006
Batch: 40 | Loss: 0.566 | Acc: 83.803,99.333,99.981,% | Adaptive Acc: 93.083% | clf_exit: 0.824 0.170 0.006
Batch: 60 | Loss: 0.583 | Acc: 83.312,99.244,99.987,% | Adaptive Acc: 92.738% | clf_exit: 0.821 0.173 0.006
Batch: 80 | Loss: 0.590 | Acc: 83.073,99.180,99.981,% | Adaptive Acc: 92.458% | clf_exit: 0.823 0.171 0.006
Batch: 100 | Loss: 0.591 | Acc: 83.130,99.172,99.985,% | Adaptive Acc: 92.528% | clf_exit: 0.821 0.173 0.006
Batch: 120 | Loss: 0.587 | Acc: 83.335,99.206,99.974,% | Adaptive Acc: 92.698% | clf_exit: 0.821 0.173 0.006
Batch: 140 | Loss: 0.584 | Acc: 83.500,99.213,99.978,% | Adaptive Acc: 92.664% | clf_exit: 0.822 0.171 0.006
Batch: 160 | Loss: 0.587 | Acc: 83.458,99.185,99.976,% | Adaptive Acc: 92.639% | clf_exit: 0.821 0.172 0.006
Batch: 180 | Loss: 0.587 | Acc: 83.494,99.150,99.965,% | Adaptive Acc: 92.619% | clf_exit: 0.822 0.172 0.007
Batch: 200 | Loss: 0.587 | Acc: 83.481,99.164,99.965,% | Adaptive Acc: 92.596% | clf_exit: 0.822 0.171 0.006
Batch: 220 | Loss: 0.589 | Acc: 83.445,99.166,99.965,% | Adaptive Acc: 92.580% | clf_exit: 0.821 0.172 0.007
Batch: 240 | Loss: 0.588 | Acc: 83.516,99.183,99.964,% | Adaptive Acc: 92.589% | clf_exit: 0.821 0.172 0.007
Batch: 260 | Loss: 0.588 | Acc: 83.513,99.183,99.961,% | Adaptive Acc: 92.607% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.588 | Acc: 83.574,99.177,99.961,% | Adaptive Acc: 92.593% | clf_exit: 0.822 0.171 0.006
Batch: 300 | Loss: 0.588 | Acc: 83.568,99.185,99.961,% | Adaptive Acc: 92.566% | clf_exit: 0.822 0.171 0.006
Batch: 320 | Loss: 0.588 | Acc: 83.567,99.190,99.959,% | Adaptive Acc: 92.538% | clf_exit: 0.822 0.172 0.006
Batch: 340 | Loss: 0.588 | Acc: 83.516,99.180,99.956,% | Adaptive Acc: 92.497% | clf_exit: 0.822 0.172 0.006
Batch: 360 | Loss: 0.587 | Acc: 83.540,99.180,99.957,% | Adaptive Acc: 92.523% | clf_exit: 0.822 0.172 0.006
Batch: 380 | Loss: 0.589 | Acc: 83.479,99.174,99.953,% | Adaptive Acc: 92.477% | clf_exit: 0.821 0.173 0.006
Batch: 0 | Loss: 0.921 | Acc: 82.031,92.969,92.969,% | Adaptive Acc: 89.844% | clf_exit: 0.852 0.117 0.031
Batch: 20 | Loss: 1.116 | Acc: 80.469,91.220,93.229,% | Adaptive Acc: 85.938% | clf_exit: 0.839 0.141 0.020
Batch: 40 | Loss: 1.119 | Acc: 80.488,90.892,93.540,% | Adaptive Acc: 85.899% | clf_exit: 0.848 0.130 0.022
Batch: 60 | Loss: 1.099 | Acc: 80.584,90.945,93.686,% | Adaptive Acc: 86.053% | clf_exit: 0.851 0.127 0.022
Train all parameters

Epoch: 232
Batch: 0 | Loss: 0.532 | Acc: 86.719,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.558 | Acc: 84.375,99.368,99.963,% | Adaptive Acc: 93.862% | clf_exit: 0.813 0.182 0.005
Batch: 40 | Loss: 0.570 | Acc: 83.708,99.200,99.962,% | Adaptive Acc: 93.007% | clf_exit: 0.821 0.173 0.006
Batch: 60 | Loss: 0.570 | Acc: 84.055,99.193,99.974,% | Adaptive Acc: 92.853% | clf_exit: 0.825 0.168 0.006
Batch: 80 | Loss: 0.575 | Acc: 84.095,99.151,99.971,% | Adaptive Acc: 92.689% | clf_exit: 0.824 0.169 0.006
Batch: 100 | Loss: 0.580 | Acc: 83.973,99.188,99.961,% | Adaptive Acc: 92.489% | clf_exit: 0.825 0.169 0.006
Batch: 120 | Loss: 0.580 | Acc: 83.988,99.180,99.968,% | Adaptive Acc: 92.439% | clf_exit: 0.826 0.168 0.006
Batch: 140 | Loss: 0.585 | Acc: 83.854,99.147,99.967,% | Adaptive Acc: 92.404% | clf_exit: 0.825 0.169 0.006
Batch: 160 | Loss: 0.584 | Acc: 83.827,99.161,99.961,% | Adaptive Acc: 92.386% | clf_exit: 0.825 0.169 0.006
Batch: 180 | Loss: 0.585 | Acc: 83.723,99.167,99.965,% | Adaptive Acc: 92.291% | clf_exit: 0.825 0.169 0.006
Batch: 200 | Loss: 0.586 | Acc: 83.745,99.168,99.965,% | Adaptive Acc: 92.347% | clf_exit: 0.825 0.169 0.006
Batch: 220 | Loss: 0.588 | Acc: 83.664,99.162,99.965,% | Adaptive Acc: 92.297% | clf_exit: 0.825 0.169 0.006
Batch: 240 | Loss: 0.590 | Acc: 83.636,99.141,99.961,% | Adaptive Acc: 92.269% | clf_exit: 0.825 0.169 0.006
Batch: 260 | Loss: 0.587 | Acc: 83.702,99.159,99.961,% | Adaptive Acc: 92.337% | clf_exit: 0.825 0.169 0.006
Batch: 280 | Loss: 0.587 | Acc: 83.672,99.160,99.964,% | Adaptive Acc: 92.379% | clf_exit: 0.824 0.170 0.006
Batch: 300 | Loss: 0.588 | Acc: 83.648,99.164,99.964,% | Adaptive Acc: 92.431% | clf_exit: 0.824 0.171 0.006
Batch: 320 | Loss: 0.587 | Acc: 83.674,99.180,99.961,% | Adaptive Acc: 92.484% | clf_exit: 0.824 0.171 0.006
Batch: 340 | Loss: 0.588 | Acc: 83.653,99.171,99.963,% | Adaptive Acc: 92.499% | clf_exit: 0.823 0.171 0.006
Batch: 360 | Loss: 0.587 | Acc: 83.624,99.175,99.963,% | Adaptive Acc: 92.493% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.589 | Acc: 83.555,99.153,99.961,% | Adaptive Acc: 92.407% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.889 | Acc: 82.812,92.969,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.844 0.109 0.047
Batch: 20 | Loss: 1.108 | Acc: 80.432,91.034,93.415,% | Adaptive Acc: 85.863% | clf_exit: 0.842 0.141 0.017
Batch: 40 | Loss: 1.114 | Acc: 80.716,90.854,93.674,% | Adaptive Acc: 85.957% | clf_exit: 0.851 0.128 0.021
Batch: 60 | Loss: 1.091 | Acc: 80.802,90.984,93.699,% | Adaptive Acc: 86.194% | clf_exit: 0.851 0.127 0.022
Train all parameters

Epoch: 233
Batch: 0 | Loss: 0.650 | Acc: 80.469,99.219,100.000,% | Adaptive Acc: 89.062% | clf_exit: 0.812 0.180 0.008
Batch: 20 | Loss: 0.562 | Acc: 84.301,99.442,100.000,% | Adaptive Acc: 92.894% | clf_exit: 0.826 0.169 0.006
Batch: 40 | Loss: 0.575 | Acc: 83.841,99.314,99.962,% | Adaptive Acc: 92.912% | clf_exit: 0.820 0.175 0.005
Batch: 60 | Loss: 0.587 | Acc: 83.145,99.116,99.962,% | Adaptive Acc: 92.188% | clf_exit: 0.824 0.171 0.005
Batch: 80 | Loss: 0.585 | Acc: 83.198,99.161,99.971,% | Adaptive Acc: 92.294% | clf_exit: 0.822 0.173 0.005
Batch: 100 | Loss: 0.586 | Acc: 83.215,99.211,99.946,% | Adaptive Acc: 92.319% | clf_exit: 0.822 0.173 0.005
Batch: 120 | Loss: 0.585 | Acc: 83.316,99.257,99.955,% | Adaptive Acc: 92.394% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.585 | Acc: 83.372,99.224,99.956,% | Adaptive Acc: 92.332% | clf_exit: 0.824 0.170 0.005
Batch: 160 | Loss: 0.589 | Acc: 83.249,99.199,99.951,% | Adaptive Acc: 92.343% | clf_exit: 0.822 0.172 0.005
Batch: 180 | Loss: 0.587 | Acc: 83.300,99.206,99.948,% | Adaptive Acc: 92.352% | clf_exit: 0.823 0.172 0.005
Batch: 200 | Loss: 0.587 | Acc: 83.302,99.203,99.949,% | Adaptive Acc: 92.359% | clf_exit: 0.822 0.172 0.006
Batch: 220 | Loss: 0.587 | Acc: 83.293,99.212,99.954,% | Adaptive Acc: 92.343% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.587 | Acc: 83.412,99.190,99.951,% | Adaptive Acc: 92.369% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.588 | Acc: 83.411,99.198,99.952,% | Adaptive Acc: 92.328% | clf_exit: 0.824 0.171 0.006
Batch: 280 | Loss: 0.589 | Acc: 83.407,99.185,99.956,% | Adaptive Acc: 92.279% | clf_exit: 0.824 0.170 0.006
Batch: 300 | Loss: 0.589 | Acc: 83.415,99.177,99.956,% | Adaptive Acc: 92.304% | clf_exit: 0.823 0.171 0.006
Batch: 320 | Loss: 0.588 | Acc: 83.411,99.190,99.959,% | Adaptive Acc: 92.312% | clf_exit: 0.823 0.171 0.006
Batch: 340 | Loss: 0.587 | Acc: 83.461,99.189,99.961,% | Adaptive Acc: 92.327% | clf_exit: 0.823 0.171 0.006
Batch: 360 | Loss: 0.587 | Acc: 83.460,99.191,99.961,% | Adaptive Acc: 92.322% | clf_exit: 0.823 0.171 0.006
Batch: 380 | Loss: 0.588 | Acc: 83.391,99.196,99.959,% | Adaptive Acc: 92.302% | clf_exit: 0.822 0.172 0.006
Batch: 0 | Loss: 0.889 | Acc: 82.812,93.750,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.852 0.133 0.016
Batch: 20 | Loss: 1.116 | Acc: 80.208,91.146,93.304,% | Adaptive Acc: 85.900% | clf_exit: 0.844 0.134 0.022
Batch: 40 | Loss: 1.116 | Acc: 80.488,90.777,93.636,% | Adaptive Acc: 85.728% | clf_exit: 0.851 0.127 0.022
Batch: 60 | Loss: 1.095 | Acc: 80.738,90.881,93.699,% | Adaptive Acc: 85.899% | clf_exit: 0.852 0.125 0.023
Train all parameters

Epoch: 234
Batch: 0 | Loss: 0.558 | Acc: 85.156,99.219,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.820 0.172 0.008
Batch: 20 | Loss: 0.597 | Acc: 83.854,99.144,99.963,% | Adaptive Acc: 92.597% | clf_exit: 0.817 0.176 0.007
Batch: 40 | Loss: 0.599 | Acc: 83.556,99.123,99.981,% | Adaptive Acc: 92.435% | clf_exit: 0.821 0.172 0.007
Batch: 60 | Loss: 0.596 | Acc: 83.530,99.155,99.987,% | Adaptive Acc: 92.444% | clf_exit: 0.820 0.173 0.007
Batch: 80 | Loss: 0.588 | Acc: 83.661,99.161,99.981,% | Adaptive Acc: 92.506% | clf_exit: 0.826 0.168 0.007
Batch: 100 | Loss: 0.590 | Acc: 83.648,99.118,99.985,% | Adaptive Acc: 92.474% | clf_exit: 0.826 0.168 0.006
Batch: 120 | Loss: 0.585 | Acc: 83.723,99.167,99.981,% | Adaptive Acc: 92.517% | clf_exit: 0.826 0.168 0.006
Batch: 140 | Loss: 0.585 | Acc: 83.716,99.191,99.983,% | Adaptive Acc: 92.575% | clf_exit: 0.824 0.170 0.006
Batch: 160 | Loss: 0.587 | Acc: 83.608,99.194,99.981,% | Adaptive Acc: 92.571% | clf_exit: 0.822 0.171 0.006
Batch: 180 | Loss: 0.588 | Acc: 83.646,99.171,99.978,% | Adaptive Acc: 92.485% | clf_exit: 0.823 0.171 0.006
Batch: 200 | Loss: 0.588 | Acc: 83.633,99.176,99.977,% | Adaptive Acc: 92.498% | clf_exit: 0.822 0.172 0.006
Batch: 220 | Loss: 0.589 | Acc: 83.551,99.134,99.975,% | Adaptive Acc: 92.502% | clf_exit: 0.821 0.173 0.006
Batch: 240 | Loss: 0.589 | Acc: 83.581,99.141,99.971,% | Adaptive Acc: 92.557% | clf_exit: 0.821 0.172 0.006
Batch: 260 | Loss: 0.586 | Acc: 83.705,99.156,99.973,% | Adaptive Acc: 92.631% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.588 | Acc: 83.624,99.135,99.969,% | Adaptive Acc: 92.621% | clf_exit: 0.821 0.172 0.007
Batch: 300 | Loss: 0.588 | Acc: 83.604,99.133,99.966,% | Adaptive Acc: 92.598% | clf_exit: 0.821 0.172 0.006
Batch: 320 | Loss: 0.588 | Acc: 83.572,99.131,99.966,% | Adaptive Acc: 92.553% | clf_exit: 0.822 0.172 0.006
Batch: 340 | Loss: 0.588 | Acc: 83.559,99.125,99.963,% | Adaptive Acc: 92.511% | clf_exit: 0.822 0.171 0.006
Batch: 360 | Loss: 0.589 | Acc: 83.475,99.137,99.965,% | Adaptive Acc: 92.467% | clf_exit: 0.822 0.171 0.006
Batch: 380 | Loss: 0.589 | Acc: 83.514,99.141,99.965,% | Adaptive Acc: 92.470% | clf_exit: 0.823 0.171 0.006
Batch: 0 | Loss: 0.922 | Acc: 82.031,92.188,92.969,% | Adaptive Acc: 89.062% | clf_exit: 0.859 0.117 0.023
Batch: 20 | Loss: 1.117 | Acc: 80.469,91.109,93.229,% | Adaptive Acc: 85.863% | clf_exit: 0.845 0.138 0.017
Batch: 40 | Loss: 1.120 | Acc: 80.488,90.758,93.502,% | Adaptive Acc: 85.918% | clf_exit: 0.852 0.128 0.019
Batch: 60 | Loss: 1.096 | Acc: 80.622,90.804,93.558,% | Adaptive Acc: 86.155% | clf_exit: 0.852 0.128 0.020
Train classifier parameters

Epoch: 235
Batch: 0 | Loss: 0.623 | Acc: 81.250,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.797 0.188 0.016
Batch: 20 | Loss: 0.586 | Acc: 83.222,99.144,99.963,% | Adaptive Acc: 92.820% | clf_exit: 0.810 0.183 0.007
Batch: 40 | Loss: 0.590 | Acc: 83.556,99.085,99.924,% | Adaptive Acc: 93.064% | clf_exit: 0.812 0.181 0.007
Batch: 60 | Loss: 0.583 | Acc: 83.876,99.219,99.949,% | Adaptive Acc: 92.892% | clf_exit: 0.819 0.176 0.006
Batch: 80 | Loss: 0.587 | Acc: 83.758,99.171,99.952,% | Adaptive Acc: 92.882% | clf_exit: 0.817 0.177 0.006
Batch: 100 | Loss: 0.590 | Acc: 83.632,99.157,99.954,% | Adaptive Acc: 92.752% | clf_exit: 0.818 0.176 0.006
Batch: 120 | Loss: 0.595 | Acc: 83.316,99.141,99.961,% | Adaptive Acc: 92.497% | clf_exit: 0.819 0.175 0.006
Batch: 140 | Loss: 0.595 | Acc: 83.256,99.186,99.967,% | Adaptive Acc: 92.393% | clf_exit: 0.820 0.174 0.006
Batch: 160 | Loss: 0.593 | Acc: 83.240,99.170,99.961,% | Adaptive Acc: 92.343% | clf_exit: 0.820 0.174 0.006
Batch: 180 | Loss: 0.592 | Acc: 83.257,99.167,99.965,% | Adaptive Acc: 92.352% | clf_exit: 0.821 0.173 0.006
Batch: 200 | Loss: 0.589 | Acc: 83.333,99.195,99.969,% | Adaptive Acc: 92.382% | clf_exit: 0.822 0.172 0.006
Batch: 220 | Loss: 0.588 | Acc: 83.417,99.190,99.965,% | Adaptive Acc: 92.438% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.586 | Acc: 83.500,99.193,99.964,% | Adaptive Acc: 92.534% | clf_exit: 0.822 0.172 0.006
Batch: 260 | Loss: 0.587 | Acc: 83.519,99.189,99.967,% | Adaptive Acc: 92.541% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.587 | Acc: 83.488,99.185,99.969,% | Adaptive Acc: 92.510% | clf_exit: 0.821 0.172 0.006
Batch: 300 | Loss: 0.587 | Acc: 83.513,99.195,99.971,% | Adaptive Acc: 92.517% | clf_exit: 0.822 0.172 0.006
Batch: 320 | Loss: 0.586 | Acc: 83.530,99.197,99.971,% | Adaptive Acc: 92.497% | clf_exit: 0.822 0.172 0.006
Batch: 340 | Loss: 0.587 | Acc: 83.488,99.198,99.973,% | Adaptive Acc: 92.485% | clf_exit: 0.822 0.172 0.006
Batch: 360 | Loss: 0.588 | Acc: 83.462,99.195,99.972,% | Adaptive Acc: 92.434% | clf_exit: 0.822 0.172 0.006
Batch: 380 | Loss: 0.588 | Acc: 83.489,99.196,99.973,% | Adaptive Acc: 92.427% | clf_exit: 0.822 0.172 0.006
Batch: 0 | Loss: 0.927 | Acc: 82.031,93.750,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.121 | Acc: 80.208,91.109,93.341,% | Adaptive Acc: 85.863% | clf_exit: 0.841 0.141 0.018
Batch: 40 | Loss: 1.123 | Acc: 80.373,90.777,93.426,% | Adaptive Acc: 85.823% | clf_exit: 0.848 0.131 0.021
Batch: 60 | Loss: 1.100 | Acc: 80.610,90.984,93.494,% | Adaptive Acc: 86.014% | clf_exit: 0.850 0.129 0.021
Train classifier parameters

Epoch: 236
Batch: 0 | Loss: 0.593 | Acc: 82.812,99.219,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.820 0.180 0.000
Batch: 20 | Loss: 0.564 | Acc: 84.263,99.516,99.926,% | Adaptive Acc: 92.671% | clf_exit: 0.831 0.165 0.004
Batch: 40 | Loss: 0.581 | Acc: 83.861,99.371,99.924,% | Adaptive Acc: 92.378% | clf_exit: 0.834 0.161 0.005
Batch: 60 | Loss: 0.582 | Acc: 83.722,99.347,99.949,% | Adaptive Acc: 92.495% | clf_exit: 0.831 0.163 0.006
Batch: 80 | Loss: 0.580 | Acc: 83.603,99.363,99.961,% | Adaptive Acc: 92.650% | clf_exit: 0.827 0.167 0.006
Batch: 100 | Loss: 0.584 | Acc: 83.346,99.288,99.969,% | Adaptive Acc: 92.512% | clf_exit: 0.825 0.168 0.006
Batch: 120 | Loss: 0.587 | Acc: 83.290,99.238,99.968,% | Adaptive Acc: 92.504% | clf_exit: 0.824 0.170 0.006
Batch: 140 | Loss: 0.588 | Acc: 83.306,99.246,99.972,% | Adaptive Acc: 92.448% | clf_exit: 0.823 0.171 0.006
Batch: 160 | Loss: 0.587 | Acc: 83.337,99.258,99.976,% | Adaptive Acc: 92.391% | clf_exit: 0.824 0.170 0.006
Batch: 180 | Loss: 0.587 | Acc: 83.300,99.258,99.970,% | Adaptive Acc: 92.369% | clf_exit: 0.824 0.170 0.006
Batch: 200 | Loss: 0.587 | Acc: 83.322,99.246,99.969,% | Adaptive Acc: 92.382% | clf_exit: 0.823 0.171 0.006
Batch: 220 | Loss: 0.587 | Acc: 83.332,99.247,99.968,% | Adaptive Acc: 92.315% | clf_exit: 0.824 0.170 0.006
Batch: 240 | Loss: 0.587 | Acc: 83.334,99.245,99.971,% | Adaptive Acc: 92.353% | clf_exit: 0.824 0.170 0.006
Batch: 260 | Loss: 0.583 | Acc: 83.498,99.258,99.970,% | Adaptive Acc: 92.400% | clf_exit: 0.825 0.169 0.006
Batch: 280 | Loss: 0.584 | Acc: 83.513,99.269,99.972,% | Adaptive Acc: 92.377% | clf_exit: 0.825 0.169 0.006
Batch: 300 | Loss: 0.583 | Acc: 83.565,99.278,99.974,% | Adaptive Acc: 92.411% | clf_exit: 0.825 0.169 0.006
Batch: 320 | Loss: 0.583 | Acc: 83.586,99.275,99.976,% | Adaptive Acc: 92.424% | clf_exit: 0.825 0.169 0.006
Batch: 340 | Loss: 0.584 | Acc: 83.596,99.262,99.975,% | Adaptive Acc: 92.440% | clf_exit: 0.825 0.169 0.006
Batch: 360 | Loss: 0.585 | Acc: 83.576,99.251,99.972,% | Adaptive Acc: 92.434% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.583 | Acc: 83.649,99.268,99.971,% | Adaptive Acc: 92.454% | clf_exit: 0.825 0.169 0.006
Batch: 0 | Loss: 0.909 | Acc: 82.812,92.969,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.859 0.117 0.023
Batch: 20 | Loss: 1.125 | Acc: 80.097,90.885,93.266,% | Adaptive Acc: 85.603% | clf_exit: 0.846 0.136 0.018
Batch: 40 | Loss: 1.126 | Acc: 80.354,90.739,93.464,% | Adaptive Acc: 85.823% | clf_exit: 0.852 0.128 0.020
Batch: 60 | Loss: 1.102 | Acc: 80.571,90.894,93.571,% | Adaptive Acc: 85.989% | clf_exit: 0.853 0.126 0.021
Train classifier parameters

Epoch: 237
Batch: 0 | Loss: 0.547 | Acc: 84.375,99.219,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.588 | Acc: 82.850,99.442,100.000,% | Adaptive Acc: 92.225% | clf_exit: 0.816 0.179 0.005
Batch: 40 | Loss: 0.580 | Acc: 83.841,99.390,100.000,% | Adaptive Acc: 92.645% | clf_exit: 0.821 0.172 0.006
Batch: 60 | Loss: 0.572 | Acc: 84.260,99.308,99.974,% | Adaptive Acc: 92.956% | clf_exit: 0.820 0.174 0.006
Batch: 80 | Loss: 0.580 | Acc: 83.951,99.238,99.961,% | Adaptive Acc: 92.863% | clf_exit: 0.818 0.176 0.006
Batch: 100 | Loss: 0.577 | Acc: 83.841,99.219,99.969,% | Adaptive Acc: 92.652% | clf_exit: 0.822 0.172 0.006
Batch: 120 | Loss: 0.578 | Acc: 83.775,99.186,99.974,% | Adaptive Acc: 92.562% | clf_exit: 0.822 0.172 0.006
Batch: 140 | Loss: 0.579 | Acc: 83.821,99.158,99.972,% | Adaptive Acc: 92.598% | clf_exit: 0.823 0.171 0.006
Batch: 160 | Loss: 0.579 | Acc: 83.856,99.165,99.971,% | Adaptive Acc: 92.673% | clf_exit: 0.823 0.171 0.007
Batch: 180 | Loss: 0.580 | Acc: 83.861,99.158,99.970,% | Adaptive Acc: 92.714% | clf_exit: 0.823 0.170 0.007
Batch: 200 | Loss: 0.585 | Acc: 83.695,99.168,99.973,% | Adaptive Acc: 92.600% | clf_exit: 0.822 0.172 0.007
Batch: 220 | Loss: 0.583 | Acc: 83.820,99.159,99.972,% | Adaptive Acc: 92.636% | clf_exit: 0.822 0.171 0.007
Batch: 240 | Loss: 0.583 | Acc: 83.821,99.157,99.968,% | Adaptive Acc: 92.615% | clf_exit: 0.823 0.170 0.007
Batch: 260 | Loss: 0.584 | Acc: 83.740,99.159,99.961,% | Adaptive Acc: 92.589% | clf_exit: 0.823 0.171 0.007
Batch: 280 | Loss: 0.582 | Acc: 83.749,99.166,99.964,% | Adaptive Acc: 92.621% | clf_exit: 0.823 0.170 0.007
Batch: 300 | Loss: 0.583 | Acc: 83.656,99.167,99.966,% | Adaptive Acc: 92.556% | clf_exit: 0.824 0.170 0.007
Batch: 320 | Loss: 0.583 | Acc: 83.684,99.170,99.966,% | Adaptive Acc: 92.557% | clf_exit: 0.824 0.169 0.007
Batch: 340 | Loss: 0.584 | Acc: 83.660,99.178,99.968,% | Adaptive Acc: 92.579% | clf_exit: 0.823 0.170 0.007
Batch: 360 | Loss: 0.584 | Acc: 83.639,99.195,99.970,% | Adaptive Acc: 92.558% | clf_exit: 0.823 0.170 0.007
Batch: 380 | Loss: 0.585 | Acc: 83.643,99.192,99.969,% | Adaptive Acc: 92.557% | clf_exit: 0.823 0.170 0.007
Batch: 0 | Loss: 0.913 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.117 | Acc: 80.394,91.034,93.676,% | Adaptive Acc: 85.826% | clf_exit: 0.842 0.140 0.019
Batch: 40 | Loss: 1.118 | Acc: 80.583,90.796,93.731,% | Adaptive Acc: 85.995% | clf_exit: 0.848 0.131 0.021
Batch: 60 | Loss: 1.099 | Acc: 80.763,90.817,93.712,% | Adaptive Acc: 86.206% | clf_exit: 0.850 0.128 0.022
Train classifier parameters

Epoch: 238
Batch: 0 | Loss: 0.792 | Acc: 80.469,98.438,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.812 0.156 0.031
Batch: 20 | Loss: 0.624 | Acc: 82.143,99.293,100.000,% | Adaptive Acc: 91.592% | clf_exit: 0.813 0.179 0.007
Batch: 40 | Loss: 0.600 | Acc: 82.812,99.333,99.981,% | Adaptive Acc: 92.111% | clf_exit: 0.818 0.176 0.006
Batch: 60 | Loss: 0.591 | Acc: 83.210,99.449,99.987,% | Adaptive Acc: 92.649% | clf_exit: 0.816 0.178 0.006
Batch: 80 | Loss: 0.593 | Acc: 83.015,99.383,99.981,% | Adaptive Acc: 92.506% | clf_exit: 0.818 0.176 0.006
Batch: 100 | Loss: 0.592 | Acc: 83.145,99.373,99.985,% | Adaptive Acc: 92.644% | clf_exit: 0.817 0.178 0.006
Batch: 120 | Loss: 0.591 | Acc: 83.232,99.335,99.987,% | Adaptive Acc: 92.588% | clf_exit: 0.819 0.175 0.006
Batch: 140 | Loss: 0.585 | Acc: 83.516,99.330,99.967,% | Adaptive Acc: 92.697% | clf_exit: 0.820 0.174 0.006
Batch: 160 | Loss: 0.584 | Acc: 83.589,99.340,99.966,% | Adaptive Acc: 92.804% | clf_exit: 0.819 0.174 0.006
Batch: 180 | Loss: 0.582 | Acc: 83.555,99.344,99.970,% | Adaptive Acc: 92.680% | clf_exit: 0.821 0.173 0.006
Batch: 200 | Loss: 0.580 | Acc: 83.703,99.328,99.969,% | Adaptive Acc: 92.704% | clf_exit: 0.822 0.172 0.006
Batch: 220 | Loss: 0.580 | Acc: 83.809,99.289,99.972,% | Adaptive Acc: 92.718% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.581 | Acc: 83.798,99.267,99.971,% | Adaptive Acc: 92.648% | clf_exit: 0.822 0.172 0.006
Batch: 260 | Loss: 0.583 | Acc: 83.746,99.267,99.973,% | Adaptive Acc: 92.580% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.582 | Acc: 83.830,99.291,99.975,% | Adaptive Acc: 92.646% | clf_exit: 0.822 0.173 0.006
Batch: 300 | Loss: 0.583 | Acc: 83.775,99.281,99.977,% | Adaptive Acc: 92.587% | clf_exit: 0.822 0.173 0.005
Batch: 320 | Loss: 0.583 | Acc: 83.803,99.267,99.976,% | Adaptive Acc: 92.577% | clf_exit: 0.822 0.173 0.006
Batch: 340 | Loss: 0.582 | Acc: 83.800,99.255,99.975,% | Adaptive Acc: 92.577% | clf_exit: 0.821 0.173 0.005
Batch: 360 | Loss: 0.584 | Acc: 83.765,99.251,99.974,% | Adaptive Acc: 92.558% | clf_exit: 0.822 0.173 0.006
Batch: 380 | Loss: 0.585 | Acc: 83.672,99.229,99.975,% | Adaptive Acc: 92.559% | clf_exit: 0.821 0.173 0.006
Batch: 0 | Loss: 0.913 | Acc: 82.031,91.406,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.875 0.094 0.031
Batch: 20 | Loss: 1.116 | Acc: 80.469,90.885,93.527,% | Adaptive Acc: 85.975% | clf_exit: 0.843 0.137 0.020
Batch: 40 | Loss: 1.119 | Acc: 80.450,90.663,93.579,% | Adaptive Acc: 85.957% | clf_exit: 0.851 0.128 0.022
Batch: 60 | Loss: 1.097 | Acc: 80.558,90.817,93.673,% | Adaptive Acc: 86.104% | clf_exit: 0.852 0.126 0.022
Train classifier parameters

Epoch: 239
Batch: 0 | Loss: 0.513 | Acc: 84.375,100.000,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.836 0.156 0.008
Batch: 20 | Loss: 0.581 | Acc: 84.301,99.070,99.963,% | Adaptive Acc: 92.746% | clf_exit: 0.823 0.172 0.005
Batch: 40 | Loss: 0.570 | Acc: 84.318,99.314,99.962,% | Adaptive Acc: 92.759% | clf_exit: 0.827 0.168 0.005
Batch: 60 | Loss: 0.560 | Acc: 84.593,99.308,99.962,% | Adaptive Acc: 92.751% | clf_exit: 0.831 0.166 0.004
Batch: 80 | Loss: 0.569 | Acc: 84.192,99.267,99.971,% | Adaptive Acc: 92.650% | clf_exit: 0.829 0.167 0.004
Batch: 100 | Loss: 0.576 | Acc: 83.864,99.273,99.977,% | Adaptive Acc: 92.543% | clf_exit: 0.828 0.167 0.005
Batch: 120 | Loss: 0.575 | Acc: 83.994,99.296,99.974,% | Adaptive Acc: 92.575% | clf_exit: 0.828 0.167 0.005
Batch: 140 | Loss: 0.577 | Acc: 83.893,99.296,99.978,% | Adaptive Acc: 92.598% | clf_exit: 0.827 0.168 0.005
Batch: 160 | Loss: 0.578 | Acc: 83.822,99.272,99.966,% | Adaptive Acc: 92.542% | clf_exit: 0.826 0.169 0.005
Batch: 180 | Loss: 0.578 | Acc: 83.848,99.262,99.965,% | Adaptive Acc: 92.610% | clf_exit: 0.826 0.169 0.005
Batch: 200 | Loss: 0.578 | Acc: 83.827,99.254,99.969,% | Adaptive Acc: 92.607% | clf_exit: 0.826 0.169 0.005
Batch: 220 | Loss: 0.579 | Acc: 83.827,99.229,99.968,% | Adaptive Acc: 92.559% | clf_exit: 0.825 0.169 0.005
Batch: 240 | Loss: 0.578 | Acc: 83.860,99.258,99.971,% | Adaptive Acc: 92.586% | clf_exit: 0.824 0.170 0.005
Batch: 260 | Loss: 0.579 | Acc: 83.836,99.258,99.973,% | Adaptive Acc: 92.562% | clf_exit: 0.825 0.170 0.005
Batch: 280 | Loss: 0.578 | Acc: 83.836,99.255,99.975,% | Adaptive Acc: 92.554% | clf_exit: 0.824 0.170 0.005
Batch: 300 | Loss: 0.580 | Acc: 83.796,99.242,99.971,% | Adaptive Acc: 92.564% | clf_exit: 0.824 0.171 0.005
Batch: 320 | Loss: 0.581 | Acc: 83.750,99.238,99.973,% | Adaptive Acc: 92.572% | clf_exit: 0.823 0.172 0.006
Batch: 340 | Loss: 0.581 | Acc: 83.740,99.232,99.975,% | Adaptive Acc: 92.566% | clf_exit: 0.823 0.171 0.006
Batch: 360 | Loss: 0.582 | Acc: 83.732,99.240,99.976,% | Adaptive Acc: 92.542% | clf_exit: 0.823 0.171 0.006
Batch: 380 | Loss: 0.583 | Acc: 83.676,99.221,99.977,% | Adaptive Acc: 92.514% | clf_exit: 0.823 0.171 0.006
Batch: 0 | Loss: 0.925 | Acc: 82.031,90.625,93.750,% | Adaptive Acc: 87.500% | clf_exit: 0.898 0.078 0.023
Batch: 20 | Loss: 1.119 | Acc: 80.357,90.960,93.378,% | Adaptive Acc: 85.677% | clf_exit: 0.848 0.134 0.018
Batch: 40 | Loss: 1.122 | Acc: 80.373,90.701,93.598,% | Adaptive Acc: 85.861% | clf_exit: 0.853 0.126 0.020
Batch: 60 | Loss: 1.101 | Acc: 80.584,90.727,93.660,% | Adaptive Acc: 85.989% | clf_exit: 0.854 0.126 0.021
Train classifier parameters

Epoch: 240
Batch: 0 | Loss: 0.657 | Acc: 77.344,99.219,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.750 0.250 0.000
Batch: 20 | Loss: 0.604 | Acc: 82.887,99.368,100.000,% | Adaptive Acc: 91.890% | clf_exit: 0.814 0.181 0.005
Batch: 40 | Loss: 0.594 | Acc: 83.194,99.295,100.000,% | Adaptive Acc: 91.997% | clf_exit: 0.825 0.169 0.006
Batch: 60 | Loss: 0.586 | Acc: 83.722,99.219,100.000,% | Adaptive Acc: 92.392% | clf_exit: 0.824 0.170 0.006
Batch: 80 | Loss: 0.587 | Acc: 83.700,99.199,99.990,% | Adaptive Acc: 92.409% | clf_exit: 0.821 0.173 0.006
Batch: 100 | Loss: 0.592 | Acc: 83.393,99.219,99.992,% | Adaptive Acc: 92.365% | clf_exit: 0.820 0.174 0.006
Batch: 120 | Loss: 0.592 | Acc: 83.439,99.206,99.981,% | Adaptive Acc: 92.349% | clf_exit: 0.821 0.174 0.006
Batch: 140 | Loss: 0.596 | Acc: 83.317,99.180,99.983,% | Adaptive Acc: 92.354% | clf_exit: 0.820 0.174 0.006
Batch: 160 | Loss: 0.593 | Acc: 83.385,99.214,99.981,% | Adaptive Acc: 92.396% | clf_exit: 0.820 0.174 0.006
Batch: 180 | Loss: 0.590 | Acc: 83.456,99.210,99.978,% | Adaptive Acc: 92.451% | clf_exit: 0.821 0.173 0.006
Batch: 200 | Loss: 0.592 | Acc: 83.302,99.172,99.969,% | Adaptive Acc: 92.362% | clf_exit: 0.821 0.174 0.006
Batch: 220 | Loss: 0.592 | Acc: 83.353,99.190,99.961,% | Adaptive Acc: 92.375% | clf_exit: 0.822 0.173 0.006
Batch: 240 | Loss: 0.591 | Acc: 83.380,99.199,99.964,% | Adaptive Acc: 92.392% | clf_exit: 0.822 0.172 0.006
Batch: 260 | Loss: 0.590 | Acc: 83.432,99.219,99.967,% | Adaptive Acc: 92.424% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.590 | Acc: 83.382,99.216,99.967,% | Adaptive Acc: 92.354% | clf_exit: 0.823 0.171 0.006
Batch: 300 | Loss: 0.590 | Acc: 83.412,99.193,99.966,% | Adaptive Acc: 92.367% | clf_exit: 0.822 0.172 0.006
Batch: 320 | Loss: 0.590 | Acc: 83.380,99.192,99.968,% | Adaptive Acc: 92.341% | clf_exit: 0.822 0.172 0.006
Batch: 340 | Loss: 0.589 | Acc: 83.415,99.184,99.968,% | Adaptive Acc: 92.391% | clf_exit: 0.822 0.171 0.006
Batch: 360 | Loss: 0.588 | Acc: 83.462,99.193,99.968,% | Adaptive Acc: 92.413% | clf_exit: 0.823 0.171 0.006
Batch: 380 | Loss: 0.587 | Acc: 83.483,99.198,99.969,% | Adaptive Acc: 92.397% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.936 | Acc: 82.031,89.844,92.969,% | Adaptive Acc: 87.500% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.124 | Acc: 80.246,91.034,93.080,% | Adaptive Acc: 85.826% | clf_exit: 0.842 0.140 0.018
Batch: 40 | Loss: 1.123 | Acc: 80.335,90.625,93.407,% | Adaptive Acc: 85.938% | clf_exit: 0.849 0.130 0.021
Batch: 60 | Loss: 1.099 | Acc: 80.571,90.830,93.519,% | Adaptive Acc: 86.130% | clf_exit: 0.851 0.127 0.022
Train classifier parameters

Epoch: 241
Batch: 0 | Loss: 0.583 | Acc: 82.031,99.219,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 0.553 | Acc: 84.338,99.405,100.000,% | Adaptive Acc: 92.746% | clf_exit: 0.833 0.160 0.006
Batch: 40 | Loss: 0.567 | Acc: 84.108,99.409,100.000,% | Adaptive Acc: 92.302% | clf_exit: 0.834 0.159 0.006
Batch: 60 | Loss: 0.573 | Acc: 84.080,99.334,100.000,% | Adaptive Acc: 92.341% | clf_exit: 0.828 0.166 0.006
Batch: 80 | Loss: 0.578 | Acc: 83.893,99.334,100.000,% | Adaptive Acc: 92.313% | clf_exit: 0.826 0.168 0.006
Batch: 100 | Loss: 0.580 | Acc: 83.795,99.373,99.992,% | Adaptive Acc: 92.404% | clf_exit: 0.823 0.171 0.006
Batch: 120 | Loss: 0.577 | Acc: 83.917,99.380,99.994,% | Adaptive Acc: 92.497% | clf_exit: 0.822 0.172 0.006
Batch: 140 | Loss: 0.581 | Acc: 83.766,99.341,99.994,% | Adaptive Acc: 92.348% | clf_exit: 0.822 0.172 0.006
Batch: 160 | Loss: 0.581 | Acc: 83.739,99.350,99.990,% | Adaptive Acc: 92.386% | clf_exit: 0.823 0.172 0.006
Batch: 180 | Loss: 0.582 | Acc: 83.654,99.305,99.987,% | Adaptive Acc: 92.395% | clf_exit: 0.822 0.172 0.006
Batch: 200 | Loss: 0.584 | Acc: 83.687,99.285,99.977,% | Adaptive Acc: 92.382% | clf_exit: 0.823 0.171 0.006
Batch: 220 | Loss: 0.582 | Acc: 83.742,99.275,99.972,% | Adaptive Acc: 92.410% | clf_exit: 0.823 0.171 0.006
Batch: 240 | Loss: 0.583 | Acc: 83.769,99.271,99.971,% | Adaptive Acc: 92.395% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.584 | Acc: 83.728,99.252,99.967,% | Adaptive Acc: 92.400% | clf_exit: 0.822 0.171 0.006
Batch: 280 | Loss: 0.585 | Acc: 83.697,99.255,99.967,% | Adaptive Acc: 92.441% | clf_exit: 0.822 0.172 0.006
Batch: 300 | Loss: 0.584 | Acc: 83.757,99.258,99.969,% | Adaptive Acc: 92.504% | clf_exit: 0.821 0.172 0.006
Batch: 320 | Loss: 0.585 | Acc: 83.706,99.236,99.968,% | Adaptive Acc: 92.484% | clf_exit: 0.821 0.173 0.006
Batch: 340 | Loss: 0.584 | Acc: 83.736,99.251,99.970,% | Adaptive Acc: 92.522% | clf_exit: 0.821 0.173 0.006
Batch: 360 | Loss: 0.585 | Acc: 83.695,99.230,99.972,% | Adaptive Acc: 92.506% | clf_exit: 0.821 0.173 0.006
Batch: 380 | Loss: 0.585 | Acc: 83.713,99.239,99.973,% | Adaptive Acc: 92.499% | clf_exit: 0.821 0.173 0.006
Batch: 0 | Loss: 0.933 | Acc: 81.250,92.969,92.969,% | Adaptive Acc: 87.500% | clf_exit: 0.875 0.117 0.008
Batch: 20 | Loss: 1.124 | Acc: 80.469,91.220,93.080,% | Adaptive Acc: 85.528% | clf_exit: 0.846 0.137 0.017
Batch: 40 | Loss: 1.121 | Acc: 80.545,90.816,93.426,% | Adaptive Acc: 85.804% | clf_exit: 0.852 0.127 0.020
Batch: 60 | Loss: 1.099 | Acc: 80.725,90.945,93.571,% | Adaptive Acc: 86.002% | clf_exit: 0.853 0.126 0.021
Train classifier parameters

Epoch: 242
Batch: 0 | Loss: 0.544 | Acc: 84.375,99.219,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.859 0.133 0.008
Batch: 20 | Loss: 0.602 | Acc: 83.519,99.070,100.000,% | Adaptive Acc: 92.225% | clf_exit: 0.824 0.168 0.007
Batch: 40 | Loss: 0.593 | Acc: 83.899,99.200,99.981,% | Adaptive Acc: 92.550% | clf_exit: 0.823 0.170 0.007
Batch: 60 | Loss: 0.588 | Acc: 83.799,99.206,99.974,% | Adaptive Acc: 92.546% | clf_exit: 0.823 0.171 0.006
Batch: 80 | Loss: 0.596 | Acc: 83.468,99.122,99.961,% | Adaptive Acc: 92.535% | clf_exit: 0.818 0.176 0.006
Batch: 100 | Loss: 0.591 | Acc: 83.594,99.149,99.961,% | Adaptive Acc: 92.489% | clf_exit: 0.820 0.174 0.006
Batch: 120 | Loss: 0.586 | Acc: 83.665,99.128,99.961,% | Adaptive Acc: 92.510% | clf_exit: 0.821 0.173 0.006
Batch: 140 | Loss: 0.587 | Acc: 83.488,99.125,99.961,% | Adaptive Acc: 92.437% | clf_exit: 0.820 0.174 0.006
Batch: 160 | Loss: 0.589 | Acc: 83.472,99.141,99.966,% | Adaptive Acc: 92.425% | clf_exit: 0.820 0.174 0.006
Batch: 180 | Loss: 0.589 | Acc: 83.516,99.141,99.970,% | Adaptive Acc: 92.442% | clf_exit: 0.820 0.174 0.006
Batch: 200 | Loss: 0.587 | Acc: 83.586,99.160,99.973,% | Adaptive Acc: 92.483% | clf_exit: 0.821 0.173 0.006
Batch: 220 | Loss: 0.588 | Acc: 83.512,99.180,99.975,% | Adaptive Acc: 92.463% | clf_exit: 0.822 0.173 0.006
Batch: 240 | Loss: 0.587 | Acc: 83.616,99.190,99.974,% | Adaptive Acc: 92.489% | clf_exit: 0.822 0.172 0.006
Batch: 260 | Loss: 0.586 | Acc: 83.669,99.210,99.976,% | Adaptive Acc: 92.532% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.587 | Acc: 83.616,99.208,99.972,% | Adaptive Acc: 92.513% | clf_exit: 0.822 0.173 0.006
Batch: 300 | Loss: 0.586 | Acc: 83.664,99.206,99.966,% | Adaptive Acc: 92.522% | clf_exit: 0.822 0.172 0.006
Batch: 320 | Loss: 0.584 | Acc: 83.725,99.211,99.968,% | Adaptive Acc: 92.543% | clf_exit: 0.822 0.172 0.006
Batch: 340 | Loss: 0.586 | Acc: 83.676,99.203,99.963,% | Adaptive Acc: 92.492% | clf_exit: 0.822 0.172 0.006
Batch: 360 | Loss: 0.585 | Acc: 83.704,99.197,99.963,% | Adaptive Acc: 92.503% | clf_exit: 0.823 0.171 0.006
Batch: 380 | Loss: 0.585 | Acc: 83.653,99.202,99.963,% | Adaptive Acc: 92.503% | clf_exit: 0.822 0.172 0.006
Batch: 0 | Loss: 0.929 | Acc: 82.031,92.188,92.969,% | Adaptive Acc: 87.500% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.122 | Acc: 80.246,90.960,93.266,% | Adaptive Acc: 85.231% | clf_exit: 0.851 0.131 0.017
Batch: 40 | Loss: 1.125 | Acc: 80.297,90.720,93.483,% | Adaptive Acc: 85.537% | clf_exit: 0.853 0.127 0.020
Batch: 60 | Loss: 1.104 | Acc: 80.520,90.779,93.494,% | Adaptive Acc: 85.745% | clf_exit: 0.855 0.125 0.020
Train classifier parameters

Epoch: 243
Batch: 0 | Loss: 0.567 | Acc: 85.156,98.438,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.852 0.141 0.008
Batch: 20 | Loss: 0.576 | Acc: 83.371,99.330,100.000,% | Adaptive Acc: 92.374% | clf_exit: 0.823 0.170 0.007
Batch: 40 | Loss: 0.579 | Acc: 83.708,99.314,100.000,% | Adaptive Acc: 92.130% | clf_exit: 0.823 0.171 0.006
Batch: 60 | Loss: 0.583 | Acc: 83.594,99.180,100.000,% | Adaptive Acc: 92.508% | clf_exit: 0.819 0.175 0.006
Batch: 80 | Loss: 0.575 | Acc: 83.729,99.248,99.990,% | Adaptive Acc: 92.641% | clf_exit: 0.821 0.174 0.005
Batch: 100 | Loss: 0.578 | Acc: 83.563,99.250,99.985,% | Adaptive Acc: 92.543% | clf_exit: 0.822 0.172 0.005
Batch: 120 | Loss: 0.578 | Acc: 83.536,99.174,99.981,% | Adaptive Acc: 92.504% | clf_exit: 0.822 0.172 0.006
Batch: 140 | Loss: 0.585 | Acc: 83.533,99.169,99.978,% | Adaptive Acc: 92.520% | clf_exit: 0.821 0.173 0.006
Batch: 160 | Loss: 0.588 | Acc: 83.482,99.175,99.976,% | Adaptive Acc: 92.576% | clf_exit: 0.819 0.175 0.007
Batch: 180 | Loss: 0.590 | Acc: 83.400,99.158,99.978,% | Adaptive Acc: 92.563% | clf_exit: 0.818 0.176 0.007
Batch: 200 | Loss: 0.587 | Acc: 83.489,99.176,99.977,% | Adaptive Acc: 92.533% | clf_exit: 0.820 0.174 0.006
Batch: 220 | Loss: 0.588 | Acc: 83.474,99.169,99.979,% | Adaptive Acc: 92.552% | clf_exit: 0.819 0.174 0.007
Batch: 240 | Loss: 0.586 | Acc: 83.578,99.173,99.977,% | Adaptive Acc: 92.564% | clf_exit: 0.820 0.174 0.006
Batch: 260 | Loss: 0.585 | Acc: 83.666,99.156,99.979,% | Adaptive Acc: 92.598% | clf_exit: 0.820 0.173 0.006
Batch: 280 | Loss: 0.585 | Acc: 83.674,99.177,99.978,% | Adaptive Acc: 92.588% | clf_exit: 0.820 0.173 0.006
Batch: 300 | Loss: 0.585 | Acc: 83.718,99.185,99.979,% | Adaptive Acc: 92.613% | clf_exit: 0.820 0.173 0.006
Batch: 320 | Loss: 0.585 | Acc: 83.735,99.175,99.981,% | Adaptive Acc: 92.640% | clf_exit: 0.820 0.174 0.006
Batch: 340 | Loss: 0.584 | Acc: 83.752,99.168,99.982,% | Adaptive Acc: 92.627% | clf_exit: 0.821 0.173 0.006
Batch: 360 | Loss: 0.585 | Acc: 83.676,99.184,99.976,% | Adaptive Acc: 92.620% | clf_exit: 0.821 0.173 0.006
Batch: 380 | Loss: 0.587 | Acc: 83.625,99.163,99.975,% | Adaptive Acc: 92.575% | clf_exit: 0.821 0.173 0.006
Batch: 0 | Loss: 0.921 | Acc: 82.031,91.406,93.750,% | Adaptive Acc: 88.281% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.120 | Acc: 80.208,90.737,93.378,% | Adaptive Acc: 85.417% | clf_exit: 0.846 0.135 0.019
Batch: 40 | Loss: 1.124 | Acc: 80.335,90.625,93.483,% | Adaptive Acc: 85.537% | clf_exit: 0.852 0.127 0.021
Batch: 60 | Loss: 1.102 | Acc: 80.558,90.766,93.571,% | Adaptive Acc: 85.861% | clf_exit: 0.852 0.126 0.022
Train classifier parameters

Epoch: 244
Batch: 0 | Loss: 0.684 | Acc: 81.250,97.656,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.820 0.164 0.016
Batch: 20 | Loss: 0.587 | Acc: 83.073,99.293,100.000,% | Adaptive Acc: 92.448% | clf_exit: 0.814 0.179 0.007
Batch: 40 | Loss: 0.593 | Acc: 82.984,99.162,99.962,% | Adaptive Acc: 92.378% | clf_exit: 0.814 0.179 0.007
Batch: 60 | Loss: 0.582 | Acc: 83.504,99.219,99.974,% | Adaptive Acc: 92.572% | clf_exit: 0.822 0.173 0.006
Batch: 80 | Loss: 0.579 | Acc: 83.681,99.228,99.981,% | Adaptive Acc: 92.438% | clf_exit: 0.825 0.170 0.005
Batch: 100 | Loss: 0.580 | Acc: 83.632,99.273,99.985,% | Adaptive Acc: 92.543% | clf_exit: 0.823 0.172 0.005
Batch: 120 | Loss: 0.580 | Acc: 83.600,99.296,99.987,% | Adaptive Acc: 92.568% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.579 | Acc: 83.699,99.335,99.983,% | Adaptive Acc: 92.542% | clf_exit: 0.826 0.169 0.005
Batch: 160 | Loss: 0.578 | Acc: 83.662,99.345,99.981,% | Adaptive Acc: 92.440% | clf_exit: 0.826 0.168 0.005
Batch: 180 | Loss: 0.576 | Acc: 83.788,99.353,99.983,% | Adaptive Acc: 92.537% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.578 | Acc: 83.757,99.324,99.984,% | Adaptive Acc: 92.518% | clf_exit: 0.827 0.167 0.005
Batch: 220 | Loss: 0.581 | Acc: 83.601,99.286,99.982,% | Adaptive Acc: 92.474% | clf_exit: 0.827 0.168 0.006
Batch: 240 | Loss: 0.582 | Acc: 83.584,99.222,99.971,% | Adaptive Acc: 92.492% | clf_exit: 0.826 0.168 0.006
Batch: 260 | Loss: 0.585 | Acc: 83.564,99.225,99.973,% | Adaptive Acc: 92.421% | clf_exit: 0.825 0.169 0.006
Batch: 280 | Loss: 0.584 | Acc: 83.599,99.224,99.972,% | Adaptive Acc: 92.421% | clf_exit: 0.825 0.169 0.006
Batch: 300 | Loss: 0.585 | Acc: 83.511,99.224,99.971,% | Adaptive Acc: 92.411% | clf_exit: 0.825 0.169 0.006
Batch: 320 | Loss: 0.585 | Acc: 83.560,99.233,99.973,% | Adaptive Acc: 92.428% | clf_exit: 0.824 0.170 0.006
Batch: 340 | Loss: 0.585 | Acc: 83.539,99.249,99.973,% | Adaptive Acc: 92.407% | clf_exit: 0.824 0.170 0.006
Batch: 360 | Loss: 0.585 | Acc: 83.566,99.243,99.970,% | Adaptive Acc: 92.419% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.584 | Acc: 83.567,99.256,99.969,% | Adaptive Acc: 92.436% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.910 | Acc: 82.031,92.969,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.086 0.031
Batch: 20 | Loss: 1.118 | Acc: 80.022,90.960,93.266,% | Adaptive Acc: 85.379% | clf_exit: 0.849 0.133 0.018
Batch: 40 | Loss: 1.119 | Acc: 80.240,90.701,93.540,% | Adaptive Acc: 85.709% | clf_exit: 0.853 0.126 0.021
Batch: 60 | Loss: 1.097 | Acc: 80.520,90.830,93.519,% | Adaptive Acc: 85.938% | clf_exit: 0.855 0.124 0.021
Train classifier parameters

Epoch: 245
Batch: 0 | Loss: 0.503 | Acc: 86.719,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.828 0.164 0.008
Batch: 20 | Loss: 0.576 | Acc: 82.887,99.330,100.000,% | Adaptive Acc: 92.150% | clf_exit: 0.820 0.176 0.003
Batch: 40 | Loss: 0.584 | Acc: 83.346,99.257,99.962,% | Adaptive Acc: 92.188% | clf_exit: 0.822 0.174 0.004
Batch: 60 | Loss: 0.586 | Acc: 83.478,99.206,99.974,% | Adaptive Acc: 92.123% | clf_exit: 0.823 0.172 0.005
Batch: 80 | Loss: 0.585 | Acc: 83.507,99.238,99.971,% | Adaptive Acc: 92.255% | clf_exit: 0.824 0.171 0.005
Batch: 100 | Loss: 0.585 | Acc: 83.369,99.273,99.977,% | Adaptive Acc: 92.420% | clf_exit: 0.822 0.173 0.005
Batch: 120 | Loss: 0.584 | Acc: 83.381,99.264,99.974,% | Adaptive Acc: 92.323% | clf_exit: 0.824 0.170 0.005
Batch: 140 | Loss: 0.580 | Acc: 83.633,99.269,99.978,% | Adaptive Acc: 92.398% | clf_exit: 0.826 0.169 0.006
Batch: 160 | Loss: 0.581 | Acc: 83.565,99.253,99.981,% | Adaptive Acc: 92.372% | clf_exit: 0.826 0.169 0.005
Batch: 180 | Loss: 0.584 | Acc: 83.503,99.258,99.974,% | Adaptive Acc: 92.364% | clf_exit: 0.823 0.171 0.006
Batch: 200 | Loss: 0.584 | Acc: 83.555,99.238,99.977,% | Adaptive Acc: 92.444% | clf_exit: 0.823 0.172 0.006
Batch: 220 | Loss: 0.582 | Acc: 83.583,99.261,99.979,% | Adaptive Acc: 92.499% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.584 | Acc: 83.480,99.241,99.974,% | Adaptive Acc: 92.372% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.582 | Acc: 83.588,99.210,99.976,% | Adaptive Acc: 92.442% | clf_exit: 0.823 0.171 0.006
Batch: 280 | Loss: 0.583 | Acc: 83.647,99.208,99.975,% | Adaptive Acc: 92.482% | clf_exit: 0.823 0.170 0.006
Batch: 300 | Loss: 0.584 | Acc: 83.640,99.177,99.977,% | Adaptive Acc: 92.442% | clf_exit: 0.823 0.170 0.006
Batch: 320 | Loss: 0.584 | Acc: 83.628,99.180,99.973,% | Adaptive Acc: 92.448% | clf_exit: 0.823 0.171 0.006
Batch: 340 | Loss: 0.583 | Acc: 83.646,99.196,99.973,% | Adaptive Acc: 92.453% | clf_exit: 0.823 0.171 0.006
Batch: 360 | Loss: 0.584 | Acc: 83.600,99.195,99.974,% | Adaptive Acc: 92.452% | clf_exit: 0.822 0.171 0.006
Batch: 380 | Loss: 0.586 | Acc: 83.532,99.178,99.975,% | Adaptive Acc: 92.429% | clf_exit: 0.822 0.171 0.006
Batch: 0 | Loss: 0.923 | Acc: 82.031,92.969,92.969,% | Adaptive Acc: 89.062% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.124 | Acc: 80.283,90.848,93.229,% | Adaptive Acc: 85.305% | clf_exit: 0.849 0.132 0.018
Batch: 40 | Loss: 1.126 | Acc: 80.450,90.720,93.407,% | Adaptive Acc: 85.633% | clf_exit: 0.853 0.127 0.020
Batch: 60 | Loss: 1.101 | Acc: 80.622,90.804,93.494,% | Adaptive Acc: 85.912% | clf_exit: 0.855 0.124 0.021
Train classifier parameters

Epoch: 246
Batch: 0 | Loss: 0.555 | Acc: 85.156,97.656,99.219,% | Adaptive Acc: 91.406% | clf_exit: 0.820 0.172 0.008
Batch: 20 | Loss: 0.593 | Acc: 84.338,98.847,99.888,% | Adaptive Acc: 92.671% | clf_exit: 0.827 0.164 0.009
Batch: 40 | Loss: 0.587 | Acc: 83.841,99.047,99.924,% | Adaptive Acc: 92.359% | clf_exit: 0.826 0.168 0.006
Batch: 60 | Loss: 0.578 | Acc: 84.068,99.091,99.949,% | Adaptive Acc: 92.533% | clf_exit: 0.824 0.169 0.007
Batch: 80 | Loss: 0.581 | Acc: 83.999,99.142,99.961,% | Adaptive Acc: 92.564% | clf_exit: 0.824 0.169 0.007
Batch: 100 | Loss: 0.582 | Acc: 83.926,99.157,99.961,% | Adaptive Acc: 92.690% | clf_exit: 0.822 0.171 0.007
Batch: 120 | Loss: 0.583 | Acc: 83.923,99.154,99.961,% | Adaptive Acc: 92.736% | clf_exit: 0.821 0.172 0.007
Batch: 140 | Loss: 0.582 | Acc: 83.893,99.180,99.961,% | Adaptive Acc: 92.658% | clf_exit: 0.822 0.172 0.006
Batch: 160 | Loss: 0.582 | Acc: 83.832,99.190,99.966,% | Adaptive Acc: 92.678% | clf_exit: 0.822 0.171 0.007
Batch: 180 | Loss: 0.580 | Acc: 83.905,99.214,99.965,% | Adaptive Acc: 92.636% | clf_exit: 0.823 0.170 0.007
Batch: 200 | Loss: 0.581 | Acc: 83.920,99.238,99.957,% | Adaptive Acc: 92.677% | clf_exit: 0.822 0.171 0.007
Batch: 220 | Loss: 0.583 | Acc: 83.802,99.222,99.961,% | Adaptive Acc: 92.636% | clf_exit: 0.822 0.171 0.007
Batch: 240 | Loss: 0.582 | Acc: 83.892,99.212,99.955,% | Adaptive Acc: 92.635% | clf_exit: 0.822 0.171 0.007
Batch: 260 | Loss: 0.580 | Acc: 83.857,99.219,99.958,% | Adaptive Acc: 92.613% | clf_exit: 0.823 0.170 0.007
Batch: 280 | Loss: 0.582 | Acc: 83.786,99.216,99.958,% | Adaptive Acc: 92.602% | clf_exit: 0.823 0.171 0.007
Batch: 300 | Loss: 0.582 | Acc: 83.755,99.203,99.956,% | Adaptive Acc: 92.585% | clf_exit: 0.823 0.171 0.007
Batch: 320 | Loss: 0.585 | Acc: 83.645,99.199,99.959,% | Adaptive Acc: 92.528% | clf_exit: 0.822 0.171 0.007
Batch: 340 | Loss: 0.585 | Acc: 83.587,99.203,99.956,% | Adaptive Acc: 92.472% | clf_exit: 0.823 0.171 0.006
Batch: 360 | Loss: 0.584 | Acc: 83.602,99.206,99.959,% | Adaptive Acc: 92.499% | clf_exit: 0.823 0.171 0.006
Batch: 380 | Loss: 0.585 | Acc: 83.553,99.196,99.957,% | Adaptive Acc: 92.448% | clf_exit: 0.823 0.170 0.007
Batch: 0 | Loss: 0.931 | Acc: 82.031,92.188,93.750,% | Adaptive Acc: 88.281% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.123 | Acc: 80.246,91.109,93.192,% | Adaptive Acc: 85.565% | clf_exit: 0.846 0.137 0.017
Batch: 40 | Loss: 1.122 | Acc: 80.450,90.854,93.388,% | Adaptive Acc: 85.747% | clf_exit: 0.852 0.128 0.020
Batch: 60 | Loss: 1.098 | Acc: 80.571,90.971,93.571,% | Adaptive Acc: 86.002% | clf_exit: 0.851 0.128 0.021
Train classifier parameters

Epoch: 247
Batch: 0 | Loss: 0.626 | Acc: 78.125,99.219,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.820 0.180 0.000
Batch: 20 | Loss: 0.566 | Acc: 84.152,99.554,100.000,% | Adaptive Acc: 92.485% | clf_exit: 0.828 0.170 0.002
Batch: 40 | Loss: 0.595 | Acc: 83.194,99.371,99.981,% | Adaptive Acc: 91.883% | clf_exit: 0.824 0.172 0.004
Batch: 60 | Loss: 0.585 | Acc: 83.555,99.334,99.974,% | Adaptive Acc: 92.047% | clf_exit: 0.827 0.168 0.005
Batch: 80 | Loss: 0.587 | Acc: 83.478,99.277,99.971,% | Adaptive Acc: 92.024% | clf_exit: 0.826 0.168 0.006
Batch: 100 | Loss: 0.589 | Acc: 83.509,99.281,99.969,% | Adaptive Acc: 92.141% | clf_exit: 0.823 0.171 0.005
Batch: 120 | Loss: 0.586 | Acc: 83.749,99.257,99.974,% | Adaptive Acc: 92.226% | clf_exit: 0.825 0.170 0.005
Batch: 140 | Loss: 0.585 | Acc: 83.793,99.280,99.967,% | Adaptive Acc: 92.309% | clf_exit: 0.825 0.170 0.005
Batch: 160 | Loss: 0.586 | Acc: 83.778,99.228,99.966,% | Adaptive Acc: 92.285% | clf_exit: 0.824 0.171 0.006
Batch: 180 | Loss: 0.585 | Acc: 83.840,99.197,99.965,% | Adaptive Acc: 92.304% | clf_exit: 0.824 0.171 0.005
Batch: 200 | Loss: 0.586 | Acc: 83.835,99.180,99.961,% | Adaptive Acc: 92.300% | clf_exit: 0.824 0.171 0.006
Batch: 220 | Loss: 0.586 | Acc: 83.827,99.180,99.965,% | Adaptive Acc: 92.308% | clf_exit: 0.823 0.171 0.005
Batch: 240 | Loss: 0.587 | Acc: 83.791,99.203,99.968,% | Adaptive Acc: 92.330% | clf_exit: 0.824 0.171 0.005
Batch: 260 | Loss: 0.585 | Acc: 83.845,99.225,99.967,% | Adaptive Acc: 92.364% | clf_exit: 0.824 0.170 0.006
Batch: 280 | Loss: 0.586 | Acc: 83.811,99.216,99.967,% | Adaptive Acc: 92.363% | clf_exit: 0.824 0.171 0.005
Batch: 300 | Loss: 0.588 | Acc: 83.726,99.211,99.969,% | Adaptive Acc: 92.374% | clf_exit: 0.823 0.171 0.006
Batch: 320 | Loss: 0.588 | Acc: 83.701,99.219,99.971,% | Adaptive Acc: 92.394% | clf_exit: 0.823 0.171 0.006
Batch: 340 | Loss: 0.586 | Acc: 83.731,99.207,99.968,% | Adaptive Acc: 92.382% | clf_exit: 0.824 0.170 0.006
Batch: 360 | Loss: 0.586 | Acc: 83.687,99.210,99.970,% | Adaptive Acc: 92.322% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.586 | Acc: 83.637,99.217,99.965,% | Adaptive Acc: 92.325% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.935 | Acc: 82.031,92.188,92.969,% | Adaptive Acc: 89.062% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.125 | Acc: 80.469,91.183,93.155,% | Adaptive Acc: 85.640% | clf_exit: 0.845 0.139 0.016
Batch: 40 | Loss: 1.124 | Acc: 80.621,90.911,93.483,% | Adaptive Acc: 85.899% | clf_exit: 0.852 0.129 0.019
Batch: 60 | Loss: 1.099 | Acc: 80.802,91.009,93.648,% | Adaptive Acc: 86.130% | clf_exit: 0.852 0.127 0.021
Train classifier parameters

Epoch: 248
Batch: 0 | Loss: 0.724 | Acc: 78.125,98.438,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.766 0.219 0.016
Batch: 20 | Loss: 0.574 | Acc: 84.263,99.144,99.963,% | Adaptive Acc: 92.708% | clf_exit: 0.821 0.172 0.007
Batch: 40 | Loss: 0.573 | Acc: 83.918,99.162,99.962,% | Adaptive Acc: 92.702% | clf_exit: 0.819 0.175 0.006
Batch: 60 | Loss: 0.574 | Acc: 83.952,99.193,99.962,% | Adaptive Acc: 92.546% | clf_exit: 0.823 0.172 0.005
Batch: 80 | Loss: 0.576 | Acc: 83.767,99.199,99.952,% | Adaptive Acc: 92.679% | clf_exit: 0.821 0.174 0.005
Batch: 100 | Loss: 0.579 | Acc: 83.625,99.126,99.954,% | Adaptive Acc: 92.644% | clf_exit: 0.818 0.176 0.005
Batch: 120 | Loss: 0.578 | Acc: 83.665,99.161,99.955,% | Adaptive Acc: 92.659% | clf_exit: 0.820 0.174 0.005
Batch: 140 | Loss: 0.577 | Acc: 83.766,99.158,99.961,% | Adaptive Acc: 92.664% | clf_exit: 0.821 0.174 0.005
Batch: 160 | Loss: 0.581 | Acc: 83.603,99.141,99.966,% | Adaptive Acc: 92.585% | clf_exit: 0.820 0.175 0.005
Batch: 180 | Loss: 0.579 | Acc: 83.706,99.154,99.970,% | Adaptive Acc: 92.680% | clf_exit: 0.820 0.175 0.005
Batch: 200 | Loss: 0.582 | Acc: 83.563,99.145,99.973,% | Adaptive Acc: 92.580% | clf_exit: 0.820 0.175 0.006
Batch: 220 | Loss: 0.581 | Acc: 83.682,99.159,99.972,% | Adaptive Acc: 92.658% | clf_exit: 0.819 0.175 0.006
Batch: 240 | Loss: 0.583 | Acc: 83.613,99.151,99.974,% | Adaptive Acc: 92.560% | clf_exit: 0.820 0.174 0.006
Batch: 260 | Loss: 0.584 | Acc: 83.576,99.141,99.976,% | Adaptive Acc: 92.511% | clf_exit: 0.820 0.174 0.006
Batch: 280 | Loss: 0.584 | Acc: 83.574,99.152,99.978,% | Adaptive Acc: 92.466% | clf_exit: 0.821 0.174 0.006
Batch: 300 | Loss: 0.585 | Acc: 83.529,99.141,99.977,% | Adaptive Acc: 92.468% | clf_exit: 0.820 0.174 0.006
Batch: 320 | Loss: 0.585 | Acc: 83.440,99.146,99.978,% | Adaptive Acc: 92.465% | clf_exit: 0.820 0.174 0.005
Batch: 340 | Loss: 0.584 | Acc: 83.500,99.129,99.979,% | Adaptive Acc: 92.478% | clf_exit: 0.821 0.174 0.005
Batch: 360 | Loss: 0.583 | Acc: 83.555,99.134,99.978,% | Adaptive Acc: 92.497% | clf_exit: 0.822 0.173 0.005
Batch: 380 | Loss: 0.584 | Acc: 83.483,99.151,99.977,% | Adaptive Acc: 92.481% | clf_exit: 0.821 0.174 0.005
Batch: 0 | Loss: 0.919 | Acc: 82.031,91.406,94.531,% | Adaptive Acc: 87.500% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.117 | Acc: 80.357,90.923,93.601,% | Adaptive Acc: 85.305% | clf_exit: 0.848 0.135 0.017
Batch: 40 | Loss: 1.120 | Acc: 80.431,90.796,93.674,% | Adaptive Acc: 85.747% | clf_exit: 0.852 0.128 0.020
Batch: 60 | Loss: 1.100 | Acc: 80.571,90.804,93.686,% | Adaptive Acc: 86.027% | clf_exit: 0.853 0.126 0.021
Train classifier parameters

Epoch: 249
Batch: 0 | Loss: 0.603 | Acc: 82.812,98.438,99.219,% | Adaptive Acc: 90.625% | clf_exit: 0.852 0.148 0.000
Batch: 20 | Loss: 0.559 | Acc: 84.077,99.033,99.963,% | Adaptive Acc: 92.894% | clf_exit: 0.831 0.163 0.007
Batch: 40 | Loss: 0.572 | Acc: 83.594,99.219,99.981,% | Adaptive Acc: 92.969% | clf_exit: 0.821 0.173 0.006
Batch: 60 | Loss: 0.578 | Acc: 83.568,99.193,99.974,% | Adaptive Acc: 92.815% | clf_exit: 0.818 0.176 0.006
Batch: 80 | Loss: 0.577 | Acc: 83.603,99.238,99.971,% | Adaptive Acc: 92.785% | clf_exit: 0.820 0.174 0.006
Batch: 100 | Loss: 0.580 | Acc: 83.586,99.250,99.977,% | Adaptive Acc: 92.729% | clf_exit: 0.821 0.174 0.006
Batch: 120 | Loss: 0.583 | Acc: 83.471,99.225,99.981,% | Adaptive Acc: 92.639% | clf_exit: 0.821 0.173 0.006
Batch: 140 | Loss: 0.585 | Acc: 83.367,99.230,99.983,% | Adaptive Acc: 92.503% | clf_exit: 0.822 0.173 0.006
Batch: 160 | Loss: 0.587 | Acc: 83.317,99.219,99.971,% | Adaptive Acc: 92.469% | clf_exit: 0.821 0.174 0.006
Batch: 180 | Loss: 0.588 | Acc: 83.326,99.210,99.974,% | Adaptive Acc: 92.477% | clf_exit: 0.820 0.174 0.006
Batch: 200 | Loss: 0.592 | Acc: 83.259,99.188,99.973,% | Adaptive Acc: 92.401% | clf_exit: 0.820 0.174 0.006
Batch: 220 | Loss: 0.592 | Acc: 83.318,99.166,99.975,% | Adaptive Acc: 92.470% | clf_exit: 0.819 0.174 0.006
Batch: 240 | Loss: 0.591 | Acc: 83.390,99.144,99.968,% | Adaptive Acc: 92.457% | clf_exit: 0.821 0.173 0.006
Batch: 260 | Loss: 0.589 | Acc: 83.375,99.171,99.970,% | Adaptive Acc: 92.472% | clf_exit: 0.820 0.173 0.006
Batch: 280 | Loss: 0.587 | Acc: 83.483,99.171,99.972,% | Adaptive Acc: 92.457% | clf_exit: 0.822 0.172 0.006
Batch: 300 | Loss: 0.585 | Acc: 83.524,99.156,99.969,% | Adaptive Acc: 92.465% | clf_exit: 0.823 0.171 0.006
Batch: 320 | Loss: 0.585 | Acc: 83.545,99.146,99.968,% | Adaptive Acc: 92.484% | clf_exit: 0.822 0.171 0.006
Batch: 340 | Loss: 0.584 | Acc: 83.628,99.139,99.968,% | Adaptive Acc: 92.499% | clf_exit: 0.823 0.170 0.006
Batch: 360 | Loss: 0.585 | Acc: 83.598,99.141,99.970,% | Adaptive Acc: 92.458% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.585 | Acc: 83.579,99.145,99.971,% | Adaptive Acc: 92.448% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.936 | Acc: 82.812,90.625,93.750,% | Adaptive Acc: 89.062% | clf_exit: 0.859 0.117 0.023
Batch: 20 | Loss: 1.123 | Acc: 80.394,91.034,93.304,% | Adaptive Acc: 85.603% | clf_exit: 0.846 0.137 0.017
Batch: 40 | Loss: 1.123 | Acc: 80.488,90.663,93.483,% | Adaptive Acc: 85.785% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.098 | Acc: 80.661,90.843,93.635,% | Adaptive Acc: 86.104% | clf_exit: 0.850 0.129 0.022
Train all parameters

Epoch: 250
Batch: 0 | Loss: 0.588 | Acc: 84.375,100.000,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.805 0.188 0.008
Batch: 20 | Loss: 0.583 | Acc: 83.408,99.330,99.963,% | Adaptive Acc: 93.155% | clf_exit: 0.820 0.174 0.005
Batch: 40 | Loss: 0.578 | Acc: 83.498,99.257,99.962,% | Adaptive Acc: 92.893% | clf_exit: 0.823 0.173 0.005
Batch: 60 | Loss: 0.582 | Acc: 83.350,99.193,99.974,% | Adaptive Acc: 92.610% | clf_exit: 0.823 0.172 0.005
Batch: 80 | Loss: 0.576 | Acc: 83.767,99.228,99.971,% | Adaptive Acc: 92.785% | clf_exit: 0.824 0.171 0.006
Batch: 100 | Loss: 0.575 | Acc: 83.772,99.234,99.969,% | Adaptive Acc: 92.853% | clf_exit: 0.823 0.171 0.005
Batch: 120 | Loss: 0.576 | Acc: 83.794,99.212,99.974,% | Adaptive Acc: 92.814% | clf_exit: 0.822 0.172 0.006
Batch: 140 | Loss: 0.578 | Acc: 83.754,99.224,99.972,% | Adaptive Acc: 92.697% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.580 | Acc: 83.734,99.253,99.971,% | Adaptive Acc: 92.629% | clf_exit: 0.824 0.171 0.005
Batch: 180 | Loss: 0.581 | Acc: 83.723,99.214,99.970,% | Adaptive Acc: 92.580% | clf_exit: 0.825 0.170 0.005
Batch: 200 | Loss: 0.579 | Acc: 83.835,99.195,99.973,% | Adaptive Acc: 92.557% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.579 | Acc: 83.799,99.198,99.975,% | Adaptive Acc: 92.562% | clf_exit: 0.827 0.168 0.005
Batch: 240 | Loss: 0.582 | Acc: 83.720,99.154,99.977,% | Adaptive Acc: 92.518% | clf_exit: 0.826 0.168 0.005
Batch: 260 | Loss: 0.583 | Acc: 83.687,99.129,99.970,% | Adaptive Acc: 92.520% | clf_exit: 0.826 0.168 0.006
Batch: 280 | Loss: 0.583 | Acc: 83.635,99.146,99.972,% | Adaptive Acc: 92.541% | clf_exit: 0.825 0.169 0.006
Batch: 300 | Loss: 0.585 | Acc: 83.596,99.159,99.974,% | Adaptive Acc: 92.520% | clf_exit: 0.825 0.169 0.006
Batch: 320 | Loss: 0.586 | Acc: 83.562,99.141,99.976,% | Adaptive Acc: 92.506% | clf_exit: 0.825 0.170 0.006
Batch: 340 | Loss: 0.586 | Acc: 83.582,99.159,99.977,% | Adaptive Acc: 92.540% | clf_exit: 0.824 0.170 0.006
Batch: 360 | Loss: 0.588 | Acc: 83.486,99.137,99.976,% | Adaptive Acc: 92.490% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.589 | Acc: 83.440,99.137,99.977,% | Adaptive Acc: 92.448% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.900 | Acc: 81.250,92.969,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.125 | Acc: 80.506,90.923,93.266,% | Adaptive Acc: 86.049% | clf_exit: 0.839 0.143 0.018
Batch: 40 | Loss: 1.122 | Acc: 80.488,90.854,93.445,% | Adaptive Acc: 85.995% | clf_exit: 0.848 0.132 0.020
Batch: 60 | Loss: 1.099 | Acc: 80.661,90.945,93.558,% | Adaptive Acc: 86.270% | clf_exit: 0.849 0.130 0.021
Train all parameters

Epoch: 251
Batch: 0 | Loss: 0.632 | Acc: 84.375,98.438,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.812 0.188 0.000
Batch: 20 | Loss: 0.566 | Acc: 84.635,99.591,100.000,% | Adaptive Acc: 93.043% | clf_exit: 0.824 0.170 0.006
Batch: 40 | Loss: 0.568 | Acc: 84.546,99.466,99.981,% | Adaptive Acc: 92.988% | clf_exit: 0.825 0.170 0.005
Batch: 60 | Loss: 0.572 | Acc: 84.285,99.462,99.974,% | Adaptive Acc: 92.713% | clf_exit: 0.826 0.169 0.005
Batch: 80 | Loss: 0.575 | Acc: 84.307,99.392,99.961,% | Adaptive Acc: 92.863% | clf_exit: 0.826 0.168 0.006
Batch: 100 | Loss: 0.582 | Acc: 83.864,99.335,99.954,% | Adaptive Acc: 92.590% | clf_exit: 0.824 0.170 0.006
Batch: 120 | Loss: 0.582 | Acc: 83.749,99.322,99.955,% | Adaptive Acc: 92.549% | clf_exit: 0.824 0.170 0.006
Batch: 140 | Loss: 0.584 | Acc: 83.815,99.263,99.945,% | Adaptive Acc: 92.487% | clf_exit: 0.824 0.170 0.006
Batch: 160 | Loss: 0.585 | Acc: 83.754,99.214,99.947,% | Adaptive Acc: 92.484% | clf_exit: 0.824 0.170 0.006
Batch: 180 | Loss: 0.587 | Acc: 83.745,99.206,99.953,% | Adaptive Acc: 92.446% | clf_exit: 0.825 0.169 0.006
Batch: 200 | Loss: 0.589 | Acc: 83.644,99.192,99.957,% | Adaptive Acc: 92.394% | clf_exit: 0.824 0.170 0.006
Batch: 220 | Loss: 0.587 | Acc: 83.686,99.194,99.954,% | Adaptive Acc: 92.410% | clf_exit: 0.824 0.170 0.006
Batch: 240 | Loss: 0.586 | Acc: 83.701,99.180,99.958,% | Adaptive Acc: 92.405% | clf_exit: 0.824 0.170 0.006
Batch: 260 | Loss: 0.585 | Acc: 83.693,99.189,99.961,% | Adaptive Acc: 92.352% | clf_exit: 0.825 0.170 0.006
Batch: 280 | Loss: 0.584 | Acc: 83.680,99.169,99.964,% | Adaptive Acc: 92.379% | clf_exit: 0.825 0.170 0.006
Batch: 300 | Loss: 0.584 | Acc: 83.666,99.182,99.964,% | Adaptive Acc: 92.380% | clf_exit: 0.825 0.170 0.006
Batch: 320 | Loss: 0.586 | Acc: 83.586,99.175,99.966,% | Adaptive Acc: 92.407% | clf_exit: 0.824 0.170 0.006
Batch: 340 | Loss: 0.586 | Acc: 83.548,99.184,99.968,% | Adaptive Acc: 92.391% | clf_exit: 0.824 0.170 0.006
Batch: 360 | Loss: 0.584 | Acc: 83.563,99.191,99.970,% | Adaptive Acc: 92.408% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.584 | Acc: 83.571,99.176,99.971,% | Adaptive Acc: 92.405% | clf_exit: 0.825 0.170 0.006
Batch: 0 | Loss: 0.910 | Acc: 81.250,92.969,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.094 0.039
Batch: 20 | Loss: 1.130 | Acc: 80.655,91.109,93.118,% | Adaptive Acc: 86.272% | clf_exit: 0.843 0.138 0.020
Batch: 40 | Loss: 1.134 | Acc: 80.431,90.777,93.255,% | Adaptive Acc: 85.880% | clf_exit: 0.852 0.126 0.021
Batch: 60 | Loss: 1.106 | Acc: 80.597,91.022,93.468,% | Adaptive Acc: 86.078% | clf_exit: 0.852 0.127 0.021
Train all parameters

Epoch: 252
Batch: 0 | Loss: 0.534 | Acc: 85.938,99.219,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.828 0.156 0.016
Batch: 20 | Loss: 0.601 | Acc: 82.738,99.256,99.926,% | Adaptive Acc: 92.448% | clf_exit: 0.811 0.183 0.007
Batch: 40 | Loss: 0.591 | Acc: 83.346,99.181,99.886,% | Adaptive Acc: 92.550% | clf_exit: 0.818 0.176 0.007
Batch: 60 | Loss: 0.586 | Acc: 83.389,99.257,99.923,% | Adaptive Acc: 92.456% | clf_exit: 0.823 0.172 0.005
Batch: 80 | Loss: 0.591 | Acc: 83.324,99.219,99.913,% | Adaptive Acc: 92.342% | clf_exit: 0.824 0.171 0.006
Batch: 100 | Loss: 0.588 | Acc: 83.540,99.250,99.923,% | Adaptive Acc: 92.358% | clf_exit: 0.824 0.171 0.005
Batch: 120 | Loss: 0.590 | Acc: 83.549,99.232,99.935,% | Adaptive Acc: 92.259% | clf_exit: 0.825 0.170 0.005
Batch: 140 | Loss: 0.585 | Acc: 83.671,99.246,99.945,% | Adaptive Acc: 92.359% | clf_exit: 0.825 0.169 0.005
Batch: 160 | Loss: 0.586 | Acc: 83.613,99.199,99.947,% | Adaptive Acc: 92.289% | clf_exit: 0.826 0.169 0.005
Batch: 180 | Loss: 0.588 | Acc: 83.512,99.201,99.948,% | Adaptive Acc: 92.261% | clf_exit: 0.825 0.169 0.005
Batch: 200 | Loss: 0.587 | Acc: 83.590,99.199,99.953,% | Adaptive Acc: 92.289% | clf_exit: 0.826 0.169 0.005
Batch: 220 | Loss: 0.587 | Acc: 83.551,99.215,99.954,% | Adaptive Acc: 92.248% | clf_exit: 0.826 0.168 0.005
Batch: 240 | Loss: 0.587 | Acc: 83.500,99.212,99.958,% | Adaptive Acc: 92.249% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.586 | Acc: 83.519,99.228,99.961,% | Adaptive Acc: 92.265% | clf_exit: 0.826 0.168 0.005
Batch: 280 | Loss: 0.585 | Acc: 83.541,99.241,99.964,% | Adaptive Acc: 92.313% | clf_exit: 0.825 0.169 0.005
Batch: 300 | Loss: 0.585 | Acc: 83.529,99.219,99.966,% | Adaptive Acc: 92.302% | clf_exit: 0.826 0.169 0.005
Batch: 320 | Loss: 0.584 | Acc: 83.567,99.226,99.966,% | Adaptive Acc: 92.285% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.585 | Acc: 83.553,99.223,99.966,% | Adaptive Acc: 92.259% | clf_exit: 0.826 0.168 0.005
Batch: 360 | Loss: 0.584 | Acc: 83.566,99.212,99.963,% | Adaptive Acc: 92.244% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.584 | Acc: 83.559,99.200,99.965,% | Adaptive Acc: 92.278% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.932 | Acc: 82.031,92.188,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.119 | Acc: 80.543,90.774,93.192,% | Adaptive Acc: 86.049% | clf_exit: 0.840 0.143 0.016
Batch: 40 | Loss: 1.124 | Acc: 80.469,90.377,93.388,% | Adaptive Acc: 86.033% | clf_exit: 0.849 0.132 0.019
Batch: 60 | Loss: 1.101 | Acc: 80.546,90.548,93.519,% | Adaptive Acc: 86.155% | clf_exit: 0.851 0.128 0.021
Train all parameters

Epoch: 253
Batch: 0 | Loss: 0.686 | Acc: 81.250,99.219,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.812 0.188 0.000
Batch: 20 | Loss: 0.591 | Acc: 83.705,99.256,100.000,% | Adaptive Acc: 92.597% | clf_exit: 0.819 0.174 0.007
Batch: 40 | Loss: 0.587 | Acc: 83.632,99.123,99.981,% | Adaptive Acc: 92.226% | clf_exit: 0.824 0.171 0.006
Batch: 60 | Loss: 0.582 | Acc: 83.876,99.168,99.974,% | Adaptive Acc: 92.559% | clf_exit: 0.821 0.174 0.005
Batch: 80 | Loss: 0.576 | Acc: 84.230,99.199,99.981,% | Adaptive Acc: 92.679% | clf_exit: 0.825 0.170 0.005
Batch: 100 | Loss: 0.575 | Acc: 84.321,99.172,99.977,% | Adaptive Acc: 92.675% | clf_exit: 0.826 0.168 0.006
Batch: 120 | Loss: 0.582 | Acc: 83.852,99.186,99.981,% | Adaptive Acc: 92.581% | clf_exit: 0.824 0.170 0.006
Batch: 140 | Loss: 0.586 | Acc: 83.633,99.197,99.983,% | Adaptive Acc: 92.492% | clf_exit: 0.822 0.172 0.006
Batch: 160 | Loss: 0.583 | Acc: 83.647,99.199,99.976,% | Adaptive Acc: 92.459% | clf_exit: 0.824 0.171 0.006
Batch: 180 | Loss: 0.585 | Acc: 83.555,99.219,99.978,% | Adaptive Acc: 92.399% | clf_exit: 0.823 0.171 0.006
Batch: 200 | Loss: 0.585 | Acc: 83.493,99.203,99.977,% | Adaptive Acc: 92.425% | clf_exit: 0.822 0.172 0.006
Batch: 220 | Loss: 0.585 | Acc: 83.466,99.233,99.975,% | Adaptive Acc: 92.389% | clf_exit: 0.823 0.172 0.006
Batch: 240 | Loss: 0.585 | Acc: 83.399,99.238,99.977,% | Adaptive Acc: 92.379% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.585 | Acc: 83.393,99.228,99.979,% | Adaptive Acc: 92.337% | clf_exit: 0.824 0.171 0.005
Batch: 280 | Loss: 0.584 | Acc: 83.485,99.233,99.972,% | Adaptive Acc: 92.327% | clf_exit: 0.825 0.170 0.005
Batch: 300 | Loss: 0.585 | Acc: 83.487,99.229,99.974,% | Adaptive Acc: 92.284% | clf_exit: 0.825 0.170 0.005
Batch: 320 | Loss: 0.585 | Acc: 83.438,99.238,99.973,% | Adaptive Acc: 92.304% | clf_exit: 0.824 0.170 0.005
Batch: 340 | Loss: 0.585 | Acc: 83.470,99.228,99.973,% | Adaptive Acc: 92.343% | clf_exit: 0.824 0.171 0.005
Batch: 360 | Loss: 0.584 | Acc: 83.494,99.225,99.974,% | Adaptive Acc: 92.350% | clf_exit: 0.824 0.170 0.005
Batch: 380 | Loss: 0.585 | Acc: 83.516,99.229,99.973,% | Adaptive Acc: 92.343% | clf_exit: 0.825 0.170 0.005
Batch: 0 | Loss: 0.898 | Acc: 81.250,92.188,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.108 | Acc: 80.022,91.220,93.304,% | Adaptive Acc: 85.789% | clf_exit: 0.842 0.142 0.016
Batch: 40 | Loss: 1.119 | Acc: 80.259,90.644,93.464,% | Adaptive Acc: 85.671% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.097 | Acc: 80.392,90.766,93.532,% | Adaptive Acc: 85.822% | clf_exit: 0.851 0.128 0.020
Train all parameters

Epoch: 254
Batch: 0 | Loss: 0.496 | Acc: 86.719,100.000,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.555 | Acc: 84.189,99.293,99.963,% | Adaptive Acc: 92.336% | clf_exit: 0.840 0.156 0.004
Batch: 40 | Loss: 0.576 | Acc: 83.308,99.314,99.962,% | Adaptive Acc: 92.207% | clf_exit: 0.832 0.162 0.006
Batch: 60 | Loss: 0.579 | Acc: 83.133,99.296,99.962,% | Adaptive Acc: 92.111% | clf_exit: 0.830 0.164 0.006
Batch: 80 | Loss: 0.579 | Acc: 83.160,99.286,99.961,% | Adaptive Acc: 92.323% | clf_exit: 0.827 0.168 0.005
Batch: 100 | Loss: 0.584 | Acc: 83.006,99.219,99.969,% | Adaptive Acc: 92.242% | clf_exit: 0.826 0.168 0.005
Batch: 120 | Loss: 0.585 | Acc: 83.161,99.206,99.961,% | Adaptive Acc: 92.239% | clf_exit: 0.825 0.169 0.005
Batch: 140 | Loss: 0.586 | Acc: 83.411,99.158,99.950,% | Adaptive Acc: 92.337% | clf_exit: 0.825 0.169 0.006
Batch: 160 | Loss: 0.587 | Acc: 83.463,99.175,99.956,% | Adaptive Acc: 92.348% | clf_exit: 0.824 0.170 0.006
Batch: 180 | Loss: 0.588 | Acc: 83.482,99.201,99.957,% | Adaptive Acc: 92.412% | clf_exit: 0.822 0.172 0.006
Batch: 200 | Loss: 0.589 | Acc: 83.520,99.207,99.961,% | Adaptive Acc: 92.421% | clf_exit: 0.822 0.172 0.006
Batch: 220 | Loss: 0.591 | Acc: 83.449,99.226,99.965,% | Adaptive Acc: 92.364% | clf_exit: 0.822 0.172 0.006
Batch: 240 | Loss: 0.589 | Acc: 83.548,99.261,99.964,% | Adaptive Acc: 92.405% | clf_exit: 0.822 0.172 0.006
Batch: 260 | Loss: 0.588 | Acc: 83.573,99.252,99.967,% | Adaptive Acc: 92.436% | clf_exit: 0.822 0.172 0.006
Batch: 280 | Loss: 0.586 | Acc: 83.616,99.244,99.967,% | Adaptive Acc: 92.452% | clf_exit: 0.823 0.172 0.006
Batch: 300 | Loss: 0.585 | Acc: 83.711,99.229,99.966,% | Adaptive Acc: 92.515% | clf_exit: 0.823 0.172 0.006
Batch: 320 | Loss: 0.586 | Acc: 83.655,99.231,99.968,% | Adaptive Acc: 92.509% | clf_exit: 0.823 0.172 0.006
Batch: 340 | Loss: 0.585 | Acc: 83.633,99.228,99.966,% | Adaptive Acc: 92.478% | clf_exit: 0.823 0.172 0.006
Batch: 360 | Loss: 0.586 | Acc: 83.596,99.212,99.968,% | Adaptive Acc: 92.467% | clf_exit: 0.823 0.172 0.006
Batch: 380 | Loss: 0.586 | Acc: 83.542,99.223,99.969,% | Adaptive Acc: 92.479% | clf_exit: 0.822 0.172 0.006
Batch: 0 | Loss: 0.879 | Acc: 82.031,94.531,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.112 | Acc: 80.283,91.220,93.676,% | Adaptive Acc: 85.900% | clf_exit: 0.844 0.140 0.017
Batch: 40 | Loss: 1.116 | Acc: 80.507,90.911,93.636,% | Adaptive Acc: 85.957% | clf_exit: 0.852 0.129 0.019
Batch: 60 | Loss: 1.096 | Acc: 80.751,91.009,93.699,% | Adaptive Acc: 86.014% | clf_exit: 0.853 0.127 0.020
Train all parameters

Epoch: 255
Batch: 0 | Loss: 0.607 | Acc: 82.812,99.219,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.581 | Acc: 82.961,99.256,100.000,% | Adaptive Acc: 92.411% | clf_exit: 0.826 0.170 0.004
Batch: 40 | Loss: 0.581 | Acc: 83.098,99.314,99.943,% | Adaptive Acc: 92.511% | clf_exit: 0.824 0.171 0.006
Batch: 60 | Loss: 0.578 | Acc: 83.453,99.270,99.962,% | Adaptive Acc: 92.431% | clf_exit: 0.827 0.167 0.006
Batch: 80 | Loss: 0.574 | Acc: 83.816,99.219,99.952,% | Adaptive Acc: 92.612% | clf_exit: 0.826 0.168 0.006
Batch: 100 | Loss: 0.579 | Acc: 83.609,99.180,99.954,% | Adaptive Acc: 92.497% | clf_exit: 0.825 0.169 0.006
Batch: 120 | Loss: 0.581 | Acc: 83.536,99.193,99.961,% | Adaptive Acc: 92.478% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.585 | Acc: 83.494,99.191,99.961,% | Adaptive Acc: 92.531% | clf_exit: 0.822 0.172 0.006
Batch: 160 | Loss: 0.587 | Acc: 83.511,99.141,99.951,% | Adaptive Acc: 92.474% | clf_exit: 0.823 0.171 0.006
Batch: 180 | Loss: 0.585 | Acc: 83.585,99.154,99.953,% | Adaptive Acc: 92.412% | clf_exit: 0.825 0.169 0.006
Batch: 200 | Loss: 0.586 | Acc: 83.539,99.172,99.949,% | Adaptive Acc: 92.487% | clf_exit: 0.824 0.171 0.006
Batch: 220 | Loss: 0.584 | Acc: 83.608,99.176,99.943,% | Adaptive Acc: 92.612% | clf_exit: 0.823 0.171 0.006
Batch: 240 | Loss: 0.583 | Acc: 83.613,99.186,99.948,% | Adaptive Acc: 92.589% | clf_exit: 0.823 0.171 0.006
Batch: 260 | Loss: 0.583 | Acc: 83.615,99.192,99.952,% | Adaptive Acc: 92.610% | clf_exit: 0.823 0.171 0.006
Batch: 280 | Loss: 0.581 | Acc: 83.680,99.233,99.956,% | Adaptive Acc: 92.621% | clf_exit: 0.824 0.171 0.006
Batch: 300 | Loss: 0.582 | Acc: 83.666,99.214,99.958,% | Adaptive Acc: 92.585% | clf_exit: 0.824 0.171 0.006
Batch: 320 | Loss: 0.580 | Acc: 83.728,99.231,99.956,% | Adaptive Acc: 92.567% | clf_exit: 0.824 0.170 0.006
Batch: 340 | Loss: 0.581 | Acc: 83.674,99.230,99.959,% | Adaptive Acc: 92.515% | clf_exit: 0.825 0.170 0.006
Batch: 360 | Loss: 0.580 | Acc: 83.721,99.240,99.959,% | Adaptive Acc: 92.568% | clf_exit: 0.824 0.170 0.006
Batch: 380 | Loss: 0.580 | Acc: 83.748,99.241,99.961,% | Adaptive Acc: 92.589% | clf_exit: 0.824 0.170 0.006
Batch: 0 | Loss: 0.913 | Acc: 81.250,92.188,95.312,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.109 | Acc: 80.134,91.220,93.676,% | Adaptive Acc: 86.198% | clf_exit: 0.841 0.141 0.018
Batch: 40 | Loss: 1.117 | Acc: 80.278,90.911,93.731,% | Adaptive Acc: 85.938% | clf_exit: 0.851 0.127 0.022
Batch: 60 | Loss: 1.095 | Acc: 80.392,91.060,93.763,% | Adaptive Acc: 86.078% | clf_exit: 0.851 0.128 0.021
Train all parameters

Epoch: 256
Batch: 0 | Loss: 0.532 | Acc: 85.156,99.219,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.820 0.172 0.008
Batch: 20 | Loss: 0.551 | Acc: 84.598,99.293,100.000,% | Adaptive Acc: 93.192% | clf_exit: 0.839 0.154 0.006
Batch: 40 | Loss: 0.571 | Acc: 84.432,99.295,99.981,% | Adaptive Acc: 93.083% | clf_exit: 0.831 0.164 0.006
Batch: 60 | Loss: 0.579 | Acc: 84.004,99.283,99.974,% | Adaptive Acc: 92.853% | clf_exit: 0.828 0.166 0.005
Batch: 80 | Loss: 0.578 | Acc: 84.086,99.248,99.981,% | Adaptive Acc: 92.872% | clf_exit: 0.826 0.169 0.005
Batch: 100 | Loss: 0.575 | Acc: 84.189,99.265,99.985,% | Adaptive Acc: 92.899% | clf_exit: 0.826 0.168 0.005
Batch: 120 | Loss: 0.577 | Acc: 83.910,99.257,99.974,% | Adaptive Acc: 92.672% | clf_exit: 0.825 0.169 0.005
Batch: 140 | Loss: 0.579 | Acc: 83.832,99.241,99.978,% | Adaptive Acc: 92.631% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.582 | Acc: 83.574,99.238,99.976,% | Adaptive Acc: 92.493% | clf_exit: 0.823 0.172 0.005
Batch: 180 | Loss: 0.582 | Acc: 83.568,99.245,99.978,% | Adaptive Acc: 92.507% | clf_exit: 0.823 0.172 0.005
Batch: 200 | Loss: 0.582 | Acc: 83.578,99.246,99.977,% | Adaptive Acc: 92.498% | clf_exit: 0.823 0.172 0.005
Batch: 220 | Loss: 0.585 | Acc: 83.438,99.247,99.979,% | Adaptive Acc: 92.495% | clf_exit: 0.822 0.173 0.005
Batch: 240 | Loss: 0.585 | Acc: 83.484,99.264,99.977,% | Adaptive Acc: 92.515% | clf_exit: 0.822 0.173 0.005
Batch: 260 | Loss: 0.583 | Acc: 83.522,99.288,99.979,% | Adaptive Acc: 92.541% | clf_exit: 0.822 0.172 0.005
Batch: 280 | Loss: 0.581 | Acc: 83.538,99.288,99.981,% | Adaptive Acc: 92.577% | clf_exit: 0.823 0.172 0.005
Batch: 300 | Loss: 0.580 | Acc: 83.607,99.286,99.982,% | Adaptive Acc: 92.603% | clf_exit: 0.823 0.172 0.005
Batch: 320 | Loss: 0.581 | Acc: 83.562,99.282,99.983,% | Adaptive Acc: 92.572% | clf_exit: 0.823 0.172 0.005
Batch: 340 | Loss: 0.582 | Acc: 83.546,99.281,99.984,% | Adaptive Acc: 92.549% | clf_exit: 0.823 0.172 0.005
Batch: 360 | Loss: 0.581 | Acc: 83.587,99.292,99.985,% | Adaptive Acc: 92.549% | clf_exit: 0.823 0.171 0.005
Batch: 380 | Loss: 0.580 | Acc: 83.647,99.284,99.986,% | Adaptive Acc: 92.550% | clf_exit: 0.824 0.171 0.005
Batch: 0 | Loss: 0.891 | Acc: 80.469,93.750,94.531,% | Adaptive Acc: 86.719% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.114 | Acc: 80.246,90.960,93.378,% | Adaptive Acc: 85.863% | clf_exit: 0.844 0.140 0.016
Batch: 40 | Loss: 1.119 | Acc: 80.450,90.758,93.579,% | Adaptive Acc: 85.766% | clf_exit: 0.851 0.129 0.020
Batch: 60 | Loss: 1.095 | Acc: 80.674,90.907,93.660,% | Adaptive Acc: 86.002% | clf_exit: 0.852 0.128 0.020
Train all parameters

Epoch: 257
Batch: 0 | Loss: 0.487 | Acc: 85.938,100.000,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.585 | Acc: 83.445,99.107,100.000,% | Adaptive Acc: 92.262% | clf_exit: 0.825 0.170 0.005
Batch: 40 | Loss: 0.573 | Acc: 83.956,99.276,100.000,% | Adaptive Acc: 92.226% | clf_exit: 0.829 0.167 0.004
Batch: 60 | Loss: 0.574 | Acc: 84.004,99.296,99.987,% | Adaptive Acc: 92.111% | clf_exit: 0.830 0.164 0.005
Batch: 80 | Loss: 0.575 | Acc: 83.777,99.257,99.990,% | Adaptive Acc: 92.033% | clf_exit: 0.830 0.165 0.005
Batch: 100 | Loss: 0.579 | Acc: 83.679,99.149,99.985,% | Adaptive Acc: 92.164% | clf_exit: 0.829 0.166 0.005
Batch: 120 | Loss: 0.583 | Acc: 83.400,99.167,99.981,% | Adaptive Acc: 92.175% | clf_exit: 0.826 0.168 0.005
Batch: 140 | Loss: 0.582 | Acc: 83.577,99.191,99.983,% | Adaptive Acc: 92.282% | clf_exit: 0.826 0.168 0.005
Batch: 160 | Loss: 0.582 | Acc: 83.618,99.194,99.985,% | Adaptive Acc: 92.255% | clf_exit: 0.826 0.169 0.005
Batch: 180 | Loss: 0.582 | Acc: 83.615,99.184,99.983,% | Adaptive Acc: 92.222% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.582 | Acc: 83.625,99.180,99.984,% | Adaptive Acc: 92.203% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.584 | Acc: 83.523,99.137,99.986,% | Adaptive Acc: 92.131% | clf_exit: 0.827 0.167 0.005
Batch: 240 | Loss: 0.582 | Acc: 83.578,99.160,99.987,% | Adaptive Acc: 92.165% | clf_exit: 0.827 0.167 0.005
Batch: 260 | Loss: 0.582 | Acc: 83.567,99.147,99.982,% | Adaptive Acc: 92.199% | clf_exit: 0.827 0.168 0.005
Batch: 280 | Loss: 0.584 | Acc: 83.496,99.155,99.983,% | Adaptive Acc: 92.157% | clf_exit: 0.827 0.167 0.005
Batch: 300 | Loss: 0.584 | Acc: 83.498,99.175,99.982,% | Adaptive Acc: 92.167% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.585 | Acc: 83.528,99.182,99.981,% | Adaptive Acc: 92.222% | clf_exit: 0.826 0.169 0.005
Batch: 340 | Loss: 0.584 | Acc: 83.571,99.173,99.979,% | Adaptive Acc: 92.236% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.583 | Acc: 83.613,99.175,99.978,% | Adaptive Acc: 92.272% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.583 | Acc: 83.635,99.178,99.975,% | Adaptive Acc: 92.311% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.893 | Acc: 81.250,92.188,96.094,% | Adaptive Acc: 87.500% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.106 | Acc: 80.655,91.146,93.750,% | Adaptive Acc: 86.496% | clf_exit: 0.843 0.138 0.019
Batch: 40 | Loss: 1.114 | Acc: 80.659,90.873,93.693,% | Adaptive Acc: 86.147% | clf_exit: 0.853 0.126 0.022
Batch: 60 | Loss: 1.092 | Acc: 80.763,91.035,93.750,% | Adaptive Acc: 86.219% | clf_exit: 0.854 0.124 0.022
Train all parameters

Epoch: 258
Batch: 0 | Loss: 0.667 | Acc: 81.250,99.219,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.766 0.227 0.008
Batch: 20 | Loss: 0.610 | Acc: 83.147,99.293,99.926,% | Adaptive Acc: 90.960% | clf_exit: 0.832 0.164 0.004
Batch: 40 | Loss: 0.592 | Acc: 83.575,99.295,99.962,% | Adaptive Acc: 91.597% | clf_exit: 0.830 0.165 0.004
Batch: 60 | Loss: 0.594 | Acc: 83.389,99.308,99.936,% | Adaptive Acc: 91.790% | clf_exit: 0.824 0.172 0.004
Batch: 80 | Loss: 0.586 | Acc: 83.613,99.248,99.942,% | Adaptive Acc: 92.139% | clf_exit: 0.825 0.170 0.005
Batch: 100 | Loss: 0.584 | Acc: 83.601,99.219,99.954,% | Adaptive Acc: 92.280% | clf_exit: 0.824 0.171 0.005
Batch: 120 | Loss: 0.583 | Acc: 83.684,99.219,99.961,% | Adaptive Acc: 92.342% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.582 | Acc: 83.721,99.213,99.956,% | Adaptive Acc: 92.271% | clf_exit: 0.824 0.170 0.005
Batch: 160 | Loss: 0.584 | Acc: 83.701,99.199,99.956,% | Adaptive Acc: 92.314% | clf_exit: 0.824 0.170 0.005
Batch: 180 | Loss: 0.588 | Acc: 83.507,99.176,99.957,% | Adaptive Acc: 92.278% | clf_exit: 0.822 0.172 0.005
Batch: 200 | Loss: 0.589 | Acc: 83.489,99.168,99.961,% | Adaptive Acc: 92.292% | clf_exit: 0.823 0.172 0.006
Batch: 220 | Loss: 0.587 | Acc: 83.604,99.201,99.965,% | Adaptive Acc: 92.378% | clf_exit: 0.823 0.172 0.006
Batch: 240 | Loss: 0.587 | Acc: 83.597,99.203,99.968,% | Adaptive Acc: 92.440% | clf_exit: 0.822 0.173 0.006
Batch: 260 | Loss: 0.587 | Acc: 83.651,99.198,99.970,% | Adaptive Acc: 92.409% | clf_exit: 0.823 0.172 0.006
Batch: 280 | Loss: 0.586 | Acc: 83.638,99.166,99.969,% | Adaptive Acc: 92.371% | clf_exit: 0.824 0.171 0.005
Batch: 300 | Loss: 0.585 | Acc: 83.607,99.156,99.969,% | Adaptive Acc: 92.369% | clf_exit: 0.824 0.170 0.006
Batch: 320 | Loss: 0.585 | Acc: 83.621,99.165,99.971,% | Adaptive Acc: 92.375% | clf_exit: 0.824 0.170 0.005
Batch: 340 | Loss: 0.584 | Acc: 83.665,99.178,99.970,% | Adaptive Acc: 92.398% | clf_exit: 0.824 0.170 0.005
Batch: 360 | Loss: 0.583 | Acc: 83.672,99.184,99.972,% | Adaptive Acc: 92.400% | clf_exit: 0.824 0.170 0.005
Batch: 380 | Loss: 0.581 | Acc: 83.715,99.182,99.969,% | Adaptive Acc: 92.403% | clf_exit: 0.825 0.170 0.005
Batch: 0 | Loss: 0.896 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.836 0.148 0.016
Batch: 20 | Loss: 1.106 | Acc: 80.171,91.109,93.155,% | Adaptive Acc: 86.198% | clf_exit: 0.843 0.140 0.018
Batch: 40 | Loss: 1.114 | Acc: 80.278,90.911,93.464,% | Adaptive Acc: 85.995% | clf_exit: 0.853 0.127 0.021
Batch: 60 | Loss: 1.094 | Acc: 80.571,91.022,93.571,% | Adaptive Acc: 86.142% | clf_exit: 0.853 0.127 0.021
Train all parameters

Epoch: 259
Batch: 0 | Loss: 0.586 | Acc: 80.469,100.000,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.820 0.172 0.008
Batch: 20 | Loss: 0.575 | Acc: 83.631,99.293,99.963,% | Adaptive Acc: 91.964% | clf_exit: 0.830 0.166 0.004
Batch: 40 | Loss: 0.581 | Acc: 83.651,99.276,99.943,% | Adaptive Acc: 92.340% | clf_exit: 0.825 0.169 0.006
Batch: 60 | Loss: 0.586 | Acc: 83.453,99.308,99.962,% | Adaptive Acc: 92.239% | clf_exit: 0.826 0.168 0.006
Batch: 80 | Loss: 0.592 | Acc: 83.256,99.267,99.961,% | Adaptive Acc: 92.149% | clf_exit: 0.824 0.170 0.006
Batch: 100 | Loss: 0.589 | Acc: 83.176,99.296,99.969,% | Adaptive Acc: 92.188% | clf_exit: 0.824 0.170 0.006
Batch: 120 | Loss: 0.590 | Acc: 83.168,99.264,99.968,% | Adaptive Acc: 92.149% | clf_exit: 0.823 0.171 0.006
Batch: 140 | Loss: 0.586 | Acc: 83.350,99.246,99.972,% | Adaptive Acc: 92.315% | clf_exit: 0.824 0.170 0.006
Batch: 160 | Loss: 0.585 | Acc: 83.332,99.253,99.971,% | Adaptive Acc: 92.357% | clf_exit: 0.823 0.171 0.006
Batch: 180 | Loss: 0.585 | Acc: 83.227,99.262,99.970,% | Adaptive Acc: 92.343% | clf_exit: 0.822 0.172 0.006
Batch: 200 | Loss: 0.585 | Acc: 83.275,99.242,99.973,% | Adaptive Acc: 92.269% | clf_exit: 0.823 0.171 0.006
Batch: 220 | Loss: 0.583 | Acc: 83.279,99.268,99.975,% | Adaptive Acc: 92.241% | clf_exit: 0.824 0.170 0.006
Batch: 240 | Loss: 0.581 | Acc: 83.402,99.267,99.974,% | Adaptive Acc: 92.256% | clf_exit: 0.824 0.170 0.006
Batch: 260 | Loss: 0.582 | Acc: 83.471,99.252,99.976,% | Adaptive Acc: 92.259% | clf_exit: 0.824 0.170 0.006
Batch: 280 | Loss: 0.581 | Acc: 83.516,99.244,99.978,% | Adaptive Acc: 92.263% | clf_exit: 0.825 0.170 0.006
Batch: 300 | Loss: 0.581 | Acc: 83.498,99.250,99.979,% | Adaptive Acc: 92.281% | clf_exit: 0.824 0.170 0.006
Batch: 320 | Loss: 0.582 | Acc: 83.472,99.248,99.981,% | Adaptive Acc: 92.270% | clf_exit: 0.825 0.169 0.006
Batch: 340 | Loss: 0.582 | Acc: 83.486,99.246,99.982,% | Adaptive Acc: 92.291% | clf_exit: 0.825 0.169 0.006
Batch: 360 | Loss: 0.580 | Acc: 83.563,99.245,99.976,% | Adaptive Acc: 92.352% | clf_exit: 0.826 0.169 0.006
Batch: 380 | Loss: 0.580 | Acc: 83.590,99.237,99.975,% | Adaptive Acc: 92.354% | clf_exit: 0.826 0.169 0.006
Batch: 0 | Loss: 0.915 | Acc: 82.031,92.969,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.113 | Acc: 80.618,90.774,93.378,% | Adaptive Acc: 86.384% | clf_exit: 0.843 0.139 0.017
Batch: 40 | Loss: 1.119 | Acc: 80.488,90.701,93.521,% | Adaptive Acc: 86.166% | clf_exit: 0.851 0.129 0.020
Batch: 60 | Loss: 1.096 | Acc: 80.648,90.856,93.673,% | Adaptive Acc: 86.219% | clf_exit: 0.853 0.127 0.020
Train all parameters

Epoch: 260
Batch: 0 | Loss: 0.639 | Acc: 83.594,97.656,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.844 0.141 0.016
Batch: 20 | Loss: 0.573 | Acc: 84.524,99.182,99.963,% | Adaptive Acc: 92.597% | clf_exit: 0.828 0.169 0.003
Batch: 40 | Loss: 0.565 | Acc: 84.242,99.295,99.962,% | Adaptive Acc: 92.359% | clf_exit: 0.832 0.165 0.003
Batch: 60 | Loss: 0.569 | Acc: 83.901,99.244,99.974,% | Adaptive Acc: 92.380% | clf_exit: 0.828 0.168 0.004
Batch: 80 | Loss: 0.571 | Acc: 83.922,99.267,99.981,% | Adaptive Acc: 92.390% | clf_exit: 0.829 0.166 0.005
Batch: 100 | Loss: 0.579 | Acc: 83.640,99.250,99.985,% | Adaptive Acc: 92.350% | clf_exit: 0.826 0.168 0.006
Batch: 120 | Loss: 0.576 | Acc: 83.749,99.257,99.974,% | Adaptive Acc: 92.336% | clf_exit: 0.827 0.167 0.006
Batch: 140 | Loss: 0.573 | Acc: 83.871,99.285,99.978,% | Adaptive Acc: 92.481% | clf_exit: 0.826 0.168 0.006
Batch: 160 | Loss: 0.577 | Acc: 83.798,99.253,99.981,% | Adaptive Acc: 92.488% | clf_exit: 0.825 0.170 0.006
Batch: 180 | Loss: 0.578 | Acc: 83.797,99.249,99.978,% | Adaptive Acc: 92.498% | clf_exit: 0.824 0.170 0.006
Batch: 200 | Loss: 0.578 | Acc: 83.730,99.262,99.981,% | Adaptive Acc: 92.495% | clf_exit: 0.824 0.170 0.006
Batch: 220 | Loss: 0.579 | Acc: 83.700,99.226,99.975,% | Adaptive Acc: 92.453% | clf_exit: 0.824 0.170 0.006
Batch: 240 | Loss: 0.580 | Acc: 83.668,99.225,99.974,% | Adaptive Acc: 92.405% | clf_exit: 0.825 0.169 0.006
Batch: 260 | Loss: 0.580 | Acc: 83.681,99.237,99.973,% | Adaptive Acc: 92.397% | clf_exit: 0.826 0.169 0.006
Batch: 280 | Loss: 0.581 | Acc: 83.613,99.227,99.975,% | Adaptive Acc: 92.374% | clf_exit: 0.825 0.169 0.006
Batch: 300 | Loss: 0.582 | Acc: 83.586,99.227,99.974,% | Adaptive Acc: 92.354% | clf_exit: 0.825 0.170 0.006
Batch: 320 | Loss: 0.580 | Acc: 83.652,99.231,99.976,% | Adaptive Acc: 92.368% | clf_exit: 0.825 0.169 0.006
Batch: 340 | Loss: 0.579 | Acc: 83.695,99.221,99.977,% | Adaptive Acc: 92.380% | clf_exit: 0.826 0.168 0.006
Batch: 360 | Loss: 0.580 | Acc: 83.706,99.208,99.974,% | Adaptive Acc: 92.378% | clf_exit: 0.826 0.168 0.006
Batch: 380 | Loss: 0.580 | Acc: 83.686,99.200,99.971,% | Adaptive Acc: 92.345% | clf_exit: 0.826 0.168 0.006
Batch: 0 | Loss: 0.909 | Acc: 82.812,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.875 0.109 0.016
Batch: 20 | Loss: 1.121 | Acc: 80.506,90.997,93.452,% | Adaptive Acc: 85.491% | clf_exit: 0.849 0.134 0.016
Batch: 40 | Loss: 1.125 | Acc: 80.888,90.701,93.540,% | Adaptive Acc: 85.785% | clf_exit: 0.855 0.126 0.019
Batch: 60 | Loss: 1.103 | Acc: 80.866,90.856,93.622,% | Adaptive Acc: 86.014% | clf_exit: 0.855 0.126 0.020
Train all parameters

Epoch: 261
Batch: 0 | Loss: 0.606 | Acc: 78.125,99.219,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.601 | Acc: 82.738,99.144,100.000,% | Adaptive Acc: 91.667% | clf_exit: 0.826 0.167 0.007
Batch: 40 | Loss: 0.580 | Acc: 83.594,99.295,100.000,% | Adaptive Acc: 92.359% | clf_exit: 0.824 0.171 0.006
Batch: 60 | Loss: 0.586 | Acc: 83.312,99.219,99.974,% | Adaptive Acc: 92.034% | clf_exit: 0.824 0.171 0.005
Batch: 80 | Loss: 0.583 | Acc: 83.401,99.248,99.981,% | Adaptive Acc: 92.072% | clf_exit: 0.825 0.170 0.006
Batch: 100 | Loss: 0.585 | Acc: 83.447,99.265,99.985,% | Adaptive Acc: 92.188% | clf_exit: 0.823 0.171 0.006
Batch: 120 | Loss: 0.582 | Acc: 83.600,99.264,99.987,% | Adaptive Acc: 92.104% | clf_exit: 0.827 0.168 0.006
Batch: 140 | Loss: 0.579 | Acc: 83.666,99.263,99.989,% | Adaptive Acc: 92.237% | clf_exit: 0.826 0.168 0.006
Batch: 160 | Loss: 0.582 | Acc: 83.657,99.262,99.985,% | Adaptive Acc: 92.333% | clf_exit: 0.824 0.170 0.006
Batch: 180 | Loss: 0.581 | Acc: 83.646,99.283,99.983,% | Adaptive Acc: 92.287% | clf_exit: 0.825 0.169 0.006
Batch: 200 | Loss: 0.580 | Acc: 83.691,99.296,99.977,% | Adaptive Acc: 92.370% | clf_exit: 0.825 0.169 0.005
Batch: 220 | Loss: 0.580 | Acc: 83.725,99.272,99.979,% | Adaptive Acc: 92.446% | clf_exit: 0.825 0.169 0.006
Batch: 240 | Loss: 0.576 | Acc: 83.860,99.261,99.977,% | Adaptive Acc: 92.489% | clf_exit: 0.827 0.168 0.005
Batch: 260 | Loss: 0.576 | Acc: 83.869,99.285,99.979,% | Adaptive Acc: 92.523% | clf_exit: 0.827 0.168 0.005
Batch: 280 | Loss: 0.575 | Acc: 83.880,99.269,99.975,% | Adaptive Acc: 92.510% | clf_exit: 0.827 0.167 0.005
Batch: 300 | Loss: 0.576 | Acc: 83.827,99.281,99.977,% | Adaptive Acc: 92.517% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.577 | Acc: 83.747,99.255,99.976,% | Adaptive Acc: 92.472% | clf_exit: 0.826 0.168 0.005
Batch: 340 | Loss: 0.576 | Acc: 83.766,99.253,99.975,% | Adaptive Acc: 92.488% | clf_exit: 0.826 0.168 0.005
Batch: 360 | Loss: 0.578 | Acc: 83.685,99.243,99.976,% | Adaptive Acc: 92.439% | clf_exit: 0.826 0.168 0.006
Batch: 380 | Loss: 0.581 | Acc: 83.577,99.223,99.973,% | Adaptive Acc: 92.374% | clf_exit: 0.826 0.168 0.006
Batch: 0 | Loss: 0.924 | Acc: 81.250,92.969,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.898 0.094 0.008
Batch: 20 | Loss: 1.125 | Acc: 79.911,90.997,93.378,% | Adaptive Acc: 86.049% | clf_exit: 0.839 0.146 0.015
Batch: 40 | Loss: 1.129 | Acc: 80.393,90.739,93.407,% | Adaptive Acc: 85.880% | clf_exit: 0.850 0.132 0.018
Batch: 60 | Loss: 1.108 | Acc: 80.597,90.843,93.635,% | Adaptive Acc: 86.027% | clf_exit: 0.852 0.129 0.018
Train all parameters

Epoch: 262
Batch: 0 | Loss: 0.618 | Acc: 82.031,100.000,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.586 | Acc: 83.408,99.182,99.963,% | Adaptive Acc: 92.708% | clf_exit: 0.831 0.164 0.004
Batch: 40 | Loss: 0.580 | Acc: 83.651,99.238,99.962,% | Adaptive Acc: 92.492% | clf_exit: 0.830 0.166 0.004
Batch: 60 | Loss: 0.570 | Acc: 83.824,99.244,99.962,% | Adaptive Acc: 92.585% | clf_exit: 0.829 0.167 0.004
Batch: 80 | Loss: 0.567 | Acc: 83.796,99.296,99.971,% | Adaptive Acc: 92.544% | clf_exit: 0.833 0.163 0.004
Batch: 100 | Loss: 0.569 | Acc: 83.810,99.250,99.977,% | Adaptive Acc: 92.528% | clf_exit: 0.833 0.163 0.004
Batch: 120 | Loss: 0.573 | Acc: 83.775,99.199,99.981,% | Adaptive Acc: 92.517% | clf_exit: 0.831 0.165 0.004
Batch: 140 | Loss: 0.572 | Acc: 83.832,99.258,99.978,% | Adaptive Acc: 92.592% | clf_exit: 0.830 0.166 0.004
Batch: 160 | Loss: 0.571 | Acc: 83.730,99.282,99.981,% | Adaptive Acc: 92.615% | clf_exit: 0.828 0.167 0.005
Batch: 180 | Loss: 0.571 | Acc: 83.697,99.309,99.983,% | Adaptive Acc: 92.632% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.570 | Acc: 83.792,99.324,99.984,% | Adaptive Acc: 92.689% | clf_exit: 0.828 0.168 0.005
Batch: 220 | Loss: 0.571 | Acc: 83.710,99.339,99.986,% | Adaptive Acc: 92.661% | clf_exit: 0.827 0.168 0.005
Batch: 240 | Loss: 0.571 | Acc: 83.730,99.322,99.984,% | Adaptive Acc: 92.602% | clf_exit: 0.828 0.167 0.005
Batch: 260 | Loss: 0.570 | Acc: 83.785,99.338,99.985,% | Adaptive Acc: 92.604% | clf_exit: 0.828 0.167 0.004
Batch: 280 | Loss: 0.569 | Acc: 83.811,99.338,99.986,% | Adaptive Acc: 92.613% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.572 | Acc: 83.778,99.341,99.984,% | Adaptive Acc: 92.605% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.573 | Acc: 83.742,99.336,99.985,% | Adaptive Acc: 92.582% | clf_exit: 0.826 0.169 0.005
Batch: 340 | Loss: 0.572 | Acc: 83.830,99.331,99.982,% | Adaptive Acc: 92.566% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.572 | Acc: 83.823,99.314,99.981,% | Adaptive Acc: 92.558% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.573 | Acc: 83.780,99.305,99.979,% | Adaptive Acc: 92.522% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.895 | Acc: 82.031,93.750,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.891 0.094 0.016
Batch: 20 | Loss: 1.106 | Acc: 80.394,91.443,93.601,% | Adaptive Acc: 86.421% | clf_exit: 0.840 0.143 0.016
Batch: 40 | Loss: 1.113 | Acc: 80.602,90.987,93.636,% | Adaptive Acc: 86.109% | clf_exit: 0.851 0.130 0.019
Batch: 60 | Loss: 1.093 | Acc: 80.776,91.048,93.712,% | Adaptive Acc: 86.283% | clf_exit: 0.851 0.129 0.020
Train all parameters

Epoch: 263
Batch: 0 | Loss: 0.566 | Acc: 85.156,99.219,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.575 | Acc: 84.487,99.516,99.963,% | Adaptive Acc: 92.188% | clf_exit: 0.837 0.160 0.003
Batch: 40 | Loss: 0.576 | Acc: 83.822,99.428,99.981,% | Adaptive Acc: 92.054% | clf_exit: 0.833 0.163 0.004
Batch: 60 | Loss: 0.575 | Acc: 83.683,99.308,99.974,% | Adaptive Acc: 92.175% | clf_exit: 0.832 0.163 0.005
Batch: 80 | Loss: 0.578 | Acc: 83.584,99.306,99.981,% | Adaptive Acc: 92.197% | clf_exit: 0.832 0.163 0.005
Batch: 100 | Loss: 0.577 | Acc: 83.741,99.327,99.985,% | Adaptive Acc: 92.218% | clf_exit: 0.832 0.163 0.005
Batch: 120 | Loss: 0.577 | Acc: 83.736,99.296,99.981,% | Adaptive Acc: 92.200% | clf_exit: 0.833 0.162 0.005
Batch: 140 | Loss: 0.576 | Acc: 83.660,99.324,99.983,% | Adaptive Acc: 92.265% | clf_exit: 0.831 0.164 0.005
Batch: 160 | Loss: 0.576 | Acc: 83.618,99.321,99.985,% | Adaptive Acc: 92.236% | clf_exit: 0.831 0.164 0.005
Batch: 180 | Loss: 0.572 | Acc: 83.779,99.331,99.987,% | Adaptive Acc: 92.356% | clf_exit: 0.832 0.164 0.005
Batch: 200 | Loss: 0.572 | Acc: 83.738,99.331,99.988,% | Adaptive Acc: 92.401% | clf_exit: 0.830 0.165 0.005
Batch: 220 | Loss: 0.567 | Acc: 83.923,99.360,99.989,% | Adaptive Acc: 92.481% | clf_exit: 0.831 0.164 0.005
Batch: 240 | Loss: 0.567 | Acc: 83.918,99.381,99.987,% | Adaptive Acc: 92.547% | clf_exit: 0.831 0.164 0.005
Batch: 260 | Loss: 0.569 | Acc: 83.869,99.389,99.982,% | Adaptive Acc: 92.502% | clf_exit: 0.830 0.165 0.005
Batch: 280 | Loss: 0.571 | Acc: 83.858,99.383,99.981,% | Adaptive Acc: 92.499% | clf_exit: 0.830 0.165 0.005
Batch: 300 | Loss: 0.570 | Acc: 83.864,99.385,99.979,% | Adaptive Acc: 92.535% | clf_exit: 0.830 0.166 0.005
Batch: 320 | Loss: 0.570 | Acc: 83.827,99.387,99.981,% | Adaptive Acc: 92.511% | clf_exit: 0.829 0.166 0.005
Batch: 340 | Loss: 0.571 | Acc: 83.816,99.388,99.982,% | Adaptive Acc: 92.492% | clf_exit: 0.829 0.166 0.005
Batch: 360 | Loss: 0.572 | Acc: 83.836,99.388,99.983,% | Adaptive Acc: 92.519% | clf_exit: 0.829 0.166 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.862,99.381,99.984,% | Adaptive Acc: 92.516% | clf_exit: 0.829 0.166 0.005
Batch: 0 | Loss: 0.921 | Acc: 81.250,92.969,93.750,% | Adaptive Acc: 89.062% | clf_exit: 0.859 0.117 0.023
Batch: 20 | Loss: 1.110 | Acc: 80.841,91.332,93.452,% | Adaptive Acc: 86.272% | clf_exit: 0.839 0.141 0.019
Batch: 40 | Loss: 1.117 | Acc: 80.774,91.025,93.540,% | Adaptive Acc: 85.938% | clf_exit: 0.848 0.132 0.020
Batch: 60 | Loss: 1.095 | Acc: 80.802,91.060,93.673,% | Adaptive Acc: 86.168% | clf_exit: 0.850 0.129 0.021
Train all parameters

Epoch: 264
Batch: 0 | Loss: 0.542 | Acc: 82.031,100.000,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.576 | Acc: 83.854,99.479,99.963,% | Adaptive Acc: 93.006% | clf_exit: 0.811 0.186 0.003
Batch: 40 | Loss: 0.572 | Acc: 84.013,99.314,99.981,% | Adaptive Acc: 92.988% | clf_exit: 0.815 0.182 0.003
Batch: 60 | Loss: 0.568 | Acc: 84.209,99.257,99.974,% | Adaptive Acc: 92.930% | clf_exit: 0.819 0.177 0.004
Batch: 80 | Loss: 0.570 | Acc: 84.346,99.248,99.971,% | Adaptive Acc: 92.747% | clf_exit: 0.825 0.171 0.004
Batch: 100 | Loss: 0.567 | Acc: 84.414,99.257,99.977,% | Adaptive Acc: 92.860% | clf_exit: 0.824 0.171 0.005
Batch: 120 | Loss: 0.569 | Acc: 84.336,99.277,99.981,% | Adaptive Acc: 92.769% | clf_exit: 0.825 0.170 0.005
Batch: 140 | Loss: 0.569 | Acc: 84.203,99.291,99.983,% | Adaptive Acc: 92.858% | clf_exit: 0.823 0.173 0.004
Batch: 160 | Loss: 0.568 | Acc: 84.297,99.296,99.981,% | Adaptive Acc: 92.881% | clf_exit: 0.824 0.171 0.005
Batch: 180 | Loss: 0.567 | Acc: 84.228,99.318,99.978,% | Adaptive Acc: 92.770% | clf_exit: 0.826 0.170 0.004
Batch: 200 | Loss: 0.570 | Acc: 84.014,99.339,99.977,% | Adaptive Acc: 92.681% | clf_exit: 0.825 0.171 0.004
Batch: 220 | Loss: 0.569 | Acc: 84.004,99.325,99.979,% | Adaptive Acc: 92.704% | clf_exit: 0.825 0.171 0.004
Batch: 240 | Loss: 0.571 | Acc: 83.983,99.300,99.974,% | Adaptive Acc: 92.713% | clf_exit: 0.824 0.171 0.005
Batch: 260 | Loss: 0.572 | Acc: 83.974,99.303,99.970,% | Adaptive Acc: 92.717% | clf_exit: 0.825 0.171 0.005
Batch: 280 | Loss: 0.571 | Acc: 83.986,99.308,99.972,% | Adaptive Acc: 92.691% | clf_exit: 0.825 0.170 0.005
Batch: 300 | Loss: 0.570 | Acc: 83.944,99.315,99.971,% | Adaptive Acc: 92.675% | clf_exit: 0.825 0.170 0.005
Batch: 320 | Loss: 0.570 | Acc: 83.976,99.316,99.973,% | Adaptive Acc: 92.694% | clf_exit: 0.825 0.170 0.005
Batch: 340 | Loss: 0.570 | Acc: 84.002,99.317,99.970,% | Adaptive Acc: 92.714% | clf_exit: 0.825 0.170 0.005
Batch: 360 | Loss: 0.571 | Acc: 83.910,99.310,99.972,% | Adaptive Acc: 92.677% | clf_exit: 0.825 0.170 0.005
Batch: 380 | Loss: 0.571 | Acc: 83.928,99.295,99.971,% | Adaptive Acc: 92.692% | clf_exit: 0.825 0.170 0.005
Batch: 0 | Loss: 0.895 | Acc: 82.812,92.969,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.875 0.109 0.016
Batch: 20 | Loss: 1.111 | Acc: 80.432,90.923,93.304,% | Adaptive Acc: 85.789% | clf_exit: 0.844 0.141 0.016
Batch: 40 | Loss: 1.118 | Acc: 80.640,90.644,93.464,% | Adaptive Acc: 85.899% | clf_exit: 0.851 0.129 0.019
Batch: 60 | Loss: 1.096 | Acc: 80.776,90.766,93.622,% | Adaptive Acc: 86.066% | clf_exit: 0.853 0.127 0.020
Train all parameters

Epoch: 265
Batch: 0 | Loss: 0.514 | Acc: 87.500,100.000,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.569 | Acc: 84.747,99.256,99.888,% | Adaptive Acc: 92.783% | clf_exit: 0.825 0.173 0.003
Batch: 40 | Loss: 0.569 | Acc: 84.184,99.200,99.924,% | Adaptive Acc: 92.721% | clf_exit: 0.826 0.170 0.004
Batch: 60 | Loss: 0.569 | Acc: 83.876,99.193,99.949,% | Adaptive Acc: 92.405% | clf_exit: 0.831 0.164 0.005
Batch: 80 | Loss: 0.563 | Acc: 84.134,99.257,99.961,% | Adaptive Acc: 92.515% | clf_exit: 0.831 0.165 0.005
Batch: 100 | Loss: 0.564 | Acc: 84.097,99.296,99.969,% | Adaptive Acc: 92.443% | clf_exit: 0.831 0.165 0.005
Batch: 120 | Loss: 0.563 | Acc: 84.123,99.270,99.961,% | Adaptive Acc: 92.517% | clf_exit: 0.830 0.166 0.005
Batch: 140 | Loss: 0.565 | Acc: 84.020,99.274,99.967,% | Adaptive Acc: 92.487% | clf_exit: 0.830 0.166 0.005
Batch: 160 | Loss: 0.565 | Acc: 83.997,99.296,99.971,% | Adaptive Acc: 92.571% | clf_exit: 0.829 0.166 0.005
Batch: 180 | Loss: 0.564 | Acc: 84.021,99.283,99.974,% | Adaptive Acc: 92.550% | clf_exit: 0.830 0.164 0.005
Batch: 200 | Loss: 0.566 | Acc: 83.971,99.277,99.977,% | Adaptive Acc: 92.603% | clf_exit: 0.830 0.165 0.005
Batch: 220 | Loss: 0.567 | Acc: 83.990,99.286,99.972,% | Adaptive Acc: 92.580% | clf_exit: 0.831 0.165 0.005
Batch: 240 | Loss: 0.567 | Acc: 84.002,99.303,99.974,% | Adaptive Acc: 92.586% | clf_exit: 0.830 0.165 0.005
Batch: 260 | Loss: 0.567 | Acc: 83.968,99.318,99.976,% | Adaptive Acc: 92.538% | clf_exit: 0.830 0.165 0.005
Batch: 280 | Loss: 0.569 | Acc: 83.939,99.302,99.978,% | Adaptive Acc: 92.518% | clf_exit: 0.830 0.165 0.005
Batch: 300 | Loss: 0.568 | Acc: 83.975,99.294,99.979,% | Adaptive Acc: 92.572% | clf_exit: 0.829 0.166 0.005
Batch: 320 | Loss: 0.568 | Acc: 83.971,99.297,99.981,% | Adaptive Acc: 92.548% | clf_exit: 0.830 0.165 0.005
Batch: 340 | Loss: 0.570 | Acc: 83.905,99.308,99.982,% | Adaptive Acc: 92.554% | clf_exit: 0.829 0.166 0.005
Batch: 360 | Loss: 0.570 | Acc: 83.888,99.318,99.983,% | Adaptive Acc: 92.568% | clf_exit: 0.829 0.166 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.805,99.307,99.982,% | Adaptive Acc: 92.526% | clf_exit: 0.828 0.166 0.005
Batch: 0 | Loss: 0.894 | Acc: 82.812,92.188,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.859 0.117 0.023
Batch: 20 | Loss: 1.102 | Acc: 80.580,91.257,93.527,% | Adaptive Acc: 86.235% | clf_exit: 0.840 0.143 0.017
Batch: 40 | Loss: 1.111 | Acc: 80.640,91.082,93.750,% | Adaptive Acc: 86.223% | clf_exit: 0.849 0.130 0.020
Batch: 60 | Loss: 1.092 | Acc: 80.776,91.060,93.763,% | Adaptive Acc: 86.322% | clf_exit: 0.850 0.129 0.020
Train all parameters

Epoch: 266
Batch: 0 | Loss: 0.504 | Acc: 85.938,100.000,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.569 | Acc: 84.226,99.405,99.963,% | Adaptive Acc: 92.336% | clf_exit: 0.827 0.172 0.001
Batch: 40 | Loss: 0.555 | Acc: 84.947,99.486,99.981,% | Adaptive Acc: 92.683% | clf_exit: 0.829 0.170 0.002
Batch: 60 | Loss: 0.563 | Acc: 84.503,99.360,99.987,% | Adaptive Acc: 92.764% | clf_exit: 0.829 0.168 0.003
Batch: 80 | Loss: 0.569 | Acc: 84.327,99.392,99.990,% | Adaptive Acc: 92.824% | clf_exit: 0.827 0.170 0.004
Batch: 100 | Loss: 0.567 | Acc: 84.352,99.443,99.992,% | Adaptive Acc: 92.907% | clf_exit: 0.826 0.169 0.004
Batch: 120 | Loss: 0.569 | Acc: 84.285,99.432,99.987,% | Adaptive Acc: 92.859% | clf_exit: 0.825 0.170 0.004
Batch: 140 | Loss: 0.567 | Acc: 84.309,99.424,99.989,% | Adaptive Acc: 92.852% | clf_exit: 0.826 0.170 0.004
Batch: 160 | Loss: 0.568 | Acc: 84.322,99.393,99.981,% | Adaptive Acc: 92.823% | clf_exit: 0.827 0.169 0.004
Batch: 180 | Loss: 0.565 | Acc: 84.362,99.426,99.983,% | Adaptive Acc: 92.805% | clf_exit: 0.828 0.168 0.004
Batch: 200 | Loss: 0.567 | Acc: 84.223,99.405,99.984,% | Adaptive Acc: 92.786% | clf_exit: 0.826 0.169 0.004
Batch: 220 | Loss: 0.570 | Acc: 84.181,99.399,99.986,% | Adaptive Acc: 92.750% | clf_exit: 0.826 0.169 0.004
Batch: 240 | Loss: 0.571 | Acc: 84.061,99.371,99.987,% | Adaptive Acc: 92.719% | clf_exit: 0.826 0.170 0.004
Batch: 260 | Loss: 0.570 | Acc: 84.115,99.353,99.985,% | Adaptive Acc: 92.759% | clf_exit: 0.826 0.170 0.004
Batch: 280 | Loss: 0.571 | Acc: 84.061,99.341,99.986,% | Adaptive Acc: 92.732% | clf_exit: 0.826 0.170 0.004
Batch: 300 | Loss: 0.572 | Acc: 84.019,99.325,99.979,% | Adaptive Acc: 92.701% | clf_exit: 0.825 0.170 0.004
Batch: 320 | Loss: 0.571 | Acc: 84.034,99.336,99.978,% | Adaptive Acc: 92.703% | clf_exit: 0.826 0.169 0.004
Batch: 340 | Loss: 0.571 | Acc: 83.997,99.338,99.977,% | Adaptive Acc: 92.694% | clf_exit: 0.826 0.169 0.005
Batch: 360 | Loss: 0.571 | Acc: 84.016,99.323,99.974,% | Adaptive Acc: 92.718% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.571 | Acc: 83.969,99.315,99.973,% | Adaptive Acc: 92.690% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.911 | Acc: 82.812,92.969,92.969,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.114 | Acc: 80.618,91.146,93.378,% | Adaptive Acc: 86.012% | clf_exit: 0.846 0.138 0.015
Batch: 40 | Loss: 1.121 | Acc: 80.678,90.816,93.483,% | Adaptive Acc: 85.976% | clf_exit: 0.855 0.127 0.019
Batch: 60 | Loss: 1.099 | Acc: 80.751,90.881,93.660,% | Adaptive Acc: 86.053% | clf_exit: 0.855 0.127 0.019
Train all parameters

Epoch: 267
Batch: 0 | Loss: 0.780 | Acc: 77.344,98.438,98.438,% | Adaptive Acc: 89.062% | clf_exit: 0.758 0.227 0.016
Batch: 20 | Loss: 0.578 | Acc: 83.482,99.256,99.926,% | Adaptive Acc: 92.225% | clf_exit: 0.832 0.162 0.006
Batch: 40 | Loss: 0.571 | Acc: 83.880,99.314,99.943,% | Adaptive Acc: 92.092% | clf_exit: 0.831 0.164 0.005
Batch: 60 | Loss: 0.569 | Acc: 84.209,99.270,99.962,% | Adaptive Acc: 92.328% | clf_exit: 0.833 0.163 0.005
Batch: 80 | Loss: 0.571 | Acc: 84.018,99.257,99.952,% | Adaptive Acc: 92.564% | clf_exit: 0.830 0.165 0.005
Batch: 100 | Loss: 0.571 | Acc: 83.965,99.226,99.961,% | Adaptive Acc: 92.582% | clf_exit: 0.829 0.166 0.005
Batch: 120 | Loss: 0.569 | Acc: 84.072,99.257,99.968,% | Adaptive Acc: 92.568% | clf_exit: 0.831 0.164 0.005
Batch: 140 | Loss: 0.573 | Acc: 83.965,99.258,99.967,% | Adaptive Acc: 92.614% | clf_exit: 0.829 0.166 0.005
Batch: 160 | Loss: 0.571 | Acc: 84.035,99.238,99.966,% | Adaptive Acc: 92.605% | clf_exit: 0.830 0.165 0.005
Batch: 180 | Loss: 0.571 | Acc: 84.038,99.245,99.961,% | Adaptive Acc: 92.628% | clf_exit: 0.829 0.166 0.005
Batch: 200 | Loss: 0.571 | Acc: 84.041,99.246,99.965,% | Adaptive Acc: 92.677% | clf_exit: 0.829 0.166 0.005
Batch: 220 | Loss: 0.570 | Acc: 84.032,99.222,99.968,% | Adaptive Acc: 92.732% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.571 | Acc: 83.999,99.245,99.971,% | Adaptive Acc: 92.683% | clf_exit: 0.828 0.167 0.005
Batch: 260 | Loss: 0.569 | Acc: 84.001,99.255,99.967,% | Adaptive Acc: 92.660% | clf_exit: 0.829 0.167 0.005
Batch: 280 | Loss: 0.570 | Acc: 83.888,99.266,99.969,% | Adaptive Acc: 92.591% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.570 | Acc: 83.952,99.268,99.966,% | Adaptive Acc: 92.631% | clf_exit: 0.828 0.168 0.005
Batch: 320 | Loss: 0.571 | Acc: 83.891,99.275,99.966,% | Adaptive Acc: 92.630% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.572 | Acc: 83.926,99.276,99.968,% | Adaptive Acc: 92.648% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.571 | Acc: 83.972,99.273,99.970,% | Adaptive Acc: 92.666% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.573 | Acc: 83.928,99.268,99.971,% | Adaptive Acc: 92.657% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.924 | Acc: 82.031,92.188,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.891 0.094 0.016
Batch: 20 | Loss: 1.109 | Acc: 80.766,91.369,93.266,% | Adaptive Acc: 86.347% | clf_exit: 0.839 0.143 0.018
Batch: 40 | Loss: 1.115 | Acc: 80.774,91.101,93.293,% | Adaptive Acc: 86.071% | clf_exit: 0.849 0.130 0.021
Batch: 60 | Loss: 1.092 | Acc: 80.827,91.137,93.468,% | Adaptive Acc: 86.309% | clf_exit: 0.850 0.129 0.021
Train all parameters

Epoch: 268
Batch: 0 | Loss: 0.665 | Acc: 81.250,96.875,98.438,% | Adaptive Acc: 87.500% | clf_exit: 0.812 0.188 0.000
Batch: 20 | Loss: 0.571 | Acc: 83.557,98.958,99.926,% | Adaptive Acc: 92.448% | clf_exit: 0.826 0.169 0.005
Batch: 40 | Loss: 0.581 | Acc: 83.270,99.162,99.943,% | Adaptive Acc: 92.035% | clf_exit: 0.830 0.164 0.006
Batch: 60 | Loss: 0.573 | Acc: 83.722,99.244,99.962,% | Adaptive Acc: 92.392% | clf_exit: 0.831 0.163 0.006
Batch: 80 | Loss: 0.570 | Acc: 83.719,99.267,99.961,% | Adaptive Acc: 92.602% | clf_exit: 0.827 0.168 0.005
Batch: 100 | Loss: 0.570 | Acc: 83.725,99.288,99.969,% | Adaptive Acc: 92.574% | clf_exit: 0.826 0.169 0.005
Batch: 120 | Loss: 0.569 | Acc: 83.878,99.335,99.974,% | Adaptive Acc: 92.646% | clf_exit: 0.826 0.169 0.005
Batch: 140 | Loss: 0.571 | Acc: 83.865,99.357,99.978,% | Adaptive Acc: 92.708% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.572 | Acc: 83.841,99.350,99.976,% | Adaptive Acc: 92.648% | clf_exit: 0.825 0.170 0.005
Batch: 180 | Loss: 0.571 | Acc: 83.896,99.340,99.970,% | Adaptive Acc: 92.697% | clf_exit: 0.825 0.169 0.005
Batch: 200 | Loss: 0.571 | Acc: 83.889,99.343,99.969,% | Adaptive Acc: 92.708% | clf_exit: 0.825 0.169 0.005
Batch: 220 | Loss: 0.569 | Acc: 83.961,99.360,99.972,% | Adaptive Acc: 92.753% | clf_exit: 0.825 0.169 0.005
Batch: 240 | Loss: 0.571 | Acc: 83.895,99.352,99.971,% | Adaptive Acc: 92.719% | clf_exit: 0.824 0.170 0.005
Batch: 260 | Loss: 0.571 | Acc: 83.875,99.350,99.970,% | Adaptive Acc: 92.732% | clf_exit: 0.824 0.171 0.005
Batch: 280 | Loss: 0.572 | Acc: 83.833,99.330,99.969,% | Adaptive Acc: 92.696% | clf_exit: 0.825 0.170 0.005
Batch: 300 | Loss: 0.572 | Acc: 83.840,99.341,99.971,% | Adaptive Acc: 92.727% | clf_exit: 0.825 0.170 0.005
Batch: 320 | Loss: 0.570 | Acc: 83.891,99.343,99.973,% | Adaptive Acc: 92.689% | clf_exit: 0.826 0.169 0.005
Batch: 340 | Loss: 0.571 | Acc: 83.882,99.336,99.975,% | Adaptive Acc: 92.630% | clf_exit: 0.827 0.169 0.005
Batch: 360 | Loss: 0.570 | Acc: 83.957,99.338,99.976,% | Adaptive Acc: 92.614% | clf_exit: 0.827 0.169 0.005
Batch: 380 | Loss: 0.570 | Acc: 83.907,99.340,99.977,% | Adaptive Acc: 92.587% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.916 | Acc: 82.031,92.188,93.750,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.102 0.016
Batch: 20 | Loss: 1.105 | Acc: 80.692,91.481,93.527,% | Adaptive Acc: 86.421% | clf_exit: 0.841 0.142 0.017
Batch: 40 | Loss: 1.112 | Acc: 80.678,91.063,93.502,% | Adaptive Acc: 86.242% | clf_exit: 0.852 0.128 0.020
Batch: 60 | Loss: 1.090 | Acc: 80.725,91.137,93.635,% | Adaptive Acc: 86.373% | clf_exit: 0.852 0.127 0.020
Train all parameters

Epoch: 269
Batch: 0 | Loss: 0.496 | Acc: 84.375,100.000,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.563 | Acc: 84.189,99.070,99.963,% | Adaptive Acc: 93.006% | clf_exit: 0.832 0.164 0.004
Batch: 40 | Loss: 0.566 | Acc: 84.070,99.314,99.981,% | Adaptive Acc: 93.102% | clf_exit: 0.826 0.169 0.006
Batch: 60 | Loss: 0.569 | Acc: 83.837,99.321,99.987,% | Adaptive Acc: 92.956% | clf_exit: 0.825 0.170 0.006
Batch: 80 | Loss: 0.570 | Acc: 83.999,99.334,99.990,% | Adaptive Acc: 92.814% | clf_exit: 0.827 0.168 0.005
Batch: 100 | Loss: 0.576 | Acc: 83.818,99.327,99.992,% | Adaptive Acc: 92.783% | clf_exit: 0.824 0.171 0.005
Batch: 120 | Loss: 0.574 | Acc: 83.852,99.335,99.994,% | Adaptive Acc: 92.814% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.578 | Acc: 83.655,99.318,99.989,% | Adaptive Acc: 92.664% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.576 | Acc: 83.691,99.345,99.985,% | Adaptive Acc: 92.687% | clf_exit: 0.824 0.171 0.005
Batch: 180 | Loss: 0.574 | Acc: 83.766,99.331,99.987,% | Adaptive Acc: 92.684% | clf_exit: 0.824 0.170 0.005
Batch: 200 | Loss: 0.574 | Acc: 83.683,99.328,99.981,% | Adaptive Acc: 92.596% | clf_exit: 0.825 0.170 0.005
Batch: 220 | Loss: 0.577 | Acc: 83.576,99.307,99.982,% | Adaptive Acc: 92.506% | clf_exit: 0.824 0.171 0.005
Batch: 240 | Loss: 0.579 | Acc: 83.639,99.303,99.981,% | Adaptive Acc: 92.528% | clf_exit: 0.823 0.171 0.005
Batch: 260 | Loss: 0.578 | Acc: 83.627,99.306,99.982,% | Adaptive Acc: 92.493% | clf_exit: 0.824 0.170 0.005
Batch: 280 | Loss: 0.578 | Acc: 83.663,99.305,99.981,% | Adaptive Acc: 92.479% | clf_exit: 0.825 0.170 0.005
Batch: 300 | Loss: 0.578 | Acc: 83.669,99.297,99.982,% | Adaptive Acc: 92.460% | clf_exit: 0.825 0.170 0.005
Batch: 320 | Loss: 0.576 | Acc: 83.788,99.287,99.981,% | Adaptive Acc: 92.489% | clf_exit: 0.826 0.169 0.005
Batch: 340 | Loss: 0.574 | Acc: 83.889,99.292,99.979,% | Adaptive Acc: 92.549% | clf_exit: 0.826 0.169 0.005
Batch: 360 | Loss: 0.576 | Acc: 83.840,99.297,99.978,% | Adaptive Acc: 92.516% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.574 | Acc: 83.926,99.297,99.975,% | Adaptive Acc: 92.540% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.915 | Acc: 82.812,93.750,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.891 0.094 0.016
Batch: 20 | Loss: 1.117 | Acc: 80.655,91.071,93.266,% | Adaptive Acc: 85.640% | clf_exit: 0.847 0.137 0.016
Batch: 40 | Loss: 1.123 | Acc: 80.774,90.777,93.388,% | Adaptive Acc: 85.918% | clf_exit: 0.853 0.128 0.019
Batch: 60 | Loss: 1.102 | Acc: 80.853,90.868,93.532,% | Adaptive Acc: 86.053% | clf_exit: 0.853 0.128 0.019
Train all parameters

Epoch: 270
Batch: 0 | Loss: 0.721 | Acc: 79.688,100.000,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.789 0.211 0.000
Batch: 20 | Loss: 0.553 | Acc: 84.673,99.330,99.963,% | Adaptive Acc: 93.638% | clf_exit: 0.818 0.176 0.006
Batch: 40 | Loss: 0.573 | Acc: 84.146,99.371,99.962,% | Adaptive Acc: 93.045% | clf_exit: 0.822 0.173 0.005
Batch: 60 | Loss: 0.568 | Acc: 84.183,99.347,99.962,% | Adaptive Acc: 93.020% | clf_exit: 0.827 0.168 0.005
Batch: 80 | Loss: 0.568 | Acc: 84.153,99.363,99.971,% | Adaptive Acc: 92.892% | clf_exit: 0.826 0.168 0.005
Batch: 100 | Loss: 0.570 | Acc: 83.895,99.389,99.969,% | Adaptive Acc: 92.806% | clf_exit: 0.826 0.168 0.006
Batch: 120 | Loss: 0.573 | Acc: 83.878,99.354,99.968,% | Adaptive Acc: 92.723% | clf_exit: 0.828 0.167 0.006
Batch: 140 | Loss: 0.575 | Acc: 83.771,99.357,99.967,% | Adaptive Acc: 92.653% | clf_exit: 0.826 0.168 0.006
Batch: 160 | Loss: 0.574 | Acc: 83.715,99.335,99.971,% | Adaptive Acc: 92.712% | clf_exit: 0.826 0.169 0.005
Batch: 180 | Loss: 0.573 | Acc: 83.866,99.335,99.974,% | Adaptive Acc: 92.727% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.572 | Acc: 83.916,99.351,99.973,% | Adaptive Acc: 92.732% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.572 | Acc: 83.873,99.364,99.975,% | Adaptive Acc: 92.746% | clf_exit: 0.827 0.168 0.005
Batch: 240 | Loss: 0.571 | Acc: 83.934,99.345,99.977,% | Adaptive Acc: 92.777% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.571 | Acc: 83.959,99.347,99.976,% | Adaptive Acc: 92.792% | clf_exit: 0.826 0.169 0.005
Batch: 280 | Loss: 0.570 | Acc: 84.044,99.341,99.978,% | Adaptive Acc: 92.816% | clf_exit: 0.827 0.168 0.005
Batch: 300 | Loss: 0.570 | Acc: 84.030,99.343,99.979,% | Adaptive Acc: 92.753% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.570 | Acc: 84.032,99.343,99.976,% | Adaptive Acc: 92.701% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.570 | Acc: 83.997,99.333,99.970,% | Adaptive Acc: 92.689% | clf_exit: 0.827 0.169 0.005
Batch: 360 | Loss: 0.571 | Acc: 83.972,99.331,99.972,% | Adaptive Acc: 92.655% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.922,99.313,99.973,% | Adaptive Acc: 92.610% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.906 | Acc: 81.250,92.969,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.114 | Acc: 80.804,91.220,93.452,% | Adaptive Acc: 86.235% | clf_exit: 0.841 0.144 0.015
Batch: 40 | Loss: 1.119 | Acc: 80.774,90.892,93.540,% | Adaptive Acc: 86.052% | clf_exit: 0.853 0.129 0.018
Batch: 60 | Loss: 1.096 | Acc: 80.789,90.971,93.635,% | Adaptive Acc: 86.258% | clf_exit: 0.853 0.129 0.018
Train all parameters

Epoch: 271
Batch: 0 | Loss: 0.488 | Acc: 84.375,98.438,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.859 0.141 0.000
Batch: 20 | Loss: 0.602 | Acc: 82.552,99.070,99.926,% | Adaptive Acc: 91.481% | clf_exit: 0.818 0.178 0.004
Batch: 40 | Loss: 0.590 | Acc: 83.003,99.257,99.962,% | Adaptive Acc: 91.864% | clf_exit: 0.819 0.177 0.004
Batch: 60 | Loss: 0.579 | Acc: 83.632,99.308,99.962,% | Adaptive Acc: 92.226% | clf_exit: 0.825 0.170 0.005
Batch: 80 | Loss: 0.578 | Acc: 83.488,99.296,99.952,% | Adaptive Acc: 92.284% | clf_exit: 0.825 0.171 0.005
Batch: 100 | Loss: 0.578 | Acc: 83.733,99.234,99.961,% | Adaptive Acc: 92.497% | clf_exit: 0.822 0.173 0.005
Batch: 120 | Loss: 0.579 | Acc: 83.716,99.238,99.968,% | Adaptive Acc: 92.420% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.580 | Acc: 83.738,99.252,99.972,% | Adaptive Acc: 92.509% | clf_exit: 0.824 0.171 0.006
Batch: 160 | Loss: 0.581 | Acc: 83.696,99.228,99.956,% | Adaptive Acc: 92.430% | clf_exit: 0.825 0.170 0.006
Batch: 180 | Loss: 0.577 | Acc: 83.857,99.258,99.961,% | Adaptive Acc: 92.546% | clf_exit: 0.826 0.168 0.006
Batch: 200 | Loss: 0.579 | Acc: 83.788,99.262,99.965,% | Adaptive Acc: 92.537% | clf_exit: 0.825 0.169 0.006
Batch: 220 | Loss: 0.578 | Acc: 83.781,99.282,99.965,% | Adaptive Acc: 92.548% | clf_exit: 0.825 0.169 0.006
Batch: 240 | Loss: 0.577 | Acc: 83.795,99.277,99.968,% | Adaptive Acc: 92.557% | clf_exit: 0.826 0.168 0.006
Batch: 260 | Loss: 0.576 | Acc: 83.821,99.273,99.970,% | Adaptive Acc: 92.568% | clf_exit: 0.826 0.168 0.006
Batch: 280 | Loss: 0.576 | Acc: 83.869,99.288,99.972,% | Adaptive Acc: 92.599% | clf_exit: 0.826 0.168 0.006
Batch: 300 | Loss: 0.573 | Acc: 83.936,99.304,99.971,% | Adaptive Acc: 92.608% | clf_exit: 0.827 0.168 0.006
Batch: 320 | Loss: 0.572 | Acc: 83.983,99.311,99.973,% | Adaptive Acc: 92.635% | clf_exit: 0.827 0.167 0.006
Batch: 340 | Loss: 0.572 | Acc: 83.953,99.322,99.973,% | Adaptive Acc: 92.602% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.572 | Acc: 83.936,99.327,99.974,% | Adaptive Acc: 92.594% | clf_exit: 0.827 0.167 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.895,99.329,99.975,% | Adaptive Acc: 92.565% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.913 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.117 | Acc: 80.543,91.257,93.415,% | Adaptive Acc: 86.272% | clf_exit: 0.842 0.142 0.017
Batch: 40 | Loss: 1.122 | Acc: 80.736,90.816,93.540,% | Adaptive Acc: 86.109% | clf_exit: 0.851 0.130 0.019
Batch: 60 | Loss: 1.098 | Acc: 80.789,90.881,93.686,% | Adaptive Acc: 86.335% | clf_exit: 0.851 0.129 0.020
Train all parameters

Epoch: 272
Batch: 0 | Loss: 0.502 | Acc: 87.500,99.219,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.828 0.164 0.008
Batch: 20 | Loss: 0.554 | Acc: 84.784,99.442,100.000,% | Adaptive Acc: 92.448% | clf_exit: 0.842 0.156 0.002
Batch: 40 | Loss: 0.573 | Acc: 83.841,99.257,99.981,% | Adaptive Acc: 92.473% | clf_exit: 0.829 0.167 0.004
Batch: 60 | Loss: 0.572 | Acc: 83.927,99.244,99.974,% | Adaptive Acc: 92.482% | clf_exit: 0.829 0.167 0.004
Batch: 80 | Loss: 0.567 | Acc: 83.999,99.267,99.961,% | Adaptive Acc: 92.477% | clf_exit: 0.830 0.166 0.004
Batch: 100 | Loss: 0.566 | Acc: 84.166,99.242,99.961,% | Adaptive Acc: 92.567% | clf_exit: 0.829 0.167 0.004
Batch: 120 | Loss: 0.567 | Acc: 84.149,99.251,99.955,% | Adaptive Acc: 92.568% | clf_exit: 0.828 0.167 0.004
Batch: 140 | Loss: 0.567 | Acc: 83.987,99.302,99.961,% | Adaptive Acc: 92.476% | clf_exit: 0.829 0.166 0.005
Batch: 160 | Loss: 0.568 | Acc: 83.948,99.292,99.966,% | Adaptive Acc: 92.488% | clf_exit: 0.829 0.167 0.005
Batch: 180 | Loss: 0.572 | Acc: 83.805,99.279,99.970,% | Adaptive Acc: 92.377% | clf_exit: 0.829 0.167 0.004
Batch: 200 | Loss: 0.571 | Acc: 83.905,99.296,99.969,% | Adaptive Acc: 92.436% | clf_exit: 0.829 0.167 0.005
Batch: 220 | Loss: 0.569 | Acc: 83.993,99.335,99.968,% | Adaptive Acc: 92.495% | clf_exit: 0.829 0.167 0.004
Batch: 240 | Loss: 0.570 | Acc: 83.934,99.348,99.968,% | Adaptive Acc: 92.515% | clf_exit: 0.827 0.168 0.005
Batch: 260 | Loss: 0.568 | Acc: 84.010,99.353,99.970,% | Adaptive Acc: 92.592% | clf_exit: 0.828 0.168 0.004
Batch: 280 | Loss: 0.570 | Acc: 83.958,99.330,99.967,% | Adaptive Acc: 92.507% | clf_exit: 0.828 0.168 0.005
Batch: 300 | Loss: 0.569 | Acc: 83.996,99.325,99.969,% | Adaptive Acc: 92.585% | clf_exit: 0.827 0.168 0.004
Batch: 320 | Loss: 0.569 | Acc: 83.983,99.328,99.971,% | Adaptive Acc: 92.606% | clf_exit: 0.827 0.169 0.004
Batch: 340 | Loss: 0.569 | Acc: 84.043,99.326,99.973,% | Adaptive Acc: 92.621% | clf_exit: 0.827 0.168 0.004
Batch: 360 | Loss: 0.570 | Acc: 83.970,99.325,99.970,% | Adaptive Acc: 92.610% | clf_exit: 0.827 0.169 0.004
Batch: 380 | Loss: 0.570 | Acc: 83.977,99.340,99.971,% | Adaptive Acc: 92.606% | clf_exit: 0.827 0.169 0.005
Batch: 0 | Loss: 0.900 | Acc: 82.031,92.969,95.312,% | Adaptive Acc: 89.062% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.110 | Acc: 80.618,90.923,93.415,% | Adaptive Acc: 85.826% | clf_exit: 0.846 0.138 0.016
Batch: 40 | Loss: 1.116 | Acc: 80.640,90.663,93.559,% | Adaptive Acc: 85.995% | clf_exit: 0.853 0.127 0.020
Batch: 60 | Loss: 1.095 | Acc: 80.827,90.856,93.660,% | Adaptive Acc: 86.270% | clf_exit: 0.852 0.127 0.021
Train all parameters

Epoch: 273
Batch: 0 | Loss: 0.576 | Acc: 85.156,99.219,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.844 0.148 0.008
Batch: 20 | Loss: 0.596 | Acc: 83.333,99.293,100.000,% | Adaptive Acc: 92.374% | clf_exit: 0.816 0.177 0.007
Batch: 40 | Loss: 0.589 | Acc: 83.365,99.333,99.981,% | Adaptive Acc: 92.283% | clf_exit: 0.828 0.167 0.005
Batch: 60 | Loss: 0.574 | Acc: 83.850,99.360,99.987,% | Adaptive Acc: 92.482% | clf_exit: 0.831 0.164 0.005
Batch: 80 | Loss: 0.572 | Acc: 83.758,99.325,99.981,% | Adaptive Acc: 92.535% | clf_exit: 0.830 0.165 0.005
Batch: 100 | Loss: 0.574 | Acc: 83.764,99.273,99.977,% | Adaptive Acc: 92.590% | clf_exit: 0.827 0.168 0.005
Batch: 120 | Loss: 0.570 | Acc: 84.039,99.322,99.981,% | Adaptive Acc: 92.840% | clf_exit: 0.826 0.168 0.005
Batch: 140 | Loss: 0.574 | Acc: 83.887,99.346,99.978,% | Adaptive Acc: 92.747% | clf_exit: 0.826 0.168 0.005
Batch: 160 | Loss: 0.572 | Acc: 84.011,99.374,99.976,% | Adaptive Acc: 92.736% | clf_exit: 0.828 0.167 0.005
Batch: 180 | Loss: 0.573 | Acc: 83.887,99.396,99.974,% | Adaptive Acc: 92.753% | clf_exit: 0.826 0.169 0.005
Batch: 200 | Loss: 0.572 | Acc: 83.881,99.394,99.977,% | Adaptive Acc: 92.767% | clf_exit: 0.826 0.169 0.005
Batch: 220 | Loss: 0.573 | Acc: 83.880,99.381,99.979,% | Adaptive Acc: 92.764% | clf_exit: 0.826 0.169 0.005
Batch: 240 | Loss: 0.572 | Acc: 83.895,99.381,99.977,% | Adaptive Acc: 92.771% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.571 | Acc: 83.914,99.377,99.979,% | Adaptive Acc: 92.783% | clf_exit: 0.826 0.168 0.005
Batch: 280 | Loss: 0.569 | Acc: 83.966,99.386,99.981,% | Adaptive Acc: 92.869% | clf_exit: 0.826 0.169 0.005
Batch: 300 | Loss: 0.570 | Acc: 83.936,99.367,99.979,% | Adaptive Acc: 92.836% | clf_exit: 0.826 0.169 0.005
Batch: 320 | Loss: 0.569 | Acc: 83.988,99.370,99.978,% | Adaptive Acc: 92.842% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.570 | Acc: 83.972,99.356,99.977,% | Adaptive Acc: 92.774% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.570 | Acc: 83.972,99.355,99.978,% | Adaptive Acc: 92.724% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.570 | Acc: 83.977,99.362,99.979,% | Adaptive Acc: 92.712% | clf_exit: 0.827 0.167 0.005
Batch: 0 | Loss: 0.902 | Acc: 82.031,93.750,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.108 | Acc: 80.692,91.109,93.341,% | Adaptive Acc: 86.496% | clf_exit: 0.839 0.146 0.016
Batch: 40 | Loss: 1.116 | Acc: 80.621,90.701,93.540,% | Adaptive Acc: 86.223% | clf_exit: 0.849 0.132 0.019
Batch: 60 | Loss: 1.094 | Acc: 80.789,90.791,93.648,% | Adaptive Acc: 86.360% | clf_exit: 0.850 0.130 0.020
Train all parameters

Epoch: 274
Batch: 0 | Loss: 0.454 | Acc: 91.406,98.438,100.000,% | Adaptive Acc: 96.875% | clf_exit: 0.828 0.156 0.016
Batch: 20 | Loss: 0.571 | Acc: 84.487,99.107,99.963,% | Adaptive Acc: 92.783% | clf_exit: 0.820 0.171 0.009
Batch: 40 | Loss: 0.564 | Acc: 84.394,99.238,99.962,% | Adaptive Acc: 92.931% | clf_exit: 0.822 0.170 0.008
Batch: 60 | Loss: 0.571 | Acc: 83.978,99.257,99.974,% | Adaptive Acc: 92.828% | clf_exit: 0.820 0.173 0.007
Batch: 80 | Loss: 0.570 | Acc: 84.008,99.277,99.981,% | Adaptive Acc: 93.065% | clf_exit: 0.818 0.175 0.007
Batch: 100 | Loss: 0.569 | Acc: 84.158,99.312,99.985,% | Adaptive Acc: 93.162% | clf_exit: 0.819 0.175 0.007
Batch: 120 | Loss: 0.575 | Acc: 84.001,99.270,99.987,% | Adaptive Acc: 92.969% | clf_exit: 0.820 0.174 0.007
Batch: 140 | Loss: 0.576 | Acc: 83.982,99.269,99.978,% | Adaptive Acc: 92.991% | clf_exit: 0.819 0.174 0.007
Batch: 160 | Loss: 0.579 | Acc: 83.977,99.282,99.981,% | Adaptive Acc: 92.998% | clf_exit: 0.820 0.174 0.007
Batch: 180 | Loss: 0.575 | Acc: 84.047,99.309,99.978,% | Adaptive Acc: 92.977% | clf_exit: 0.823 0.171 0.006
Batch: 200 | Loss: 0.575 | Acc: 83.940,99.312,99.973,% | Adaptive Acc: 92.934% | clf_exit: 0.823 0.171 0.006
Batch: 220 | Loss: 0.575 | Acc: 83.940,99.328,99.975,% | Adaptive Acc: 92.902% | clf_exit: 0.823 0.171 0.006
Batch: 240 | Loss: 0.574 | Acc: 83.950,99.332,99.977,% | Adaptive Acc: 92.833% | clf_exit: 0.824 0.170 0.006
Batch: 260 | Loss: 0.574 | Acc: 83.926,99.330,99.970,% | Adaptive Acc: 92.822% | clf_exit: 0.824 0.170 0.006
Batch: 280 | Loss: 0.575 | Acc: 83.891,99.310,99.972,% | Adaptive Acc: 92.805% | clf_exit: 0.825 0.170 0.006
Batch: 300 | Loss: 0.575 | Acc: 83.853,99.312,99.971,% | Adaptive Acc: 92.712% | clf_exit: 0.825 0.169 0.006
Batch: 320 | Loss: 0.574 | Acc: 83.830,99.321,99.973,% | Adaptive Acc: 92.694% | clf_exit: 0.826 0.169 0.006
Batch: 340 | Loss: 0.574 | Acc: 83.866,99.317,99.973,% | Adaptive Acc: 92.692% | clf_exit: 0.826 0.169 0.005
Batch: 360 | Loss: 0.574 | Acc: 83.851,99.307,99.974,% | Adaptive Acc: 92.720% | clf_exit: 0.825 0.169 0.006
Batch: 380 | Loss: 0.573 | Acc: 83.891,99.321,99.973,% | Adaptive Acc: 92.737% | clf_exit: 0.825 0.169 0.006
Batch: 0 | Loss: 0.906 | Acc: 82.031,92.969,93.750,% | Adaptive Acc: 89.062% | clf_exit: 0.883 0.102 0.016
Batch: 20 | Loss: 1.111 | Acc: 80.766,91.034,93.452,% | Adaptive Acc: 86.347% | clf_exit: 0.841 0.142 0.017
Batch: 40 | Loss: 1.119 | Acc: 80.640,90.816,93.540,% | Adaptive Acc: 86.109% | clf_exit: 0.849 0.131 0.020
Batch: 60 | Loss: 1.096 | Acc: 80.763,90.984,93.686,% | Adaptive Acc: 86.296% | clf_exit: 0.851 0.128 0.021
Train all parameters

Epoch: 275
Batch: 0 | Loss: 0.525 | Acc: 83.594,100.000,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.578 | Acc: 83.854,99.256,100.000,% | Adaptive Acc: 93.006% | clf_exit: 0.817 0.179 0.004
Batch: 40 | Loss: 0.577 | Acc: 83.765,99.295,100.000,% | Adaptive Acc: 92.950% | clf_exit: 0.819 0.176 0.005
Batch: 60 | Loss: 0.570 | Acc: 83.863,99.372,100.000,% | Adaptive Acc: 93.084% | clf_exit: 0.822 0.173 0.005
Batch: 80 | Loss: 0.571 | Acc: 83.767,99.383,100.000,% | Adaptive Acc: 92.911% | clf_exit: 0.824 0.171 0.005
Batch: 100 | Loss: 0.573 | Acc: 83.663,99.327,99.992,% | Adaptive Acc: 92.799% | clf_exit: 0.825 0.170 0.005
Batch: 120 | Loss: 0.573 | Acc: 83.865,99.303,99.994,% | Adaptive Acc: 92.788% | clf_exit: 0.827 0.169 0.004
Batch: 140 | Loss: 0.571 | Acc: 83.932,99.313,99.994,% | Adaptive Acc: 92.780% | clf_exit: 0.828 0.168 0.005
Batch: 160 | Loss: 0.572 | Acc: 83.865,99.311,99.990,% | Adaptive Acc: 92.692% | clf_exit: 0.828 0.168 0.005
Batch: 180 | Loss: 0.569 | Acc: 84.034,99.296,99.987,% | Adaptive Acc: 92.714% | clf_exit: 0.828 0.167 0.005
Batch: 200 | Loss: 0.569 | Acc: 84.060,99.331,99.988,% | Adaptive Acc: 92.771% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.572 | Acc: 84.032,99.335,99.989,% | Adaptive Acc: 92.679% | clf_exit: 0.827 0.168 0.005
Batch: 240 | Loss: 0.574 | Acc: 83.954,99.332,99.987,% | Adaptive Acc: 92.641% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.574 | Acc: 83.974,99.344,99.988,% | Adaptive Acc: 92.607% | clf_exit: 0.827 0.169 0.005
Batch: 280 | Loss: 0.573 | Acc: 83.977,99.347,99.989,% | Adaptive Acc: 92.652% | clf_exit: 0.827 0.169 0.005
Batch: 300 | Loss: 0.571 | Acc: 84.035,99.346,99.990,% | Adaptive Acc: 92.660% | clf_exit: 0.827 0.169 0.004
Batch: 320 | Loss: 0.570 | Acc: 84.049,99.353,99.985,% | Adaptive Acc: 92.679% | clf_exit: 0.827 0.169 0.004
Batch: 340 | Loss: 0.570 | Acc: 84.004,99.342,99.982,% | Adaptive Acc: 92.623% | clf_exit: 0.827 0.169 0.004
Batch: 360 | Loss: 0.570 | Acc: 83.990,99.336,99.983,% | Adaptive Acc: 92.594% | clf_exit: 0.827 0.169 0.005
Batch: 380 | Loss: 0.570 | Acc: 84.016,99.327,99.984,% | Adaptive Acc: 92.587% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.916 | Acc: 81.250,92.969,93.750,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.110 | Acc: 80.655,90.997,93.601,% | Adaptive Acc: 86.161% | clf_exit: 0.843 0.141 0.017
Batch: 40 | Loss: 1.121 | Acc: 80.716,90.701,93.559,% | Adaptive Acc: 85.976% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.101 | Acc: 80.763,90.791,93.635,% | Adaptive Acc: 86.078% | clf_exit: 0.853 0.126 0.021
Train all parameters

Epoch: 276
Batch: 0 | Loss: 0.591 | Acc: 82.812,100.000,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.558 | Acc: 83.780,99.591,100.000,% | Adaptive Acc: 93.601% | clf_exit: 0.810 0.184 0.006
Batch: 40 | Loss: 0.561 | Acc: 83.994,99.371,99.981,% | Adaptive Acc: 93.674% | clf_exit: 0.816 0.178 0.006
Batch: 60 | Loss: 0.564 | Acc: 83.850,99.321,99.987,% | Adaptive Acc: 93.494% | clf_exit: 0.817 0.177 0.006
Batch: 80 | Loss: 0.568 | Acc: 83.873,99.296,99.981,% | Adaptive Acc: 93.268% | clf_exit: 0.820 0.174 0.006
Batch: 100 | Loss: 0.566 | Acc: 84.104,99.319,99.985,% | Adaptive Acc: 93.154% | clf_exit: 0.821 0.173 0.006
Batch: 120 | Loss: 0.561 | Acc: 84.310,99.367,99.987,% | Adaptive Acc: 93.259% | clf_exit: 0.823 0.172 0.005
Batch: 140 | Loss: 0.563 | Acc: 84.142,99.374,99.983,% | Adaptive Acc: 93.052% | clf_exit: 0.824 0.172 0.005
Batch: 160 | Loss: 0.565 | Acc: 83.992,99.350,99.976,% | Adaptive Acc: 92.930% | clf_exit: 0.824 0.171 0.005
Batch: 180 | Loss: 0.566 | Acc: 83.982,99.361,99.978,% | Adaptive Acc: 92.904% | clf_exit: 0.824 0.171 0.005
Batch: 200 | Loss: 0.565 | Acc: 84.010,99.374,99.981,% | Adaptive Acc: 92.903% | clf_exit: 0.826 0.170 0.005
Batch: 220 | Loss: 0.566 | Acc: 84.053,99.381,99.979,% | Adaptive Acc: 92.859% | clf_exit: 0.826 0.169 0.005
Batch: 240 | Loss: 0.566 | Acc: 84.018,99.374,99.977,% | Adaptive Acc: 92.800% | clf_exit: 0.827 0.168 0.005
Batch: 260 | Loss: 0.566 | Acc: 84.025,99.386,99.976,% | Adaptive Acc: 92.759% | clf_exit: 0.827 0.169 0.004
Batch: 280 | Loss: 0.566 | Acc: 84.033,99.388,99.972,% | Adaptive Acc: 92.749% | clf_exit: 0.827 0.168 0.004
Batch: 300 | Loss: 0.566 | Acc: 83.988,99.395,99.974,% | Adaptive Acc: 92.743% | clf_exit: 0.827 0.169 0.004
Batch: 320 | Loss: 0.566 | Acc: 83.990,99.375,99.973,% | Adaptive Acc: 92.711% | clf_exit: 0.828 0.168 0.004
Batch: 340 | Loss: 0.566 | Acc: 84.008,99.354,99.975,% | Adaptive Acc: 92.710% | clf_exit: 0.828 0.168 0.004
Batch: 360 | Loss: 0.568 | Acc: 83.977,99.366,99.976,% | Adaptive Acc: 92.703% | clf_exit: 0.827 0.168 0.004
Batch: 380 | Loss: 0.568 | Acc: 83.996,99.364,99.975,% | Adaptive Acc: 92.694% | clf_exit: 0.828 0.168 0.004
Batch: 0 | Loss: 0.894 | Acc: 82.031,92.969,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.108 | Acc: 80.655,90.997,93.452,% | Adaptive Acc: 85.900% | clf_exit: 0.845 0.137 0.018
Batch: 40 | Loss: 1.116 | Acc: 80.793,90.777,93.579,% | Adaptive Acc: 86.242% | clf_exit: 0.850 0.129 0.021
Batch: 60 | Loss: 1.096 | Acc: 80.827,90.830,93.609,% | Adaptive Acc: 86.258% | clf_exit: 0.852 0.127 0.021
Train all parameters

Epoch: 277
Batch: 0 | Loss: 0.499 | Acc: 87.500,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.828 0.172 0.000
Batch: 20 | Loss: 0.583 | Acc: 83.445,99.479,100.000,% | Adaptive Acc: 92.485% | clf_exit: 0.821 0.175 0.004
Batch: 40 | Loss: 0.581 | Acc: 83.575,99.447,100.000,% | Adaptive Acc: 92.550% | clf_exit: 0.820 0.176 0.004
Batch: 60 | Loss: 0.565 | Acc: 84.183,99.449,99.987,% | Adaptive Acc: 92.879% | clf_exit: 0.823 0.173 0.004
Batch: 80 | Loss: 0.572 | Acc: 83.845,99.392,99.981,% | Adaptive Acc: 92.708% | clf_exit: 0.822 0.174 0.004
Batch: 100 | Loss: 0.576 | Acc: 83.609,99.397,99.985,% | Adaptive Acc: 92.551% | clf_exit: 0.823 0.173 0.004
Batch: 120 | Loss: 0.576 | Acc: 83.490,99.387,99.981,% | Adaptive Acc: 92.491% | clf_exit: 0.823 0.173 0.004
Batch: 140 | Loss: 0.575 | Acc: 83.677,99.396,99.983,% | Adaptive Acc: 92.531% | clf_exit: 0.824 0.172 0.004
Batch: 160 | Loss: 0.575 | Acc: 83.618,99.379,99.985,% | Adaptive Acc: 92.450% | clf_exit: 0.825 0.171 0.004
Batch: 180 | Loss: 0.574 | Acc: 83.676,99.335,99.987,% | Adaptive Acc: 92.438% | clf_exit: 0.826 0.170 0.004
Batch: 200 | Loss: 0.574 | Acc: 83.664,99.324,99.988,% | Adaptive Acc: 92.487% | clf_exit: 0.826 0.170 0.004
Batch: 220 | Loss: 0.573 | Acc: 83.710,99.339,99.982,% | Adaptive Acc: 92.530% | clf_exit: 0.826 0.170 0.005
Batch: 240 | Loss: 0.572 | Acc: 83.733,99.358,99.981,% | Adaptive Acc: 92.573% | clf_exit: 0.825 0.171 0.004
Batch: 260 | Loss: 0.573 | Acc: 83.666,99.353,99.979,% | Adaptive Acc: 92.514% | clf_exit: 0.825 0.171 0.004
Batch: 280 | Loss: 0.572 | Acc: 83.763,99.361,99.978,% | Adaptive Acc: 92.568% | clf_exit: 0.825 0.171 0.005
Batch: 300 | Loss: 0.571 | Acc: 83.794,99.346,99.977,% | Adaptive Acc: 92.592% | clf_exit: 0.825 0.170 0.005
Batch: 320 | Loss: 0.570 | Acc: 83.869,99.345,99.978,% | Adaptive Acc: 92.609% | clf_exit: 0.825 0.170 0.005
Batch: 340 | Loss: 0.570 | Acc: 83.885,99.345,99.979,% | Adaptive Acc: 92.611% | clf_exit: 0.826 0.169 0.005
Batch: 360 | Loss: 0.570 | Acc: 83.934,99.333,99.981,% | Adaptive Acc: 92.642% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.570 | Acc: 83.963,99.315,99.977,% | Adaptive Acc: 92.645% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.896 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 90.625% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.108 | Acc: 80.729,91.146,93.304,% | Adaptive Acc: 86.458% | clf_exit: 0.840 0.141 0.019
Batch: 40 | Loss: 1.116 | Acc: 80.716,90.701,93.464,% | Adaptive Acc: 86.319% | clf_exit: 0.850 0.128 0.022
Batch: 60 | Loss: 1.093 | Acc: 80.815,90.881,93.584,% | Adaptive Acc: 86.373% | clf_exit: 0.852 0.126 0.022
Train all parameters

Epoch: 278
Batch: 0 | Loss: 0.606 | Acc: 85.156,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.781 0.219 0.000
Batch: 20 | Loss: 0.560 | Acc: 83.891,99.516,100.000,% | Adaptive Acc: 92.932% | clf_exit: 0.825 0.170 0.005
Batch: 40 | Loss: 0.562 | Acc: 84.013,99.371,99.981,% | Adaptive Acc: 93.007% | clf_exit: 0.829 0.166 0.006
Batch: 60 | Loss: 0.561 | Acc: 84.144,99.360,99.987,% | Adaptive Acc: 92.828% | clf_exit: 0.831 0.164 0.005
Batch: 80 | Loss: 0.564 | Acc: 84.240,99.431,99.981,% | Adaptive Acc: 92.853% | clf_exit: 0.829 0.166 0.005
Batch: 100 | Loss: 0.567 | Acc: 84.127,99.373,99.985,% | Adaptive Acc: 92.628% | clf_exit: 0.829 0.166 0.005
Batch: 120 | Loss: 0.568 | Acc: 84.039,99.393,99.987,% | Adaptive Acc: 92.594% | clf_exit: 0.828 0.167 0.005
Batch: 140 | Loss: 0.567 | Acc: 84.076,99.363,99.983,% | Adaptive Acc: 92.725% | clf_exit: 0.827 0.168 0.005
Batch: 160 | Loss: 0.565 | Acc: 84.186,99.398,99.985,% | Adaptive Acc: 92.741% | clf_exit: 0.829 0.166 0.005
Batch: 180 | Loss: 0.565 | Acc: 84.151,99.374,99.983,% | Adaptive Acc: 92.688% | clf_exit: 0.829 0.167 0.005
Batch: 200 | Loss: 0.567 | Acc: 84.095,99.370,99.984,% | Adaptive Acc: 92.650% | clf_exit: 0.828 0.167 0.005
Batch: 220 | Loss: 0.569 | Acc: 83.926,99.381,99.986,% | Adaptive Acc: 92.559% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.570 | Acc: 83.860,99.365,99.984,% | Adaptive Acc: 92.521% | clf_exit: 0.828 0.168 0.005
Batch: 260 | Loss: 0.571 | Acc: 83.845,99.353,99.985,% | Adaptive Acc: 92.499% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.571 | Acc: 83.925,99.338,99.983,% | Adaptive Acc: 92.529% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.571 | Acc: 83.988,99.328,99.984,% | Adaptive Acc: 92.559% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.571 | Acc: 83.973,99.331,99.985,% | Adaptive Acc: 92.562% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.571 | Acc: 83.983,99.331,99.986,% | Adaptive Acc: 92.595% | clf_exit: 0.828 0.168 0.005
Batch: 360 | Loss: 0.572 | Acc: 83.966,99.323,99.987,% | Adaptive Acc: 92.612% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.948,99.327,99.988,% | Adaptive Acc: 92.600% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.888 | Acc: 82.031,92.188,94.531,% | Adaptive Acc: 90.625% | clf_exit: 0.859 0.125 0.016
Batch: 20 | Loss: 1.106 | Acc: 80.841,91.146,93.378,% | Adaptive Acc: 86.458% | clf_exit: 0.845 0.138 0.017
Batch: 40 | Loss: 1.112 | Acc: 80.869,90.777,93.502,% | Adaptive Acc: 86.223% | clf_exit: 0.853 0.128 0.019
Batch: 60 | Loss: 1.090 | Acc: 80.879,90.920,93.622,% | Adaptive Acc: 86.399% | clf_exit: 0.853 0.127 0.020
Train all parameters

Epoch: 279
Batch: 0 | Loss: 0.576 | Acc: 78.906,99.219,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.549 | Acc: 84.561,99.330,99.963,% | Adaptive Acc: 92.708% | clf_exit: 0.828 0.167 0.005
Batch: 40 | Loss: 0.570 | Acc: 84.356,99.143,99.962,% | Adaptive Acc: 92.702% | clf_exit: 0.827 0.167 0.006
Batch: 60 | Loss: 0.572 | Acc: 84.209,99.193,99.974,% | Adaptive Acc: 92.725% | clf_exit: 0.824 0.170 0.005
Batch: 80 | Loss: 0.577 | Acc: 84.008,99.257,99.971,% | Adaptive Acc: 92.631% | clf_exit: 0.826 0.169 0.005
Batch: 100 | Loss: 0.576 | Acc: 84.050,99.234,99.946,% | Adaptive Acc: 92.752% | clf_exit: 0.823 0.171 0.006
Batch: 120 | Loss: 0.570 | Acc: 84.065,99.270,99.948,% | Adaptive Acc: 92.717% | clf_exit: 0.825 0.171 0.005
Batch: 140 | Loss: 0.569 | Acc: 83.937,99.269,99.956,% | Adaptive Acc: 92.620% | clf_exit: 0.825 0.170 0.005
Batch: 160 | Loss: 0.567 | Acc: 84.064,99.277,99.956,% | Adaptive Acc: 92.721% | clf_exit: 0.825 0.170 0.005
Batch: 180 | Loss: 0.568 | Acc: 84.103,99.275,99.961,% | Adaptive Acc: 92.762% | clf_exit: 0.826 0.169 0.005
Batch: 200 | Loss: 0.565 | Acc: 84.208,99.285,99.961,% | Adaptive Acc: 92.782% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.563 | Acc: 84.343,99.304,99.965,% | Adaptive Acc: 92.813% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.565 | Acc: 84.268,99.339,99.964,% | Adaptive Acc: 92.726% | clf_exit: 0.829 0.166 0.005
Batch: 260 | Loss: 0.566 | Acc: 84.225,99.347,99.967,% | Adaptive Acc: 92.717% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.568 | Acc: 84.158,99.347,99.969,% | Adaptive Acc: 92.660% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.567 | Acc: 84.188,99.338,99.971,% | Adaptive Acc: 92.694% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.569 | Acc: 84.068,99.321,99.971,% | Adaptive Acc: 92.672% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.569 | Acc: 84.093,99.315,99.973,% | Adaptive Acc: 92.669% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.570 | Acc: 84.070,99.310,99.972,% | Adaptive Acc: 92.657% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.571 | Acc: 84.037,99.311,99.971,% | Adaptive Acc: 92.643% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.891 | Acc: 81.250,93.750,95.312,% | Adaptive Acc: 88.281% | clf_exit: 0.875 0.094 0.031
Batch: 20 | Loss: 1.101 | Acc: 80.915,91.295,93.638,% | Adaptive Acc: 86.347% | clf_exit: 0.843 0.140 0.017
Batch: 40 | Loss: 1.112 | Acc: 80.850,91.044,93.636,% | Adaptive Acc: 86.166% | clf_exit: 0.851 0.129 0.020
Batch: 60 | Loss: 1.093 | Acc: 80.879,91.099,93.699,% | Adaptive Acc: 86.386% | clf_exit: 0.851 0.128 0.021
Train all parameters

Epoch: 280
Batch: 0 | Loss: 0.619 | Acc: 83.594,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.852 0.148 0.000
Batch: 20 | Loss: 0.596 | Acc: 82.775,99.405,100.000,% | Adaptive Acc: 91.704% | clf_exit: 0.826 0.171 0.003
Batch: 40 | Loss: 0.575 | Acc: 83.518,99.447,99.981,% | Adaptive Acc: 92.207% | clf_exit: 0.826 0.170 0.003
Batch: 60 | Loss: 0.568 | Acc: 84.029,99.360,99.974,% | Adaptive Acc: 92.444% | clf_exit: 0.829 0.168 0.004
Batch: 80 | Loss: 0.561 | Acc: 84.211,99.412,99.981,% | Adaptive Acc: 92.670% | clf_exit: 0.832 0.164 0.004
Batch: 100 | Loss: 0.567 | Acc: 84.050,99.397,99.985,% | Adaptive Acc: 92.628% | clf_exit: 0.829 0.166 0.004
Batch: 120 | Loss: 0.572 | Acc: 83.768,99.329,99.981,% | Adaptive Acc: 92.601% | clf_exit: 0.828 0.167 0.005
Batch: 140 | Loss: 0.567 | Acc: 83.954,99.379,99.983,% | Adaptive Acc: 92.697% | clf_exit: 0.830 0.166 0.004
Batch: 160 | Loss: 0.568 | Acc: 83.880,99.379,99.981,% | Adaptive Acc: 92.629% | clf_exit: 0.831 0.165 0.004
Batch: 180 | Loss: 0.569 | Acc: 83.814,99.366,99.974,% | Adaptive Acc: 92.623% | clf_exit: 0.829 0.166 0.004
Batch: 200 | Loss: 0.569 | Acc: 83.862,99.335,99.973,% | Adaptive Acc: 92.623% | clf_exit: 0.830 0.166 0.004
Batch: 220 | Loss: 0.569 | Acc: 83.968,99.360,99.968,% | Adaptive Acc: 92.672% | clf_exit: 0.831 0.165 0.004
Batch: 240 | Loss: 0.567 | Acc: 84.005,99.365,99.968,% | Adaptive Acc: 92.726% | clf_exit: 0.831 0.165 0.004
Batch: 260 | Loss: 0.567 | Acc: 83.941,99.338,99.970,% | Adaptive Acc: 92.708% | clf_exit: 0.830 0.166 0.004
Batch: 280 | Loss: 0.571 | Acc: 83.797,99.302,99.969,% | Adaptive Acc: 92.593% | clf_exit: 0.829 0.166 0.005
Batch: 300 | Loss: 0.571 | Acc: 83.817,99.307,99.971,% | Adaptive Acc: 92.587% | clf_exit: 0.829 0.167 0.005
Batch: 320 | Loss: 0.573 | Acc: 83.818,99.282,99.971,% | Adaptive Acc: 92.594% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.572 | Acc: 83.775,99.292,99.970,% | Adaptive Acc: 92.588% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.571 | Acc: 83.808,99.297,99.972,% | Adaptive Acc: 92.581% | clf_exit: 0.828 0.167 0.005
Batch: 380 | Loss: 0.571 | Acc: 83.834,99.297,99.973,% | Adaptive Acc: 92.622% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.929 | Acc: 81.250,92.188,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.844 0.133 0.023
Batch: 20 | Loss: 1.113 | Acc: 80.580,91.220,93.155,% | Adaptive Acc: 86.570% | clf_exit: 0.842 0.142 0.016
Batch: 40 | Loss: 1.117 | Acc: 80.564,90.873,93.388,% | Adaptive Acc: 86.261% | clf_exit: 0.852 0.128 0.020
Batch: 60 | Loss: 1.094 | Acc: 80.699,90.971,93.532,% | Adaptive Acc: 86.347% | clf_exit: 0.853 0.127 0.019
Train all parameters

Epoch: 281
Batch: 0 | Loss: 0.455 | Acc: 89.844,100.000,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.898 0.102 0.000
Batch: 20 | Loss: 0.566 | Acc: 83.854,99.405,100.000,% | Adaptive Acc: 92.225% | clf_exit: 0.830 0.168 0.003
Batch: 40 | Loss: 0.569 | Acc: 83.689,99.390,99.981,% | Adaptive Acc: 92.340% | clf_exit: 0.827 0.168 0.005
Batch: 60 | Loss: 0.571 | Acc: 83.594,99.398,99.987,% | Adaptive Acc: 92.277% | clf_exit: 0.827 0.169 0.005
Batch: 80 | Loss: 0.564 | Acc: 83.787,99.412,99.981,% | Adaptive Acc: 92.400% | clf_exit: 0.828 0.167 0.005
Batch: 100 | Loss: 0.562 | Acc: 84.019,99.350,99.985,% | Adaptive Acc: 92.512% | clf_exit: 0.831 0.165 0.005
Batch: 120 | Loss: 0.561 | Acc: 84.001,99.341,99.981,% | Adaptive Acc: 92.388% | clf_exit: 0.832 0.163 0.005
Batch: 140 | Loss: 0.562 | Acc: 83.976,99.368,99.983,% | Adaptive Acc: 92.498% | clf_exit: 0.831 0.164 0.005
Batch: 160 | Loss: 0.560 | Acc: 84.084,99.379,99.985,% | Adaptive Acc: 92.566% | clf_exit: 0.832 0.163 0.005
Batch: 180 | Loss: 0.563 | Acc: 84.051,99.361,99.983,% | Adaptive Acc: 92.615% | clf_exit: 0.831 0.164 0.005
Batch: 200 | Loss: 0.565 | Acc: 83.951,99.374,99.981,% | Adaptive Acc: 92.615% | clf_exit: 0.829 0.166 0.005
Batch: 220 | Loss: 0.565 | Acc: 83.979,99.378,99.982,% | Adaptive Acc: 92.619% | clf_exit: 0.829 0.166 0.005
Batch: 240 | Loss: 0.567 | Acc: 83.957,99.381,99.984,% | Adaptive Acc: 92.625% | clf_exit: 0.828 0.167 0.005
Batch: 260 | Loss: 0.569 | Acc: 83.881,99.368,99.982,% | Adaptive Acc: 92.547% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.571 | Acc: 83.769,99.352,99.983,% | Adaptive Acc: 92.438% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.571 | Acc: 83.786,99.359,99.982,% | Adaptive Acc: 92.465% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.570 | Acc: 83.810,99.365,99.981,% | Adaptive Acc: 92.482% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.570 | Acc: 83.846,99.365,99.982,% | Adaptive Acc: 92.524% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.568 | Acc: 83.916,99.377,99.983,% | Adaptive Acc: 92.564% | clf_exit: 0.828 0.167 0.005
Batch: 380 | Loss: 0.568 | Acc: 83.951,99.377,99.982,% | Adaptive Acc: 92.608% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.918 | Acc: 81.250,92.188,93.750,% | Adaptive Acc: 87.500% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.110 | Acc: 80.580,91.257,93.415,% | Adaptive Acc: 86.310% | clf_exit: 0.843 0.141 0.015
Batch: 40 | Loss: 1.116 | Acc: 80.583,90.701,93.483,% | Adaptive Acc: 86.166% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.094 | Acc: 80.712,90.856,93.635,% | Adaptive Acc: 86.194% | clf_exit: 0.852 0.128 0.020
Train all parameters

Epoch: 282
Batch: 0 | Loss: 0.599 | Acc: 82.031,100.000,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.789 0.211 0.000
Batch: 20 | Loss: 0.597 | Acc: 83.631,99.219,99.963,% | Adaptive Acc: 91.815% | clf_exit: 0.830 0.164 0.005
Batch: 40 | Loss: 0.592 | Acc: 83.594,99.181,99.981,% | Adaptive Acc: 92.092% | clf_exit: 0.823 0.171 0.006
Batch: 60 | Loss: 0.588 | Acc: 83.389,99.244,99.987,% | Adaptive Acc: 92.008% | clf_exit: 0.825 0.170 0.005
Batch: 80 | Loss: 0.586 | Acc: 83.526,99.219,99.981,% | Adaptive Acc: 92.130% | clf_exit: 0.826 0.169 0.005
Batch: 100 | Loss: 0.582 | Acc: 83.617,99.203,99.977,% | Adaptive Acc: 92.157% | clf_exit: 0.828 0.167 0.005
Batch: 120 | Loss: 0.581 | Acc: 83.697,99.225,99.974,% | Adaptive Acc: 92.207% | clf_exit: 0.827 0.168 0.005
Batch: 140 | Loss: 0.579 | Acc: 83.682,99.241,99.972,% | Adaptive Acc: 92.343% | clf_exit: 0.826 0.168 0.005
Batch: 160 | Loss: 0.576 | Acc: 83.817,99.258,99.976,% | Adaptive Acc: 92.464% | clf_exit: 0.827 0.168 0.005
Batch: 180 | Loss: 0.574 | Acc: 83.840,99.292,99.978,% | Adaptive Acc: 92.537% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.575 | Acc: 83.753,99.285,99.981,% | Adaptive Acc: 92.498% | clf_exit: 0.826 0.169 0.005
Batch: 220 | Loss: 0.574 | Acc: 83.824,99.311,99.982,% | Adaptive Acc: 92.583% | clf_exit: 0.826 0.169 0.005
Batch: 240 | Loss: 0.573 | Acc: 83.801,99.322,99.981,% | Adaptive Acc: 92.538% | clf_exit: 0.826 0.168 0.005
Batch: 260 | Loss: 0.573 | Acc: 83.878,99.318,99.979,% | Adaptive Acc: 92.538% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.573 | Acc: 83.902,99.308,99.978,% | Adaptive Acc: 92.552% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.573 | Acc: 83.861,99.307,99.979,% | Adaptive Acc: 92.538% | clf_exit: 0.827 0.167 0.005
Batch: 320 | Loss: 0.574 | Acc: 83.810,99.311,99.981,% | Adaptive Acc: 92.494% | clf_exit: 0.827 0.167 0.005
Batch: 340 | Loss: 0.573 | Acc: 83.839,99.315,99.979,% | Adaptive Acc: 92.552% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.572 | Acc: 83.869,99.318,99.978,% | Adaptive Acc: 92.581% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.871,99.311,99.979,% | Adaptive Acc: 92.606% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.906 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 88.281% | clf_exit: 0.883 0.094 0.023
Batch: 20 | Loss: 1.113 | Acc: 80.804,91.220,93.527,% | Adaptive Acc: 86.272% | clf_exit: 0.847 0.134 0.019
Batch: 40 | Loss: 1.122 | Acc: 80.716,90.930,93.483,% | Adaptive Acc: 85.938% | clf_exit: 0.853 0.126 0.021
Batch: 60 | Loss: 1.099 | Acc: 80.802,90.971,93.648,% | Adaptive Acc: 86.117% | clf_exit: 0.854 0.125 0.021
Train all parameters

Epoch: 283
Batch: 0 | Loss: 0.623 | Acc: 81.250,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.797 0.195 0.008
Batch: 20 | Loss: 0.604 | Acc: 82.961,99.107,100.000,% | Adaptive Acc: 92.001% | clf_exit: 0.818 0.179 0.003
Batch: 40 | Loss: 0.592 | Acc: 83.270,99.143,99.981,% | Adaptive Acc: 92.092% | clf_exit: 0.823 0.172 0.005
Batch: 60 | Loss: 0.584 | Acc: 83.261,99.232,99.987,% | Adaptive Acc: 92.059% | clf_exit: 0.823 0.172 0.005
Batch: 80 | Loss: 0.582 | Acc: 83.372,99.228,99.990,% | Adaptive Acc: 92.342% | clf_exit: 0.822 0.172 0.005
Batch: 100 | Loss: 0.577 | Acc: 83.524,99.219,99.985,% | Adaptive Acc: 92.350% | clf_exit: 0.825 0.169 0.006
Batch: 120 | Loss: 0.568 | Acc: 83.897,99.219,99.981,% | Adaptive Acc: 92.633% | clf_exit: 0.828 0.167 0.005
Batch: 140 | Loss: 0.566 | Acc: 83.948,99.274,99.983,% | Adaptive Acc: 92.736% | clf_exit: 0.828 0.167 0.005
Batch: 160 | Loss: 0.567 | Acc: 83.933,99.262,99.985,% | Adaptive Acc: 92.639% | clf_exit: 0.828 0.167 0.005
Batch: 180 | Loss: 0.573 | Acc: 83.792,99.240,99.983,% | Adaptive Acc: 92.528% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.573 | Acc: 83.769,99.227,99.984,% | Adaptive Acc: 92.561% | clf_exit: 0.827 0.168 0.006
Batch: 220 | Loss: 0.573 | Acc: 83.756,99.229,99.982,% | Adaptive Acc: 92.523% | clf_exit: 0.827 0.168 0.005
Batch: 240 | Loss: 0.572 | Acc: 83.840,99.222,99.981,% | Adaptive Acc: 92.547% | clf_exit: 0.827 0.168 0.005
Batch: 260 | Loss: 0.570 | Acc: 83.914,99.252,99.982,% | Adaptive Acc: 92.562% | clf_exit: 0.827 0.168 0.005
Batch: 280 | Loss: 0.567 | Acc: 84.044,99.269,99.983,% | Adaptive Acc: 92.635% | clf_exit: 0.829 0.166 0.005
Batch: 300 | Loss: 0.566 | Acc: 84.066,99.276,99.984,% | Adaptive Acc: 92.660% | clf_exit: 0.828 0.166 0.005
Batch: 320 | Loss: 0.567 | Acc: 84.022,99.284,99.983,% | Adaptive Acc: 92.606% | clf_exit: 0.829 0.166 0.005
Batch: 340 | Loss: 0.568 | Acc: 84.018,99.285,99.982,% | Adaptive Acc: 92.602% | clf_exit: 0.828 0.166 0.005
Batch: 360 | Loss: 0.569 | Acc: 84.001,99.286,99.981,% | Adaptive Acc: 92.601% | clf_exit: 0.828 0.167 0.005
Batch: 380 | Loss: 0.569 | Acc: 84.031,99.280,99.979,% | Adaptive Acc: 92.612% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.895 | Acc: 80.469,92.188,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.852 0.125 0.023
Batch: 20 | Loss: 1.102 | Acc: 80.729,91.369,93.490,% | Adaptive Acc: 86.235% | clf_exit: 0.843 0.141 0.015
Batch: 40 | Loss: 1.111 | Acc: 80.640,91.063,93.598,% | Adaptive Acc: 86.204% | clf_exit: 0.852 0.128 0.020
Batch: 60 | Loss: 1.091 | Acc: 80.712,91.086,93.699,% | Adaptive Acc: 86.373% | clf_exit: 0.853 0.126 0.021
Train all parameters

Epoch: 284
Batch: 0 | Loss: 0.458 | Acc: 88.281,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.836 0.164 0.000
Batch: 20 | Loss: 0.559 | Acc: 84.933,99.516,100.000,% | Adaptive Acc: 92.783% | clf_exit: 0.824 0.172 0.004
Batch: 40 | Loss: 0.556 | Acc: 84.966,99.524,100.000,% | Adaptive Acc: 93.159% | clf_exit: 0.828 0.167 0.005
Batch: 60 | Loss: 0.567 | Acc: 84.298,99.424,99.974,% | Adaptive Acc: 92.815% | clf_exit: 0.827 0.167 0.005
Batch: 80 | Loss: 0.565 | Acc: 84.346,99.479,99.981,% | Adaptive Acc: 92.805% | clf_exit: 0.829 0.166 0.005
Batch: 100 | Loss: 0.567 | Acc: 84.274,99.489,99.977,% | Adaptive Acc: 92.891% | clf_exit: 0.826 0.169 0.005
Batch: 120 | Loss: 0.564 | Acc: 84.233,99.509,99.974,% | Adaptive Acc: 92.833% | clf_exit: 0.828 0.167 0.004
Batch: 140 | Loss: 0.564 | Acc: 84.331,99.451,99.967,% | Adaptive Acc: 92.852% | clf_exit: 0.829 0.167 0.005
Batch: 160 | Loss: 0.565 | Acc: 84.268,99.461,99.971,% | Adaptive Acc: 92.828% | clf_exit: 0.828 0.167 0.005
Batch: 180 | Loss: 0.564 | Acc: 84.198,99.452,99.970,% | Adaptive Acc: 92.857% | clf_exit: 0.828 0.167 0.005
Batch: 200 | Loss: 0.565 | Acc: 84.169,99.436,99.973,% | Adaptive Acc: 92.794% | clf_exit: 0.828 0.167 0.005
Batch: 220 | Loss: 0.566 | Acc: 84.092,99.396,99.975,% | Adaptive Acc: 92.725% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.566 | Acc: 84.041,99.391,99.977,% | Adaptive Acc: 92.680% | clf_exit: 0.828 0.167 0.005
Batch: 260 | Loss: 0.566 | Acc: 84.073,99.383,99.979,% | Adaptive Acc: 92.696% | clf_exit: 0.827 0.168 0.005
Batch: 280 | Loss: 0.568 | Acc: 83.975,99.369,99.978,% | Adaptive Acc: 92.685% | clf_exit: 0.826 0.169 0.005
Batch: 300 | Loss: 0.567 | Acc: 84.027,99.377,99.979,% | Adaptive Acc: 92.688% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.570 | Acc: 83.988,99.375,99.981,% | Adaptive Acc: 92.652% | clf_exit: 0.826 0.169 0.005
Batch: 340 | Loss: 0.569 | Acc: 83.995,99.377,99.982,% | Adaptive Acc: 92.669% | clf_exit: 0.826 0.169 0.005
Batch: 360 | Loss: 0.569 | Acc: 83.994,99.379,99.978,% | Adaptive Acc: 92.698% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.568 | Acc: 84.039,99.379,99.979,% | Adaptive Acc: 92.725% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.891 | Acc: 82.812,92.188,95.312,% | Adaptive Acc: 90.625% | clf_exit: 0.852 0.125 0.023
Batch: 20 | Loss: 1.105 | Acc: 80.841,91.071,93.601,% | Adaptive Acc: 86.049% | clf_exit: 0.844 0.139 0.017
Batch: 40 | Loss: 1.113 | Acc: 80.869,90.854,93.693,% | Adaptive Acc: 86.090% | clf_exit: 0.852 0.127 0.020
Batch: 60 | Loss: 1.092 | Acc: 80.943,90.971,93.660,% | Adaptive Acc: 86.232% | clf_exit: 0.853 0.126 0.021
Train classifier parameters

Epoch: 285
Batch: 0 | Loss: 0.739 | Acc: 74.219,99.219,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.781 0.203 0.016
Batch: 20 | Loss: 0.586 | Acc: 83.668,99.293,100.000,% | Adaptive Acc: 92.894% | clf_exit: 0.815 0.177 0.007
Batch: 40 | Loss: 0.584 | Acc: 84.051,99.200,100.000,% | Adaptive Acc: 92.588% | clf_exit: 0.825 0.169 0.006
Batch: 60 | Loss: 0.575 | Acc: 84.068,99.180,99.987,% | Adaptive Acc: 92.751% | clf_exit: 0.824 0.170 0.006
Batch: 80 | Loss: 0.580 | Acc: 83.700,99.161,99.990,% | Adaptive Acc: 92.631% | clf_exit: 0.823 0.171 0.006
Batch: 100 | Loss: 0.586 | Acc: 83.617,99.157,99.992,% | Adaptive Acc: 92.481% | clf_exit: 0.822 0.172 0.006
Batch: 120 | Loss: 0.577 | Acc: 83.929,99.199,99.994,% | Adaptive Acc: 92.639% | clf_exit: 0.825 0.170 0.006
Batch: 140 | Loss: 0.577 | Acc: 83.910,99.197,99.989,% | Adaptive Acc: 92.570% | clf_exit: 0.827 0.168 0.006
Batch: 160 | Loss: 0.577 | Acc: 83.744,99.199,99.990,% | Adaptive Acc: 92.498% | clf_exit: 0.827 0.168 0.005
Batch: 180 | Loss: 0.578 | Acc: 83.654,99.236,99.987,% | Adaptive Acc: 92.421% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.579 | Acc: 83.660,99.242,99.988,% | Adaptive Acc: 92.456% | clf_exit: 0.826 0.169 0.005
Batch: 220 | Loss: 0.579 | Acc: 83.693,99.243,99.986,% | Adaptive Acc: 92.438% | clf_exit: 0.826 0.169 0.005
Batch: 240 | Loss: 0.578 | Acc: 83.714,99.264,99.987,% | Adaptive Acc: 92.440% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.578 | Acc: 83.725,99.270,99.985,% | Adaptive Acc: 92.442% | clf_exit: 0.826 0.168 0.005
Batch: 280 | Loss: 0.576 | Acc: 83.724,99.283,99.986,% | Adaptive Acc: 92.399% | clf_exit: 0.827 0.168 0.005
Batch: 300 | Loss: 0.574 | Acc: 83.788,99.304,99.987,% | Adaptive Acc: 92.478% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.574 | Acc: 83.774,99.301,99.988,% | Adaptive Acc: 92.482% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.573 | Acc: 83.763,99.310,99.989,% | Adaptive Acc: 92.488% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.573 | Acc: 83.808,99.314,99.989,% | Adaptive Acc: 92.499% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.834,99.313,99.988,% | Adaptive Acc: 92.507% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.899 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.109 | Acc: 80.952,91.183,93.452,% | Adaptive Acc: 86.049% | clf_exit: 0.844 0.138 0.017
Batch: 40 | Loss: 1.114 | Acc: 80.850,90.816,93.559,% | Adaptive Acc: 85.957% | clf_exit: 0.851 0.128 0.021
Batch: 60 | Loss: 1.091 | Acc: 80.904,90.945,93.712,% | Adaptive Acc: 86.181% | clf_exit: 0.852 0.127 0.021
Train classifier parameters

Epoch: 286
Batch: 0 | Loss: 0.490 | Acc: 88.281,99.219,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.867 0.133 0.000
Batch: 20 | Loss: 0.566 | Acc: 84.412,99.330,99.963,% | Adaptive Acc: 92.522% | clf_exit: 0.828 0.168 0.004
Batch: 40 | Loss: 0.562 | Acc: 83.918,99.295,99.981,% | Adaptive Acc: 92.569% | clf_exit: 0.826 0.171 0.003
Batch: 60 | Loss: 0.559 | Acc: 84.157,99.283,99.987,% | Adaptive Acc: 92.559% | clf_exit: 0.830 0.166 0.004
Batch: 80 | Loss: 0.563 | Acc: 84.028,99.325,99.990,% | Adaptive Acc: 92.573% | clf_exit: 0.829 0.167 0.004
Batch: 100 | Loss: 0.559 | Acc: 84.189,99.350,99.985,% | Adaptive Acc: 92.628% | clf_exit: 0.831 0.165 0.004
Batch: 120 | Loss: 0.563 | Acc: 84.226,99.303,99.981,% | Adaptive Acc: 92.549% | clf_exit: 0.832 0.163 0.004
Batch: 140 | Loss: 0.567 | Acc: 84.048,99.324,99.983,% | Adaptive Acc: 92.570% | clf_exit: 0.830 0.166 0.005
Batch: 160 | Loss: 0.567 | Acc: 84.142,99.316,99.981,% | Adaptive Acc: 92.581% | clf_exit: 0.830 0.165 0.005
Batch: 180 | Loss: 0.566 | Acc: 84.142,99.309,99.978,% | Adaptive Acc: 92.589% | clf_exit: 0.831 0.164 0.005
Batch: 200 | Loss: 0.569 | Acc: 84.014,99.304,99.965,% | Adaptive Acc: 92.588% | clf_exit: 0.830 0.165 0.005
Batch: 220 | Loss: 0.569 | Acc: 84.046,99.297,99.968,% | Adaptive Acc: 92.615% | clf_exit: 0.830 0.165 0.005
Batch: 240 | Loss: 0.569 | Acc: 84.080,99.297,99.968,% | Adaptive Acc: 92.628% | clf_exit: 0.829 0.166 0.005
Batch: 260 | Loss: 0.570 | Acc: 84.082,99.312,99.970,% | Adaptive Acc: 92.645% | clf_exit: 0.829 0.166 0.005
Batch: 280 | Loss: 0.569 | Acc: 84.122,99.308,99.969,% | Adaptive Acc: 92.710% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.569 | Acc: 84.058,99.307,99.971,% | Adaptive Acc: 92.660% | clf_exit: 0.829 0.166 0.005
Batch: 320 | Loss: 0.569 | Acc: 84.063,99.314,99.971,% | Adaptive Acc: 92.677% | clf_exit: 0.829 0.166 0.005
Batch: 340 | Loss: 0.569 | Acc: 84.022,99.306,99.970,% | Adaptive Acc: 92.639% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.568 | Acc: 84.029,99.312,99.968,% | Adaptive Acc: 92.648% | clf_exit: 0.829 0.167 0.005
Batch: 380 | Loss: 0.570 | Acc: 83.930,99.301,99.965,% | Adaptive Acc: 92.639% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.906 | Acc: 82.031,92.969,92.969,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.109 | Acc: 80.990,91.071,93.378,% | Adaptive Acc: 86.272% | clf_exit: 0.843 0.140 0.018
Batch: 40 | Loss: 1.116 | Acc: 80.869,90.701,93.521,% | Adaptive Acc: 86.166% | clf_exit: 0.848 0.130 0.022
Batch: 60 | Loss: 1.095 | Acc: 80.904,90.753,93.699,% | Adaptive Acc: 86.335% | clf_exit: 0.850 0.128 0.022
Train classifier parameters

Epoch: 287
Batch: 0 | Loss: 0.815 | Acc: 79.688,97.656,100.000,% | Adaptive Acc: 85.938% | clf_exit: 0.820 0.180 0.000
Batch: 20 | Loss: 0.556 | Acc: 84.189,99.256,100.000,% | Adaptive Acc: 92.150% | clf_exit: 0.843 0.152 0.005
Batch: 40 | Loss: 0.554 | Acc: 84.299,99.314,99.981,% | Adaptive Acc: 92.321% | clf_exit: 0.841 0.153 0.006
Batch: 60 | Loss: 0.567 | Acc: 83.837,99.296,99.987,% | Adaptive Acc: 92.008% | clf_exit: 0.835 0.159 0.006
Batch: 80 | Loss: 0.572 | Acc: 83.873,99.277,99.990,% | Adaptive Acc: 92.168% | clf_exit: 0.831 0.163 0.006
Batch: 100 | Loss: 0.573 | Acc: 83.872,99.257,99.977,% | Adaptive Acc: 92.311% | clf_exit: 0.830 0.164 0.006
Batch: 120 | Loss: 0.574 | Acc: 83.710,99.277,99.981,% | Adaptive Acc: 92.349% | clf_exit: 0.829 0.165 0.006
Batch: 140 | Loss: 0.570 | Acc: 83.838,99.274,99.978,% | Adaptive Acc: 92.337% | clf_exit: 0.831 0.164 0.006
Batch: 160 | Loss: 0.571 | Acc: 83.822,99.287,99.981,% | Adaptive Acc: 92.372% | clf_exit: 0.829 0.166 0.005
Batch: 180 | Loss: 0.571 | Acc: 83.861,99.296,99.983,% | Adaptive Acc: 92.416% | clf_exit: 0.828 0.167 0.005
Batch: 200 | Loss: 0.571 | Acc: 83.843,99.296,99.984,% | Adaptive Acc: 92.452% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.571 | Acc: 83.859,99.297,99.979,% | Adaptive Acc: 92.410% | clf_exit: 0.827 0.168 0.005
Batch: 240 | Loss: 0.570 | Acc: 83.895,99.303,99.981,% | Adaptive Acc: 92.440% | clf_exit: 0.827 0.168 0.005
Batch: 260 | Loss: 0.569 | Acc: 83.938,99.303,99.979,% | Adaptive Acc: 92.487% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.569 | Acc: 83.925,99.322,99.981,% | Adaptive Acc: 92.438% | clf_exit: 0.829 0.166 0.005
Batch: 300 | Loss: 0.567 | Acc: 83.988,99.328,99.979,% | Adaptive Acc: 92.476% | clf_exit: 0.829 0.166 0.005
Batch: 320 | Loss: 0.568 | Acc: 83.976,99.348,99.981,% | Adaptive Acc: 92.463% | clf_exit: 0.829 0.166 0.005
Batch: 340 | Loss: 0.567 | Acc: 83.967,99.347,99.982,% | Adaptive Acc: 92.492% | clf_exit: 0.829 0.166 0.005
Batch: 360 | Loss: 0.568 | Acc: 83.908,99.359,99.983,% | Adaptive Acc: 92.480% | clf_exit: 0.829 0.167 0.005
Batch: 380 | Loss: 0.569 | Acc: 83.856,99.354,99.982,% | Adaptive Acc: 92.497% | clf_exit: 0.828 0.168 0.005
Batch: 0 | Loss: 0.895 | Acc: 82.031,93.750,93.750,% | Adaptive Acc: 89.062% | clf_exit: 0.883 0.102 0.016
Batch: 20 | Loss: 1.109 | Acc: 81.064,91.332,93.452,% | Adaptive Acc: 86.086% | clf_exit: 0.843 0.141 0.016
Batch: 40 | Loss: 1.115 | Acc: 80.907,90.949,93.502,% | Adaptive Acc: 86.014% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.094 | Acc: 80.943,90.945,93.660,% | Adaptive Acc: 86.219% | clf_exit: 0.851 0.128 0.021
Train classifier parameters

Epoch: 288
Batch: 0 | Loss: 0.581 | Acc: 81.250,100.000,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.836 0.164 0.000
Batch: 20 | Loss: 0.563 | Acc: 84.338,99.479,100.000,% | Adaptive Acc: 92.225% | clf_exit: 0.833 0.160 0.006
Batch: 40 | Loss: 0.565 | Acc: 84.013,99.466,99.962,% | Adaptive Acc: 92.435% | clf_exit: 0.833 0.161 0.006
Batch: 60 | Loss: 0.568 | Acc: 83.837,99.398,99.974,% | Adaptive Acc: 92.316% | clf_exit: 0.831 0.163 0.005
Batch: 80 | Loss: 0.577 | Acc: 83.632,99.373,99.981,% | Adaptive Acc: 92.265% | clf_exit: 0.828 0.166 0.005
Batch: 100 | Loss: 0.571 | Acc: 83.965,99.412,99.985,% | Adaptive Acc: 92.543% | clf_exit: 0.829 0.166 0.005
Batch: 120 | Loss: 0.570 | Acc: 83.981,99.432,99.987,% | Adaptive Acc: 92.420% | clf_exit: 0.830 0.165 0.005
Batch: 140 | Loss: 0.571 | Acc: 83.915,99.374,99.983,% | Adaptive Acc: 92.448% | clf_exit: 0.830 0.166 0.005
Batch: 160 | Loss: 0.573 | Acc: 83.865,99.384,99.981,% | Adaptive Acc: 92.503% | clf_exit: 0.829 0.167 0.005
Batch: 180 | Loss: 0.572 | Acc: 83.866,99.340,99.983,% | Adaptive Acc: 92.472% | clf_exit: 0.829 0.166 0.005
Batch: 200 | Loss: 0.575 | Acc: 83.808,99.304,99.981,% | Adaptive Acc: 92.428% | clf_exit: 0.829 0.166 0.005
Batch: 220 | Loss: 0.575 | Acc: 83.841,99.293,99.982,% | Adaptive Acc: 92.449% | clf_exit: 0.829 0.166 0.005
Batch: 240 | Loss: 0.572 | Acc: 83.950,99.287,99.981,% | Adaptive Acc: 92.447% | clf_exit: 0.829 0.166 0.005
Batch: 260 | Loss: 0.573 | Acc: 83.917,99.291,99.976,% | Adaptive Acc: 92.445% | clf_exit: 0.829 0.166 0.005
Batch: 280 | Loss: 0.572 | Acc: 83.947,99.283,99.975,% | Adaptive Acc: 92.438% | clf_exit: 0.829 0.166 0.005
Batch: 300 | Loss: 0.572 | Acc: 83.921,99.291,99.977,% | Adaptive Acc: 92.468% | clf_exit: 0.830 0.166 0.005
Batch: 320 | Loss: 0.572 | Acc: 83.917,99.284,99.973,% | Adaptive Acc: 92.501% | clf_exit: 0.830 0.166 0.005
Batch: 340 | Loss: 0.573 | Acc: 83.864,99.281,99.975,% | Adaptive Acc: 92.499% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.572 | Acc: 83.879,99.286,99.976,% | Adaptive Acc: 92.490% | clf_exit: 0.829 0.166 0.005
Batch: 380 | Loss: 0.572 | Acc: 83.858,99.284,99.977,% | Adaptive Acc: 92.440% | clf_exit: 0.829 0.166 0.005
Batch: 0 | Loss: 0.911 | Acc: 81.250,92.188,93.750,% | Adaptive Acc: 88.281% | clf_exit: 0.875 0.109 0.016
Batch: 20 | Loss: 1.109 | Acc: 80.990,91.109,93.304,% | Adaptive Acc: 86.310% | clf_exit: 0.840 0.142 0.018
Batch: 40 | Loss: 1.116 | Acc: 80.812,90.835,93.426,% | Adaptive Acc: 86.090% | clf_exit: 0.850 0.129 0.021
Batch: 60 | Loss: 1.094 | Acc: 80.879,90.894,93.571,% | Adaptive Acc: 86.245% | clf_exit: 0.852 0.127 0.021
Train classifier parameters

Epoch: 289
Batch: 0 | Loss: 0.631 | Acc: 79.688,99.219,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.766 0.219 0.016
Batch: 20 | Loss: 0.573 | Acc: 84.226,99.256,100.000,% | Adaptive Acc: 93.601% | clf_exit: 0.816 0.177 0.007
Batch: 40 | Loss: 0.572 | Acc: 84.127,99.333,99.981,% | Adaptive Acc: 93.026% | clf_exit: 0.820 0.173 0.007
Batch: 60 | Loss: 0.557 | Acc: 84.798,99.372,99.974,% | Adaptive Acc: 93.276% | clf_exit: 0.824 0.170 0.006
Batch: 80 | Loss: 0.558 | Acc: 84.587,99.325,99.971,% | Adaptive Acc: 92.969% | clf_exit: 0.828 0.166 0.006
Batch: 100 | Loss: 0.560 | Acc: 84.607,99.265,99.977,% | Adaptive Acc: 93.093% | clf_exit: 0.827 0.168 0.005
Batch: 120 | Loss: 0.567 | Acc: 84.272,99.316,99.981,% | Adaptive Acc: 92.904% | clf_exit: 0.825 0.170 0.005
Batch: 140 | Loss: 0.563 | Acc: 84.397,99.307,99.983,% | Adaptive Acc: 92.913% | clf_exit: 0.826 0.169 0.005
Batch: 160 | Loss: 0.564 | Acc: 84.302,99.330,99.985,% | Adaptive Acc: 92.780% | clf_exit: 0.827 0.168 0.005
Batch: 180 | Loss: 0.565 | Acc: 84.263,99.327,99.983,% | Adaptive Acc: 92.710% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.563 | Acc: 84.328,99.335,99.984,% | Adaptive Acc: 92.767% | clf_exit: 0.828 0.167 0.005
Batch: 220 | Loss: 0.563 | Acc: 84.304,99.350,99.986,% | Adaptive Acc: 92.735% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.565 | Acc: 84.190,99.335,99.984,% | Adaptive Acc: 92.674% | clf_exit: 0.829 0.166 0.005
Batch: 260 | Loss: 0.563 | Acc: 84.258,99.350,99.985,% | Adaptive Acc: 92.747% | clf_exit: 0.829 0.166 0.005
Batch: 280 | Loss: 0.567 | Acc: 84.203,99.324,99.986,% | Adaptive Acc: 92.694% | clf_exit: 0.829 0.166 0.005
Batch: 300 | Loss: 0.569 | Acc: 84.113,99.320,99.987,% | Adaptive Acc: 92.681% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.569 | Acc: 84.068,99.333,99.985,% | Adaptive Acc: 92.694% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.570 | Acc: 84.029,99.331,99.986,% | Adaptive Acc: 92.682% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.570 | Acc: 84.007,99.340,99.987,% | Adaptive Acc: 92.664% | clf_exit: 0.828 0.167 0.005
Batch: 380 | Loss: 0.570 | Acc: 83.985,99.348,99.988,% | Adaptive Acc: 92.641% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.894 | Acc: 81.250,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.109 0.023
Batch: 20 | Loss: 1.102 | Acc: 80.952,91.109,93.527,% | Adaptive Acc: 86.384% | clf_exit: 0.839 0.144 0.016
Batch: 40 | Loss: 1.112 | Acc: 80.869,90.911,93.617,% | Adaptive Acc: 86.452% | clf_exit: 0.847 0.132 0.020
Batch: 60 | Loss: 1.092 | Acc: 80.968,91.022,93.660,% | Adaptive Acc: 86.527% | clf_exit: 0.849 0.130 0.021
Train classifier parameters

Epoch: 290
Batch: 0 | Loss: 0.727 | Acc: 78.906,98.438,100.000,% | Adaptive Acc: 89.062% | clf_exit: 0.781 0.219 0.000
Batch: 20 | Loss: 0.572 | Acc: 84.561,99.293,99.926,% | Adaptive Acc: 92.634% | clf_exit: 0.831 0.163 0.006
Batch: 40 | Loss: 0.568 | Acc: 84.051,99.486,99.962,% | Adaptive Acc: 92.435% | clf_exit: 0.830 0.166 0.004
Batch: 60 | Loss: 0.575 | Acc: 83.952,99.475,99.974,% | Adaptive Acc: 92.508% | clf_exit: 0.827 0.168 0.005
Batch: 80 | Loss: 0.574 | Acc: 83.845,99.470,99.981,% | Adaptive Acc: 92.525% | clf_exit: 0.827 0.168 0.005
Batch: 100 | Loss: 0.577 | Acc: 83.911,99.404,99.985,% | Adaptive Acc: 92.466% | clf_exit: 0.828 0.167 0.005
Batch: 120 | Loss: 0.575 | Acc: 83.968,99.406,99.981,% | Adaptive Acc: 92.556% | clf_exit: 0.828 0.167 0.005
Batch: 140 | Loss: 0.576 | Acc: 83.898,99.407,99.983,% | Adaptive Acc: 92.586% | clf_exit: 0.827 0.168 0.005
Batch: 160 | Loss: 0.578 | Acc: 83.768,99.423,99.976,% | Adaptive Acc: 92.585% | clf_exit: 0.825 0.170 0.005
Batch: 180 | Loss: 0.576 | Acc: 83.896,99.430,99.978,% | Adaptive Acc: 92.667% | clf_exit: 0.825 0.171 0.004
Batch: 200 | Loss: 0.576 | Acc: 83.796,99.425,99.977,% | Adaptive Acc: 92.642% | clf_exit: 0.824 0.172 0.005
Batch: 220 | Loss: 0.573 | Acc: 83.877,99.424,99.979,% | Adaptive Acc: 92.693% | clf_exit: 0.825 0.170 0.005
Batch: 240 | Loss: 0.572 | Acc: 83.892,99.433,99.981,% | Adaptive Acc: 92.645% | clf_exit: 0.826 0.169 0.004
Batch: 260 | Loss: 0.574 | Acc: 83.833,99.398,99.976,% | Adaptive Acc: 92.577% | clf_exit: 0.826 0.169 0.005
Batch: 280 | Loss: 0.572 | Acc: 83.841,99.411,99.975,% | Adaptive Acc: 92.549% | clf_exit: 0.827 0.169 0.005
Batch: 300 | Loss: 0.572 | Acc: 83.905,99.400,99.974,% | Adaptive Acc: 92.525% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.572 | Acc: 83.879,99.367,99.973,% | Adaptive Acc: 92.463% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.571 | Acc: 83.944,99.368,99.975,% | Adaptive Acc: 92.522% | clf_exit: 0.828 0.168 0.005
Batch: 360 | Loss: 0.570 | Acc: 83.964,99.362,99.974,% | Adaptive Acc: 92.514% | clf_exit: 0.828 0.167 0.005
Batch: 380 | Loss: 0.569 | Acc: 83.983,99.362,99.975,% | Adaptive Acc: 92.538% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.885 | Acc: 82.031,92.969,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.104 | Acc: 80.804,91.071,93.490,% | Adaptive Acc: 86.496% | clf_exit: 0.844 0.138 0.017
Batch: 40 | Loss: 1.112 | Acc: 80.678,90.854,93.540,% | Adaptive Acc: 86.357% | clf_exit: 0.852 0.127 0.021
Batch: 60 | Loss: 1.092 | Acc: 80.840,90.996,93.635,% | Adaptive Acc: 86.527% | clf_exit: 0.853 0.126 0.022
Train classifier parameters

Epoch: 291
Batch: 0 | Loss: 0.518 | Acc: 87.500,99.219,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.859 0.133 0.008
Batch: 20 | Loss: 0.576 | Acc: 83.408,99.628,100.000,% | Adaptive Acc: 93.192% | clf_exit: 0.824 0.172 0.004
Batch: 40 | Loss: 0.570 | Acc: 83.727,99.505,99.981,% | Adaptive Acc: 93.102% | clf_exit: 0.825 0.171 0.004
Batch: 60 | Loss: 0.578 | Acc: 83.466,99.385,99.987,% | Adaptive Acc: 92.918% | clf_exit: 0.822 0.173 0.005
Batch: 80 | Loss: 0.573 | Acc: 83.748,99.402,99.990,% | Adaptive Acc: 92.805% | clf_exit: 0.826 0.170 0.004
Batch: 100 | Loss: 0.572 | Acc: 83.748,99.312,99.992,% | Adaptive Acc: 92.868% | clf_exit: 0.827 0.169 0.005
Batch: 120 | Loss: 0.574 | Acc: 83.594,99.316,99.994,% | Adaptive Acc: 92.769% | clf_exit: 0.827 0.168 0.005
Batch: 140 | Loss: 0.574 | Acc: 83.544,99.296,99.994,% | Adaptive Acc: 92.858% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.573 | Acc: 83.569,99.292,99.990,% | Adaptive Acc: 92.833% | clf_exit: 0.824 0.171 0.005
Batch: 180 | Loss: 0.571 | Acc: 83.758,99.309,99.991,% | Adaptive Acc: 92.921% | clf_exit: 0.824 0.171 0.005
Batch: 200 | Loss: 0.572 | Acc: 83.792,99.312,99.992,% | Adaptive Acc: 92.895% | clf_exit: 0.825 0.170 0.005
Batch: 220 | Loss: 0.572 | Acc: 83.728,99.314,99.993,% | Adaptive Acc: 92.813% | clf_exit: 0.824 0.171 0.005
Batch: 240 | Loss: 0.573 | Acc: 83.707,99.319,99.994,% | Adaptive Acc: 92.732% | clf_exit: 0.825 0.171 0.005
Batch: 260 | Loss: 0.576 | Acc: 83.660,99.297,99.994,% | Adaptive Acc: 92.613% | clf_exit: 0.825 0.170 0.005
Batch: 280 | Loss: 0.576 | Acc: 83.702,99.299,99.992,% | Adaptive Acc: 92.610% | clf_exit: 0.825 0.170 0.005
Batch: 300 | Loss: 0.574 | Acc: 83.781,99.304,99.990,% | Adaptive Acc: 92.670% | clf_exit: 0.826 0.170 0.005
Batch: 320 | Loss: 0.571 | Acc: 83.805,99.326,99.990,% | Adaptive Acc: 92.723% | clf_exit: 0.825 0.170 0.005
Batch: 340 | Loss: 0.571 | Acc: 83.763,99.326,99.991,% | Adaptive Acc: 92.696% | clf_exit: 0.826 0.170 0.005
Batch: 360 | Loss: 0.569 | Acc: 83.858,99.338,99.991,% | Adaptive Acc: 92.720% | clf_exit: 0.827 0.169 0.005
Batch: 380 | Loss: 0.570 | Acc: 83.873,99.334,99.992,% | Adaptive Acc: 92.708% | clf_exit: 0.827 0.169 0.004
Batch: 0 | Loss: 0.884 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.103 | Acc: 81.138,91.257,93.490,% | Adaptive Acc: 86.905% | clf_exit: 0.837 0.145 0.017
Batch: 40 | Loss: 1.110 | Acc: 80.926,90.892,93.521,% | Adaptive Acc: 86.509% | clf_exit: 0.847 0.131 0.021
Batch: 60 | Loss: 1.089 | Acc: 81.019,90.958,93.673,% | Adaptive Acc: 86.591% | clf_exit: 0.848 0.130 0.021
Train classifier parameters

Epoch: 292
Batch: 0 | Loss: 0.586 | Acc: 83.594,99.219,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.828 0.164 0.008
Batch: 20 | Loss: 0.546 | Acc: 84.561,99.479,99.963,% | Adaptive Acc: 92.894% | clf_exit: 0.836 0.160 0.005
Batch: 40 | Loss: 0.544 | Acc: 84.889,99.390,99.962,% | Adaptive Acc: 93.178% | clf_exit: 0.834 0.160 0.006
Batch: 60 | Loss: 0.553 | Acc: 84.618,99.424,99.974,% | Adaptive Acc: 93.161% | clf_exit: 0.831 0.163 0.005
Batch: 80 | Loss: 0.559 | Acc: 84.336,99.441,99.981,% | Adaptive Acc: 93.017% | clf_exit: 0.831 0.164 0.005
Batch: 100 | Loss: 0.564 | Acc: 84.220,99.435,99.985,% | Adaptive Acc: 92.938% | clf_exit: 0.829 0.165 0.006
Batch: 120 | Loss: 0.562 | Acc: 84.278,99.464,99.987,% | Adaptive Acc: 92.891% | clf_exit: 0.831 0.164 0.005
Batch: 140 | Loss: 0.565 | Acc: 84.148,99.451,99.983,% | Adaptive Acc: 92.819% | clf_exit: 0.831 0.164 0.005
Batch: 160 | Loss: 0.565 | Acc: 84.152,99.466,99.981,% | Adaptive Acc: 92.886% | clf_exit: 0.829 0.166 0.005
Batch: 180 | Loss: 0.563 | Acc: 84.280,99.452,99.983,% | Adaptive Acc: 92.900% | clf_exit: 0.830 0.165 0.005
Batch: 200 | Loss: 0.562 | Acc: 84.328,99.448,99.981,% | Adaptive Acc: 92.988% | clf_exit: 0.829 0.165 0.005
Batch: 220 | Loss: 0.562 | Acc: 84.280,99.449,99.982,% | Adaptive Acc: 92.986% | clf_exit: 0.829 0.166 0.005
Batch: 240 | Loss: 0.565 | Acc: 84.103,99.400,99.984,% | Adaptive Acc: 92.839% | clf_exit: 0.828 0.166 0.005
Batch: 260 | Loss: 0.565 | Acc: 84.091,99.395,99.979,% | Adaptive Acc: 92.807% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.564 | Acc: 84.111,99.399,99.978,% | Adaptive Acc: 92.805% | clf_exit: 0.829 0.166 0.005
Batch: 300 | Loss: 0.565 | Acc: 84.175,99.390,99.979,% | Adaptive Acc: 92.816% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.567 | Acc: 84.105,99.389,99.978,% | Adaptive Acc: 92.779% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.567 | Acc: 84.063,99.379,99.979,% | Adaptive Acc: 92.758% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.568 | Acc: 84.066,99.366,99.981,% | Adaptive Acc: 92.724% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.567 | Acc: 84.072,99.358,99.979,% | Adaptive Acc: 92.735% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.896 | Acc: 82.031,92.188,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.105 | Acc: 80.952,91.518,93.452,% | Adaptive Acc: 86.496% | clf_exit: 0.844 0.138 0.017
Batch: 40 | Loss: 1.112 | Acc: 80.926,91.063,93.483,% | Adaptive Acc: 86.300% | clf_exit: 0.853 0.127 0.020
Batch: 60 | Loss: 1.091 | Acc: 80.904,91.048,93.660,% | Adaptive Acc: 86.373% | clf_exit: 0.854 0.125 0.021
Train classifier parameters

Epoch: 293
Batch: 0 | Loss: 0.550 | Acc: 83.594,100.000,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.576 | Acc: 82.924,99.442,100.000,% | Adaptive Acc: 92.336% | clf_exit: 0.821 0.174 0.005
Batch: 40 | Loss: 0.578 | Acc: 82.946,99.352,99.981,% | Adaptive Acc: 92.664% | clf_exit: 0.816 0.179 0.005
Batch: 60 | Loss: 0.581 | Acc: 82.941,99.360,99.987,% | Adaptive Acc: 92.431% | clf_exit: 0.815 0.180 0.005
Batch: 80 | Loss: 0.575 | Acc: 83.266,99.344,99.981,% | Adaptive Acc: 92.612% | clf_exit: 0.818 0.177 0.005
Batch: 100 | Loss: 0.577 | Acc: 83.222,99.366,99.985,% | Adaptive Acc: 92.505% | clf_exit: 0.819 0.176 0.005
Batch: 120 | Loss: 0.576 | Acc: 83.355,99.393,99.987,% | Adaptive Acc: 92.627% | clf_exit: 0.818 0.177 0.005
Batch: 140 | Loss: 0.574 | Acc: 83.494,99.402,99.989,% | Adaptive Acc: 92.647% | clf_exit: 0.821 0.175 0.005
Batch: 160 | Loss: 0.571 | Acc: 83.574,99.350,99.985,% | Adaptive Acc: 92.673% | clf_exit: 0.823 0.172 0.005
Batch: 180 | Loss: 0.572 | Acc: 83.490,99.348,99.983,% | Adaptive Acc: 92.615% | clf_exit: 0.824 0.172 0.005
Batch: 200 | Loss: 0.570 | Acc: 83.644,99.366,99.984,% | Adaptive Acc: 92.685% | clf_exit: 0.824 0.171 0.005
Batch: 220 | Loss: 0.568 | Acc: 83.799,99.378,99.986,% | Adaptive Acc: 92.753% | clf_exit: 0.825 0.170 0.005
Batch: 240 | Loss: 0.566 | Acc: 83.876,99.387,99.987,% | Adaptive Acc: 92.803% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.566 | Acc: 83.905,99.392,99.988,% | Adaptive Acc: 92.720% | clf_exit: 0.828 0.168 0.005
Batch: 280 | Loss: 0.567 | Acc: 83.933,99.369,99.989,% | Adaptive Acc: 92.699% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.568 | Acc: 83.796,99.380,99.987,% | Adaptive Acc: 92.665% | clf_exit: 0.828 0.168 0.005
Batch: 320 | Loss: 0.567 | Acc: 83.876,99.372,99.985,% | Adaptive Acc: 92.694% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.566 | Acc: 83.882,99.361,99.986,% | Adaptive Acc: 92.694% | clf_exit: 0.828 0.167 0.005
Batch: 360 | Loss: 0.567 | Acc: 83.856,99.355,99.983,% | Adaptive Acc: 92.677% | clf_exit: 0.828 0.168 0.005
Batch: 380 | Loss: 0.567 | Acc: 83.869,99.360,99.982,% | Adaptive Acc: 92.684% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.910 | Acc: 81.250,92.969,93.750,% | Adaptive Acc: 86.719% | clf_exit: 0.898 0.094 0.008
Batch: 20 | Loss: 1.110 | Acc: 81.213,91.146,93.304,% | Adaptive Acc: 86.458% | clf_exit: 0.842 0.141 0.017
Batch: 40 | Loss: 1.117 | Acc: 81.040,90.777,93.407,% | Adaptive Acc: 86.109% | clf_exit: 0.853 0.128 0.019
Batch: 60 | Loss: 1.094 | Acc: 80.968,90.945,93.571,% | Adaptive Acc: 86.296% | clf_exit: 0.853 0.127 0.020
Train classifier parameters

Epoch: 294
Batch: 0 | Loss: 0.558 | Acc: 82.812,100.000,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.844 0.156 0.000
Batch: 20 | Loss: 0.560 | Acc: 84.784,99.219,100.000,% | Adaptive Acc: 92.783% | clf_exit: 0.829 0.166 0.005
Batch: 40 | Loss: 0.568 | Acc: 84.375,99.238,100.000,% | Adaptive Acc: 92.664% | clf_exit: 0.830 0.164 0.006
Batch: 60 | Loss: 0.572 | Acc: 84.401,99.257,100.000,% | Adaptive Acc: 92.610% | clf_exit: 0.831 0.164 0.006
Batch: 80 | Loss: 0.578 | Acc: 84.086,99.306,100.000,% | Adaptive Acc: 92.612% | clf_exit: 0.825 0.169 0.006
Batch: 100 | Loss: 0.577 | Acc: 84.042,99.327,99.992,% | Adaptive Acc: 92.675% | clf_exit: 0.824 0.170 0.005
Batch: 120 | Loss: 0.580 | Acc: 83.755,99.322,99.994,% | Adaptive Acc: 92.633% | clf_exit: 0.823 0.172 0.005
Batch: 140 | Loss: 0.578 | Acc: 83.782,99.313,99.994,% | Adaptive Acc: 92.631% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.576 | Acc: 83.899,99.306,99.995,% | Adaptive Acc: 92.692% | clf_exit: 0.824 0.170 0.005
Batch: 180 | Loss: 0.577 | Acc: 83.771,99.283,99.996,% | Adaptive Acc: 92.606% | clf_exit: 0.824 0.171 0.005
Batch: 200 | Loss: 0.572 | Acc: 83.912,99.293,99.996,% | Adaptive Acc: 92.701% | clf_exit: 0.826 0.169 0.005
Batch: 220 | Loss: 0.569 | Acc: 84.060,99.304,99.996,% | Adaptive Acc: 92.781% | clf_exit: 0.826 0.169 0.005
Batch: 240 | Loss: 0.569 | Acc: 84.041,99.326,99.997,% | Adaptive Acc: 92.820% | clf_exit: 0.826 0.169 0.005
Batch: 260 | Loss: 0.571 | Acc: 83.953,99.344,99.997,% | Adaptive Acc: 92.780% | clf_exit: 0.825 0.170 0.005
Batch: 280 | Loss: 0.570 | Acc: 83.952,99.355,99.997,% | Adaptive Acc: 92.782% | clf_exit: 0.826 0.169 0.005
Batch: 300 | Loss: 0.570 | Acc: 83.936,99.349,99.997,% | Adaptive Acc: 92.792% | clf_exit: 0.826 0.169 0.005
Batch: 320 | Loss: 0.571 | Acc: 83.952,99.350,99.995,% | Adaptive Acc: 92.762% | clf_exit: 0.826 0.168 0.005
Batch: 340 | Loss: 0.571 | Acc: 83.960,99.347,99.993,% | Adaptive Acc: 92.730% | clf_exit: 0.826 0.168 0.005
Batch: 360 | Loss: 0.571 | Acc: 83.951,99.349,99.994,% | Adaptive Acc: 92.731% | clf_exit: 0.826 0.168 0.005
Batch: 380 | Loss: 0.569 | Acc: 84.004,99.348,99.994,% | Adaptive Acc: 92.710% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.892 | Acc: 82.031,92.188,95.312,% | Adaptive Acc: 90.625% | clf_exit: 0.852 0.125 0.023
Batch: 20 | Loss: 1.100 | Acc: 80.729,91.257,93.713,% | Adaptive Acc: 86.570% | clf_exit: 0.842 0.142 0.016
Batch: 40 | Loss: 1.108 | Acc: 80.755,90.873,93.769,% | Adaptive Acc: 86.357% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.087 | Acc: 80.815,90.984,93.763,% | Adaptive Acc: 86.463% | clf_exit: 0.852 0.126 0.022
Train classifier parameters

Epoch: 295
Batch: 0 | Loss: 0.550 | Acc: 87.500,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.859 0.141 0.000
Batch: 20 | Loss: 0.546 | Acc: 84.933,99.442,100.000,% | Adaptive Acc: 93.080% | clf_exit: 0.834 0.163 0.003
Batch: 40 | Loss: 0.570 | Acc: 83.918,99.295,100.000,% | Adaptive Acc: 92.912% | clf_exit: 0.824 0.171 0.004
Batch: 60 | Loss: 0.566 | Acc: 84.055,99.360,100.000,% | Adaptive Acc: 93.033% | clf_exit: 0.820 0.176 0.005
Batch: 80 | Loss: 0.565 | Acc: 84.047,99.412,100.000,% | Adaptive Acc: 93.056% | clf_exit: 0.821 0.174 0.005
Batch: 100 | Loss: 0.565 | Acc: 84.081,99.389,99.985,% | Adaptive Acc: 92.907% | clf_exit: 0.823 0.172 0.005
Batch: 120 | Loss: 0.565 | Acc: 84.020,99.361,99.987,% | Adaptive Acc: 92.853% | clf_exit: 0.824 0.171 0.005
Batch: 140 | Loss: 0.566 | Acc: 83.954,99.357,99.989,% | Adaptive Acc: 92.808% | clf_exit: 0.825 0.170 0.005
Batch: 160 | Loss: 0.567 | Acc: 83.948,99.369,99.985,% | Adaptive Acc: 92.707% | clf_exit: 0.826 0.169 0.005
Batch: 180 | Loss: 0.567 | Acc: 83.978,99.357,99.987,% | Adaptive Acc: 92.705% | clf_exit: 0.826 0.169 0.005
Batch: 200 | Loss: 0.567 | Acc: 83.963,99.355,99.988,% | Adaptive Acc: 92.654% | clf_exit: 0.828 0.167 0.005
Batch: 220 | Loss: 0.565 | Acc: 84.032,99.357,99.989,% | Adaptive Acc: 92.668% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.566 | Acc: 83.992,99.355,99.990,% | Adaptive Acc: 92.564% | clf_exit: 0.829 0.166 0.005
Batch: 260 | Loss: 0.567 | Acc: 83.923,99.347,99.991,% | Adaptive Acc: 92.532% | clf_exit: 0.829 0.166 0.005
Batch: 280 | Loss: 0.568 | Acc: 83.897,99.355,99.992,% | Adaptive Acc: 92.560% | clf_exit: 0.828 0.167 0.005
Batch: 300 | Loss: 0.570 | Acc: 83.814,99.362,99.992,% | Adaptive Acc: 92.574% | clf_exit: 0.827 0.168 0.005
Batch: 320 | Loss: 0.569 | Acc: 83.871,99.365,99.983,% | Adaptive Acc: 92.572% | clf_exit: 0.828 0.168 0.005
Batch: 340 | Loss: 0.570 | Acc: 83.896,99.359,99.984,% | Adaptive Acc: 92.561% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.569 | Acc: 83.953,99.359,99.985,% | Adaptive Acc: 92.594% | clf_exit: 0.828 0.168 0.005
Batch: 380 | Loss: 0.569 | Acc: 83.926,99.354,99.986,% | Adaptive Acc: 92.598% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.897 | Acc: 82.031,92.188,93.750,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.102 | Acc: 81.250,91.109,93.527,% | Adaptive Acc: 87.054% | clf_exit: 0.839 0.144 0.017
Batch: 40 | Loss: 1.109 | Acc: 80.964,90.835,93.540,% | Adaptive Acc: 86.566% | clf_exit: 0.849 0.131 0.021
Batch: 60 | Loss: 1.089 | Acc: 81.032,90.932,93.699,% | Adaptive Acc: 86.668% | clf_exit: 0.850 0.129 0.021
Train classifier parameters

Epoch: 296
Batch: 0 | Loss: 0.617 | Acc: 80.469,100.000,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.820 0.180 0.000
Batch: 20 | Loss: 0.572 | Acc: 83.705,99.070,100.000,% | Adaptive Acc: 92.597% | clf_exit: 0.827 0.166 0.007
Batch: 40 | Loss: 0.579 | Acc: 83.479,99.219,99.981,% | Adaptive Acc: 92.683% | clf_exit: 0.820 0.174 0.007
Batch: 60 | Loss: 0.567 | Acc: 83.901,99.308,99.987,% | Adaptive Acc: 92.738% | clf_exit: 0.826 0.168 0.006
Batch: 80 | Loss: 0.578 | Acc: 83.806,99.248,99.990,% | Adaptive Acc: 92.544% | clf_exit: 0.825 0.170 0.006
Batch: 100 | Loss: 0.578 | Acc: 83.733,99.265,99.992,% | Adaptive Acc: 92.590% | clf_exit: 0.824 0.171 0.005
Batch: 120 | Loss: 0.573 | Acc: 83.781,99.303,99.981,% | Adaptive Acc: 92.607% | clf_exit: 0.826 0.169 0.005
Batch: 140 | Loss: 0.577 | Acc: 83.693,99.280,99.983,% | Adaptive Acc: 92.520% | clf_exit: 0.824 0.171 0.005
Batch: 160 | Loss: 0.581 | Acc: 83.560,99.248,99.981,% | Adaptive Acc: 92.488% | clf_exit: 0.823 0.172 0.005
Batch: 180 | Loss: 0.582 | Acc: 83.564,99.232,99.983,% | Adaptive Acc: 92.498% | clf_exit: 0.823 0.171 0.005
Batch: 200 | Loss: 0.582 | Acc: 83.423,99.281,99.977,% | Adaptive Acc: 92.436% | clf_exit: 0.824 0.171 0.005
Batch: 220 | Loss: 0.581 | Acc: 83.527,99.304,99.975,% | Adaptive Acc: 92.488% | clf_exit: 0.824 0.171 0.005
Batch: 240 | Loss: 0.580 | Acc: 83.574,99.322,99.977,% | Adaptive Acc: 92.560% | clf_exit: 0.823 0.172 0.005
Batch: 260 | Loss: 0.578 | Acc: 83.609,99.332,99.976,% | Adaptive Acc: 92.592% | clf_exit: 0.824 0.171 0.005
Batch: 280 | Loss: 0.576 | Acc: 83.597,99.336,99.978,% | Adaptive Acc: 92.491% | clf_exit: 0.825 0.170 0.005
Batch: 300 | Loss: 0.575 | Acc: 83.653,99.351,99.979,% | Adaptive Acc: 92.530% | clf_exit: 0.826 0.169 0.005
Batch: 320 | Loss: 0.573 | Acc: 83.735,99.348,99.978,% | Adaptive Acc: 92.555% | clf_exit: 0.826 0.169 0.005
Batch: 340 | Loss: 0.573 | Acc: 83.736,99.352,99.977,% | Adaptive Acc: 92.529% | clf_exit: 0.827 0.169 0.005
Batch: 360 | Loss: 0.573 | Acc: 83.750,99.342,99.974,% | Adaptive Acc: 92.581% | clf_exit: 0.826 0.169 0.005
Batch: 380 | Loss: 0.574 | Acc: 83.709,99.348,99.973,% | Adaptive Acc: 92.575% | clf_exit: 0.826 0.169 0.005
Batch: 0 | Loss: 0.882 | Acc: 82.031,92.969,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.867 0.117 0.016
Batch: 20 | Loss: 1.101 | Acc: 80.990,91.220,93.638,% | Adaptive Acc: 86.719% | clf_exit: 0.837 0.145 0.018
Batch: 40 | Loss: 1.110 | Acc: 80.736,90.911,93.617,% | Adaptive Acc: 86.471% | clf_exit: 0.846 0.133 0.021
Batch: 60 | Loss: 1.089 | Acc: 80.981,91.048,93.635,% | Adaptive Acc: 86.616% | clf_exit: 0.848 0.131 0.021
Train classifier parameters

Epoch: 297
Batch: 0 | Loss: 0.606 | Acc: 79.688,100.000,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.820 0.180 0.000
Batch: 20 | Loss: 0.584 | Acc: 83.222,99.405,99.963,% | Adaptive Acc: 92.150% | clf_exit: 0.824 0.172 0.004
Batch: 40 | Loss: 0.562 | Acc: 84.451,99.333,99.981,% | Adaptive Acc: 92.797% | clf_exit: 0.830 0.165 0.005
Batch: 60 | Loss: 0.567 | Acc: 84.080,99.321,99.987,% | Adaptive Acc: 92.610% | clf_exit: 0.829 0.165 0.005
Batch: 80 | Loss: 0.572 | Acc: 83.922,99.248,99.990,% | Adaptive Acc: 92.486% | clf_exit: 0.831 0.164 0.005
Batch: 100 | Loss: 0.568 | Acc: 83.996,99.281,99.977,% | Adaptive Acc: 92.621% | clf_exit: 0.832 0.163 0.005
Batch: 120 | Loss: 0.566 | Acc: 84.039,99.309,99.981,% | Adaptive Acc: 92.594% | clf_exit: 0.832 0.163 0.005
Batch: 140 | Loss: 0.566 | Acc: 84.137,99.330,99.978,% | Adaptive Acc: 92.697% | clf_exit: 0.832 0.163 0.005
Batch: 160 | Loss: 0.563 | Acc: 84.229,99.340,99.981,% | Adaptive Acc: 92.755% | clf_exit: 0.833 0.162 0.005
Batch: 180 | Loss: 0.562 | Acc: 84.189,99.344,99.974,% | Adaptive Acc: 92.727% | clf_exit: 0.833 0.162 0.005
Batch: 200 | Loss: 0.563 | Acc: 84.173,99.335,99.977,% | Adaptive Acc: 92.798% | clf_exit: 0.833 0.162 0.005
Batch: 220 | Loss: 0.563 | Acc: 84.170,99.335,99.979,% | Adaptive Acc: 92.700% | clf_exit: 0.833 0.162 0.005
Batch: 240 | Loss: 0.563 | Acc: 84.177,99.345,99.981,% | Adaptive Acc: 92.687% | clf_exit: 0.834 0.161 0.005
Batch: 260 | Loss: 0.563 | Acc: 84.189,99.347,99.982,% | Adaptive Acc: 92.708% | clf_exit: 0.833 0.162 0.005
Batch: 280 | Loss: 0.564 | Acc: 84.192,99.352,99.983,% | Adaptive Acc: 92.760% | clf_exit: 0.832 0.163 0.005
Batch: 300 | Loss: 0.566 | Acc: 84.084,99.343,99.982,% | Adaptive Acc: 92.738% | clf_exit: 0.831 0.164 0.005
Batch: 320 | Loss: 0.567 | Acc: 84.122,99.355,99.983,% | Adaptive Acc: 92.781% | clf_exit: 0.831 0.164 0.005
Batch: 340 | Loss: 0.566 | Acc: 84.114,99.354,99.982,% | Adaptive Acc: 92.792% | clf_exit: 0.830 0.165 0.005
Batch: 360 | Loss: 0.567 | Acc: 84.074,99.366,99.983,% | Adaptive Acc: 92.778% | clf_exit: 0.829 0.166 0.005
Batch: 380 | Loss: 0.567 | Acc: 83.998,99.375,99.984,% | Adaptive Acc: 92.727% | clf_exit: 0.829 0.166 0.005
Batch: 0 | Loss: 0.889 | Acc: 82.031,92.969,94.531,% | Adaptive Acc: 89.844% | clf_exit: 0.859 0.117 0.023
Batch: 20 | Loss: 1.106 | Acc: 80.580,91.295,93.638,% | Adaptive Acc: 86.347% | clf_exit: 0.842 0.140 0.019
Batch: 40 | Loss: 1.113 | Acc: 80.640,90.892,93.674,% | Adaptive Acc: 86.166% | clf_exit: 0.850 0.130 0.020
Batch: 60 | Loss: 1.092 | Acc: 80.763,90.984,93.712,% | Adaptive Acc: 86.283% | clf_exit: 0.852 0.127 0.021
Train classifier parameters

Epoch: 298
Batch: 0 | Loss: 0.478 | Acc: 89.062,100.000,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.867 0.133 0.000
Batch: 20 | Loss: 0.577 | Acc: 83.110,99.479,100.000,% | Adaptive Acc: 92.634% | clf_exit: 0.812 0.186 0.002
Batch: 40 | Loss: 0.575 | Acc: 83.632,99.428,100.000,% | Adaptive Acc: 92.797% | clf_exit: 0.821 0.175 0.004
Batch: 60 | Loss: 0.573 | Acc: 83.671,99.308,99.987,% | Adaptive Acc: 92.738% | clf_exit: 0.823 0.172 0.005
Batch: 80 | Loss: 0.574 | Acc: 83.690,99.277,99.981,% | Adaptive Acc: 92.583% | clf_exit: 0.824 0.170 0.005
Batch: 100 | Loss: 0.569 | Acc: 83.965,99.296,99.985,% | Adaptive Acc: 92.659% | clf_exit: 0.827 0.167 0.005
Batch: 120 | Loss: 0.571 | Acc: 83.917,99.251,99.981,% | Adaptive Acc: 92.633% | clf_exit: 0.827 0.167 0.006
Batch: 140 | Loss: 0.569 | Acc: 83.943,99.280,99.978,% | Adaptive Acc: 92.681% | clf_exit: 0.827 0.168 0.005
Batch: 160 | Loss: 0.568 | Acc: 83.953,99.296,99.981,% | Adaptive Acc: 92.615% | clf_exit: 0.827 0.167 0.005
Batch: 180 | Loss: 0.567 | Acc: 83.999,99.301,99.974,% | Adaptive Acc: 92.684% | clf_exit: 0.827 0.168 0.005
Batch: 200 | Loss: 0.568 | Acc: 84.014,99.304,99.977,% | Adaptive Acc: 92.638% | clf_exit: 0.827 0.168 0.005
Batch: 220 | Loss: 0.568 | Acc: 84.053,99.311,99.979,% | Adaptive Acc: 92.644% | clf_exit: 0.828 0.167 0.005
Batch: 240 | Loss: 0.568 | Acc: 84.015,99.310,99.977,% | Adaptive Acc: 92.687% | clf_exit: 0.827 0.168 0.005
Batch: 260 | Loss: 0.568 | Acc: 84.079,99.303,99.979,% | Adaptive Acc: 92.678% | clf_exit: 0.827 0.167 0.005
Batch: 280 | Loss: 0.568 | Acc: 84.083,99.305,99.975,% | Adaptive Acc: 92.657% | clf_exit: 0.827 0.168 0.005
Batch: 300 | Loss: 0.569 | Acc: 84.069,99.317,99.974,% | Adaptive Acc: 92.644% | clf_exit: 0.827 0.167 0.005
Batch: 320 | Loss: 0.569 | Acc: 84.066,99.331,99.973,% | Adaptive Acc: 92.628% | clf_exit: 0.827 0.168 0.005
Batch: 340 | Loss: 0.570 | Acc: 84.036,99.324,99.970,% | Adaptive Acc: 92.641% | clf_exit: 0.827 0.168 0.005
Batch: 360 | Loss: 0.571 | Acc: 83.998,99.318,99.968,% | Adaptive Acc: 92.603% | clf_exit: 0.827 0.168 0.005
Batch: 380 | Loss: 0.571 | Acc: 83.973,99.305,99.969,% | Adaptive Acc: 92.575% | clf_exit: 0.827 0.168 0.005
Batch: 0 | Loss: 0.881 | Acc: 82.031,92.969,96.094,% | Adaptive Acc: 90.625% | clf_exit: 0.859 0.109 0.031
Batch: 20 | Loss: 1.104 | Acc: 80.618,91.146,93.601,% | Adaptive Acc: 86.198% | clf_exit: 0.842 0.140 0.018
Batch: 40 | Loss: 1.112 | Acc: 80.774,90.854,93.693,% | Adaptive Acc: 86.300% | clf_exit: 0.849 0.130 0.022
Batch: 60 | Loss: 1.094 | Acc: 81.019,90.945,93.686,% | Adaptive Acc: 86.463% | clf_exit: 0.850 0.128 0.022
Train classifier parameters

Epoch: 299
Batch: 0 | Loss: 0.648 | Acc: 82.812,100.000,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.805 0.195 0.000
Batch: 20 | Loss: 0.576 | Acc: 83.408,99.479,99.963,% | Adaptive Acc: 92.894% | clf_exit: 0.822 0.173 0.006
Batch: 40 | Loss: 0.556 | Acc: 84.165,99.390,99.981,% | Adaptive Acc: 93.350% | clf_exit: 0.826 0.168 0.006
Batch: 60 | Loss: 0.566 | Acc: 83.927,99.372,99.987,% | Adaptive Acc: 93.135% | clf_exit: 0.825 0.169 0.006
Batch: 80 | Loss: 0.568 | Acc: 83.758,99.373,99.990,% | Adaptive Acc: 93.065% | clf_exit: 0.825 0.170 0.005
Batch: 100 | Loss: 0.567 | Acc: 83.694,99.420,99.992,% | Adaptive Acc: 92.876% | clf_exit: 0.826 0.169 0.005
Batch: 120 | Loss: 0.566 | Acc: 83.749,99.412,99.994,% | Adaptive Acc: 92.846% | clf_exit: 0.827 0.168 0.005
Batch: 140 | Loss: 0.566 | Acc: 83.732,99.385,99.994,% | Adaptive Acc: 92.803% | clf_exit: 0.827 0.168 0.005
Batch: 160 | Loss: 0.564 | Acc: 83.836,99.423,99.995,% | Adaptive Acc: 92.823% | clf_exit: 0.828 0.167 0.005
Batch: 180 | Loss: 0.562 | Acc: 83.913,99.417,99.996,% | Adaptive Acc: 92.800% | clf_exit: 0.829 0.167 0.005
Batch: 200 | Loss: 0.564 | Acc: 83.881,99.405,99.992,% | Adaptive Acc: 92.685% | clf_exit: 0.829 0.167 0.005
Batch: 220 | Loss: 0.566 | Acc: 83.838,99.388,99.993,% | Adaptive Acc: 92.697% | clf_exit: 0.828 0.168 0.005
Batch: 240 | Loss: 0.566 | Acc: 83.902,99.374,99.987,% | Adaptive Acc: 92.709% | clf_exit: 0.828 0.167 0.005
Batch: 260 | Loss: 0.565 | Acc: 83.947,99.395,99.988,% | Adaptive Acc: 92.726% | clf_exit: 0.828 0.167 0.005
Batch: 280 | Loss: 0.564 | Acc: 84.044,99.397,99.989,% | Adaptive Acc: 92.777% | clf_exit: 0.829 0.167 0.005
Batch: 300 | Loss: 0.565 | Acc: 83.980,99.382,99.987,% | Adaptive Acc: 92.751% | clf_exit: 0.828 0.167 0.005
Batch: 320 | Loss: 0.566 | Acc: 83.988,99.377,99.988,% | Adaptive Acc: 92.755% | clf_exit: 0.828 0.167 0.005
Batch: 340 | Loss: 0.565 | Acc: 84.057,99.368,99.989,% | Adaptive Acc: 92.763% | clf_exit: 0.829 0.166 0.005
Batch: 360 | Loss: 0.567 | Acc: 83.996,99.364,99.989,% | Adaptive Acc: 92.737% | clf_exit: 0.828 0.167 0.005
Batch: 380 | Loss: 0.568 | Acc: 83.994,99.358,99.990,% | Adaptive Acc: 92.710% | clf_exit: 0.828 0.167 0.005
Batch: 0 | Loss: 0.883 | Acc: 82.812,92.969,94.531,% | Adaptive Acc: 89.062% | clf_exit: 0.875 0.102 0.023
Batch: 20 | Loss: 1.112 | Acc: 80.655,91.220,93.452,% | Adaptive Acc: 86.458% | clf_exit: 0.840 0.141 0.018
Batch: 40 | Loss: 1.115 | Acc: 80.774,90.796,93.579,% | Adaptive Acc: 86.319% | clf_exit: 0.850 0.128 0.022
Batch: 60 | Loss: 1.092 | Acc: 80.853,90.920,93.648,% | Adaptive Acc: 86.411% | clf_exit: 0.851 0.127 0.022
