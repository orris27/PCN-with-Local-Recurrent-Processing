==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=128, out_features=100, bias=True)
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=256, out_features=100, bias=True)
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=512, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Train all parameters

Epoch: 0
Batch: 0 | Loss: 13.963 | Acc: 0.000,0.000,0.781,% | Adaptive Acc: 0.781% | clf_exit: 0.000 0.000 1.000
Batch: 20 | Loss: 13.663 | Acc: 1.004,1.860,4.204,% | Adaptive Acc: 4.204% | clf_exit: 0.000 0.000 1.000
Batch: 40 | Loss: 13.416 | Acc: 1.963,3.049,5.412,% | Adaptive Acc: 5.412% | clf_exit: 0.000 0.000 1.000
Batch: 60 | Loss: 13.221 | Acc: 2.907,4.188,6.506,% | Adaptive Acc: 6.519% | clf_exit: 0.000 0.001 0.999
Batch: 80 | Loss: 13.065 | Acc: 3.279,4.794,7.301,% | Adaptive Acc: 7.311% | clf_exit: 0.000 0.002 0.998
Batch: 100 | Loss: 12.938 | Acc: 3.465,5.399,7.890,% | Adaptive Acc: 7.905% | clf_exit: 0.001 0.002 0.998
Batch: 120 | Loss: 12.827 | Acc: 3.777,5.882,8.445,% | Adaptive Acc: 8.458% | clf_exit: 0.001 0.002 0.998
Batch: 140 | Loss: 12.735 | Acc: 4.089,6.278,8.976,% | Adaptive Acc: 8.987% | clf_exit: 0.001 0.002 0.997
Batch: 160 | Loss: 12.646 | Acc: 4.440,6.706,9.385,% | Adaptive Acc: 9.399% | clf_exit: 0.001 0.002 0.997
Batch: 180 | Loss: 12.571 | Acc: 4.748,7.135,9.725,% | Adaptive Acc: 9.738% | clf_exit: 0.002 0.003 0.996
Batch: 200 | Loss: 12.498 | Acc: 5.006,7.432,9.997,% | Adaptive Acc: 10.005% | clf_exit: 0.002 0.003 0.995
Batch: 220 | Loss: 12.436 | Acc: 5.158,7.664,10.252,% | Adaptive Acc: 10.262% | clf_exit: 0.002 0.003 0.995
Batch: 240 | Loss: 12.373 | Acc: 5.423,7.949,10.532,% | Adaptive Acc: 10.548% | clf_exit: 0.002 0.003 0.995
Batch: 260 | Loss: 12.314 | Acc: 5.645,8.226,10.779,% | Adaptive Acc: 10.794% | clf_exit: 0.002 0.003 0.995
Batch: 280 | Loss: 12.257 | Acc: 5.833,8.477,11.029,% | Adaptive Acc: 11.040% | clf_exit: 0.002 0.003 0.995
Batch: 300 | Loss: 12.206 | Acc: 6.009,8.711,11.288,% | Adaptive Acc: 11.298% | clf_exit: 0.002 0.003 0.995
Batch: 320 | Loss: 12.159 | Acc: 6.150,8.954,11.519,% | Adaptive Acc: 11.531% | clf_exit: 0.002 0.003 0.995
Batch: 340 | Loss: 12.113 | Acc: 6.339,9.169,11.810,% | Adaptive Acc: 11.817% | clf_exit: 0.002 0.003 0.995
Batch: 360 | Loss: 12.071 | Acc: 6.482,9.407,12.056,% | Adaptive Acc: 12.063% | clf_exit: 0.002 0.004 0.994
Batch: 380 | Loss: 12.029 | Acc: 6.617,9.613,12.219,% | Adaptive Acc: 12.229% | clf_exit: 0.002 0.004 0.994
Batch: 0 | Loss: 11.218 | Acc: 7.812,13.281,13.281,% | Adaptive Acc: 13.281% | clf_exit: 0.000 0.016 0.984
Batch: 20 | Loss: 11.333 | Acc: 9.152,12.351,14.769,% | Adaptive Acc: 14.844% | clf_exit: 0.006 0.012 0.982
Batch: 40 | Loss: 11.300 | Acc: 8.479,12.462,15.473,% | Adaptive Acc: 15.530% | clf_exit: 0.005 0.012 0.984
Batch: 60 | Loss: 11.306 | Acc: 8.632,12.538,15.612,% | Adaptive Acc: 15.651% | clf_exit: 0.004 0.012 0.984
Train all parameters

Epoch: 1
Batch: 0 | Loss: 10.732 | Acc: 10.156,11.719,19.531,% | Adaptive Acc: 19.531% | clf_exit: 0.000 0.000 1.000
Batch: 20 | Loss: 11.065 | Acc: 9.040,13.802,17.560,% | Adaptive Acc: 17.597% | clf_exit: 0.006 0.006 0.988
Batch: 40 | Loss: 11.038 | Acc: 9.623,14.482,18.083,% | Adaptive Acc: 18.102% | clf_exit: 0.004 0.006 0.990
Batch: 60 | Loss: 11.045 | Acc: 9.618,14.242,17.802,% | Adaptive Acc: 17.841% | clf_exit: 0.004 0.007 0.989
Batch: 80 | Loss: 11.034 | Acc: 9.645,14.400,17.930,% | Adaptive Acc: 17.969% | clf_exit: 0.004 0.006 0.990
Batch: 100 | Loss: 11.013 | Acc: 9.615,14.534,18.239,% | Adaptive Acc: 18.270% | clf_exit: 0.004 0.006 0.990
Batch: 120 | Loss: 10.987 | Acc: 9.879,14.837,18.634,% | Adaptive Acc: 18.666% | clf_exit: 0.004 0.006 0.990
Batch: 140 | Loss: 10.971 | Acc: 10.029,15.038,18.722,% | Adaptive Acc: 18.750% | clf_exit: 0.004 0.007 0.989
Batch: 160 | Loss: 10.951 | Acc: 9.948,15.188,18.997,% | Adaptive Acc: 19.017% | clf_exit: 0.004 0.007 0.989
Batch: 180 | Loss: 10.930 | Acc: 9.953,15.366,19.052,% | Adaptive Acc: 19.069% | clf_exit: 0.005 0.006 0.989
Batch: 200 | Loss: 10.908 | Acc: 10.047,15.543,19.364,% | Adaptive Acc: 19.380% | clf_exit: 0.005 0.007 0.989
Batch: 220 | Loss: 10.874 | Acc: 10.234,15.791,19.669,% | Adaptive Acc: 19.683% | clf_exit: 0.005 0.007 0.988
Batch: 240 | Loss: 10.842 | Acc: 10.309,15.926,19.898,% | Adaptive Acc: 19.914% | clf_exit: 0.005 0.007 0.988
Batch: 260 | Loss: 10.813 | Acc: 10.420,16.149,20.169,% | Adaptive Acc: 20.187% | clf_exit: 0.005 0.008 0.987
Batch: 280 | Loss: 10.787 | Acc: 10.501,16.367,20.390,% | Adaptive Acc: 20.413% | clf_exit: 0.005 0.008 0.987
Batch: 300 | Loss: 10.765 | Acc: 10.494,16.440,20.554,% | Adaptive Acc: 20.575% | clf_exit: 0.005 0.008 0.987
Batch: 320 | Loss: 10.747 | Acc: 10.526,16.564,20.709,% | Adaptive Acc: 20.726% | clf_exit: 0.005 0.008 0.987
Batch: 340 | Loss: 10.723 | Acc: 10.543,16.697,20.927,% | Adaptive Acc: 20.943% | clf_exit: 0.005 0.009 0.987
Batch: 360 | Loss: 10.699 | Acc: 10.691,16.910,21.204,% | Adaptive Acc: 21.217% | clf_exit: 0.005 0.009 0.986
Batch: 380 | Loss: 10.674 | Acc: 10.794,17.079,21.457,% | Adaptive Acc: 21.467% | clf_exit: 0.005 0.009 0.986
Batch: 0 | Loss: 10.179 | Acc: 10.938,20.312,26.562,% | Adaptive Acc: 26.562% | clf_exit: 0.000 0.031 0.969
Batch: 20 | Loss: 10.339 | Acc: 12.574,18.378,24.033,% | Adaptive Acc: 24.107% | clf_exit: 0.006 0.038 0.956
Batch: 40 | Loss: 10.299 | Acc: 11.662,18.369,24.352,% | Adaptive Acc: 24.409% | clf_exit: 0.006 0.034 0.960
Batch: 60 | Loss: 10.299 | Acc: 11.552,18.878,24.257,% | Adaptive Acc: 24.283% | clf_exit: 0.006 0.034 0.960
Train all parameters

Epoch: 2
Batch: 0 | Loss: 10.252 | Acc: 11.719,20.312,25.781,% | Adaptive Acc: 25.781% | clf_exit: 0.008 0.016 0.977
Batch: 20 | Loss: 10.067 | Acc: 12.426,19.792,25.818,% | Adaptive Acc: 25.818% | clf_exit: 0.004 0.015 0.981
Batch: 40 | Loss: 10.102 | Acc: 11.433,19.912,26.562,% | Adaptive Acc: 26.524% | clf_exit: 0.006 0.013 0.981
Batch: 60 | Loss: 10.061 | Acc: 12.077,20.492,26.934,% | Adaptive Acc: 26.895% | clf_exit: 0.006 0.012 0.982
Batch: 80 | Loss: 10.060 | Acc: 12.008,20.438,26.784,% | Adaptive Acc: 26.746% | clf_exit: 0.006 0.015 0.979
Batch: 100 | Loss: 10.044 | Acc: 12.175,20.653,26.787,% | Adaptive Acc: 26.756% | clf_exit: 0.006 0.017 0.977
Batch: 120 | Loss: 10.046 | Acc: 12.164,20.693,26.956,% | Adaptive Acc: 26.943% | clf_exit: 0.005 0.016 0.978
Batch: 140 | Loss: 10.016 | Acc: 12.472,21.044,27.189,% | Adaptive Acc: 27.178% | clf_exit: 0.005 0.018 0.977
Batch: 160 | Loss: 10.007 | Acc: 12.515,21.118,27.329,% | Adaptive Acc: 27.305% | clf_exit: 0.005 0.018 0.976
Batch: 180 | Loss: 9.998 | Acc: 12.569,21.146,27.331,% | Adaptive Acc: 27.309% | clf_exit: 0.005 0.019 0.976
Batch: 200 | Loss: 9.998 | Acc: 12.570,21.144,27.126,% | Adaptive Acc: 27.103% | clf_exit: 0.006 0.019 0.975
Batch: 220 | Loss: 9.974 | Acc: 12.666,21.207,27.287,% | Adaptive Acc: 27.266% | clf_exit: 0.005 0.020 0.975
Batch: 240 | Loss: 9.958 | Acc: 12.672,21.288,27.396,% | Adaptive Acc: 27.383% | clf_exit: 0.005 0.020 0.975
Batch: 260 | Loss: 9.944 | Acc: 12.757,21.441,27.481,% | Adaptive Acc: 27.469% | clf_exit: 0.005 0.020 0.974
Batch: 280 | Loss: 9.933 | Acc: 12.806,21.533,27.602,% | Adaptive Acc: 27.591% | clf_exit: 0.006 0.021 0.973
Batch: 300 | Loss: 9.922 | Acc: 12.788,21.680,27.743,% | Adaptive Acc: 27.736% | clf_exit: 0.006 0.021 0.973
Batch: 320 | Loss: 9.902 | Acc: 12.921,21.756,27.872,% | Adaptive Acc: 27.862% | clf_exit: 0.006 0.021 0.973
Batch: 340 | Loss: 9.884 | Acc: 12.954,21.891,28.049,% | Adaptive Acc: 28.043% | clf_exit: 0.006 0.022 0.972
Batch: 360 | Loss: 9.871 | Acc: 12.939,21.920,28.110,% | Adaptive Acc: 28.090% | clf_exit: 0.006 0.022 0.971
Batch: 380 | Loss: 9.859 | Acc: 12.994,21.973,28.223,% | Adaptive Acc: 28.203% | clf_exit: 0.007 0.022 0.971
Batch: 0 | Loss: 10.011 | Acc: 15.625,23.438,24.219,% | Adaptive Acc: 24.219% | clf_exit: 0.008 0.070 0.922
Batch: 20 | Loss: 9.692 | Acc: 13.207,22.879,28.051,% | Adaptive Acc: 28.051% | clf_exit: 0.015 0.056 0.929
Batch: 40 | Loss: 9.691 | Acc: 13.681,22.523,28.239,% | Adaptive Acc: 28.296% | clf_exit: 0.014 0.056 0.930
Batch: 60 | Loss: 9.717 | Acc: 13.525,22.336,27.843,% | Adaptive Acc: 27.933% | clf_exit: 0.013 0.057 0.930
Train all parameters

Epoch: 3
Batch: 0 | Loss: 9.610 | Acc: 16.406,25.000,35.938,% | Adaptive Acc: 35.938% | clf_exit: 0.016 0.039 0.945
Batch: 20 | Loss: 9.431 | Acc: 14.137,25.037,33.929,% | Adaptive Acc: 34.077% | clf_exit: 0.011 0.031 0.959
Batch: 40 | Loss: 9.390 | Acc: 14.329,24.809,33.060,% | Adaptive Acc: 33.041% | clf_exit: 0.010 0.031 0.959
Batch: 60 | Loss: 9.390 | Acc: 14.447,25.000,32.748,% | Adaptive Acc: 32.748% | clf_exit: 0.010 0.032 0.958
Batch: 80 | Loss: 9.405 | Acc: 14.390,25.010,32.456,% | Adaptive Acc: 32.456% | clf_exit: 0.009 0.032 0.959
Batch: 100 | Loss: 9.406 | Acc: 14.341,25.248,32.356,% | Adaptive Acc: 32.341% | clf_exit: 0.009 0.033 0.958
Batch: 120 | Loss: 9.379 | Acc: 14.463,25.510,32.761,% | Adaptive Acc: 32.735% | clf_exit: 0.008 0.034 0.958
Batch: 140 | Loss: 9.357 | Acc: 14.423,25.576,32.968,% | Adaptive Acc: 32.929% | clf_exit: 0.008 0.036 0.955
Batch: 160 | Loss: 9.342 | Acc: 14.426,25.713,33.099,% | Adaptive Acc: 33.055% | clf_exit: 0.008 0.036 0.955
Batch: 180 | Loss: 9.333 | Acc: 14.550,25.816,33.197,% | Adaptive Acc: 33.153% | clf_exit: 0.009 0.037 0.955
Batch: 200 | Loss: 9.320 | Acc: 14.750,25.855,33.287,% | Adaptive Acc: 33.248% | clf_exit: 0.009 0.037 0.954
Batch: 220 | Loss: 9.305 | Acc: 14.854,25.891,33.371,% | Adaptive Acc: 33.332% | clf_exit: 0.009 0.038 0.953
Batch: 240 | Loss: 9.301 | Acc: 14.980,25.989,33.412,% | Adaptive Acc: 33.370% | clf_exit: 0.009 0.038 0.954
Batch: 260 | Loss: 9.294 | Acc: 14.972,26.039,33.396,% | Adaptive Acc: 33.360% | clf_exit: 0.008 0.038 0.953
Batch: 280 | Loss: 9.289 | Acc: 15.016,26.051,33.394,% | Adaptive Acc: 33.363% | clf_exit: 0.008 0.039 0.953
Batch: 300 | Loss: 9.278 | Acc: 15.051,26.178,33.552,% | Adaptive Acc: 33.516% | clf_exit: 0.008 0.039 0.953
Batch: 320 | Loss: 9.275 | Acc: 15.068,26.188,33.533,% | Adaptive Acc: 33.499% | clf_exit: 0.008 0.040 0.952
Batch: 340 | Loss: 9.259 | Acc: 15.123,26.320,33.649,% | Adaptive Acc: 33.612% | clf_exit: 0.008 0.040 0.952
Batch: 360 | Loss: 9.247 | Acc: 15.205,26.415,33.732,% | Adaptive Acc: 33.702% | clf_exit: 0.008 0.040 0.951
Batch: 380 | Loss: 9.231 | Acc: 15.283,26.489,33.815,% | Adaptive Acc: 33.789% | clf_exit: 0.008 0.041 0.951
Batch: 0 | Loss: 8.949 | Acc: 17.188,28.906,34.375,% | Adaptive Acc: 35.156% | clf_exit: 0.008 0.102 0.891
Batch: 20 | Loss: 9.007 | Acc: 16.109,27.121,35.268,% | Adaptive Acc: 35.082% | clf_exit: 0.014 0.080 0.906
Batch: 40 | Loss: 8.994 | Acc: 15.511,27.210,35.004,% | Adaptive Acc: 34.909% | clf_exit: 0.012 0.078 0.910
Batch: 60 | Loss: 9.003 | Acc: 15.766,27.408,34.452,% | Adaptive Acc: 34.285% | clf_exit: 0.013 0.075 0.912
Train all parameters

Epoch: 4
Batch: 0 | Loss: 8.749 | Acc: 21.094,32.812,40.625,% | Adaptive Acc: 39.844% | clf_exit: 0.016 0.070 0.914
Batch: 20 | Loss: 8.879 | Acc: 17.411,27.344,37.240,% | Adaptive Acc: 37.202% | clf_exit: 0.007 0.051 0.942
Batch: 40 | Loss: 8.841 | Acc: 17.473,28.182,37.329,% | Adaptive Acc: 37.329% | clf_exit: 0.008 0.053 0.939
Batch: 60 | Loss: 8.868 | Acc: 16.931,28.368,37.231,% | Adaptive Acc: 37.231% | clf_exit: 0.008 0.050 0.941
Batch: 80 | Loss: 8.849 | Acc: 16.898,28.694,37.220,% | Adaptive Acc: 37.220% | clf_exit: 0.009 0.051 0.940
Batch: 100 | Loss: 8.845 | Acc: 16.878,28.767,37.160,% | Adaptive Acc: 37.160% | clf_exit: 0.011 0.053 0.937
Batch: 120 | Loss: 8.829 | Acc: 16.936,28.777,37.442,% | Adaptive Acc: 37.397% | clf_exit: 0.011 0.053 0.936
Batch: 140 | Loss: 8.805 | Acc: 16.977,29.045,37.699,% | Adaptive Acc: 37.661% | clf_exit: 0.011 0.054 0.935
Batch: 160 | Loss: 8.782 | Acc: 17.139,29.479,37.898,% | Adaptive Acc: 37.874% | clf_exit: 0.012 0.056 0.933
Batch: 180 | Loss: 8.760 | Acc: 17.097,29.575,38.078,% | Adaptive Acc: 38.044% | clf_exit: 0.011 0.057 0.932
Batch: 200 | Loss: 8.762 | Acc: 17.063,29.649,38.048,% | Adaptive Acc: 38.005% | clf_exit: 0.012 0.057 0.932
Batch: 220 | Loss: 8.754 | Acc: 17.262,29.751,38.108,% | Adaptive Acc: 38.059% | clf_exit: 0.011 0.057 0.931
Batch: 240 | Loss: 8.748 | Acc: 17.188,29.830,38.216,% | Adaptive Acc: 38.168% | clf_exit: 0.011 0.058 0.931
Batch: 260 | Loss: 8.736 | Acc: 17.205,29.870,38.269,% | Adaptive Acc: 38.206% | clf_exit: 0.011 0.058 0.930
Batch: 280 | Loss: 8.723 | Acc: 17.246,29.949,38.459,% | Adaptive Acc: 38.398% | clf_exit: 0.011 0.058 0.930
Batch: 300 | Loss: 8.727 | Acc: 17.258,29.911,38.411,% | Adaptive Acc: 38.341% | clf_exit: 0.011 0.059 0.930
Batch: 320 | Loss: 8.720 | Acc: 17.353,29.924,38.444,% | Adaptive Acc: 38.369% | clf_exit: 0.011 0.059 0.929
Batch: 340 | Loss: 8.706 | Acc: 17.430,30.013,38.611,% | Adaptive Acc: 38.533% | clf_exit: 0.011 0.060 0.929
Batch: 360 | Loss: 8.697 | Acc: 17.469,30.058,38.608,% | Adaptive Acc: 38.528% | clf_exit: 0.011 0.061 0.928
Batch: 380 | Loss: 8.682 | Acc: 17.559,30.219,38.739,% | Adaptive Acc: 38.665% | clf_exit: 0.011 0.061 0.928
Batch: 0 | Loss: 8.377 | Acc: 20.312,32.812,37.500,% | Adaptive Acc: 36.719% | clf_exit: 0.031 0.156 0.812
Batch: 20 | Loss: 8.658 | Acc: 16.778,29.390,39.881,% | Adaptive Acc: 39.695% | clf_exit: 0.029 0.109 0.861
Batch: 40 | Loss: 8.651 | Acc: 16.559,29.592,39.596,% | Adaptive Acc: 39.310% | clf_exit: 0.027 0.105 0.868
Batch: 60 | Loss: 8.655 | Acc: 16.470,29.342,38.998,% | Adaptive Acc: 38.704% | clf_exit: 0.027 0.103 0.870
Train all parameters

Epoch: 5
Batch: 0 | Loss: 8.963 | Acc: 13.281,21.875,32.031,% | Adaptive Acc: 32.031% | clf_exit: 0.008 0.039 0.953
Batch: 20 | Loss: 8.420 | Acc: 20.015,31.324,39.955,% | Adaptive Acc: 39.769% | clf_exit: 0.015 0.076 0.909
Batch: 40 | Loss: 8.465 | Acc: 18.693,30.469,40.339,% | Adaptive Acc: 40.244% | clf_exit: 0.014 0.072 0.914
Batch: 60 | Loss: 8.482 | Acc: 18.763,30.725,40.369,% | Adaptive Acc: 40.241% | clf_exit: 0.012 0.070 0.918
Batch: 80 | Loss: 8.426 | Acc: 18.953,31.269,40.953,% | Adaptive Acc: 40.818% | clf_exit: 0.013 0.071 0.916
Batch: 100 | Loss: 8.415 | Acc: 18.812,31.327,41.112,% | Adaptive Acc: 40.989% | clf_exit: 0.014 0.069 0.917
Batch: 120 | Loss: 8.405 | Acc: 18.750,31.573,40.999,% | Adaptive Acc: 40.883% | clf_exit: 0.014 0.070 0.916
Batch: 140 | Loss: 8.411 | Acc: 18.744,31.793,40.963,% | Adaptive Acc: 40.847% | clf_exit: 0.014 0.070 0.916
Batch: 160 | Loss: 8.405 | Acc: 18.760,31.949,41.091,% | Adaptive Acc: 40.984% | clf_exit: 0.013 0.072 0.915
Batch: 180 | Loss: 8.401 | Acc: 18.728,32.023,41.048,% | Adaptive Acc: 40.944% | clf_exit: 0.014 0.073 0.914
Batch: 200 | Loss: 8.402 | Acc: 18.630,32.000,41.025,% | Adaptive Acc: 40.913% | clf_exit: 0.014 0.073 0.913
Batch: 220 | Loss: 8.380 | Acc: 18.683,32.173,41.141,% | Adaptive Acc: 41.014% | clf_exit: 0.014 0.074 0.912
Batch: 240 | Loss: 8.360 | Acc: 18.896,32.372,41.322,% | Adaptive Acc: 41.205% | clf_exit: 0.014 0.076 0.911
Batch: 260 | Loss: 8.342 | Acc: 18.948,32.525,41.445,% | Adaptive Acc: 41.331% | clf_exit: 0.014 0.076 0.910
Batch: 280 | Loss: 8.317 | Acc: 18.989,32.651,41.631,% | Adaptive Acc: 41.520% | clf_exit: 0.014 0.077 0.909
Batch: 300 | Loss: 8.303 | Acc: 19.142,32.800,41.715,% | Adaptive Acc: 41.596% | clf_exit: 0.014 0.078 0.908
Batch: 320 | Loss: 8.290 | Acc: 19.191,32.822,41.842,% | Adaptive Acc: 41.728% | clf_exit: 0.015 0.078 0.907
Batch: 340 | Loss: 8.272 | Acc: 19.300,32.984,42.027,% | Adaptive Acc: 41.894% | clf_exit: 0.015 0.079 0.906
Batch: 360 | Loss: 8.258 | Acc: 19.423,33.165,42.133,% | Adaptive Acc: 42.001% | clf_exit: 0.015 0.081 0.904
Batch: 380 | Loss: 8.241 | Acc: 19.466,33.288,42.302,% | Adaptive Acc: 42.167% | clf_exit: 0.015 0.082 0.903
Batch: 0 | Loss: 8.021 | Acc: 21.875,36.719,39.062,% | Adaptive Acc: 41.406% | clf_exit: 0.016 0.195 0.789
Batch: 20 | Loss: 8.242 | Acc: 19.940,32.143,40.737,% | Adaptive Acc: 40.997% | clf_exit: 0.026 0.140 0.834
Batch: 40 | Loss: 8.221 | Acc: 19.588,32.393,41.120,% | Adaptive Acc: 41.025% | clf_exit: 0.023 0.139 0.837
Batch: 60 | Loss: 8.222 | Acc: 19.608,32.198,41.099,% | Adaptive Acc: 41.073% | clf_exit: 0.024 0.140 0.835
Train all parameters

Epoch: 6
Batch: 0 | Loss: 8.200 | Acc: 14.062,34.375,41.406,% | Adaptive Acc: 41.406% | clf_exit: 0.016 0.086 0.898
Batch: 20 | Loss: 8.032 | Acc: 19.568,33.817,44.494,% | Adaptive Acc: 44.457% | clf_exit: 0.010 0.096 0.894
Batch: 40 | Loss: 8.090 | Acc: 18.960,33.384,43.540,% | Adaptive Acc: 43.483% | clf_exit: 0.012 0.089 0.899
Batch: 60 | Loss: 8.026 | Acc: 19.672,34.298,43.955,% | Adaptive Acc: 43.814% | clf_exit: 0.017 0.090 0.893
Batch: 80 | Loss: 7.970 | Acc: 20.255,34.905,44.657,% | Adaptive Acc: 44.522% | clf_exit: 0.016 0.096 0.888
Batch: 100 | Loss: 7.951 | Acc: 20.529,35.172,45.135,% | Adaptive Acc: 44.964% | clf_exit: 0.016 0.096 0.888
Batch: 120 | Loss: 7.930 | Acc: 20.823,35.331,45.254,% | Adaptive Acc: 45.048% | clf_exit: 0.019 0.095 0.886
Batch: 140 | Loss: 7.925 | Acc: 20.878,35.256,45.246,% | Adaptive Acc: 45.019% | clf_exit: 0.019 0.096 0.885
Batch: 160 | Loss: 7.917 | Acc: 20.841,35.341,45.206,% | Adaptive Acc: 45.002% | clf_exit: 0.019 0.096 0.886
Batch: 180 | Loss: 7.898 | Acc: 20.977,35.484,45.407,% | Adaptive Acc: 45.205% | clf_exit: 0.019 0.097 0.884
Batch: 200 | Loss: 7.891 | Acc: 21.028,35.463,45.328,% | Adaptive Acc: 45.138% | clf_exit: 0.020 0.097 0.883
Batch: 220 | Loss: 7.869 | Acc: 21.164,35.708,45.553,% | Adaptive Acc: 45.351% | clf_exit: 0.020 0.097 0.882
Batch: 240 | Loss: 7.850 | Acc: 21.311,35.918,45.666,% | Adaptive Acc: 45.471% | clf_exit: 0.021 0.099 0.881
Batch: 260 | Loss: 7.833 | Acc: 21.471,36.099,45.705,% | Adaptive Acc: 45.501% | clf_exit: 0.021 0.100 0.878
Batch: 280 | Loss: 7.833 | Acc: 21.547,36.093,45.774,% | Adaptive Acc: 45.557% | clf_exit: 0.022 0.101 0.877
Batch: 300 | Loss: 7.831 | Acc: 21.613,36.124,45.754,% | Adaptive Acc: 45.549% | clf_exit: 0.022 0.102 0.876
Batch: 320 | Loss: 7.821 | Acc: 21.671,36.305,45.855,% | Adaptive Acc: 45.648% | clf_exit: 0.022 0.102 0.876
Batch: 340 | Loss: 7.817 | Acc: 21.650,36.334,45.901,% | Adaptive Acc: 45.688% | clf_exit: 0.022 0.103 0.875
Batch: 360 | Loss: 7.815 | Acc: 21.695,36.385,45.901,% | Adaptive Acc: 45.683% | clf_exit: 0.021 0.104 0.874
Batch: 380 | Loss: 7.811 | Acc: 21.699,36.411,45.944,% | Adaptive Acc: 45.723% | clf_exit: 0.022 0.105 0.874
Batch: 0 | Loss: 7.637 | Acc: 23.438,35.156,46.094,% | Adaptive Acc: 46.094% | clf_exit: 0.031 0.148 0.820
Batch: 20 | Loss: 7.937 | Acc: 20.424,35.007,44.159,% | Adaptive Acc: 43.676% | clf_exit: 0.038 0.126 0.836
Batch: 40 | Loss: 7.916 | Acc: 20.351,35.213,44.931,% | Adaptive Acc: 44.341% | clf_exit: 0.032 0.133 0.835
Batch: 60 | Loss: 7.932 | Acc: 20.248,34.721,44.647,% | Adaptive Acc: 44.121% | clf_exit: 0.032 0.132 0.837
Train all parameters

Epoch: 7
Batch: 0 | Loss: 7.390 | Acc: 17.188,35.938,51.562,% | Adaptive Acc: 50.781% | clf_exit: 0.023 0.086 0.891
Batch: 20 | Loss: 7.521 | Acc: 22.842,38.876,48.400,% | Adaptive Acc: 48.103% | clf_exit: 0.019 0.113 0.867
Batch: 40 | Loss: 7.562 | Acc: 23.152,38.319,48.495,% | Adaptive Acc: 48.228% | clf_exit: 0.017 0.119 0.864
Batch: 60 | Loss: 7.579 | Acc: 22.631,38.153,48.527,% | Adaptive Acc: 48.322% | clf_exit: 0.018 0.118 0.864
Batch: 80 | Loss: 7.590 | Acc: 22.492,38.166,48.341,% | Adaptive Acc: 48.167% | clf_exit: 0.019 0.116 0.865
Batch: 100 | Loss: 7.548 | Acc: 22.618,38.699,48.654,% | Adaptive Acc: 48.492% | clf_exit: 0.021 0.117 0.862
Batch: 120 | Loss: 7.561 | Acc: 22.540,38.785,48.580,% | Adaptive Acc: 48.405% | clf_exit: 0.022 0.117 0.861
Batch: 140 | Loss: 7.523 | Acc: 22.573,38.918,48.864,% | Adaptive Acc: 48.643% | clf_exit: 0.023 0.118 0.859
Batch: 160 | Loss: 7.521 | Acc: 22.647,38.752,48.811,% | Adaptive Acc: 48.588% | clf_exit: 0.023 0.119 0.858
Batch: 180 | Loss: 7.506 | Acc: 22.777,38.838,48.740,% | Adaptive Acc: 48.541% | clf_exit: 0.023 0.121 0.856
Batch: 200 | Loss: 7.513 | Acc: 22.746,38.790,48.713,% | Adaptive Acc: 48.472% | clf_exit: 0.023 0.121 0.856
Batch: 220 | Loss: 7.515 | Acc: 22.773,38.812,48.607,% | Adaptive Acc: 48.377% | clf_exit: 0.024 0.122 0.855
Batch: 240 | Loss: 7.518 | Acc: 22.828,38.790,48.587,% | Adaptive Acc: 48.347% | clf_exit: 0.024 0.122 0.854
Batch: 260 | Loss: 7.516 | Acc: 22.908,38.847,48.563,% | Adaptive Acc: 48.324% | clf_exit: 0.024 0.123 0.852
Batch: 280 | Loss: 7.518 | Acc: 22.929,38.887,48.593,% | Adaptive Acc: 48.365% | clf_exit: 0.025 0.124 0.851
Batch: 300 | Loss: 7.509 | Acc: 23.007,39.005,48.681,% | Adaptive Acc: 48.443% | clf_exit: 0.025 0.125 0.850
Batch: 320 | Loss: 7.505 | Acc: 23.014,39.075,48.717,% | Adaptive Acc: 48.484% | clf_exit: 0.026 0.125 0.849
Batch: 340 | Loss: 7.497 | Acc: 23.057,39.145,48.813,% | Adaptive Acc: 48.577% | clf_exit: 0.025 0.126 0.848
Batch: 360 | Loss: 7.486 | Acc: 23.115,39.238,48.909,% | Adaptive Acc: 48.652% | clf_exit: 0.026 0.127 0.847
Batch: 380 | Loss: 7.480 | Acc: 23.161,39.278,48.874,% | Adaptive Acc: 48.614% | clf_exit: 0.026 0.128 0.846
Batch: 0 | Loss: 7.794 | Acc: 25.000,40.625,45.312,% | Adaptive Acc: 42.969% | clf_exit: 0.047 0.203 0.750
Batch: 20 | Loss: 7.762 | Acc: 21.391,36.607,45.536,% | Adaptive Acc: 45.015% | clf_exit: 0.041 0.176 0.783
Batch: 40 | Loss: 7.725 | Acc: 21.456,36.947,45.960,% | Adaptive Acc: 45.541% | clf_exit: 0.037 0.176 0.787
Batch: 60 | Loss: 7.729 | Acc: 21.311,36.783,45.710,% | Adaptive Acc: 45.261% | clf_exit: 0.037 0.175 0.788
Train all parameters

Epoch: 8
Batch: 0 | Loss: 8.232 | Acc: 19.531,32.812,35.156,% | Adaptive Acc: 35.156% | clf_exit: 0.031 0.125 0.844
Batch: 20 | Loss: 7.365 | Acc: 23.140,39.435,49.219,% | Adaptive Acc: 48.958% | clf_exit: 0.028 0.136 0.836
Batch: 40 | Loss: 7.331 | Acc: 23.780,39.806,49.981,% | Adaptive Acc: 49.676% | clf_exit: 0.028 0.135 0.837
Batch: 60 | Loss: 7.273 | Acc: 24.270,40.215,50.922,% | Adaptive Acc: 50.538% | clf_exit: 0.030 0.139 0.831
Batch: 80 | Loss: 7.232 | Acc: 24.585,41.001,51.418,% | Adaptive Acc: 51.022% | clf_exit: 0.030 0.141 0.829
Batch: 100 | Loss: 7.235 | Acc: 24.219,40.633,51.323,% | Adaptive Acc: 50.975% | clf_exit: 0.030 0.139 0.831
Batch: 120 | Loss: 7.227 | Acc: 24.141,40.761,51.459,% | Adaptive Acc: 51.130% | clf_exit: 0.030 0.139 0.831
Batch: 140 | Loss: 7.220 | Acc: 24.075,40.647,51.324,% | Adaptive Acc: 50.970% | clf_exit: 0.030 0.140 0.829
Batch: 160 | Loss: 7.219 | Acc: 24.102,40.591,51.354,% | Adaptive Acc: 51.014% | clf_exit: 0.031 0.139 0.830
Batch: 180 | Loss: 7.201 | Acc: 24.361,40.871,51.433,% | Adaptive Acc: 51.114% | clf_exit: 0.032 0.139 0.829
Batch: 200 | Loss: 7.194 | Acc: 24.491,41.041,51.450,% | Adaptive Acc: 51.119% | clf_exit: 0.032 0.140 0.828
Batch: 220 | Loss: 7.197 | Acc: 24.463,41.106,51.315,% | Adaptive Acc: 50.976% | clf_exit: 0.033 0.141 0.827
Batch: 240 | Loss: 7.198 | Acc: 24.394,41.196,51.371,% | Adaptive Acc: 51.037% | clf_exit: 0.032 0.141 0.827
Batch: 260 | Loss: 7.195 | Acc: 24.455,41.245,51.356,% | Adaptive Acc: 51.042% | clf_exit: 0.032 0.142 0.826
Batch: 280 | Loss: 7.188 | Acc: 24.530,41.348,51.368,% | Adaptive Acc: 51.051% | clf_exit: 0.032 0.143 0.825
Batch: 300 | Loss: 7.185 | Acc: 24.559,41.375,51.352,% | Adaptive Acc: 51.017% | clf_exit: 0.032 0.144 0.824
Batch: 320 | Loss: 7.179 | Acc: 24.569,41.423,51.472,% | Adaptive Acc: 51.098% | clf_exit: 0.033 0.145 0.823
Batch: 340 | Loss: 7.171 | Acc: 24.645,41.539,51.542,% | Adaptive Acc: 51.164% | clf_exit: 0.033 0.145 0.822
Batch: 360 | Loss: 7.172 | Acc: 24.738,41.538,51.474,% | Adaptive Acc: 51.110% | clf_exit: 0.033 0.146 0.821
Batch: 380 | Loss: 7.165 | Acc: 24.801,41.591,51.540,% | Adaptive Acc: 51.163% | clf_exit: 0.033 0.147 0.820
Batch: 0 | Loss: 6.957 | Acc: 30.469,38.281,57.031,% | Adaptive Acc: 57.812% | clf_exit: 0.070 0.211 0.719
Batch: 20 | Loss: 7.351 | Acc: 24.405,39.249,48.996,% | Adaptive Acc: 48.289% | clf_exit: 0.067 0.201 0.731
Batch: 40 | Loss: 7.308 | Acc: 24.143,40.015,48.704,% | Adaptive Acc: 48.133% | clf_exit: 0.062 0.206 0.732
Batch: 60 | Loss: 7.319 | Acc: 23.668,40.164,48.527,% | Adaptive Acc: 47.823% | clf_exit: 0.061 0.207 0.732
Train all parameters

Epoch: 9
Batch: 0 | Loss: 6.520 | Acc: 28.125,46.094,56.250,% | Adaptive Acc: 55.469% | clf_exit: 0.008 0.172 0.820
Batch: 20 | Loss: 7.049 | Acc: 24.814,41.518,52.753,% | Adaptive Acc: 52.158% | clf_exit: 0.041 0.147 0.813
Batch: 40 | Loss: 6.936 | Acc: 25.457,42.588,53.944,% | Adaptive Acc: 53.487% | clf_exit: 0.037 0.156 0.807
Batch: 60 | Loss: 6.892 | Acc: 25.743,43.251,54.470,% | Adaptive Acc: 53.881% | clf_exit: 0.039 0.158 0.802
Batch: 80 | Loss: 6.884 | Acc: 26.100,43.480,53.954,% | Adaptive Acc: 53.530% | clf_exit: 0.038 0.162 0.800
Batch: 100 | Loss: 6.889 | Acc: 25.758,43.472,53.960,% | Adaptive Acc: 53.558% | clf_exit: 0.039 0.159 0.802
Batch: 120 | Loss: 6.885 | Acc: 25.626,43.621,54.100,% | Adaptive Acc: 53.713% | clf_exit: 0.039 0.161 0.800
Batch: 140 | Loss: 6.871 | Acc: 25.875,43.689,54.150,% | Adaptive Acc: 53.701% | clf_exit: 0.040 0.162 0.798
Batch: 160 | Loss: 6.891 | Acc: 25.752,43.386,53.969,% | Adaptive Acc: 53.542% | clf_exit: 0.040 0.160 0.800
Batch: 180 | Loss: 6.892 | Acc: 25.799,43.387,54.057,% | Adaptive Acc: 53.578% | clf_exit: 0.040 0.161 0.799
Batch: 200 | Loss: 6.905 | Acc: 25.676,43.264,53.887,% | Adaptive Acc: 53.397% | clf_exit: 0.039 0.163 0.798
Batch: 220 | Loss: 6.915 | Acc: 25.583,43.273,53.818,% | Adaptive Acc: 53.316% | clf_exit: 0.039 0.163 0.797
Batch: 240 | Loss: 6.907 | Acc: 25.661,43.358,53.838,% | Adaptive Acc: 53.332% | clf_exit: 0.040 0.163 0.797
Batch: 260 | Loss: 6.909 | Acc: 25.685,43.316,53.831,% | Adaptive Acc: 53.317% | clf_exit: 0.040 0.163 0.797
Batch: 280 | Loss: 6.900 | Acc: 25.829,43.503,53.901,% | Adaptive Acc: 53.397% | clf_exit: 0.040 0.164 0.796
Batch: 300 | Loss: 6.895 | Acc: 25.812,43.516,53.956,% | Adaptive Acc: 53.436% | clf_exit: 0.039 0.166 0.795
Batch: 320 | Loss: 6.888 | Acc: 25.932,43.602,54.011,% | Adaptive Acc: 53.485% | clf_exit: 0.040 0.166 0.794
Batch: 340 | Loss: 6.883 | Acc: 25.932,43.757,53.938,% | Adaptive Acc: 53.441% | clf_exit: 0.040 0.166 0.793
Batch: 360 | Loss: 6.885 | Acc: 25.915,43.791,53.924,% | Adaptive Acc: 53.441% | clf_exit: 0.040 0.167 0.793
Batch: 380 | Loss: 6.889 | Acc: 25.818,43.740,53.816,% | Adaptive Acc: 53.344% | clf_exit: 0.040 0.167 0.793
Batch: 0 | Loss: 6.650 | Acc: 28.906,48.438,53.906,% | Adaptive Acc: 54.688% | clf_exit: 0.078 0.242 0.680
Batch: 20 | Loss: 7.055 | Acc: 25.744,41.295,52.679,% | Adaptive Acc: 50.856% | clf_exit: 0.083 0.244 0.673
Batch: 40 | Loss: 7.039 | Acc: 24.981,41.425,52.306,% | Adaptive Acc: 50.629% | clf_exit: 0.084 0.244 0.671
Batch: 60 | Loss: 7.071 | Acc: 24.705,41.201,52.088,% | Adaptive Acc: 50.359% | clf_exit: 0.082 0.243 0.675
Train all parameters

Epoch: 10
Batch: 0 | Loss: 6.945 | Acc: 20.312,40.625,53.125,% | Adaptive Acc: 51.562% | clf_exit: 0.055 0.156 0.789
Batch: 20 | Loss: 6.693 | Acc: 26.600,43.824,55.804,% | Adaptive Acc: 55.171% | clf_exit: 0.048 0.179 0.773
Batch: 40 | Loss: 6.691 | Acc: 26.734,45.103,55.716,% | Adaptive Acc: 55.088% | clf_exit: 0.045 0.180 0.776
Batch: 60 | Loss: 6.693 | Acc: 26.498,45.005,55.674,% | Adaptive Acc: 55.020% | clf_exit: 0.044 0.177 0.779
Batch: 80 | Loss: 6.714 | Acc: 26.167,45.158,55.652,% | Adaptive Acc: 55.073% | clf_exit: 0.043 0.175 0.782
Batch: 100 | Loss: 6.712 | Acc: 26.083,45.158,55.585,% | Adaptive Acc: 54.927% | clf_exit: 0.043 0.176 0.781
Batch: 120 | Loss: 6.697 | Acc: 26.324,45.164,55.804,% | Adaptive Acc: 55.152% | clf_exit: 0.043 0.179 0.778
Batch: 140 | Loss: 6.680 | Acc: 26.352,45.296,55.945,% | Adaptive Acc: 55.291% | clf_exit: 0.043 0.182 0.776
Batch: 160 | Loss: 6.686 | Acc: 26.456,45.235,55.852,% | Adaptive Acc: 55.187% | clf_exit: 0.043 0.181 0.776
Batch: 180 | Loss: 6.684 | Acc: 26.545,45.356,55.900,% | Adaptive Acc: 55.262% | clf_exit: 0.043 0.183 0.774
Batch: 200 | Loss: 6.683 | Acc: 26.648,45.363,55.822,% | Adaptive Acc: 55.173% | clf_exit: 0.043 0.184 0.773
Batch: 220 | Loss: 6.683 | Acc: 26.700,45.479,55.847,% | Adaptive Acc: 55.228% | clf_exit: 0.043 0.185 0.772
Batch: 240 | Loss: 6.676 | Acc: 26.822,45.569,55.793,% | Adaptive Acc: 55.167% | clf_exit: 0.043 0.184 0.772
Batch: 260 | Loss: 6.666 | Acc: 26.925,45.672,55.804,% | Adaptive Acc: 55.190% | clf_exit: 0.044 0.186 0.770
Batch: 280 | Loss: 6.662 | Acc: 27.021,45.746,55.911,% | Adaptive Acc: 55.285% | clf_exit: 0.044 0.186 0.769
Batch: 300 | Loss: 6.660 | Acc: 27.069,45.738,55.806,% | Adaptive Acc: 55.186% | clf_exit: 0.045 0.187 0.768
Batch: 320 | Loss: 6.659 | Acc: 27.190,45.785,55.822,% | Adaptive Acc: 55.199% | clf_exit: 0.045 0.188 0.767
Batch: 340 | Loss: 6.656 | Acc: 27.227,45.833,55.833,% | Adaptive Acc: 55.187% | clf_exit: 0.045 0.188 0.767
Batch: 360 | Loss: 6.657 | Acc: 27.181,45.817,55.802,% | Adaptive Acc: 55.135% | clf_exit: 0.046 0.188 0.767
Batch: 380 | Loss: 6.653 | Acc: 27.196,45.790,55.789,% | Adaptive Acc: 55.122% | clf_exit: 0.046 0.188 0.766
Batch: 0 | Loss: 6.767 | Acc: 29.688,45.312,57.812,% | Adaptive Acc: 57.812% | clf_exit: 0.094 0.234 0.672
Batch: 20 | Loss: 6.916 | Acc: 25.186,44.048,53.199,% | Adaptive Acc: 52.195% | clf_exit: 0.070 0.233 0.697
Batch: 40 | Loss: 6.931 | Acc: 24.809,43.464,52.268,% | Adaptive Acc: 51.315% | clf_exit: 0.069 0.231 0.700
Batch: 60 | Loss: 6.941 | Acc: 24.923,43.558,52.293,% | Adaptive Acc: 51.242% | clf_exit: 0.066 0.232 0.702
Train all parameters

Epoch: 11
Batch: 0 | Loss: 7.017 | Acc: 28.906,42.969,52.344,% | Adaptive Acc: 52.344% | clf_exit: 0.039 0.180 0.781
Batch: 20 | Loss: 6.445 | Acc: 27.604,48.140,58.185,% | Adaptive Acc: 57.478% | clf_exit: 0.049 0.191 0.760
Batch: 40 | Loss: 6.444 | Acc: 28.030,47.961,58.994,% | Adaptive Acc: 58.327% | clf_exit: 0.048 0.196 0.755
Batch: 60 | Loss: 6.433 | Acc: 28.099,47.976,58.940,% | Adaptive Acc: 58.312% | clf_exit: 0.049 0.194 0.757
Batch: 80 | Loss: 6.408 | Acc: 28.279,47.955,58.999,% | Adaptive Acc: 58.410% | clf_exit: 0.050 0.198 0.751
Batch: 100 | Loss: 6.389 | Acc: 28.481,48.074,58.826,% | Adaptive Acc: 58.246% | clf_exit: 0.053 0.200 0.748
Batch: 120 | Loss: 6.402 | Acc: 28.338,47.940,58.665,% | Adaptive Acc: 58.058% | clf_exit: 0.053 0.200 0.747
Batch: 140 | Loss: 6.418 | Acc: 28.380,47.717,58.433,% | Adaptive Acc: 57.818% | clf_exit: 0.052 0.200 0.748
Batch: 160 | Loss: 6.392 | Acc: 28.586,47.991,58.749,% | Adaptive Acc: 58.113% | clf_exit: 0.053 0.202 0.744
Batch: 180 | Loss: 6.407 | Acc: 28.488,47.717,58.451,% | Adaptive Acc: 57.795% | clf_exit: 0.053 0.202 0.744
Batch: 200 | Loss: 6.412 | Acc: 28.657,47.738,58.357,% | Adaptive Acc: 57.739% | clf_exit: 0.054 0.203 0.743
Batch: 220 | Loss: 6.418 | Acc: 28.645,47.784,58.403,% | Adaptive Acc: 57.802% | clf_exit: 0.052 0.204 0.744
Batch: 240 | Loss: 6.428 | Acc: 28.482,47.708,58.276,% | Adaptive Acc: 57.641% | clf_exit: 0.052 0.203 0.745
Batch: 260 | Loss: 6.434 | Acc: 28.373,47.629,58.049,% | Adaptive Acc: 57.417% | clf_exit: 0.053 0.203 0.744
Batch: 280 | Loss: 6.444 | Acc: 28.356,47.617,58.010,% | Adaptive Acc: 57.351% | clf_exit: 0.053 0.203 0.744
Batch: 300 | Loss: 6.441 | Acc: 28.317,47.599,58.051,% | Adaptive Acc: 57.400% | clf_exit: 0.053 0.203 0.744
Batch: 320 | Loss: 6.441 | Acc: 28.376,47.615,58.029,% | Adaptive Acc: 57.394% | clf_exit: 0.053 0.203 0.744
Batch: 340 | Loss: 6.434 | Acc: 28.421,47.803,58.026,% | Adaptive Acc: 57.386% | clf_exit: 0.053 0.205 0.742
Batch: 360 | Loss: 6.436 | Acc: 28.307,47.845,58.048,% | Adaptive Acc: 57.419% | clf_exit: 0.053 0.205 0.742
Batch: 380 | Loss: 6.433 | Acc: 28.322,47.843,58.011,% | Adaptive Acc: 57.382% | clf_exit: 0.053 0.205 0.742
Batch: 0 | Loss: 6.680 | Acc: 28.125,46.875,53.125,% | Adaptive Acc: 54.688% | clf_exit: 0.109 0.344 0.547
Batch: 20 | Loss: 6.824 | Acc: 26.488,44.978,52.976,% | Adaptive Acc: 51.600% | clf_exit: 0.100 0.285 0.615
Batch: 40 | Loss: 6.822 | Acc: 26.105,44.588,52.572,% | Adaptive Acc: 51.200% | clf_exit: 0.099 0.283 0.618
Batch: 60 | Loss: 6.855 | Acc: 25.743,44.096,52.459,% | Adaptive Acc: 50.935% | clf_exit: 0.095 0.282 0.624
Train all parameters

Epoch: 12
Batch: 0 | Loss: 5.852 | Acc: 30.469,55.469,64.844,% | Adaptive Acc: 64.062% | clf_exit: 0.070 0.234 0.695
Batch: 20 | Loss: 6.245 | Acc: 29.464,49.702,59.970,% | Adaptive Acc: 59.412% | clf_exit: 0.062 0.214 0.724
Batch: 40 | Loss: 6.272 | Acc: 28.735,48.857,59.432,% | Adaptive Acc: 58.632% | clf_exit: 0.060 0.210 0.730
Batch: 60 | Loss: 6.285 | Acc: 28.522,48.655,59.682,% | Adaptive Acc: 58.773% | clf_exit: 0.059 0.210 0.731
Batch: 80 | Loss: 6.283 | Acc: 28.443,48.254,59.452,% | Adaptive Acc: 58.536% | clf_exit: 0.057 0.210 0.733
Batch: 100 | Loss: 6.322 | Acc: 28.264,48.159,59.120,% | Adaptive Acc: 58.168% | clf_exit: 0.054 0.212 0.735
Batch: 120 | Loss: 6.281 | Acc: 28.461,48.618,59.762,% | Adaptive Acc: 58.846% | clf_exit: 0.056 0.214 0.730
Batch: 140 | Loss: 6.276 | Acc: 28.602,48.593,59.741,% | Adaptive Acc: 58.854% | clf_exit: 0.057 0.215 0.729
Batch: 160 | Loss: 6.274 | Acc: 28.654,48.646,59.671,% | Adaptive Acc: 58.798% | clf_exit: 0.057 0.214 0.729
Batch: 180 | Loss: 6.259 | Acc: 28.867,48.804,59.876,% | Adaptive Acc: 59.021% | clf_exit: 0.057 0.214 0.728
Batch: 200 | Loss: 6.261 | Acc: 28.863,48.888,59.725,% | Adaptive Acc: 58.858% | clf_exit: 0.058 0.217 0.725
Batch: 220 | Loss: 6.258 | Acc: 28.949,48.947,59.743,% | Adaptive Acc: 58.912% | clf_exit: 0.058 0.217 0.725
Batch: 240 | Loss: 6.246 | Acc: 29.094,49.005,59.790,% | Adaptive Acc: 58.950% | clf_exit: 0.059 0.218 0.723
Batch: 260 | Loss: 6.249 | Acc: 29.041,49.003,59.746,% | Adaptive Acc: 58.914% | clf_exit: 0.059 0.217 0.723
Batch: 280 | Loss: 6.244 | Acc: 28.998,49.094,59.800,% | Adaptive Acc: 58.964% | clf_exit: 0.059 0.219 0.722
Batch: 300 | Loss: 6.245 | Acc: 29.106,49.092,59.699,% | Adaptive Acc: 58.879% | clf_exit: 0.059 0.219 0.722
Batch: 320 | Loss: 6.256 | Acc: 29.064,49.048,59.616,% | Adaptive Acc: 58.801% | clf_exit: 0.059 0.218 0.723
Batch: 340 | Loss: 6.254 | Acc: 29.106,49.120,59.627,% | Adaptive Acc: 58.825% | clf_exit: 0.059 0.218 0.723
Batch: 360 | Loss: 6.252 | Acc: 29.175,49.191,59.526,% | Adaptive Acc: 58.752% | clf_exit: 0.060 0.218 0.722
Batch: 380 | Loss: 6.247 | Acc: 29.261,49.280,59.539,% | Adaptive Acc: 58.768% | clf_exit: 0.060 0.219 0.721
Batch: 0 | Loss: 6.154 | Acc: 34.375,53.125,62.500,% | Adaptive Acc: 60.938% | clf_exit: 0.125 0.242 0.633
Batch: 20 | Loss: 6.486 | Acc: 28.013,46.912,55.060,% | Adaptive Acc: 53.051% | clf_exit: 0.091 0.287 0.622
Batch: 40 | Loss: 6.463 | Acc: 27.973,47.123,55.240,% | Adaptive Acc: 53.525% | clf_exit: 0.092 0.293 0.615
Batch: 60 | Loss: 6.471 | Acc: 27.830,46.798,55.917,% | Adaptive Acc: 54.201% | clf_exit: 0.089 0.295 0.615
Train all parameters

Epoch: 13
Batch: 0 | Loss: 5.692 | Acc: 32.031,53.906,64.844,% | Adaptive Acc: 64.844% | clf_exit: 0.086 0.258 0.656
Batch: 20 | Loss: 6.070 | Acc: 30.506,50.149,63.318,% | Adaptive Acc: 62.426% | clf_exit: 0.067 0.228 0.705
Batch: 40 | Loss: 6.011 | Acc: 31.288,51.429,63.700,% | Adaptive Acc: 62.576% | clf_exit: 0.065 0.236 0.700
Batch: 60 | Loss: 6.025 | Acc: 31.058,51.178,63.268,% | Adaptive Acc: 62.269% | clf_exit: 0.065 0.236 0.699
Batch: 80 | Loss: 6.033 | Acc: 30.363,50.907,62.857,% | Adaptive Acc: 61.863% | clf_exit: 0.064 0.231 0.704
Batch: 100 | Loss: 6.013 | Acc: 30.755,51.238,62.887,% | Adaptive Acc: 61.889% | clf_exit: 0.065 0.231 0.704
Batch: 120 | Loss: 6.043 | Acc: 30.585,50.930,62.629,% | Adaptive Acc: 61.577% | clf_exit: 0.065 0.230 0.705
Batch: 140 | Loss: 6.069 | Acc: 30.336,50.765,62.483,% | Adaptive Acc: 61.458% | clf_exit: 0.064 0.230 0.706
Batch: 160 | Loss: 6.069 | Acc: 30.299,50.679,62.282,% | Adaptive Acc: 61.185% | clf_exit: 0.064 0.231 0.705
Batch: 180 | Loss: 6.066 | Acc: 30.305,50.751,62.228,% | Adaptive Acc: 61.114% | clf_exit: 0.064 0.233 0.703
Batch: 200 | Loss: 6.073 | Acc: 30.197,50.711,62.205,% | Adaptive Acc: 61.093% | clf_exit: 0.065 0.232 0.703
Batch: 220 | Loss: 6.073 | Acc: 30.133,50.753,62.288,% | Adaptive Acc: 61.164% | clf_exit: 0.065 0.233 0.703
Batch: 240 | Loss: 6.075 | Acc: 30.222,50.742,62.143,% | Adaptive Acc: 61.019% | clf_exit: 0.064 0.232 0.703
Batch: 260 | Loss: 6.077 | Acc: 30.235,50.694,62.114,% | Adaptive Acc: 60.964% | clf_exit: 0.065 0.233 0.702
Batch: 280 | Loss: 6.089 | Acc: 30.207,50.690,61.958,% | Adaptive Acc: 60.807% | clf_exit: 0.064 0.233 0.702
Batch: 300 | Loss: 6.088 | Acc: 30.212,50.618,61.898,% | Adaptive Acc: 60.730% | clf_exit: 0.065 0.233 0.702
Batch: 320 | Loss: 6.084 | Acc: 30.294,50.633,61.862,% | Adaptive Acc: 60.694% | clf_exit: 0.066 0.233 0.702
Batch: 340 | Loss: 6.087 | Acc: 30.242,50.658,61.806,% | Adaptive Acc: 60.663% | clf_exit: 0.065 0.232 0.702
Batch: 360 | Loss: 6.085 | Acc: 30.244,50.630,61.794,% | Adaptive Acc: 60.673% | clf_exit: 0.065 0.233 0.702
Batch: 380 | Loss: 6.078 | Acc: 30.243,50.689,61.766,% | Adaptive Acc: 60.652% | clf_exit: 0.065 0.233 0.702
Batch: 0 | Loss: 6.279 | Acc: 31.250,50.000,57.031,% | Adaptive Acc: 55.469% | clf_exit: 0.172 0.273 0.555
Batch: 20 | Loss: 6.575 | Acc: 25.632,47.433,55.692,% | Adaptive Acc: 53.237% | clf_exit: 0.124 0.287 0.589
Batch: 40 | Loss: 6.577 | Acc: 25.171,47.618,55.488,% | Adaptive Acc: 53.430% | clf_exit: 0.122 0.286 0.593
Batch: 60 | Loss: 6.577 | Acc: 25.307,47.567,55.494,% | Adaptive Acc: 53.266% | clf_exit: 0.120 0.286 0.594
Train all parameters

Epoch: 14
Batch: 0 | Loss: 6.487 | Acc: 29.688,50.000,63.281,% | Adaptive Acc: 65.625% | clf_exit: 0.055 0.289 0.656
Batch: 20 | Loss: 5.827 | Acc: 31.027,52.790,65.699,% | Adaptive Acc: 64.695% | clf_exit: 0.077 0.244 0.679
Batch: 40 | Loss: 5.847 | Acc: 30.507,52.382,65.111,% | Adaptive Acc: 63.967% | clf_exit: 0.075 0.242 0.683
Batch: 60 | Loss: 5.858 | Acc: 30.943,52.613,64.703,% | Adaptive Acc: 63.665% | clf_exit: 0.069 0.249 0.682
Batch: 80 | Loss: 5.855 | Acc: 31.009,52.768,64.236,% | Adaptive Acc: 63.204% | clf_exit: 0.069 0.250 0.681
Batch: 100 | Loss: 5.859 | Acc: 30.879,52.792,64.325,% | Adaptive Acc: 63.258% | clf_exit: 0.070 0.249 0.680
Batch: 120 | Loss: 5.888 | Acc: 30.824,52.518,64.082,% | Adaptive Acc: 63.029% | clf_exit: 0.070 0.246 0.684
Batch: 140 | Loss: 5.908 | Acc: 30.829,52.211,63.708,% | Adaptive Acc: 62.666% | clf_exit: 0.070 0.246 0.684
Batch: 160 | Loss: 5.915 | Acc: 30.794,52.392,63.766,% | Adaptive Acc: 62.752% | clf_exit: 0.070 0.245 0.685
Batch: 180 | Loss: 5.908 | Acc: 30.935,52.486,63.894,% | Adaptive Acc: 62.802% | clf_exit: 0.070 0.246 0.684
Batch: 200 | Loss: 5.906 | Acc: 31.028,52.456,63.790,% | Adaptive Acc: 62.671% | clf_exit: 0.070 0.246 0.684
Batch: 220 | Loss: 5.914 | Acc: 31.123,52.284,63.642,% | Adaptive Acc: 62.528% | clf_exit: 0.072 0.245 0.683
Batch: 240 | Loss: 5.925 | Acc: 31.026,52.198,63.544,% | Adaptive Acc: 62.396% | clf_exit: 0.071 0.245 0.683
Batch: 260 | Loss: 5.919 | Acc: 31.023,52.257,63.599,% | Adaptive Acc: 62.437% | clf_exit: 0.072 0.245 0.683
Batch: 280 | Loss: 5.917 | Acc: 30.986,52.288,63.620,% | Adaptive Acc: 62.483% | clf_exit: 0.072 0.245 0.683
Batch: 300 | Loss: 5.912 | Acc: 30.988,52.310,63.632,% | Adaptive Acc: 62.492% | clf_exit: 0.072 0.246 0.683
Batch: 320 | Loss: 5.913 | Acc: 31.024,52.305,63.551,% | Adaptive Acc: 62.417% | clf_exit: 0.072 0.245 0.683
Batch: 340 | Loss: 5.904 | Acc: 31.138,52.367,63.616,% | Adaptive Acc: 62.473% | clf_exit: 0.072 0.245 0.682
Batch: 360 | Loss: 5.909 | Acc: 31.077,52.350,63.472,% | Adaptive Acc: 62.331% | clf_exit: 0.072 0.246 0.682
Batch: 380 | Loss: 5.905 | Acc: 31.104,52.354,63.402,% | Adaptive Acc: 62.262% | clf_exit: 0.072 0.247 0.681
Batch: 0 | Loss: 5.890 | Acc: 31.250,54.688,63.281,% | Adaptive Acc: 62.500% | clf_exit: 0.156 0.273 0.570
Batch: 20 | Loss: 6.317 | Acc: 30.283,48.177,58.854,% | Adaptive Acc: 56.882% | clf_exit: 0.118 0.304 0.578
Batch: 40 | Loss: 6.338 | Acc: 29.935,48.171,57.774,% | Adaptive Acc: 56.059% | clf_exit: 0.121 0.300 0.579
Batch: 60 | Loss: 6.335 | Acc: 29.764,47.477,57.441,% | Adaptive Acc: 55.558% | clf_exit: 0.120 0.294 0.586
Train all parameters

Epoch: 15
Batch: 0 | Loss: 5.975 | Acc: 29.688,50.000,59.375,% | Adaptive Acc: 60.156% | clf_exit: 0.070 0.258 0.672
Batch: 20 | Loss: 5.723 | Acc: 31.138,53.162,66.518,% | Adaptive Acc: 65.551% | clf_exit: 0.076 0.240 0.684
Batch: 40 | Loss: 5.706 | Acc: 31.517,53.182,66.425,% | Adaptive Acc: 65.377% | clf_exit: 0.079 0.242 0.678
Batch: 60 | Loss: 5.711 | Acc: 31.468,53.356,66.022,% | Adaptive Acc: 64.895% | clf_exit: 0.079 0.245 0.677
Batch: 80 | Loss: 5.716 | Acc: 31.414,53.270,65.702,% | Adaptive Acc: 64.545% | clf_exit: 0.078 0.247 0.674
Batch: 100 | Loss: 5.712 | Acc: 31.846,53.481,65.486,% | Adaptive Acc: 64.325% | clf_exit: 0.080 0.249 0.671
Batch: 120 | Loss: 5.744 | Acc: 31.624,53.267,65.063,% | Adaptive Acc: 63.966% | clf_exit: 0.078 0.249 0.673
Batch: 140 | Loss: 5.767 | Acc: 31.405,53.103,64.683,% | Adaptive Acc: 63.553% | clf_exit: 0.077 0.250 0.674
Batch: 160 | Loss: 5.785 | Acc: 31.332,53.057,64.504,% | Adaptive Acc: 63.393% | clf_exit: 0.078 0.250 0.672
Batch: 180 | Loss: 5.784 | Acc: 31.388,53.116,64.580,% | Adaptive Acc: 63.497% | clf_exit: 0.078 0.250 0.672
Batch: 200 | Loss: 5.789 | Acc: 31.269,53.028,64.595,% | Adaptive Acc: 63.511% | clf_exit: 0.077 0.251 0.672
Batch: 220 | Loss: 5.781 | Acc: 31.413,52.977,64.692,% | Adaptive Acc: 63.624% | clf_exit: 0.076 0.251 0.672
Batch: 240 | Loss: 5.776 | Acc: 31.545,53.015,64.743,% | Adaptive Acc: 63.628% | clf_exit: 0.077 0.252 0.671
Batch: 260 | Loss: 5.779 | Acc: 31.546,53.005,64.697,% | Adaptive Acc: 63.575% | clf_exit: 0.077 0.252 0.671
Batch: 280 | Loss: 5.777 | Acc: 31.436,52.911,64.677,% | Adaptive Acc: 63.554% | clf_exit: 0.077 0.253 0.669
Batch: 300 | Loss: 5.772 | Acc: 31.517,52.961,64.685,% | Adaptive Acc: 63.559% | clf_exit: 0.077 0.255 0.668
Batch: 320 | Loss: 5.773 | Acc: 31.549,52.945,64.632,% | Adaptive Acc: 63.481% | clf_exit: 0.077 0.256 0.667
Batch: 340 | Loss: 5.773 | Acc: 31.603,53.024,64.660,% | Adaptive Acc: 63.524% | clf_exit: 0.077 0.256 0.667
Batch: 360 | Loss: 5.777 | Acc: 31.529,52.958,64.681,% | Adaptive Acc: 63.515% | clf_exit: 0.077 0.256 0.667
Batch: 380 | Loss: 5.777 | Acc: 31.549,53.029,64.635,% | Adaptive Acc: 63.486% | clf_exit: 0.077 0.256 0.667
Batch: 0 | Loss: 6.478 | Acc: 25.000,49.219,56.250,% | Adaptive Acc: 53.125% | clf_exit: 0.109 0.312 0.578
Batch: 20 | Loss: 6.640 | Acc: 25.670,46.466,55.543,% | Adaptive Acc: 53.757% | clf_exit: 0.104 0.304 0.592
Batch: 40 | Loss: 6.647 | Acc: 24.867,46.208,55.926,% | Adaptive Acc: 53.735% | clf_exit: 0.099 0.303 0.597
Batch: 60 | Loss: 6.666 | Acc: 24.603,46.171,55.930,% | Adaptive Acc: 53.637% | clf_exit: 0.101 0.299 0.601
Train all parameters

Epoch: 16
Batch: 0 | Loss: 5.453 | Acc: 34.375,61.719,72.656,% | Adaptive Acc: 72.656% | clf_exit: 0.055 0.250 0.695
Batch: 20 | Loss: 5.533 | Acc: 31.994,55.246,69.048,% | Adaptive Acc: 67.746% | clf_exit: 0.081 0.260 0.659
Batch: 40 | Loss: 5.575 | Acc: 31.479,55.373,68.369,% | Adaptive Acc: 67.073% | clf_exit: 0.082 0.260 0.658
Batch: 60 | Loss: 5.567 | Acc: 31.609,55.289,68.135,% | Adaptive Acc: 66.906% | clf_exit: 0.080 0.261 0.658
Batch: 80 | Loss: 5.554 | Acc: 31.944,55.228,67.882,% | Adaptive Acc: 66.570% | clf_exit: 0.082 0.266 0.652
Batch: 100 | Loss: 5.568 | Acc: 31.853,54.966,67.458,% | Adaptive Acc: 66.182% | clf_exit: 0.082 0.264 0.654
Batch: 120 | Loss: 5.578 | Acc: 31.805,54.868,67.478,% | Adaptive Acc: 66.206% | clf_exit: 0.081 0.264 0.655
Batch: 140 | Loss: 5.588 | Acc: 32.070,54.505,67.110,% | Adaptive Acc: 65.802% | clf_exit: 0.081 0.265 0.654
Batch: 160 | Loss: 5.590 | Acc: 31.963,54.498,66.984,% | Adaptive Acc: 65.790% | clf_exit: 0.081 0.265 0.655
Batch: 180 | Loss: 5.598 | Acc: 31.919,54.493,66.851,% | Adaptive Acc: 65.573% | clf_exit: 0.081 0.265 0.654
Batch: 200 | Loss: 5.607 | Acc: 31.950,54.345,66.663,% | Adaptive Acc: 65.400% | clf_exit: 0.081 0.265 0.655
Batch: 220 | Loss: 5.610 | Acc: 31.975,54.334,66.576,% | Adaptive Acc: 65.332% | clf_exit: 0.081 0.266 0.654
Batch: 240 | Loss: 5.625 | Acc: 32.009,54.308,66.429,% | Adaptive Acc: 65.197% | clf_exit: 0.081 0.267 0.653
Batch: 260 | Loss: 5.636 | Acc: 32.016,54.268,66.260,% | Adaptive Acc: 65.011% | clf_exit: 0.081 0.267 0.652
Batch: 280 | Loss: 5.636 | Acc: 32.051,54.309,66.209,% | Adaptive Acc: 64.938% | clf_exit: 0.081 0.267 0.652
Batch: 300 | Loss: 5.632 | Acc: 32.091,54.397,66.266,% | Adaptive Acc: 64.997% | clf_exit: 0.081 0.267 0.652
Batch: 320 | Loss: 5.630 | Acc: 32.255,54.495,66.236,% | Adaptive Acc: 65.002% | clf_exit: 0.081 0.268 0.651
Batch: 340 | Loss: 5.627 | Acc: 32.313,54.605,66.266,% | Adaptive Acc: 65.020% | clf_exit: 0.082 0.269 0.649
Batch: 360 | Loss: 5.631 | Acc: 32.289,54.638,66.224,% | Adaptive Acc: 64.991% | clf_exit: 0.082 0.269 0.649
Batch: 380 | Loss: 5.637 | Acc: 32.230,54.550,66.164,% | Adaptive Acc: 64.903% | clf_exit: 0.082 0.269 0.649
Batch: 0 | Loss: 6.077 | Acc: 32.031,51.562,63.281,% | Adaptive Acc: 60.156% | clf_exit: 0.180 0.312 0.508
Batch: 20 | Loss: 6.342 | Acc: 27.939,49.070,58.891,% | Adaptive Acc: 55.580% | clf_exit: 0.141 0.324 0.535
Batch: 40 | Loss: 6.313 | Acc: 28.563,48.133,58.479,% | Adaptive Acc: 55.069% | clf_exit: 0.137 0.334 0.529
Batch: 60 | Loss: 6.317 | Acc: 28.356,48.220,58.299,% | Adaptive Acc: 55.213% | clf_exit: 0.134 0.331 0.535
Train all parameters

Epoch: 17
Batch: 0 | Loss: 5.850 | Acc: 28.906,54.688,71.875,% | Adaptive Acc: 68.750% | clf_exit: 0.062 0.281 0.656
Batch: 20 | Loss: 5.413 | Acc: 32.850,57.515,69.457,% | Adaptive Acc: 67.597% | clf_exit: 0.084 0.285 0.631
Batch: 40 | Loss: 5.427 | Acc: 33.194,56.612,69.150,% | Adaptive Acc: 67.550% | clf_exit: 0.088 0.278 0.634
Batch: 60 | Loss: 5.452 | Acc: 32.979,56.084,69.019,% | Adaptive Acc: 67.508% | clf_exit: 0.090 0.278 0.633
Batch: 80 | Loss: 5.487 | Acc: 32.899,55.826,68.519,% | Adaptive Acc: 66.975% | clf_exit: 0.089 0.278 0.633
Batch: 100 | Loss: 5.498 | Acc: 32.650,55.856,68.247,% | Adaptive Acc: 66.855% | clf_exit: 0.088 0.277 0.635
Batch: 120 | Loss: 5.499 | Acc: 32.651,56.005,68.304,% | Adaptive Acc: 66.955% | clf_exit: 0.088 0.276 0.636
Batch: 140 | Loss: 5.487 | Acc: 32.962,56.062,68.190,% | Adaptive Acc: 66.910% | clf_exit: 0.088 0.279 0.633
Batch: 160 | Loss: 5.495 | Acc: 32.968,56.036,67.983,% | Adaptive Acc: 66.693% | clf_exit: 0.089 0.278 0.633
Batch: 180 | Loss: 5.506 | Acc: 32.851,55.862,67.818,% | Adaptive Acc: 66.531% | clf_exit: 0.088 0.276 0.636
Batch: 200 | Loss: 5.512 | Acc: 32.824,55.947,67.751,% | Adaptive Acc: 66.449% | clf_exit: 0.088 0.277 0.636
Batch: 220 | Loss: 5.523 | Acc: 32.685,55.695,67.619,% | Adaptive Acc: 66.336% | clf_exit: 0.086 0.277 0.637
Batch: 240 | Loss: 5.521 | Acc: 32.731,55.709,67.567,% | Adaptive Acc: 66.303% | clf_exit: 0.086 0.276 0.638
Batch: 260 | Loss: 5.533 | Acc: 32.702,55.642,67.454,% | Adaptive Acc: 66.188% | clf_exit: 0.086 0.276 0.638
Batch: 280 | Loss: 5.534 | Acc: 32.785,55.711,67.441,% | Adaptive Acc: 66.195% | clf_exit: 0.086 0.277 0.637
Batch: 300 | Loss: 5.535 | Acc: 32.818,55.791,67.429,% | Adaptive Acc: 66.191% | clf_exit: 0.085 0.278 0.636
Batch: 320 | Loss: 5.534 | Acc: 32.837,55.790,67.363,% | Adaptive Acc: 66.100% | clf_exit: 0.086 0.278 0.636
Batch: 340 | Loss: 5.529 | Acc: 32.936,55.842,67.357,% | Adaptive Acc: 66.092% | clf_exit: 0.086 0.279 0.635
Batch: 360 | Loss: 5.524 | Acc: 32.992,55.869,67.341,% | Adaptive Acc: 66.073% | clf_exit: 0.087 0.279 0.634
Batch: 380 | Loss: 5.519 | Acc: 33.048,55.930,67.286,% | Adaptive Acc: 66.043% | clf_exit: 0.087 0.278 0.634
Batch: 0 | Loss: 5.461 | Acc: 39.844,53.906,64.844,% | Adaptive Acc: 64.062% | clf_exit: 0.172 0.289 0.539
Batch: 20 | Loss: 5.951 | Acc: 32.254,52.567,60.193,% | Adaptive Acc: 58.296% | clf_exit: 0.135 0.302 0.562
Batch: 40 | Loss: 5.953 | Acc: 31.936,52.115,59.585,% | Adaptive Acc: 57.851% | clf_exit: 0.134 0.306 0.560
Batch: 60 | Loss: 5.948 | Acc: 31.609,52.190,59.849,% | Adaptive Acc: 57.966% | clf_exit: 0.132 0.308 0.561
Train all parameters

Epoch: 18
Batch: 0 | Loss: 5.677 | Acc: 28.906,48.438,67.188,% | Adaptive Acc: 61.719% | clf_exit: 0.086 0.344 0.570
Batch: 20 | Loss: 5.298 | Acc: 32.850,56.808,70.238,% | Adaptive Acc: 67.783% | clf_exit: 0.088 0.304 0.608
Batch: 40 | Loss: 5.312 | Acc: 32.870,57.222,69.855,% | Adaptive Acc: 67.835% | clf_exit: 0.089 0.300 0.611
Batch: 60 | Loss: 5.315 | Acc: 33.171,56.954,70.312,% | Adaptive Acc: 68.391% | clf_exit: 0.088 0.296 0.616
Batch: 80 | Loss: 5.316 | Acc: 33.459,57.060,70.042,% | Adaptive Acc: 68.094% | clf_exit: 0.091 0.293 0.616
Batch: 100 | Loss: 5.328 | Acc: 33.269,56.969,70.034,% | Adaptive Acc: 68.100% | clf_exit: 0.091 0.291 0.619
Batch: 120 | Loss: 5.317 | Acc: 33.452,57.238,70.054,% | Adaptive Acc: 68.162% | clf_exit: 0.092 0.291 0.617
Batch: 140 | Loss: 5.324 | Acc: 33.494,57.048,69.747,% | Adaptive Acc: 67.913% | clf_exit: 0.093 0.291 0.616
Batch: 160 | Loss: 5.338 | Acc: 33.448,56.963,69.740,% | Adaptive Acc: 67.862% | clf_exit: 0.093 0.291 0.615
Batch: 180 | Loss: 5.327 | Acc: 33.576,57.031,69.678,% | Adaptive Acc: 67.835% | clf_exit: 0.093 0.290 0.617
Batch: 200 | Loss: 5.341 | Acc: 33.477,56.907,69.683,% | Adaptive Acc: 67.844% | clf_exit: 0.091 0.292 0.617
Batch: 220 | Loss: 5.343 | Acc: 33.668,56.812,69.535,% | Adaptive Acc: 67.732% | clf_exit: 0.092 0.292 0.616
Batch: 240 | Loss: 5.352 | Acc: 33.603,56.720,69.408,% | Adaptive Acc: 67.648% | clf_exit: 0.092 0.291 0.617
Batch: 260 | Loss: 5.355 | Acc: 33.588,56.732,69.358,% | Adaptive Acc: 67.625% | clf_exit: 0.092 0.291 0.616
Batch: 280 | Loss: 5.358 | Acc: 33.585,56.867,69.281,% | Adaptive Acc: 67.552% | clf_exit: 0.093 0.290 0.616
Batch: 300 | Loss: 5.370 | Acc: 33.583,56.722,69.072,% | Adaptive Acc: 67.356% | clf_exit: 0.092 0.290 0.617
Batch: 320 | Loss: 5.376 | Acc: 33.584,56.661,69.010,% | Adaptive Acc: 67.290% | clf_exit: 0.093 0.290 0.617
Batch: 340 | Loss: 5.388 | Acc: 33.539,56.573,68.917,% | Adaptive Acc: 67.210% | clf_exit: 0.092 0.289 0.618
Batch: 360 | Loss: 5.393 | Acc: 33.544,56.594,68.863,% | Adaptive Acc: 67.166% | clf_exit: 0.092 0.289 0.618
Batch: 380 | Loss: 5.395 | Acc: 33.555,56.623,68.787,% | Adaptive Acc: 67.122% | clf_exit: 0.092 0.290 0.618
Batch: 0 | Loss: 5.639 | Acc: 29.688,54.688,68.750,% | Adaptive Acc: 64.062% | clf_exit: 0.172 0.320 0.508
Batch: 20 | Loss: 6.123 | Acc: 29.650,51.525,59.747,% | Adaptive Acc: 57.366% | clf_exit: 0.132 0.323 0.545
Batch: 40 | Loss: 6.107 | Acc: 29.802,50.877,59.108,% | Adaptive Acc: 56.803% | clf_exit: 0.128 0.325 0.547
Batch: 60 | Loss: 6.116 | Acc: 29.521,50.768,58.965,% | Adaptive Acc: 56.621% | clf_exit: 0.126 0.327 0.547
Train all parameters

Epoch: 19
Batch: 0 | Loss: 4.566 | Acc: 39.844,67.969,75.781,% | Adaptive Acc: 74.219% | clf_exit: 0.078 0.328 0.594
Batch: 20 | Loss: 5.063 | Acc: 35.863,59.747,73.251,% | Adaptive Acc: 71.652% | clf_exit: 0.103 0.295 0.602
Batch: 40 | Loss: 5.172 | Acc: 34.356,58.308,72.008,% | Adaptive Acc: 70.179% | clf_exit: 0.100 0.296 0.605
Batch: 60 | Loss: 5.204 | Acc: 33.914,58.414,71.529,% | Adaptive Acc: 69.685% | clf_exit: 0.097 0.298 0.605
Batch: 80 | Loss: 5.235 | Acc: 33.777,57.996,71.181,% | Adaptive Acc: 69.329% | clf_exit: 0.095 0.302 0.603
Batch: 100 | Loss: 5.252 | Acc: 33.516,57.805,70.877,% | Adaptive Acc: 68.936% | clf_exit: 0.092 0.303 0.605
Batch: 120 | Loss: 5.242 | Acc: 33.878,58.013,70.919,% | Adaptive Acc: 69.053% | clf_exit: 0.093 0.305 0.602
Batch: 140 | Loss: 5.251 | Acc: 33.882,57.879,70.673,% | Adaptive Acc: 68.839% | clf_exit: 0.093 0.303 0.604
Batch: 160 | Loss: 5.268 | Acc: 33.759,57.623,70.468,% | Adaptive Acc: 68.701% | clf_exit: 0.094 0.300 0.606
Batch: 180 | Loss: 5.268 | Acc: 33.874,57.575,70.312,% | Adaptive Acc: 68.590% | clf_exit: 0.094 0.298 0.608
Batch: 200 | Loss: 5.257 | Acc: 34.142,57.840,70.367,% | Adaptive Acc: 68.680% | clf_exit: 0.095 0.299 0.606
Batch: 220 | Loss: 5.259 | Acc: 34.110,57.745,70.320,% | Adaptive Acc: 68.644% | clf_exit: 0.095 0.298 0.607
Batch: 240 | Loss: 5.265 | Acc: 34.213,57.744,70.296,% | Adaptive Acc: 68.620% | clf_exit: 0.095 0.298 0.607
Batch: 260 | Loss: 5.271 | Acc: 34.234,57.872,70.253,% | Adaptive Acc: 68.588% | clf_exit: 0.096 0.297 0.607
Batch: 280 | Loss: 5.277 | Acc: 34.305,57.849,70.148,% | Adaptive Acc: 68.491% | clf_exit: 0.096 0.298 0.606
Batch: 300 | Loss: 5.285 | Acc: 34.222,57.784,70.035,% | Adaptive Acc: 68.343% | clf_exit: 0.096 0.299 0.605
Batch: 320 | Loss: 5.290 | Acc: 34.256,57.766,69.930,% | Adaptive Acc: 68.251% | clf_exit: 0.097 0.299 0.604
Batch: 340 | Loss: 5.295 | Acc: 34.205,57.682,69.891,% | Adaptive Acc: 68.230% | clf_exit: 0.096 0.299 0.605
Batch: 360 | Loss: 5.299 | Acc: 34.165,57.579,69.828,% | Adaptive Acc: 68.196% | clf_exit: 0.096 0.298 0.605
Batch: 380 | Loss: 5.304 | Acc: 34.145,57.573,69.773,% | Adaptive Acc: 68.129% | clf_exit: 0.096 0.298 0.606
Batch: 0 | Loss: 5.372 | Acc: 38.281,53.125,68.750,% | Adaptive Acc: 66.406% | clf_exit: 0.203 0.344 0.453
Batch: 20 | Loss: 6.067 | Acc: 30.655,52.307,59.040,% | Adaptive Acc: 56.734% | clf_exit: 0.151 0.345 0.504
Batch: 40 | Loss: 6.073 | Acc: 29.707,51.562,59.089,% | Adaptive Acc: 56.402% | clf_exit: 0.148 0.345 0.507
Batch: 60 | Loss: 6.067 | Acc: 29.828,51.345,59.260,% | Adaptive Acc: 56.391% | clf_exit: 0.149 0.343 0.508
Train all parameters

Epoch: 20
Batch: 0 | Loss: 5.093 | Acc: 34.375,58.594,75.781,% | Adaptive Acc: 73.438% | clf_exit: 0.070 0.297 0.633
Batch: 20 | Loss: 5.093 | Acc: 34.970,58.519,72.321,% | Adaptive Acc: 70.647% | clf_exit: 0.099 0.317 0.584
Batch: 40 | Loss: 5.110 | Acc: 34.413,58.765,72.180,% | Adaptive Acc: 70.503% | clf_exit: 0.099 0.309 0.592
Batch: 60 | Loss: 5.119 | Acc: 34.452,59.004,72.221,% | Adaptive Acc: 70.530% | clf_exit: 0.098 0.309 0.593
Batch: 80 | Loss: 5.130 | Acc: 34.414,58.555,72.000,% | Adaptive Acc: 70.419% | clf_exit: 0.099 0.311 0.590
Batch: 100 | Loss: 5.133 | Acc: 34.568,58.687,71.999,% | Adaptive Acc: 70.328% | clf_exit: 0.099 0.311 0.590
Batch: 120 | Loss: 5.146 | Acc: 34.556,58.775,71.694,% | Adaptive Acc: 70.041% | clf_exit: 0.099 0.311 0.590
Batch: 140 | Loss: 5.170 | Acc: 34.497,58.394,71.476,% | Adaptive Acc: 69.858% | clf_exit: 0.098 0.309 0.593
Batch: 160 | Loss: 5.178 | Acc: 34.438,58.206,71.438,% | Adaptive Acc: 69.764% | clf_exit: 0.099 0.308 0.593
Batch: 180 | Loss: 5.165 | Acc: 34.466,58.361,71.590,% | Adaptive Acc: 69.937% | clf_exit: 0.099 0.306 0.595
Batch: 200 | Loss: 5.163 | Acc: 34.554,58.423,71.630,% | Adaptive Acc: 69.978% | clf_exit: 0.100 0.307 0.594
Batch: 220 | Loss: 5.163 | Acc: 34.594,58.459,71.560,% | Adaptive Acc: 69.902% | clf_exit: 0.100 0.307 0.592
Batch: 240 | Loss: 5.163 | Acc: 34.605,58.477,71.677,% | Adaptive Acc: 69.969% | clf_exit: 0.101 0.306 0.593
Batch: 260 | Loss: 5.165 | Acc: 34.674,58.399,71.504,% | Adaptive Acc: 69.789% | clf_exit: 0.101 0.307 0.592
Batch: 280 | Loss: 5.166 | Acc: 34.572,58.377,71.439,% | Adaptive Acc: 69.695% | clf_exit: 0.101 0.307 0.593
Batch: 300 | Loss: 5.176 | Acc: 34.577,58.350,71.353,% | Adaptive Acc: 69.596% | clf_exit: 0.101 0.306 0.593
Batch: 320 | Loss: 5.187 | Acc: 34.562,58.282,71.181,% | Adaptive Acc: 69.434% | clf_exit: 0.101 0.305 0.594
Batch: 340 | Loss: 5.191 | Acc: 34.510,58.333,71.158,% | Adaptive Acc: 69.410% | clf_exit: 0.100 0.306 0.594
Batch: 360 | Loss: 5.199 | Acc: 34.457,58.260,71.024,% | Adaptive Acc: 69.282% | clf_exit: 0.100 0.306 0.594
Batch: 380 | Loss: 5.201 | Acc: 34.504,58.286,70.971,% | Adaptive Acc: 69.224% | clf_exit: 0.100 0.306 0.593
Batch: 0 | Loss: 5.594 | Acc: 32.812,56.250,64.062,% | Adaptive Acc: 60.156% | clf_exit: 0.211 0.320 0.469
Batch: 20 | Loss: 5.942 | Acc: 32.738,52.493,60.342,% | Adaptive Acc: 57.552% | clf_exit: 0.170 0.330 0.500
Batch: 40 | Loss: 5.988 | Acc: 31.822,51.715,59.242,% | Adaptive Acc: 56.193% | clf_exit: 0.165 0.330 0.504
Batch: 60 | Loss: 5.990 | Acc: 31.506,51.947,59.503,% | Adaptive Acc: 56.404% | clf_exit: 0.163 0.336 0.501
Train all parameters

Epoch: 21
Batch: 0 | Loss: 5.347 | Acc: 34.375,57.031,75.781,% | Adaptive Acc: 75.781% | clf_exit: 0.102 0.328 0.570
Batch: 20 | Loss: 5.096 | Acc: 34.189,59.375,73.512,% | Adaptive Acc: 71.243% | clf_exit: 0.100 0.302 0.598
Batch: 40 | Loss: 5.034 | Acc: 34.985,60.328,73.819,% | Adaptive Acc: 71.723% | clf_exit: 0.103 0.302 0.595
Batch: 60 | Loss: 4.997 | Acc: 35.207,60.284,73.745,% | Adaptive Acc: 71.644% | clf_exit: 0.106 0.304 0.591
Batch: 80 | Loss: 5.021 | Acc: 35.041,59.963,73.534,% | Adaptive Acc: 71.393% | clf_exit: 0.105 0.304 0.591
Batch: 100 | Loss: 5.007 | Acc: 35.118,59.916,73.592,% | Adaptive Acc: 71.388% | clf_exit: 0.107 0.306 0.587
Batch: 120 | Loss: 5.016 | Acc: 35.214,59.678,73.373,% | Adaptive Acc: 71.204% | clf_exit: 0.108 0.307 0.586
Batch: 140 | Loss: 5.034 | Acc: 35.068,59.613,73.122,% | Adaptive Acc: 70.977% | clf_exit: 0.106 0.308 0.586
Batch: 160 | Loss: 5.031 | Acc: 35.268,59.584,73.035,% | Adaptive Acc: 70.997% | clf_exit: 0.106 0.311 0.583
Batch: 180 | Loss: 5.046 | Acc: 35.333,59.492,72.846,% | Adaptive Acc: 70.843% | clf_exit: 0.106 0.311 0.583
Batch: 200 | Loss: 5.060 | Acc: 35.152,59.223,72.777,% | Adaptive Acc: 70.775% | clf_exit: 0.105 0.311 0.583
Batch: 220 | Loss: 5.087 | Acc: 35.015,59.025,72.571,% | Adaptive Acc: 70.532% | clf_exit: 0.104 0.310 0.585
Batch: 240 | Loss: 5.091 | Acc: 35.091,59.015,72.452,% | Adaptive Acc: 70.452% | clf_exit: 0.104 0.310 0.586
Batch: 260 | Loss: 5.100 | Acc: 35.108,58.995,72.297,% | Adaptive Acc: 70.363% | clf_exit: 0.103 0.311 0.586
Batch: 280 | Loss: 5.102 | Acc: 35.109,58.958,72.259,% | Adaptive Acc: 70.312% | clf_exit: 0.103 0.310 0.587
Batch: 300 | Loss: 5.106 | Acc: 35.138,58.887,72.119,% | Adaptive Acc: 70.211% | clf_exit: 0.103 0.310 0.587
Batch: 320 | Loss: 5.105 | Acc: 35.173,58.917,72.004,% | Adaptive Acc: 70.110% | clf_exit: 0.104 0.310 0.586
Batch: 340 | Loss: 5.105 | Acc: 35.163,58.924,71.944,% | Adaptive Acc: 70.054% | clf_exit: 0.103 0.311 0.585
Batch: 360 | Loss: 5.116 | Acc: 35.078,58.771,71.758,% | Adaptive Acc: 69.890% | clf_exit: 0.104 0.311 0.585
Batch: 380 | Loss: 5.114 | Acc: 35.056,58.789,71.744,% | Adaptive Acc: 69.861% | clf_exit: 0.103 0.312 0.585
Batch: 0 | Loss: 5.454 | Acc: 32.812,60.156,60.938,% | Adaptive Acc: 60.156% | clf_exit: 0.180 0.383 0.438
Batch: 20 | Loss: 5.839 | Acc: 31.176,53.385,62.351,% | Adaptive Acc: 58.891% | clf_exit: 0.181 0.347 0.472
Batch: 40 | Loss: 5.821 | Acc: 31.441,52.896,61.452,% | Adaptive Acc: 58.365% | clf_exit: 0.177 0.345 0.478
Batch: 60 | Loss: 5.801 | Acc: 31.673,53.394,61.732,% | Adaptive Acc: 58.581% | clf_exit: 0.177 0.346 0.476
Train all parameters

Epoch: 22
Batch: 0 | Loss: 5.274 | Acc: 30.469,57.812,72.656,% | Adaptive Acc: 71.094% | clf_exit: 0.070 0.328 0.602
Batch: 20 | Loss: 4.966 | Acc: 35.938,59.487,74.107,% | Adaptive Acc: 72.061% | clf_exit: 0.115 0.321 0.564
Batch: 40 | Loss: 4.923 | Acc: 35.747,60.290,75.019,% | Adaptive Acc: 72.809% | clf_exit: 0.112 0.325 0.563
Batch: 60 | Loss: 4.932 | Acc: 35.528,59.759,74.872,% | Adaptive Acc: 72.618% | clf_exit: 0.113 0.318 0.570
Batch: 80 | Loss: 4.950 | Acc: 35.282,60.021,74.653,% | Adaptive Acc: 72.396% | clf_exit: 0.111 0.318 0.571
Batch: 100 | Loss: 4.967 | Acc: 35.272,60.087,74.660,% | Adaptive Acc: 72.416% | clf_exit: 0.110 0.316 0.574
Batch: 120 | Loss: 4.971 | Acc: 35.240,60.124,74.600,% | Adaptive Acc: 72.372% | clf_exit: 0.108 0.317 0.576
Batch: 140 | Loss: 4.980 | Acc: 34.979,60.084,74.490,% | Adaptive Acc: 72.352% | clf_exit: 0.107 0.316 0.576
Batch: 160 | Loss: 4.986 | Acc: 34.991,60.059,74.437,% | Adaptive Acc: 72.273% | clf_exit: 0.108 0.316 0.577
Batch: 180 | Loss: 4.991 | Acc: 35.048,59.992,74.206,% | Adaptive Acc: 72.048% | clf_exit: 0.107 0.315 0.578
Batch: 200 | Loss: 4.996 | Acc: 35.040,59.907,73.861,% | Adaptive Acc: 71.758% | clf_exit: 0.108 0.316 0.577
Batch: 220 | Loss: 4.993 | Acc: 35.167,59.955,73.688,% | Adaptive Acc: 71.663% | clf_exit: 0.108 0.316 0.576
Batch: 240 | Loss: 4.998 | Acc: 35.169,59.881,73.457,% | Adaptive Acc: 71.411% | clf_exit: 0.109 0.315 0.576
Batch: 260 | Loss: 5.004 | Acc: 35.180,59.833,73.330,% | Adaptive Acc: 71.279% | clf_exit: 0.109 0.316 0.574
Batch: 280 | Loss: 5.003 | Acc: 35.290,59.853,73.304,% | Adaptive Acc: 71.238% | clf_exit: 0.110 0.316 0.574
Batch: 300 | Loss: 5.005 | Acc: 35.309,59.959,73.292,% | Adaptive Acc: 71.234% | clf_exit: 0.110 0.316 0.574
Batch: 320 | Loss: 5.014 | Acc: 35.273,59.874,73.072,% | Adaptive Acc: 71.006% | clf_exit: 0.110 0.316 0.573
Batch: 340 | Loss: 5.026 | Acc: 35.191,59.797,72.846,% | Adaptive Acc: 70.784% | clf_exit: 0.110 0.317 0.574
Batch: 360 | Loss: 5.031 | Acc: 35.295,59.778,72.715,% | Adaptive Acc: 70.637% | clf_exit: 0.109 0.317 0.574
Batch: 380 | Loss: 5.031 | Acc: 35.386,59.756,72.634,% | Adaptive Acc: 70.567% | clf_exit: 0.110 0.317 0.573
Batch: 0 | Loss: 5.449 | Acc: 37.500,58.594,66.406,% | Adaptive Acc: 61.719% | clf_exit: 0.203 0.352 0.445
Batch: 20 | Loss: 5.718 | Acc: 32.887,54.799,62.500,% | Adaptive Acc: 60.231% | clf_exit: 0.160 0.333 0.506
Batch: 40 | Loss: 5.734 | Acc: 32.603,53.944,61.338,% | Adaptive Acc: 59.165% | clf_exit: 0.159 0.334 0.507
Batch: 60 | Loss: 5.736 | Acc: 32.287,54.022,61.373,% | Adaptive Acc: 59.055% | clf_exit: 0.160 0.332 0.508
Train all parameters

Epoch: 23
Batch: 0 | Loss: 4.434 | Acc: 42.969,66.406,76.562,% | Adaptive Acc: 76.562% | clf_exit: 0.102 0.352 0.547
Batch: 20 | Loss: 4.938 | Acc: 36.235,60.007,74.963,% | Adaptive Acc: 72.656% | clf_exit: 0.114 0.330 0.556
Batch: 40 | Loss: 4.904 | Acc: 36.128,60.652,75.248,% | Adaptive Acc: 72.999% | clf_exit: 0.114 0.323 0.563
Batch: 60 | Loss: 4.908 | Acc: 35.758,60.617,75.115,% | Adaptive Acc: 72.861% | clf_exit: 0.113 0.325 0.561
Batch: 80 | Loss: 4.934 | Acc: 35.571,60.349,74.932,% | Adaptive Acc: 72.849% | clf_exit: 0.112 0.322 0.567
Batch: 100 | Loss: 4.933 | Acc: 35.953,60.257,74.590,% | Adaptive Acc: 72.440% | clf_exit: 0.111 0.321 0.568
Batch: 120 | Loss: 4.936 | Acc: 35.847,60.363,74.613,% | Adaptive Acc: 72.501% | clf_exit: 0.113 0.318 0.569
Batch: 140 | Loss: 4.927 | Acc: 35.993,60.455,74.640,% | Adaptive Acc: 72.540% | clf_exit: 0.112 0.320 0.568
Batch: 160 | Loss: 4.928 | Acc: 36.209,60.394,74.471,% | Adaptive Acc: 72.355% | clf_exit: 0.112 0.320 0.567
Batch: 180 | Loss: 4.928 | Acc: 36.089,60.329,74.469,% | Adaptive Acc: 72.281% | clf_exit: 0.112 0.321 0.567
Batch: 200 | Loss: 4.923 | Acc: 36.050,60.452,74.456,% | Adaptive Acc: 72.205% | clf_exit: 0.113 0.322 0.565
Batch: 220 | Loss: 4.924 | Acc: 36.143,60.418,74.307,% | Adaptive Acc: 72.105% | clf_exit: 0.112 0.323 0.565
Batch: 240 | Loss: 4.922 | Acc: 36.164,60.503,74.345,% | Adaptive Acc: 72.173% | clf_exit: 0.112 0.322 0.565
Batch: 260 | Loss: 4.930 | Acc: 35.946,60.432,74.240,% | Adaptive Acc: 72.135% | clf_exit: 0.111 0.323 0.566
Batch: 280 | Loss: 4.936 | Acc: 35.943,60.376,74.105,% | Adaptive Acc: 72.006% | clf_exit: 0.111 0.323 0.566
Batch: 300 | Loss: 4.939 | Acc: 35.792,60.351,74.055,% | Adaptive Acc: 71.953% | clf_exit: 0.111 0.324 0.565
Batch: 320 | Loss: 4.943 | Acc: 35.845,60.271,73.963,% | Adaptive Acc: 71.853% | clf_exit: 0.112 0.322 0.566
Batch: 340 | Loss: 4.948 | Acc: 35.816,60.209,73.884,% | Adaptive Acc: 71.756% | clf_exit: 0.112 0.322 0.566
Batch: 360 | Loss: 4.955 | Acc: 35.847,60.091,73.814,% | Adaptive Acc: 71.726% | clf_exit: 0.113 0.321 0.566
Batch: 380 | Loss: 4.962 | Acc: 35.831,60.056,73.643,% | Adaptive Acc: 71.586% | clf_exit: 0.113 0.321 0.566
Batch: 0 | Loss: 5.410 | Acc: 35.938,54.688,67.188,% | Adaptive Acc: 63.281% | clf_exit: 0.227 0.367 0.406
Batch: 20 | Loss: 5.739 | Acc: 33.147,53.609,61.756,% | Adaptive Acc: 58.222% | clf_exit: 0.198 0.352 0.450
Batch: 40 | Loss: 5.767 | Acc: 32.603,53.316,61.128,% | Adaptive Acc: 57.679% | clf_exit: 0.187 0.356 0.457
Batch: 60 | Loss: 5.767 | Acc: 32.774,53.599,61.142,% | Adaptive Acc: 57.403% | clf_exit: 0.188 0.354 0.458
Train all parameters

Epoch: 24
Batch: 0 | Loss: 5.115 | Acc: 31.250,57.031,75.781,% | Adaptive Acc: 71.094% | clf_exit: 0.117 0.297 0.586
Batch: 20 | Loss: 4.873 | Acc: 34.970,61.533,76.972,% | Adaptive Acc: 74.777% | clf_exit: 0.098 0.320 0.583
Batch: 40 | Loss: 4.847 | Acc: 35.194,61.338,76.105,% | Adaptive Acc: 73.571% | clf_exit: 0.110 0.325 0.565
Batch: 60 | Loss: 4.794 | Acc: 36.040,61.668,76.960,% | Adaptive Acc: 74.116% | clf_exit: 0.111 0.330 0.558
Batch: 80 | Loss: 4.780 | Acc: 35.899,61.410,76.900,% | Adaptive Acc: 74.122% | clf_exit: 0.113 0.332 0.555
Batch: 100 | Loss: 4.799 | Acc: 35.891,61.324,76.624,% | Adaptive Acc: 73.847% | clf_exit: 0.113 0.328 0.559
Batch: 120 | Loss: 4.815 | Acc: 36.009,61.144,76.375,% | Adaptive Acc: 73.676% | clf_exit: 0.113 0.328 0.559
Batch: 140 | Loss: 4.816 | Acc: 36.259,61.325,76.269,% | Adaptive Acc: 73.598% | clf_exit: 0.114 0.329 0.557
Batch: 160 | Loss: 4.831 | Acc: 36.098,61.185,76.029,% | Adaptive Acc: 73.428% | clf_exit: 0.113 0.329 0.557
Batch: 180 | Loss: 4.838 | Acc: 36.149,61.162,75.898,% | Adaptive Acc: 73.399% | clf_exit: 0.113 0.329 0.558
Batch: 200 | Loss: 4.838 | Acc: 36.280,61.213,75.773,% | Adaptive Acc: 73.251% | clf_exit: 0.114 0.330 0.556
Batch: 220 | Loss: 4.841 | Acc: 36.330,61.171,75.668,% | Adaptive Acc: 73.169% | clf_exit: 0.113 0.330 0.557
Batch: 240 | Loss: 4.846 | Acc: 36.346,61.093,75.486,% | Adaptive Acc: 73.039% | clf_exit: 0.113 0.330 0.557
Batch: 260 | Loss: 4.857 | Acc: 36.330,61.024,75.287,% | Adaptive Acc: 72.926% | clf_exit: 0.114 0.329 0.558
Batch: 280 | Loss: 4.859 | Acc: 36.330,61.035,75.217,% | Adaptive Acc: 72.876% | clf_exit: 0.114 0.328 0.558
Batch: 300 | Loss: 4.866 | Acc: 36.337,60.956,75.086,% | Adaptive Acc: 72.781% | clf_exit: 0.114 0.328 0.558
Batch: 320 | Loss: 4.875 | Acc: 36.273,60.862,74.968,% | Adaptive Acc: 72.690% | clf_exit: 0.114 0.327 0.559
Batch: 340 | Loss: 4.882 | Acc: 36.322,60.834,74.860,% | Adaptive Acc: 72.649% | clf_exit: 0.114 0.327 0.559
Batch: 360 | Loss: 4.887 | Acc: 36.290,60.784,74.764,% | Adaptive Acc: 72.591% | clf_exit: 0.114 0.327 0.559
Batch: 380 | Loss: 4.891 | Acc: 36.319,60.800,74.690,% | Adaptive Acc: 72.529% | clf_exit: 0.114 0.327 0.559
Batch: 0 | Loss: 5.564 | Acc: 36.719,59.375,67.188,% | Adaptive Acc: 65.625% | clf_exit: 0.219 0.312 0.469
Batch: 20 | Loss: 5.996 | Acc: 29.911,54.464,62.314,% | Adaptive Acc: 59.338% | clf_exit: 0.176 0.360 0.464
Batch: 40 | Loss: 6.029 | Acc: 29.554,53.506,61.376,% | Adaptive Acc: 58.041% | clf_exit: 0.172 0.358 0.470
Batch: 60 | Loss: 6.023 | Acc: 29.585,53.291,61.488,% | Adaptive Acc: 57.902% | clf_exit: 0.170 0.362 0.468
Train all parameters

Epoch: 25
Batch: 0 | Loss: 4.469 | Acc: 40.625,65.625,79.688,% | Adaptive Acc: 76.562% | clf_exit: 0.125 0.391 0.484
Batch: 20 | Loss: 4.716 | Acc: 36.496,61.682,78.162,% | Adaptive Acc: 75.670% | clf_exit: 0.121 0.336 0.543
Batch: 40 | Loss: 4.678 | Acc: 36.700,62.557,78.068,% | Adaptive Acc: 75.724% | clf_exit: 0.123 0.333 0.545
Batch: 60 | Loss: 4.682 | Acc: 36.808,62.474,77.792,% | Adaptive Acc: 75.653% | clf_exit: 0.120 0.334 0.546
Batch: 80 | Loss: 4.714 | Acc: 36.439,62.297,77.845,% | Adaptive Acc: 75.453% | clf_exit: 0.122 0.329 0.549
Batch: 100 | Loss: 4.695 | Acc: 36.850,62.399,77.785,% | Adaptive Acc: 75.278% | clf_exit: 0.121 0.334 0.544
Batch: 120 | Loss: 4.710 | Acc: 36.841,62.177,77.563,% | Adaptive Acc: 75.116% | clf_exit: 0.122 0.334 0.544
Batch: 140 | Loss: 4.710 | Acc: 37.012,62.240,77.460,% | Adaptive Acc: 75.050% | clf_exit: 0.122 0.334 0.544
Batch: 160 | Loss: 4.732 | Acc: 36.952,62.000,77.028,% | Adaptive Acc: 74.709% | clf_exit: 0.121 0.333 0.546
Batch: 180 | Loss: 4.745 | Acc: 36.978,61.840,76.627,% | Adaptive Acc: 74.301% | clf_exit: 0.122 0.334 0.545
Batch: 200 | Loss: 4.760 | Acc: 36.890,61.742,76.508,% | Adaptive Acc: 74.203% | clf_exit: 0.121 0.333 0.546
Batch: 220 | Loss: 4.771 | Acc: 36.857,61.712,76.266,% | Adaptive Acc: 74.031% | clf_exit: 0.121 0.332 0.547
Batch: 240 | Loss: 4.780 | Acc: 36.712,61.693,76.138,% | Adaptive Acc: 73.927% | clf_exit: 0.121 0.331 0.547
Batch: 260 | Loss: 4.789 | Acc: 36.656,61.632,75.940,% | Adaptive Acc: 73.716% | clf_exit: 0.121 0.332 0.547
Batch: 280 | Loss: 4.797 | Acc: 36.535,61.591,75.901,% | Adaptive Acc: 73.660% | clf_exit: 0.120 0.332 0.549
Batch: 300 | Loss: 4.798 | Acc: 36.607,61.641,75.818,% | Adaptive Acc: 73.580% | clf_exit: 0.120 0.333 0.547
Batch: 320 | Loss: 4.796 | Acc: 36.609,61.716,75.866,% | Adaptive Acc: 73.627% | clf_exit: 0.120 0.333 0.547
Batch: 340 | Loss: 4.798 | Acc: 36.616,61.680,75.804,% | Adaptive Acc: 73.584% | clf_exit: 0.119 0.333 0.547
Batch: 360 | Loss: 4.801 | Acc: 36.587,61.595,75.734,% | Adaptive Acc: 73.502% | clf_exit: 0.120 0.333 0.547
Batch: 380 | Loss: 4.816 | Acc: 36.540,61.389,75.498,% | Adaptive Acc: 73.280% | clf_exit: 0.119 0.332 0.549
Batch: 0 | Loss: 5.369 | Acc: 36.719,60.938,62.500,% | Adaptive Acc: 60.156% | clf_exit: 0.211 0.320 0.469
Batch: 20 | Loss: 5.622 | Acc: 34.747,56.473,63.579,% | Adaptive Acc: 61.161% | clf_exit: 0.174 0.353 0.473
Batch: 40 | Loss: 5.607 | Acc: 35.004,56.098,63.072,% | Adaptive Acc: 60.880% | clf_exit: 0.167 0.357 0.475
Batch: 60 | Loss: 5.578 | Acc: 34.746,55.981,63.256,% | Adaptive Acc: 60.976% | clf_exit: 0.167 0.358 0.475
Train all parameters

Epoch: 26
Batch: 0 | Loss: 4.913 | Acc: 39.062,62.500,75.000,% | Adaptive Acc: 71.094% | clf_exit: 0.102 0.320 0.578
Batch: 20 | Loss: 4.712 | Acc: 37.277,63.542,77.232,% | Adaptive Acc: 75.037% | clf_exit: 0.116 0.345 0.539
Batch: 40 | Loss: 4.671 | Acc: 37.481,62.786,77.820,% | Adaptive Acc: 75.572% | clf_exit: 0.117 0.343 0.540
Batch: 60 | Loss: 4.646 | Acc: 37.398,63.051,77.779,% | Adaptive Acc: 75.461% | clf_exit: 0.121 0.341 0.538
Batch: 80 | Loss: 4.654 | Acc: 37.211,63.011,77.884,% | Adaptive Acc: 75.530% | clf_exit: 0.123 0.340 0.537
Batch: 100 | Loss: 4.636 | Acc: 37.392,62.972,78.295,% | Adaptive Acc: 75.874% | clf_exit: 0.124 0.341 0.535
Batch: 120 | Loss: 4.649 | Acc: 37.345,62.926,78.022,% | Adaptive Acc: 75.594% | clf_exit: 0.124 0.340 0.536
Batch: 140 | Loss: 4.643 | Acc: 37.334,62.904,77.926,% | Adaptive Acc: 75.404% | clf_exit: 0.126 0.339 0.535
Batch: 160 | Loss: 4.659 | Acc: 37.359,62.874,77.780,% | Adaptive Acc: 75.364% | clf_exit: 0.126 0.339 0.535
Batch: 180 | Loss: 4.673 | Acc: 37.215,62.798,77.529,% | Adaptive Acc: 75.112% | clf_exit: 0.125 0.339 0.536
Batch: 200 | Loss: 4.682 | Acc: 37.383,62.702,77.503,% | Adaptive Acc: 75.062% | clf_exit: 0.124 0.340 0.536
Batch: 220 | Loss: 4.690 | Acc: 37.383,62.599,77.354,% | Adaptive Acc: 74.844% | clf_exit: 0.124 0.339 0.536
Batch: 240 | Loss: 4.704 | Acc: 37.241,62.448,77.026,% | Adaptive Acc: 74.553% | clf_exit: 0.123 0.338 0.538
Batch: 260 | Loss: 4.716 | Acc: 37.213,62.305,76.883,% | Adaptive Acc: 74.413% | clf_exit: 0.123 0.339 0.538
Batch: 280 | Loss: 4.723 | Acc: 37.236,62.214,76.643,% | Adaptive Acc: 74.191% | clf_exit: 0.123 0.338 0.538
Batch: 300 | Loss: 4.724 | Acc: 37.344,62.251,76.531,% | Adaptive Acc: 74.125% | clf_exit: 0.124 0.338 0.539
Batch: 320 | Loss: 4.722 | Acc: 37.327,62.244,76.416,% | Adaptive Acc: 74.041% | clf_exit: 0.123 0.338 0.539
Batch: 340 | Loss: 4.726 | Acc: 37.422,62.202,76.288,% | Adaptive Acc: 73.907% | clf_exit: 0.123 0.338 0.539
Batch: 360 | Loss: 4.730 | Acc: 37.349,62.184,76.201,% | Adaptive Acc: 73.810% | clf_exit: 0.123 0.338 0.539
Batch: 380 | Loss: 4.747 | Acc: 37.190,61.989,75.986,% | Adaptive Acc: 73.573% | clf_exit: 0.123 0.337 0.540
Batch: 0 | Loss: 5.064 | Acc: 37.500,62.500,70.312,% | Adaptive Acc: 67.969% | clf_exit: 0.219 0.367 0.414
Batch: 20 | Loss: 5.611 | Acc: 34.784,56.324,62.500,% | Adaptive Acc: 59.710% | clf_exit: 0.179 0.366 0.455
Batch: 40 | Loss: 5.568 | Acc: 34.889,55.755,62.329,% | Adaptive Acc: 59.889% | clf_exit: 0.175 0.365 0.460
Batch: 60 | Loss: 5.572 | Acc: 34.465,55.866,61.936,% | Adaptive Acc: 59.516% | clf_exit: 0.173 0.365 0.462
Train all parameters

Epoch: 27
Batch: 0 | Loss: 4.749 | Acc: 32.031,65.625,83.594,% | Adaptive Acc: 78.125% | clf_exit: 0.156 0.297 0.547
Batch: 20 | Loss: 4.643 | Acc: 37.091,62.798,79.539,% | Adaptive Acc: 77.083% | clf_exit: 0.123 0.333 0.544
Batch: 40 | Loss: 4.584 | Acc: 38.053,62.957,78.944,% | Adaptive Acc: 76.601% | clf_exit: 0.130 0.334 0.535
Batch: 60 | Loss: 4.600 | Acc: 37.705,62.999,78.432,% | Adaptive Acc: 76.089% | clf_exit: 0.132 0.335 0.533
Batch: 80 | Loss: 4.613 | Acc: 37.548,62.828,78.279,% | Adaptive Acc: 76.013% | clf_exit: 0.130 0.338 0.532
Batch: 100 | Loss: 4.629 | Acc: 37.554,63.057,78.210,% | Adaptive Acc: 75.774% | clf_exit: 0.129 0.339 0.532
Batch: 120 | Loss: 4.641 | Acc: 37.545,62.939,78.073,% | Adaptive Acc: 75.575% | clf_exit: 0.129 0.337 0.534
Batch: 140 | Loss: 4.639 | Acc: 37.406,62.921,77.992,% | Adaptive Acc: 75.515% | clf_exit: 0.127 0.338 0.534
Batch: 160 | Loss: 4.653 | Acc: 37.398,62.811,77.780,% | Adaptive Acc: 75.277% | clf_exit: 0.127 0.339 0.534
Batch: 180 | Loss: 4.654 | Acc: 37.267,62.871,77.866,% | Adaptive Acc: 75.341% | clf_exit: 0.128 0.338 0.534
Batch: 200 | Loss: 4.663 | Acc: 37.317,62.753,77.666,% | Adaptive Acc: 75.132% | clf_exit: 0.127 0.339 0.534
Batch: 220 | Loss: 4.663 | Acc: 37.461,62.751,77.559,% | Adaptive Acc: 75.057% | clf_exit: 0.126 0.339 0.535
Batch: 240 | Loss: 4.663 | Acc: 37.461,62.682,77.532,% | Adaptive Acc: 75.013% | clf_exit: 0.127 0.340 0.534
Batch: 260 | Loss: 4.664 | Acc: 37.488,62.692,77.514,% | Adaptive Acc: 74.997% | clf_exit: 0.127 0.339 0.533
Batch: 280 | Loss: 4.671 | Acc: 37.444,62.625,77.372,% | Adaptive Acc: 74.861% | clf_exit: 0.127 0.339 0.534
Batch: 300 | Loss: 4.674 | Acc: 37.448,62.619,77.287,% | Adaptive Acc: 74.790% | clf_exit: 0.127 0.340 0.533
Batch: 320 | Loss: 4.676 | Acc: 37.439,62.612,77.120,% | Adaptive Acc: 74.654% | clf_exit: 0.127 0.341 0.532
Batch: 340 | Loss: 4.675 | Acc: 37.484,62.566,77.064,% | Adaptive Acc: 74.622% | clf_exit: 0.127 0.341 0.533
Batch: 360 | Loss: 4.684 | Acc: 37.504,62.509,76.924,% | Adaptive Acc: 74.483% | clf_exit: 0.127 0.340 0.533
Batch: 380 | Loss: 4.692 | Acc: 37.482,62.445,76.802,% | Adaptive Acc: 74.405% | clf_exit: 0.127 0.340 0.534
Batch: 0 | Loss: 5.300 | Acc: 33.594,57.812,67.969,% | Adaptive Acc: 66.406% | clf_exit: 0.156 0.375 0.469
Batch: 20 | Loss: 5.687 | Acc: 33.817,56.250,61.979,% | Adaptive Acc: 60.156% | clf_exit: 0.145 0.371 0.484
Batch: 40 | Loss: 5.668 | Acc: 33.803,55.774,61.871,% | Adaptive Acc: 60.232% | clf_exit: 0.145 0.367 0.488
Batch: 60 | Loss: 5.678 | Acc: 33.747,55.610,61.629,% | Adaptive Acc: 60.028% | clf_exit: 0.145 0.364 0.491
Train all parameters

Epoch: 28
Batch: 0 | Loss: 4.962 | Acc: 32.812,66.406,73.438,% | Adaptive Acc: 73.438% | clf_exit: 0.055 0.352 0.594
Batch: 20 | Loss: 4.479 | Acc: 39.360,64.658,80.580,% | Adaptive Acc: 78.497% | clf_exit: 0.121 0.352 0.528
Batch: 40 | Loss: 4.561 | Acc: 38.472,63.396,79.116,% | Adaptive Acc: 76.639% | clf_exit: 0.123 0.345 0.531
Batch: 60 | Loss: 4.587 | Acc: 37.756,63.038,79.175,% | Adaptive Acc: 76.767% | clf_exit: 0.126 0.340 0.533
Batch: 80 | Loss: 4.577 | Acc: 37.915,63.339,79.090,% | Adaptive Acc: 76.630% | clf_exit: 0.130 0.340 0.530
Batch: 100 | Loss: 4.557 | Acc: 38.088,63.382,79.440,% | Adaptive Acc: 76.856% | clf_exit: 0.131 0.342 0.527
Batch: 120 | Loss: 4.561 | Acc: 38.004,63.268,79.423,% | Adaptive Acc: 76.821% | clf_exit: 0.131 0.340 0.528
Batch: 140 | Loss: 4.569 | Acc: 37.910,63.209,79.305,% | Adaptive Acc: 76.701% | clf_exit: 0.133 0.340 0.528
Batch: 160 | Loss: 4.576 | Acc: 37.859,63.383,79.091,% | Adaptive Acc: 76.567% | clf_exit: 0.131 0.341 0.528
Batch: 180 | Loss: 4.592 | Acc: 37.927,63.268,78.738,% | Adaptive Acc: 76.260% | clf_exit: 0.130 0.341 0.530
Batch: 200 | Loss: 4.612 | Acc: 37.776,63.087,78.471,% | Adaptive Acc: 75.976% | clf_exit: 0.129 0.341 0.530
Batch: 220 | Loss: 4.605 | Acc: 37.740,63.101,78.546,% | Adaptive Acc: 76.032% | clf_exit: 0.129 0.341 0.530
Batch: 240 | Loss: 4.608 | Acc: 37.834,62.993,78.349,% | Adaptive Acc: 75.775% | clf_exit: 0.131 0.339 0.530
Batch: 260 | Loss: 4.618 | Acc: 37.763,62.838,78.107,% | Adaptive Acc: 75.566% | clf_exit: 0.130 0.340 0.530
Batch: 280 | Loss: 4.627 | Acc: 37.678,62.711,77.955,% | Adaptive Acc: 75.386% | clf_exit: 0.130 0.339 0.530
Batch: 300 | Loss: 4.619 | Acc: 37.817,62.832,77.923,% | Adaptive Acc: 75.366% | clf_exit: 0.131 0.339 0.529
Batch: 320 | Loss: 4.627 | Acc: 37.848,62.770,77.762,% | Adaptive Acc: 75.204% | clf_exit: 0.131 0.339 0.530
Batch: 340 | Loss: 4.630 | Acc: 37.775,62.809,77.706,% | Adaptive Acc: 75.163% | clf_exit: 0.130 0.339 0.531
Batch: 360 | Loss: 4.629 | Acc: 37.773,62.851,77.649,% | Adaptive Acc: 75.121% | clf_exit: 0.130 0.339 0.531
Batch: 380 | Loss: 4.634 | Acc: 37.830,62.832,77.547,% | Adaptive Acc: 75.031% | clf_exit: 0.130 0.340 0.530
Batch: 0 | Loss: 5.281 | Acc: 32.812,59.375,68.750,% | Adaptive Acc: 64.062% | clf_exit: 0.203 0.383 0.414
Batch: 20 | Loss: 5.564 | Acc: 35.119,56.845,64.100,% | Adaptive Acc: 60.714% | clf_exit: 0.198 0.367 0.435
Batch: 40 | Loss: 5.593 | Acc: 34.604,56.269,63.186,% | Adaptive Acc: 60.423% | clf_exit: 0.187 0.370 0.442
Batch: 60 | Loss: 5.605 | Acc: 34.541,56.352,62.961,% | Adaptive Acc: 60.374% | clf_exit: 0.185 0.370 0.445
Train all parameters

Epoch: 29
Batch: 0 | Loss: 4.593 | Acc: 31.250,66.406,79.688,% | Adaptive Acc: 76.562% | clf_exit: 0.094 0.367 0.539
Batch: 20 | Loss: 4.309 | Acc: 38.542,66.146,82.106,% | Adaptive Acc: 79.204% | clf_exit: 0.134 0.361 0.505
Batch: 40 | Loss: 4.384 | Acc: 38.281,64.996,81.593,% | Adaptive Acc: 78.659% | clf_exit: 0.132 0.357 0.512
Batch: 60 | Loss: 4.397 | Acc: 38.358,65.279,81.673,% | Adaptive Acc: 78.753% | clf_exit: 0.134 0.352 0.514
Batch: 80 | Loss: 4.416 | Acc: 38.648,64.863,81.356,% | Adaptive Acc: 78.482% | clf_exit: 0.135 0.352 0.512
Batch: 100 | Loss: 4.455 | Acc: 38.436,64.604,81.026,% | Adaptive Acc: 78.055% | clf_exit: 0.135 0.349 0.516
Batch: 120 | Loss: 4.472 | Acc: 38.397,64.288,80.682,% | Adaptive Acc: 77.731% | clf_exit: 0.135 0.350 0.515
Batch: 140 | Loss: 4.491 | Acc: 38.237,64.129,80.336,% | Adaptive Acc: 77.427% | clf_exit: 0.134 0.349 0.517
Batch: 160 | Loss: 4.508 | Acc: 38.184,63.912,79.974,% | Adaptive Acc: 77.072% | clf_exit: 0.134 0.349 0.517
Batch: 180 | Loss: 4.512 | Acc: 38.264,63.808,79.873,% | Adaptive Acc: 76.891% | clf_exit: 0.134 0.351 0.515
Batch: 200 | Loss: 4.520 | Acc: 38.258,63.853,79.707,% | Adaptive Acc: 76.885% | clf_exit: 0.133 0.351 0.516
Batch: 220 | Loss: 4.537 | Acc: 38.129,63.628,79.479,% | Adaptive Acc: 76.633% | clf_exit: 0.134 0.348 0.518
Batch: 240 | Loss: 4.545 | Acc: 38.122,63.667,79.399,% | Adaptive Acc: 76.582% | clf_exit: 0.133 0.348 0.518
Batch: 260 | Loss: 4.554 | Acc: 38.108,63.557,79.274,% | Adaptive Acc: 76.473% | clf_exit: 0.133 0.349 0.519
Batch: 280 | Loss: 4.552 | Acc: 38.128,63.537,79.115,% | Adaptive Acc: 76.321% | clf_exit: 0.132 0.350 0.518
Batch: 300 | Loss: 4.555 | Acc: 38.146,63.510,78.935,% | Adaptive Acc: 76.116% | clf_exit: 0.133 0.349 0.518
Batch: 320 | Loss: 4.563 | Acc: 38.060,63.366,78.780,% | Adaptive Acc: 75.954% | clf_exit: 0.133 0.348 0.519
Batch: 340 | Loss: 4.564 | Acc: 38.132,63.400,78.645,% | Adaptive Acc: 75.861% | clf_exit: 0.134 0.348 0.518
Batch: 360 | Loss: 4.570 | Acc: 38.037,63.353,78.519,% | Adaptive Acc: 75.751% | clf_exit: 0.134 0.348 0.518
Batch: 380 | Loss: 4.573 | Acc: 38.111,63.351,78.363,% | Adaptive Acc: 75.638% | clf_exit: 0.134 0.347 0.519
Batch: 0 | Loss: 5.088 | Acc: 39.844,60.156,65.625,% | Adaptive Acc: 62.500% | clf_exit: 0.211 0.438 0.352
Batch: 20 | Loss: 5.645 | Acc: 32.775,55.766,63.579,% | Adaptive Acc: 59.784% | clf_exit: 0.208 0.374 0.419
Batch: 40 | Loss: 5.618 | Acc: 33.308,56.002,63.262,% | Adaptive Acc: 59.413% | clf_exit: 0.207 0.367 0.427
Batch: 60 | Loss: 5.616 | Acc: 33.043,55.968,63.691,% | Adaptive Acc: 59.708% | clf_exit: 0.203 0.367 0.429
Train all parameters

Epoch: 30
Batch: 0 | Loss: 4.556 | Acc: 36.719,61.719,80.469,% | Adaptive Acc: 75.781% | clf_exit: 0.125 0.281 0.594
Batch: 20 | Loss: 4.452 | Acc: 37.872,63.653,80.766,% | Adaptive Acc: 77.604% | clf_exit: 0.125 0.359 0.516
Batch: 40 | Loss: 4.466 | Acc: 37.976,64.139,80.602,% | Adaptive Acc: 77.553% | clf_exit: 0.129 0.355 0.516
Batch: 60 | Loss: 4.416 | Acc: 38.601,64.793,80.891,% | Adaptive Acc: 77.997% | clf_exit: 0.134 0.358 0.508
Batch: 80 | Loss: 4.426 | Acc: 38.542,64.593,80.623,% | Adaptive Acc: 77.913% | clf_exit: 0.134 0.355 0.511
Batch: 100 | Loss: 4.449 | Acc: 38.513,64.310,80.260,% | Adaptive Acc: 77.630% | clf_exit: 0.132 0.354 0.514
Batch: 120 | Loss: 4.452 | Acc: 38.488,64.379,80.385,% | Adaptive Acc: 77.712% | clf_exit: 0.132 0.353 0.515
Batch: 140 | Loss: 4.460 | Acc: 38.508,64.317,80.436,% | Adaptive Acc: 77.793% | clf_exit: 0.133 0.350 0.518
Batch: 160 | Loss: 4.462 | Acc: 38.563,64.373,80.275,% | Adaptive Acc: 77.669% | clf_exit: 0.134 0.349 0.517
Batch: 180 | Loss: 4.463 | Acc: 38.527,64.309,80.016,% | Adaptive Acc: 77.413% | clf_exit: 0.134 0.350 0.516
Batch: 200 | Loss: 4.462 | Acc: 38.476,64.338,79.925,% | Adaptive Acc: 77.371% | clf_exit: 0.134 0.350 0.516
Batch: 220 | Loss: 4.479 | Acc: 38.387,64.116,79.723,% | Adaptive Acc: 77.156% | clf_exit: 0.134 0.348 0.517
Batch: 240 | Loss: 4.494 | Acc: 38.249,64.075,79.551,% | Adaptive Acc: 76.987% | clf_exit: 0.134 0.348 0.518
Batch: 260 | Loss: 4.507 | Acc: 38.206,64.033,79.352,% | Adaptive Acc: 76.814% | clf_exit: 0.134 0.348 0.518
Batch: 280 | Loss: 4.511 | Acc: 38.162,64.051,79.298,% | Adaptive Acc: 76.746% | clf_exit: 0.134 0.347 0.519
Batch: 300 | Loss: 4.514 | Acc: 38.245,64.060,79.168,% | Adaptive Acc: 76.633% | clf_exit: 0.134 0.348 0.518
Batch: 320 | Loss: 4.519 | Acc: 38.218,64.004,79.033,% | Adaptive Acc: 76.504% | clf_exit: 0.134 0.348 0.518
Batch: 340 | Loss: 4.521 | Acc: 38.235,63.914,78.970,% | Adaptive Acc: 76.434% | clf_exit: 0.134 0.348 0.518
Batch: 360 | Loss: 4.523 | Acc: 38.262,63.846,78.820,% | Adaptive Acc: 76.277% | clf_exit: 0.134 0.348 0.518
Batch: 380 | Loss: 4.527 | Acc: 38.234,63.767,78.789,% | Adaptive Acc: 76.232% | clf_exit: 0.134 0.348 0.518
Batch: 0 | Loss: 5.150 | Acc: 34.375,60.156,72.656,% | Adaptive Acc: 65.625% | clf_exit: 0.234 0.391 0.375
Batch: 20 | Loss: 5.500 | Acc: 35.454,57.999,64.025,% | Adaptive Acc: 61.161% | clf_exit: 0.196 0.386 0.418
Batch: 40 | Loss: 5.497 | Acc: 35.537,56.936,62.824,% | Adaptive Acc: 60.232% | clf_exit: 0.191 0.384 0.425
Batch: 60 | Loss: 5.508 | Acc: 35.387,56.788,62.833,% | Adaptive Acc: 60.451% | clf_exit: 0.189 0.380 0.432
Train all parameters

Epoch: 31
Batch: 0 | Loss: 4.208 | Acc: 38.281,61.719,84.375,% | Adaptive Acc: 82.812% | clf_exit: 0.133 0.352 0.516
Batch: 20 | Loss: 4.341 | Acc: 39.174,64.955,82.403,% | Adaptive Acc: 79.204% | clf_exit: 0.134 0.358 0.508
Batch: 40 | Loss: 4.380 | Acc: 38.472,64.425,81.860,% | Adaptive Acc: 78.716% | clf_exit: 0.134 0.349 0.517
Batch: 60 | Loss: 4.387 | Acc: 38.358,64.498,81.865,% | Adaptive Acc: 78.778% | clf_exit: 0.130 0.353 0.517
Batch: 80 | Loss: 4.379 | Acc: 38.628,64.670,81.481,% | Adaptive Acc: 78.540% | clf_exit: 0.135 0.355 0.510
Batch: 100 | Loss: 4.375 | Acc: 38.606,64.743,81.474,% | Adaptive Acc: 78.504% | clf_exit: 0.135 0.356 0.509
Batch: 120 | Loss: 4.393 | Acc: 38.552,64.592,81.211,% | Adaptive Acc: 78.151% | clf_exit: 0.134 0.359 0.507
Batch: 140 | Loss: 4.411 | Acc: 38.558,64.334,80.884,% | Adaptive Acc: 77.892% | clf_exit: 0.135 0.356 0.509
Batch: 160 | Loss: 4.416 | Acc: 38.718,64.349,80.755,% | Adaptive Acc: 77.742% | clf_exit: 0.135 0.357 0.508
Batch: 180 | Loss: 4.431 | Acc: 38.657,64.321,80.633,% | Adaptive Acc: 77.689% | clf_exit: 0.135 0.355 0.510
Batch: 200 | Loss: 4.448 | Acc: 38.604,64.129,80.434,% | Adaptive Acc: 77.561% | clf_exit: 0.135 0.353 0.512
Batch: 220 | Loss: 4.448 | Acc: 38.599,64.123,80.324,% | Adaptive Acc: 77.499% | clf_exit: 0.136 0.352 0.512
Batch: 240 | Loss: 4.446 | Acc: 38.521,64.176,80.329,% | Adaptive Acc: 77.506% | clf_exit: 0.136 0.353 0.512
Batch: 260 | Loss: 4.458 | Acc: 38.548,64.021,80.047,% | Adaptive Acc: 77.242% | clf_exit: 0.136 0.353 0.511
Batch: 280 | Loss: 4.463 | Acc: 38.531,63.943,79.888,% | Adaptive Acc: 77.071% | clf_exit: 0.136 0.353 0.511
Batch: 300 | Loss: 4.469 | Acc: 38.479,63.889,79.734,% | Adaptive Acc: 76.965% | clf_exit: 0.135 0.353 0.512
Batch: 320 | Loss: 4.480 | Acc: 38.488,63.860,79.561,% | Adaptive Acc: 76.825% | clf_exit: 0.134 0.353 0.512
Batch: 340 | Loss: 4.480 | Acc: 38.563,63.868,79.468,% | Adaptive Acc: 76.723% | clf_exit: 0.135 0.354 0.512
Batch: 360 | Loss: 4.484 | Acc: 38.530,63.848,79.354,% | Adaptive Acc: 76.625% | clf_exit: 0.135 0.353 0.512
Batch: 380 | Loss: 4.488 | Acc: 38.511,63.874,79.318,% | Adaptive Acc: 76.608% | clf_exit: 0.135 0.353 0.512
Batch: 0 | Loss: 5.235 | Acc: 39.844,60.156,68.750,% | Adaptive Acc: 67.188% | clf_exit: 0.227 0.383 0.391
Batch: 20 | Loss: 5.818 | Acc: 32.515,55.208,63.802,% | Adaptive Acc: 58.631% | clf_exit: 0.204 0.369 0.427
Batch: 40 | Loss: 5.846 | Acc: 32.450,54.345,62.462,% | Adaptive Acc: 57.698% | clf_exit: 0.202 0.370 0.429
Batch: 60 | Loss: 5.852 | Acc: 32.159,54.393,62.205,% | Adaptive Acc: 57.531% | clf_exit: 0.198 0.373 0.428
Train all parameters

Epoch: 32
Batch: 0 | Loss: 4.228 | Acc: 39.844,71.094,83.594,% | Adaptive Acc: 75.781% | clf_exit: 0.164 0.383 0.453
Batch: 20 | Loss: 4.324 | Acc: 39.472,66.071,82.478,% | Adaptive Acc: 79.278% | clf_exit: 0.154 0.351 0.495
Batch: 40 | Loss: 4.336 | Acc: 38.796,65.473,82.088,% | Adaptive Acc: 78.830% | clf_exit: 0.144 0.360 0.496
Batch: 60 | Loss: 4.292 | Acc: 39.088,65.868,82.595,% | Adaptive Acc: 79.265% | clf_exit: 0.141 0.368 0.491
Batch: 80 | Loss: 4.299 | Acc: 39.236,65.721,82.436,% | Adaptive Acc: 79.215% | clf_exit: 0.144 0.364 0.492
Batch: 100 | Loss: 4.322 | Acc: 39.349,65.486,82.062,% | Adaptive Acc: 79.131% | clf_exit: 0.144 0.363 0.493
Batch: 120 | Loss: 4.350 | Acc: 39.172,65.302,81.702,% | Adaptive Acc: 78.777% | clf_exit: 0.142 0.362 0.495
Batch: 140 | Loss: 4.373 | Acc: 38.869,65.165,81.427,% | Adaptive Acc: 78.518% | clf_exit: 0.139 0.364 0.497
Batch: 160 | Loss: 4.374 | Acc: 38.815,65.145,81.405,% | Adaptive Acc: 78.513% | clf_exit: 0.138 0.363 0.499
Batch: 180 | Loss: 4.390 | Acc: 38.816,64.887,81.021,% | Adaptive Acc: 78.168% | clf_exit: 0.138 0.362 0.499
Batch: 200 | Loss: 4.395 | Acc: 38.919,64.789,80.822,% | Adaptive Acc: 77.938% | clf_exit: 0.139 0.361 0.500
Batch: 220 | Loss: 4.401 | Acc: 38.886,64.801,80.748,% | Adaptive Acc: 77.856% | clf_exit: 0.140 0.359 0.501
Batch: 240 | Loss: 4.404 | Acc: 38.959,64.811,80.663,% | Adaptive Acc: 77.824% | clf_exit: 0.141 0.358 0.502
Batch: 260 | Loss: 4.404 | Acc: 39.006,64.844,80.669,% | Adaptive Acc: 77.802% | clf_exit: 0.140 0.358 0.501
Batch: 280 | Loss: 4.411 | Acc: 38.926,64.788,80.574,% | Adaptive Acc: 77.750% | clf_exit: 0.140 0.357 0.503
Batch: 300 | Loss: 4.417 | Acc: 38.922,64.675,80.458,% | Adaptive Acc: 77.655% | clf_exit: 0.140 0.356 0.504
Batch: 320 | Loss: 4.422 | Acc: 38.878,64.557,80.340,% | Adaptive Acc: 77.509% | clf_exit: 0.140 0.356 0.504
Batch: 340 | Loss: 4.431 | Acc: 38.898,64.441,80.130,% | Adaptive Acc: 77.303% | clf_exit: 0.140 0.356 0.504
Batch: 360 | Loss: 4.442 | Acc: 38.861,64.327,79.962,% | Adaptive Acc: 77.158% | clf_exit: 0.140 0.356 0.504
Batch: 380 | Loss: 4.447 | Acc: 38.894,64.247,79.804,% | Adaptive Acc: 77.028% | clf_exit: 0.140 0.355 0.505
Batch: 0 | Loss: 4.851 | Acc: 39.062,59.375,66.406,% | Adaptive Acc: 64.062% | clf_exit: 0.219 0.398 0.383
Batch: 20 | Loss: 5.410 | Acc: 36.086,58.259,64.137,% | Adaptive Acc: 62.240% | clf_exit: 0.188 0.374 0.439
Batch: 40 | Loss: 5.405 | Acc: 36.280,57.984,64.329,% | Adaptive Acc: 62.100% | clf_exit: 0.184 0.369 0.447
Batch: 60 | Loss: 5.405 | Acc: 35.579,57.800,64.050,% | Adaptive Acc: 62.052% | clf_exit: 0.179 0.373 0.448
Train all parameters

Epoch: 33
Batch: 0 | Loss: 4.131 | Acc: 42.188,64.062,85.156,% | Adaptive Acc: 80.469% | clf_exit: 0.211 0.305 0.484
Batch: 20 | Loss: 4.290 | Acc: 38.914,64.323,82.738,% | Adaptive Acc: 79.725% | clf_exit: 0.151 0.348 0.500
Batch: 40 | Loss: 4.271 | Acc: 39.062,64.425,83.155,% | Adaptive Acc: 80.050% | clf_exit: 0.153 0.344 0.504
Batch: 60 | Loss: 4.260 | Acc: 39.075,65.382,82.915,% | Adaptive Acc: 79.662% | clf_exit: 0.152 0.348 0.500
Batch: 80 | Loss: 4.280 | Acc: 38.899,65.384,82.369,% | Adaptive Acc: 79.061% | clf_exit: 0.150 0.346 0.504
Batch: 100 | Loss: 4.291 | Acc: 38.815,65.439,82.279,% | Adaptive Acc: 78.914% | clf_exit: 0.146 0.350 0.503
Batch: 120 | Loss: 4.313 | Acc: 38.933,65.257,81.980,% | Adaptive Acc: 78.738% | clf_exit: 0.147 0.352 0.501
Batch: 140 | Loss: 4.325 | Acc: 39.123,65.370,81.749,% | Adaptive Acc: 78.624% | clf_exit: 0.147 0.352 0.501
Batch: 160 | Loss: 4.334 | Acc: 39.242,65.149,81.604,% | Adaptive Acc: 78.576% | clf_exit: 0.145 0.353 0.502
Batch: 180 | Loss: 4.338 | Acc: 39.270,65.211,81.626,% | Adaptive Acc: 78.595% | clf_exit: 0.144 0.354 0.502
Batch: 200 | Loss: 4.352 | Acc: 39.202,65.011,81.367,% | Adaptive Acc: 78.347% | clf_exit: 0.144 0.353 0.503
Batch: 220 | Loss: 4.357 | Acc: 39.229,64.936,81.215,% | Adaptive Acc: 78.185% | clf_exit: 0.145 0.353 0.502
Batch: 240 | Loss: 4.365 | Acc: 39.195,64.892,81.133,% | Adaptive Acc: 78.128% | clf_exit: 0.145 0.353 0.502
Batch: 260 | Loss: 4.371 | Acc: 39.194,64.823,81.073,% | Adaptive Acc: 78.095% | clf_exit: 0.144 0.353 0.503
Batch: 280 | Loss: 4.366 | Acc: 39.213,64.785,81.117,% | Adaptive Acc: 78.108% | clf_exit: 0.144 0.353 0.503
Batch: 300 | Loss: 4.376 | Acc: 39.151,64.730,80.975,% | Adaptive Acc: 77.990% | clf_exit: 0.143 0.355 0.502
Batch: 320 | Loss: 4.374 | Acc: 39.274,64.834,80.885,% | Adaptive Acc: 77.889% | clf_exit: 0.144 0.356 0.500
Batch: 340 | Loss: 4.384 | Acc: 39.221,64.743,80.737,% | Adaptive Acc: 77.754% | clf_exit: 0.144 0.355 0.501
Batch: 360 | Loss: 4.388 | Acc: 39.205,64.692,80.555,% | Adaptive Acc: 77.543% | clf_exit: 0.144 0.355 0.501
Batch: 380 | Loss: 4.396 | Acc: 39.134,64.618,80.446,% | Adaptive Acc: 77.428% | clf_exit: 0.145 0.354 0.501
Batch: 0 | Loss: 5.145 | Acc: 42.188,57.812,68.750,% | Adaptive Acc: 63.281% | clf_exit: 0.219 0.406 0.375
Batch: 20 | Loss: 5.647 | Acc: 35.900,56.101,63.021,% | Adaptive Acc: 59.487% | clf_exit: 0.199 0.403 0.398
Batch: 40 | Loss: 5.626 | Acc: 36.433,55.164,62.824,% | Adaptive Acc: 59.451% | clf_exit: 0.191 0.398 0.410
Batch: 60 | Loss: 5.600 | Acc: 36.399,55.610,62.807,% | Adaptive Acc: 59.541% | clf_exit: 0.192 0.396 0.412
Train all parameters

Epoch: 34
Batch: 0 | Loss: 4.464 | Acc: 35.938,59.375,80.469,% | Adaptive Acc: 75.781% | clf_exit: 0.109 0.336 0.555
Batch: 20 | Loss: 4.251 | Acc: 41.146,65.439,82.775,% | Adaptive Acc: 79.911% | clf_exit: 0.149 0.351 0.500
Batch: 40 | Loss: 4.273 | Acc: 40.377,65.835,83.041,% | Adaptive Acc: 80.145% | clf_exit: 0.140 0.354 0.506
Batch: 60 | Loss: 4.284 | Acc: 40.036,65.996,82.992,% | Adaptive Acc: 79.944% | clf_exit: 0.143 0.350 0.507
Batch: 80 | Loss: 4.287 | Acc: 39.892,65.548,82.745,% | Adaptive Acc: 79.668% | clf_exit: 0.144 0.352 0.505
Batch: 100 | Loss: 4.293 | Acc: 39.882,65.524,82.859,% | Adaptive Acc: 79.757% | clf_exit: 0.145 0.353 0.502
Batch: 120 | Loss: 4.297 | Acc: 39.934,65.651,82.748,% | Adaptive Acc: 79.649% | clf_exit: 0.144 0.356 0.501
Batch: 140 | Loss: 4.306 | Acc: 39.794,65.714,82.663,% | Adaptive Acc: 79.455% | clf_exit: 0.144 0.355 0.501
Batch: 160 | Loss: 4.317 | Acc: 39.596,65.591,82.415,% | Adaptive Acc: 79.217% | clf_exit: 0.144 0.355 0.501
Batch: 180 | Loss: 4.329 | Acc: 39.438,65.452,82.286,% | Adaptive Acc: 79.053% | clf_exit: 0.144 0.355 0.501
Batch: 200 | Loss: 4.327 | Acc: 39.451,65.372,82.194,% | Adaptive Acc: 79.019% | clf_exit: 0.143 0.356 0.500
Batch: 220 | Loss: 4.325 | Acc: 39.465,65.409,82.148,% | Adaptive Acc: 79.002% | clf_exit: 0.144 0.358 0.498
Batch: 240 | Loss: 4.325 | Acc: 39.409,65.324,81.999,% | Adaptive Acc: 78.880% | clf_exit: 0.145 0.357 0.498
Batch: 260 | Loss: 4.330 | Acc: 39.374,65.362,81.882,% | Adaptive Acc: 78.825% | clf_exit: 0.145 0.356 0.499
Batch: 280 | Loss: 4.333 | Acc: 39.416,65.269,81.809,% | Adaptive Acc: 78.706% | clf_exit: 0.145 0.355 0.500
Batch: 300 | Loss: 4.336 | Acc: 39.384,65.269,81.694,% | Adaptive Acc: 78.597% | clf_exit: 0.146 0.355 0.500
Batch: 320 | Loss: 4.346 | Acc: 39.367,65.184,81.527,% | Adaptive Acc: 78.478% | clf_exit: 0.146 0.355 0.500
Batch: 340 | Loss: 4.348 | Acc: 39.390,65.194,81.475,% | Adaptive Acc: 78.448% | clf_exit: 0.145 0.356 0.499
Batch: 360 | Loss: 4.351 | Acc: 39.400,65.188,81.384,% | Adaptive Acc: 78.370% | clf_exit: 0.145 0.356 0.499
Batch: 380 | Loss: 4.350 | Acc: 39.477,65.160,81.254,% | Adaptive Acc: 78.252% | clf_exit: 0.145 0.356 0.499
Batch: 0 | Loss: 5.342 | Acc: 38.281,62.500,64.062,% | Adaptive Acc: 62.500% | clf_exit: 0.250 0.398 0.352
Batch: 20 | Loss: 5.510 | Acc: 35.826,57.589,63.690,% | Adaptive Acc: 60.454% | clf_exit: 0.219 0.379 0.402
Batch: 40 | Loss: 5.539 | Acc: 35.556,56.421,62.881,% | Adaptive Acc: 59.909% | clf_exit: 0.214 0.376 0.410
Batch: 60 | Loss: 5.529 | Acc: 35.835,56.673,63.038,% | Adaptive Acc: 60.131% | clf_exit: 0.210 0.381 0.410
Train classifier parameters

Epoch: 35
Batch: 0 | Loss: 3.979 | Acc: 45.312,71.094,85.156,% | Adaptive Acc: 85.156% | clf_exit: 0.141 0.430 0.430
Batch: 20 | Loss: 4.593 | Acc: 36.905,63.467,78.051,% | Adaptive Acc: 74.926% | clf_exit: 0.140 0.362 0.499
Batch: 40 | Loss: 4.843 | Acc: 35.880,61.662,73.742,% | Adaptive Acc: 71.532% | clf_exit: 0.131 0.349 0.520
Batch: 60 | Loss: 4.979 | Acc: 35.284,60.579,72.554,% | Adaptive Acc: 70.735% | clf_exit: 0.124 0.340 0.536
Batch: 80 | Loss: 5.030 | Acc: 34.664,60.137,71.894,% | Adaptive Acc: 70.014% | clf_exit: 0.122 0.333 0.545
Batch: 100 | Loss: 5.063 | Acc: 34.537,59.893,71.945,% | Adaptive Acc: 70.065% | clf_exit: 0.120 0.330 0.550
Batch: 120 | Loss: 5.099 | Acc: 34.168,59.601,71.739,% | Adaptive Acc: 69.790% | clf_exit: 0.118 0.328 0.555
Batch: 140 | Loss: 5.118 | Acc: 34.142,59.464,71.393,% | Adaptive Acc: 69.476% | clf_exit: 0.117 0.328 0.554
Batch: 160 | Loss: 5.122 | Acc: 34.259,59.331,71.487,% | Adaptive Acc: 69.643% | clf_exit: 0.117 0.326 0.557
Batch: 180 | Loss: 5.129 | Acc: 34.328,59.284,71.443,% | Adaptive Acc: 69.583% | clf_exit: 0.116 0.326 0.558
Batch: 200 | Loss: 5.150 | Acc: 34.208,59.118,71.327,% | Adaptive Acc: 69.446% | clf_exit: 0.113 0.325 0.562
Batch: 220 | Loss: 5.135 | Acc: 34.467,59.248,71.451,% | Adaptive Acc: 69.560% | clf_exit: 0.114 0.325 0.562
Batch: 240 | Loss: 5.128 | Acc: 34.527,59.262,71.541,% | Adaptive Acc: 69.606% | clf_exit: 0.113 0.324 0.563
Batch: 260 | Loss: 5.134 | Acc: 34.582,59.249,71.588,% | Adaptive Acc: 69.648% | clf_exit: 0.112 0.324 0.565
Batch: 280 | Loss: 5.124 | Acc: 34.645,59.303,71.736,% | Adaptive Acc: 69.809% | clf_exit: 0.111 0.324 0.565
Batch: 300 | Loss: 5.116 | Acc: 34.751,59.352,71.789,% | Adaptive Acc: 69.895% | clf_exit: 0.111 0.324 0.565
Batch: 320 | Loss: 5.117 | Acc: 34.669,59.280,71.795,% | Adaptive Acc: 69.887% | clf_exit: 0.111 0.324 0.565
Batch: 340 | Loss: 5.113 | Acc: 34.689,59.320,71.827,% | Adaptive Acc: 69.976% | clf_exit: 0.111 0.324 0.566
Batch: 360 | Loss: 5.105 | Acc: 34.775,59.466,71.955,% | Adaptive Acc: 70.111% | clf_exit: 0.111 0.323 0.566
Batch: 380 | Loss: 5.108 | Acc: 34.763,59.447,71.969,% | Adaptive Acc: 70.146% | clf_exit: 0.111 0.322 0.567
Batch: 0 | Loss: 5.525 | Acc: 34.375,56.250,65.625,% | Adaptive Acc: 65.625% | clf_exit: 0.211 0.328 0.461
Batch: 20 | Loss: 5.708 | Acc: 34.524,55.543,62.277,% | Adaptive Acc: 59.673% | clf_exit: 0.153 0.358 0.489
Batch: 40 | Loss: 5.731 | Acc: 34.832,55.335,61.280,% | Adaptive Acc: 59.204% | clf_exit: 0.151 0.359 0.490
Batch: 60 | Loss: 5.784 | Acc: 34.554,54.867,60.617,% | Adaptive Acc: 58.888% | clf_exit: 0.152 0.353 0.495
Train classifier parameters

Epoch: 36
Batch: 0 | Loss: 4.942 | Acc: 36.719,57.812,74.219,% | Adaptive Acc: 68.750% | clf_exit: 0.133 0.344 0.523
Batch: 20 | Loss: 4.933 | Acc: 37.314,61.607,73.512,% | Adaptive Acc: 71.763% | clf_exit: 0.106 0.342 0.552
Batch: 40 | Loss: 4.905 | Acc: 36.986,61.776,73.761,% | Adaptive Acc: 71.989% | clf_exit: 0.107 0.336 0.557
Batch: 60 | Loss: 4.944 | Acc: 36.539,61.014,73.732,% | Adaptive Acc: 71.901% | clf_exit: 0.109 0.329 0.562
Batch: 80 | Loss: 4.982 | Acc: 35.889,60.426,73.553,% | Adaptive Acc: 71.518% | clf_exit: 0.110 0.326 0.563
Batch: 100 | Loss: 4.966 | Acc: 35.852,60.412,73.700,% | Adaptive Acc: 71.666% | clf_exit: 0.111 0.327 0.562
Batch: 120 | Loss: 4.976 | Acc: 35.925,60.285,73.625,% | Adaptive Acc: 71.578% | clf_exit: 0.112 0.325 0.563
Batch: 140 | Loss: 4.961 | Acc: 35.904,60.306,73.759,% | Adaptive Acc: 71.731% | clf_exit: 0.111 0.325 0.564
Batch: 160 | Loss: 4.965 | Acc: 35.845,60.219,73.821,% | Adaptive Acc: 71.885% | clf_exit: 0.110 0.323 0.567
Batch: 180 | Loss: 4.960 | Acc: 35.929,60.208,73.869,% | Adaptive Acc: 71.927% | clf_exit: 0.110 0.325 0.565
Batch: 200 | Loss: 4.940 | Acc: 36.167,60.475,74.118,% | Adaptive Acc: 72.174% | clf_exit: 0.111 0.325 0.564
Batch: 220 | Loss: 4.939 | Acc: 36.323,60.580,74.194,% | Adaptive Acc: 72.306% | clf_exit: 0.111 0.326 0.563
Batch: 240 | Loss: 4.932 | Acc: 36.450,60.727,74.310,% | Adaptive Acc: 72.429% | clf_exit: 0.111 0.326 0.563
Batch: 260 | Loss: 4.933 | Acc: 36.458,60.716,74.210,% | Adaptive Acc: 72.309% | clf_exit: 0.111 0.326 0.563
Batch: 280 | Loss: 4.927 | Acc: 36.474,60.837,74.291,% | Adaptive Acc: 72.420% | clf_exit: 0.111 0.326 0.563
Batch: 300 | Loss: 4.925 | Acc: 36.475,60.880,74.341,% | Adaptive Acc: 72.467% | clf_exit: 0.111 0.327 0.562
Batch: 320 | Loss: 4.916 | Acc: 36.483,60.967,74.401,% | Adaptive Acc: 72.552% | clf_exit: 0.111 0.328 0.562
Batch: 340 | Loss: 4.916 | Acc: 36.533,60.940,74.434,% | Adaptive Acc: 72.590% | clf_exit: 0.111 0.328 0.562
Batch: 360 | Loss: 4.911 | Acc: 36.572,61.002,74.455,% | Adaptive Acc: 72.665% | clf_exit: 0.111 0.327 0.562
Batch: 380 | Loss: 4.910 | Acc: 36.579,61.073,74.518,% | Adaptive Acc: 72.701% | clf_exit: 0.111 0.328 0.562
Batch: 0 | Loss: 5.386 | Acc: 33.594,56.250,65.625,% | Adaptive Acc: 64.062% | clf_exit: 0.195 0.359 0.445
Batch: 20 | Loss: 5.569 | Acc: 35.305,56.994,63.170,% | Adaptive Acc: 61.198% | clf_exit: 0.160 0.356 0.484
Batch: 40 | Loss: 5.594 | Acc: 35.785,56.364,62.252,% | Adaptive Acc: 60.633% | clf_exit: 0.155 0.356 0.489
Batch: 60 | Loss: 5.650 | Acc: 35.412,55.968,61.565,% | Adaptive Acc: 59.990% | clf_exit: 0.156 0.353 0.491
Train classifier parameters

Epoch: 37
Batch: 0 | Loss: 5.167 | Acc: 37.500,57.031,71.875,% | Adaptive Acc: 67.969% | clf_exit: 0.133 0.344 0.523
Batch: 20 | Loss: 4.830 | Acc: 37.202,61.384,74.926,% | Adaptive Acc: 72.470% | clf_exit: 0.115 0.321 0.564
Batch: 40 | Loss: 4.822 | Acc: 37.671,61.376,75.095,% | Adaptive Acc: 72.847% | clf_exit: 0.117 0.326 0.557
Batch: 60 | Loss: 4.841 | Acc: 37.334,61.463,75.064,% | Adaptive Acc: 72.861% | clf_exit: 0.115 0.333 0.552
Batch: 80 | Loss: 4.850 | Acc: 37.355,61.381,75.145,% | Adaptive Acc: 72.965% | clf_exit: 0.114 0.334 0.551
Batch: 100 | Loss: 4.847 | Acc: 37.144,61.448,75.224,% | Adaptive Acc: 73.043% | clf_exit: 0.114 0.331 0.554
Batch: 120 | Loss: 4.858 | Acc: 37.151,61.486,74.884,% | Adaptive Acc: 72.863% | clf_exit: 0.114 0.329 0.556
Batch: 140 | Loss: 4.858 | Acc: 37.101,61.619,74.939,% | Adaptive Acc: 72.911% | clf_exit: 0.115 0.329 0.556
Batch: 160 | Loss: 4.839 | Acc: 37.384,61.898,75.301,% | Adaptive Acc: 73.214% | clf_exit: 0.115 0.331 0.554
Batch: 180 | Loss: 4.841 | Acc: 37.250,61.762,75.324,% | Adaptive Acc: 73.222% | clf_exit: 0.113 0.331 0.556
Batch: 200 | Loss: 4.826 | Acc: 37.352,61.835,75.490,% | Adaptive Acc: 73.395% | clf_exit: 0.114 0.332 0.554
Batch: 220 | Loss: 4.821 | Acc: 37.327,61.885,75.608,% | Adaptive Acc: 73.512% | clf_exit: 0.114 0.332 0.554
Batch: 240 | Loss: 4.826 | Acc: 37.215,61.728,75.470,% | Adaptive Acc: 73.412% | clf_exit: 0.114 0.331 0.554
Batch: 260 | Loss: 4.820 | Acc: 37.290,61.749,75.500,% | Adaptive Acc: 73.432% | clf_exit: 0.115 0.331 0.554
Batch: 280 | Loss: 4.821 | Acc: 37.269,61.816,75.467,% | Adaptive Acc: 73.393% | clf_exit: 0.115 0.331 0.554
Batch: 300 | Loss: 4.823 | Acc: 37.196,61.760,75.426,% | Adaptive Acc: 73.349% | clf_exit: 0.114 0.331 0.555
Batch: 320 | Loss: 4.820 | Acc: 37.235,61.780,75.501,% | Adaptive Acc: 73.428% | clf_exit: 0.115 0.331 0.554
Batch: 340 | Loss: 4.822 | Acc: 37.124,61.751,75.541,% | Adaptive Acc: 73.460% | clf_exit: 0.115 0.330 0.555
Batch: 360 | Loss: 4.820 | Acc: 37.180,61.782,75.597,% | Adaptive Acc: 73.509% | clf_exit: 0.115 0.330 0.555
Batch: 380 | Loss: 4.819 | Acc: 37.260,61.813,75.607,% | Adaptive Acc: 73.534% | clf_exit: 0.115 0.330 0.555
Batch: 0 | Loss: 5.263 | Acc: 35.156,57.031,68.750,% | Adaptive Acc: 65.625% | clf_exit: 0.195 0.367 0.438
Batch: 20 | Loss: 5.496 | Acc: 36.198,57.701,63.616,% | Adaptive Acc: 61.198% | clf_exit: 0.166 0.365 0.469
Batch: 40 | Loss: 5.529 | Acc: 36.185,57.127,62.424,% | Adaptive Acc: 60.442% | clf_exit: 0.160 0.365 0.474
Batch: 60 | Loss: 5.586 | Acc: 35.963,56.634,61.796,% | Adaptive Acc: 59.926% | clf_exit: 0.160 0.361 0.479
Train classifier parameters

Epoch: 38
Batch: 0 | Loss: 4.899 | Acc: 37.500,64.844,78.125,% | Adaptive Acc: 73.438% | clf_exit: 0.109 0.336 0.555
Batch: 20 | Loss: 4.746 | Acc: 37.426,63.207,77.232,% | Adaptive Acc: 75.521% | clf_exit: 0.114 0.329 0.557
Batch: 40 | Loss: 4.736 | Acc: 37.919,62.938,77.325,% | Adaptive Acc: 75.476% | clf_exit: 0.113 0.330 0.557
Batch: 60 | Loss: 4.761 | Acc: 37.782,62.090,76.908,% | Adaptive Acc: 74.782% | clf_exit: 0.116 0.330 0.554
Batch: 80 | Loss: 4.765 | Acc: 37.674,61.825,76.640,% | Adaptive Acc: 74.450% | clf_exit: 0.114 0.331 0.554
Batch: 100 | Loss: 4.741 | Acc: 37.809,61.819,76.423,% | Adaptive Acc: 74.343% | clf_exit: 0.115 0.329 0.555
Batch: 120 | Loss: 4.742 | Acc: 37.816,61.770,76.227,% | Adaptive Acc: 74.096% | clf_exit: 0.116 0.328 0.556
Batch: 140 | Loss: 4.741 | Acc: 37.799,61.929,76.225,% | Adaptive Acc: 74.119% | clf_exit: 0.118 0.328 0.554
Batch: 160 | Loss: 4.742 | Acc: 37.820,62.107,76.402,% | Adaptive Acc: 74.253% | clf_exit: 0.118 0.329 0.553
Batch: 180 | Loss: 4.724 | Acc: 37.867,62.327,76.506,% | Adaptive Acc: 74.309% | clf_exit: 0.119 0.331 0.550
Batch: 200 | Loss: 4.737 | Acc: 37.776,62.306,76.329,% | Adaptive Acc: 74.137% | clf_exit: 0.119 0.331 0.550
Batch: 220 | Loss: 4.732 | Acc: 37.755,62.352,76.386,% | Adaptive Acc: 74.229% | clf_exit: 0.119 0.331 0.550
Batch: 240 | Loss: 4.731 | Acc: 37.805,62.357,76.323,% | Adaptive Acc: 74.274% | clf_exit: 0.119 0.332 0.550
Batch: 260 | Loss: 4.743 | Acc: 37.626,62.243,76.245,% | Adaptive Acc: 74.192% | clf_exit: 0.118 0.332 0.550
Batch: 280 | Loss: 4.738 | Acc: 37.639,62.344,76.276,% | Adaptive Acc: 74.269% | clf_exit: 0.118 0.333 0.549
Batch: 300 | Loss: 4.735 | Acc: 37.702,62.399,76.290,% | Adaptive Acc: 74.265% | clf_exit: 0.119 0.334 0.548
Batch: 320 | Loss: 4.733 | Acc: 37.773,62.485,76.283,% | Adaptive Acc: 74.168% | clf_exit: 0.119 0.334 0.546
Batch: 340 | Loss: 4.733 | Acc: 37.738,62.456,76.294,% | Adaptive Acc: 74.143% | clf_exit: 0.119 0.335 0.546
Batch: 360 | Loss: 4.736 | Acc: 37.697,62.364,76.223,% | Adaptive Acc: 74.089% | clf_exit: 0.120 0.334 0.546
Batch: 380 | Loss: 4.735 | Acc: 37.715,62.492,76.275,% | Adaptive Acc: 74.172% | clf_exit: 0.119 0.335 0.546
Batch: 0 | Loss: 5.230 | Acc: 38.281,61.719,70.312,% | Adaptive Acc: 64.844% | clf_exit: 0.219 0.383 0.398
Batch: 20 | Loss: 5.469 | Acc: 35.863,57.552,63.690,% | Adaptive Acc: 60.900% | clf_exit: 0.167 0.374 0.459
Batch: 40 | Loss: 5.481 | Acc: 36.395,57.241,62.919,% | Adaptive Acc: 60.671% | clf_exit: 0.164 0.372 0.464
Batch: 60 | Loss: 5.536 | Acc: 36.270,56.737,62.334,% | Adaptive Acc: 60.169% | clf_exit: 0.165 0.365 0.470
Train classifier parameters

Epoch: 39
Batch: 0 | Loss: 5.550 | Acc: 30.469,52.344,75.781,% | Adaptive Acc: 72.656% | clf_exit: 0.086 0.312 0.602
Batch: 20 | Loss: 4.690 | Acc: 38.393,63.244,77.121,% | Adaptive Acc: 75.521% | clf_exit: 0.125 0.334 0.541
Batch: 40 | Loss: 4.681 | Acc: 38.643,63.110,77.229,% | Adaptive Acc: 75.191% | clf_exit: 0.127 0.341 0.531
Batch: 60 | Loss: 4.734 | Acc: 37.987,62.359,76.870,% | Adaptive Acc: 74.949% | clf_exit: 0.123 0.337 0.540
Batch: 80 | Loss: 4.732 | Acc: 37.799,62.558,76.755,% | Adaptive Acc: 74.807% | clf_exit: 0.122 0.336 0.542
Batch: 100 | Loss: 4.712 | Acc: 37.802,62.670,76.903,% | Adaptive Acc: 74.915% | clf_exit: 0.123 0.336 0.541
Batch: 120 | Loss: 4.707 | Acc: 37.642,62.584,76.976,% | Adaptive Acc: 74.910% | clf_exit: 0.123 0.336 0.540
Batch: 140 | Loss: 4.699 | Acc: 37.832,62.805,76.995,% | Adaptive Acc: 74.911% | clf_exit: 0.123 0.337 0.540
Batch: 160 | Loss: 4.700 | Acc: 37.946,62.859,76.994,% | Adaptive Acc: 74.918% | clf_exit: 0.123 0.337 0.540
Batch: 180 | Loss: 4.697 | Acc: 37.927,62.832,76.977,% | Adaptive Acc: 74.827% | clf_exit: 0.124 0.337 0.539
Batch: 200 | Loss: 4.708 | Acc: 37.753,62.737,76.998,% | Adaptive Acc: 74.880% | clf_exit: 0.123 0.337 0.541
Batch: 220 | Loss: 4.713 | Acc: 37.666,62.613,76.863,% | Adaptive Acc: 74.703% | clf_exit: 0.122 0.336 0.542
Batch: 240 | Loss: 4.711 | Acc: 37.678,62.691,76.877,% | Adaptive Acc: 74.721% | clf_exit: 0.121 0.336 0.543
Batch: 260 | Loss: 4.716 | Acc: 37.677,62.662,76.850,% | Adaptive Acc: 74.680% | clf_exit: 0.121 0.337 0.542
Batch: 280 | Loss: 4.710 | Acc: 37.764,62.725,76.907,% | Adaptive Acc: 74.778% | clf_exit: 0.121 0.337 0.542
Batch: 300 | Loss: 4.719 | Acc: 37.630,62.601,76.887,% | Adaptive Acc: 74.720% | clf_exit: 0.121 0.336 0.543
Batch: 320 | Loss: 4.718 | Acc: 37.673,62.592,76.859,% | Adaptive Acc: 74.693% | clf_exit: 0.121 0.336 0.542
Batch: 340 | Loss: 4.713 | Acc: 37.747,62.690,76.886,% | Adaptive Acc: 74.702% | clf_exit: 0.121 0.336 0.542
Batch: 360 | Loss: 4.708 | Acc: 37.794,62.719,76.937,% | Adaptive Acc: 74.729% | clf_exit: 0.121 0.336 0.542
Batch: 380 | Loss: 4.705 | Acc: 37.828,62.762,76.880,% | Adaptive Acc: 74.703% | clf_exit: 0.122 0.336 0.542
Batch: 0 | Loss: 5.176 | Acc: 37.500,58.594,69.531,% | Adaptive Acc: 64.844% | clf_exit: 0.227 0.383 0.391
Batch: 20 | Loss: 5.452 | Acc: 36.310,57.775,64.025,% | Adaptive Acc: 61.868% | clf_exit: 0.167 0.368 0.465
Batch: 40 | Loss: 5.471 | Acc: 36.547,57.279,62.976,% | Adaptive Acc: 61.319% | clf_exit: 0.164 0.367 0.470
Batch: 60 | Loss: 5.526 | Acc: 36.514,56.993,62.500,% | Adaptive Acc: 60.771% | clf_exit: 0.166 0.361 0.473
Train classifier parameters

Epoch: 40
Batch: 0 | Loss: 4.879 | Acc: 32.031,63.281,75.781,% | Adaptive Acc: 75.000% | clf_exit: 0.102 0.312 0.586
Batch: 20 | Loss: 4.732 | Acc: 36.012,61.533,77.046,% | Adaptive Acc: 74.888% | clf_exit: 0.119 0.321 0.560
Batch: 40 | Loss: 4.717 | Acc: 36.871,62.176,77.287,% | Adaptive Acc: 74.943% | clf_exit: 0.121 0.324 0.555
Batch: 60 | Loss: 4.713 | Acc: 37.167,62.602,77.228,% | Adaptive Acc: 74.834% | clf_exit: 0.121 0.332 0.547
Batch: 80 | Loss: 4.705 | Acc: 37.162,62.741,76.861,% | Adaptive Acc: 74.479% | clf_exit: 0.121 0.334 0.545
Batch: 100 | Loss: 4.709 | Acc: 37.392,62.856,76.972,% | Adaptive Acc: 74.536% | clf_exit: 0.123 0.333 0.544
Batch: 120 | Loss: 4.694 | Acc: 37.435,63.068,77.105,% | Adaptive Acc: 74.709% | clf_exit: 0.123 0.334 0.543
Batch: 140 | Loss: 4.682 | Acc: 37.594,63.182,77.183,% | Adaptive Acc: 74.856% | clf_exit: 0.122 0.336 0.542
Batch: 160 | Loss: 4.662 | Acc: 37.854,63.276,77.392,% | Adaptive Acc: 75.073% | clf_exit: 0.123 0.337 0.540
Batch: 180 | Loss: 4.648 | Acc: 37.962,63.402,77.521,% | Adaptive Acc: 75.237% | clf_exit: 0.123 0.338 0.539
Batch: 200 | Loss: 4.646 | Acc: 37.920,63.382,77.480,% | Adaptive Acc: 75.210% | clf_exit: 0.124 0.339 0.537
Batch: 220 | Loss: 4.646 | Acc: 37.942,63.444,77.464,% | Adaptive Acc: 75.198% | clf_exit: 0.124 0.339 0.537
Batch: 240 | Loss: 4.650 | Acc: 37.938,63.453,77.477,% | Adaptive Acc: 75.201% | clf_exit: 0.124 0.339 0.537
Batch: 260 | Loss: 4.651 | Acc: 38.060,63.407,77.407,% | Adaptive Acc: 75.138% | clf_exit: 0.125 0.338 0.537
Batch: 280 | Loss: 4.658 | Acc: 38.020,63.356,77.330,% | Adaptive Acc: 75.092% | clf_exit: 0.124 0.338 0.538
Batch: 300 | Loss: 4.666 | Acc: 37.996,63.229,77.357,% | Adaptive Acc: 75.073% | clf_exit: 0.124 0.338 0.538
Batch: 320 | Loss: 4.667 | Acc: 37.989,63.216,77.336,% | Adaptive Acc: 75.027% | clf_exit: 0.123 0.338 0.539
Batch: 340 | Loss: 4.670 | Acc: 37.993,63.171,77.305,% | Adaptive Acc: 74.945% | clf_exit: 0.124 0.338 0.538
Batch: 360 | Loss: 4.660 | Acc: 38.121,63.221,77.292,% | Adaptive Acc: 74.931% | clf_exit: 0.125 0.338 0.537
Batch: 380 | Loss: 4.658 | Acc: 38.105,63.296,77.323,% | Adaptive Acc: 74.967% | clf_exit: 0.124 0.338 0.537
Batch: 0 | Loss: 5.235 | Acc: 33.594,57.812,71.875,% | Adaptive Acc: 67.188% | clf_exit: 0.219 0.375 0.406
Batch: 20 | Loss: 5.424 | Acc: 36.384,58.259,64.993,% | Adaptive Acc: 62.128% | clf_exit: 0.170 0.377 0.453
Batch: 40 | Loss: 5.450 | Acc: 36.623,57.679,63.739,% | Adaptive Acc: 61.719% | clf_exit: 0.171 0.371 0.458
Batch: 60 | Loss: 5.508 | Acc: 36.616,57.185,62.910,% | Adaptive Acc: 60.976% | clf_exit: 0.172 0.365 0.462
Train classifier parameters

Epoch: 41
Batch: 0 | Loss: 4.358 | Acc: 39.844,65.625,76.562,% | Adaptive Acc: 74.219% | clf_exit: 0.164 0.344 0.492
Batch: 20 | Loss: 4.520 | Acc: 39.844,65.104,78.571,% | Adaptive Acc: 75.558% | clf_exit: 0.132 0.346 0.523
Batch: 40 | Loss: 4.539 | Acc: 38.986,64.672,78.735,% | Adaptive Acc: 75.743% | clf_exit: 0.128 0.346 0.526
Batch: 60 | Loss: 4.574 | Acc: 38.896,64.139,78.125,% | Adaptive Acc: 75.320% | clf_exit: 0.129 0.343 0.528
Batch: 80 | Loss: 4.562 | Acc: 38.841,64.381,78.270,% | Adaptive Acc: 75.453% | clf_exit: 0.130 0.343 0.526
Batch: 100 | Loss: 4.592 | Acc: 38.792,63.838,77.970,% | Adaptive Acc: 75.278% | clf_exit: 0.128 0.343 0.529
Batch: 120 | Loss: 4.592 | Acc: 38.559,63.811,77.951,% | Adaptive Acc: 75.420% | clf_exit: 0.127 0.343 0.530
Batch: 140 | Loss: 4.582 | Acc: 38.797,63.852,77.937,% | Adaptive Acc: 75.543% | clf_exit: 0.127 0.343 0.530
Batch: 160 | Loss: 4.598 | Acc: 38.621,63.611,77.795,% | Adaptive Acc: 75.364% | clf_exit: 0.127 0.341 0.531
Batch: 180 | Loss: 4.591 | Acc: 38.575,63.640,77.831,% | Adaptive Acc: 75.371% | clf_exit: 0.129 0.340 0.531
Batch: 200 | Loss: 4.605 | Acc: 38.503,63.643,77.732,% | Adaptive Acc: 75.272% | clf_exit: 0.128 0.340 0.531
Batch: 220 | Loss: 4.616 | Acc: 38.391,63.561,77.563,% | Adaptive Acc: 75.194% | clf_exit: 0.127 0.339 0.534
Batch: 240 | Loss: 4.620 | Acc: 38.414,63.648,77.571,% | Adaptive Acc: 75.191% | clf_exit: 0.127 0.340 0.533
Batch: 260 | Loss: 4.614 | Acc: 38.512,63.736,77.508,% | Adaptive Acc: 75.168% | clf_exit: 0.128 0.340 0.532
Batch: 280 | Loss: 4.621 | Acc: 38.515,63.556,77.519,% | Adaptive Acc: 75.142% | clf_exit: 0.129 0.339 0.532
Batch: 300 | Loss: 4.624 | Acc: 38.494,63.499,77.515,% | Adaptive Acc: 75.117% | clf_exit: 0.130 0.338 0.532
Batch: 320 | Loss: 4.632 | Acc: 38.466,63.483,77.480,% | Adaptive Acc: 75.107% | clf_exit: 0.129 0.338 0.533
Batch: 340 | Loss: 4.632 | Acc: 38.460,63.558,77.419,% | Adaptive Acc: 75.064% | clf_exit: 0.129 0.338 0.533
Batch: 360 | Loss: 4.628 | Acc: 38.560,63.573,77.415,% | Adaptive Acc: 75.091% | clf_exit: 0.129 0.339 0.532
Batch: 380 | Loss: 4.627 | Acc: 38.497,63.601,77.418,% | Adaptive Acc: 75.084% | clf_exit: 0.129 0.339 0.532
Batch: 0 | Loss: 5.145 | Acc: 35.938,58.594,70.312,% | Adaptive Acc: 65.625% | clf_exit: 0.227 0.383 0.391
Batch: 20 | Loss: 5.405 | Acc: 36.570,58.594,64.435,% | Adaptive Acc: 62.016% | clf_exit: 0.178 0.365 0.458
Batch: 40 | Loss: 5.421 | Acc: 37.233,58.022,63.510,% | Adaptive Acc: 61.566% | clf_exit: 0.173 0.367 0.460
Batch: 60 | Loss: 5.476 | Acc: 37.013,57.595,62.884,% | Adaptive Acc: 60.963% | clf_exit: 0.172 0.362 0.465
Train classifier parameters

Epoch: 42
Batch: 0 | Loss: 4.126 | Acc: 42.969,63.281,77.344,% | Adaptive Acc: 69.531% | clf_exit: 0.180 0.367 0.453
Batch: 20 | Loss: 4.670 | Acc: 38.170,61.905,77.790,% | Adaptive Acc: 74.851% | clf_exit: 0.130 0.330 0.540
Batch: 40 | Loss: 4.638 | Acc: 38.700,62.614,77.915,% | Adaptive Acc: 75.438% | clf_exit: 0.130 0.334 0.536
Batch: 60 | Loss: 4.616 | Acc: 38.717,63.512,77.984,% | Adaptive Acc: 75.781% | clf_exit: 0.127 0.337 0.535
Batch: 80 | Loss: 4.610 | Acc: 38.657,63.735,78.057,% | Adaptive Acc: 75.733% | clf_exit: 0.130 0.342 0.529
Batch: 100 | Loss: 4.608 | Acc: 38.537,63.629,78.086,% | Adaptive Acc: 75.890% | clf_exit: 0.127 0.342 0.531
Batch: 120 | Loss: 4.607 | Acc: 38.733,63.527,78.093,% | Adaptive Acc: 75.801% | clf_exit: 0.128 0.344 0.529
Batch: 140 | Loss: 4.617 | Acc: 38.547,63.664,78.020,% | Adaptive Acc: 75.742% | clf_exit: 0.126 0.343 0.530
Batch: 160 | Loss: 4.617 | Acc: 38.660,63.776,78.023,% | Adaptive Acc: 75.738% | clf_exit: 0.126 0.343 0.530
Batch: 180 | Loss: 4.616 | Acc: 38.700,63.661,78.030,% | Adaptive Acc: 75.734% | clf_exit: 0.127 0.342 0.531
Batch: 200 | Loss: 4.618 | Acc: 38.619,63.495,77.946,% | Adaptive Acc: 75.575% | clf_exit: 0.128 0.340 0.532
Batch: 220 | Loss: 4.608 | Acc: 38.649,63.578,77.998,% | Adaptive Acc: 75.633% | clf_exit: 0.129 0.340 0.531
Batch: 240 | Loss: 4.605 | Acc: 38.696,63.612,77.856,% | Adaptive Acc: 75.551% | clf_exit: 0.129 0.340 0.532
Batch: 260 | Loss: 4.596 | Acc: 38.784,63.724,77.892,% | Adaptive Acc: 75.581% | clf_exit: 0.129 0.341 0.530
Batch: 280 | Loss: 4.598 | Acc: 38.640,63.626,77.850,% | Adaptive Acc: 75.481% | clf_exit: 0.129 0.341 0.530
Batch: 300 | Loss: 4.605 | Acc: 38.554,63.523,77.775,% | Adaptive Acc: 75.431% | clf_exit: 0.129 0.341 0.530
Batch: 320 | Loss: 4.610 | Acc: 38.593,63.478,77.750,% | Adaptive Acc: 75.448% | clf_exit: 0.129 0.341 0.530
Batch: 340 | Loss: 4.609 | Acc: 38.591,63.526,77.724,% | Adaptive Acc: 75.449% | clf_exit: 0.129 0.342 0.529
Batch: 360 | Loss: 4.605 | Acc: 38.651,63.584,77.816,% | Adaptive Acc: 75.550% | clf_exit: 0.129 0.342 0.529
Batch: 380 | Loss: 4.605 | Acc: 38.591,63.562,77.838,% | Adaptive Acc: 75.502% | clf_exit: 0.129 0.343 0.528
Batch: 0 | Loss: 5.162 | Acc: 34.375,58.594,70.312,% | Adaptive Acc: 65.625% | clf_exit: 0.242 0.359 0.398
Batch: 20 | Loss: 5.379 | Acc: 37.128,58.445,65.067,% | Adaptive Acc: 62.128% | clf_exit: 0.178 0.380 0.442
Batch: 40 | Loss: 5.398 | Acc: 37.500,58.098,64.139,% | Adaptive Acc: 61.890% | clf_exit: 0.177 0.373 0.450
Batch: 60 | Loss: 5.451 | Acc: 37.116,57.748,63.320,% | Adaptive Acc: 61.194% | clf_exit: 0.177 0.364 0.459
Train classifier parameters

Epoch: 43
Batch: 0 | Loss: 4.339 | Acc: 41.406,69.531,76.562,% | Adaptive Acc: 77.344% | clf_exit: 0.102 0.375 0.523
Batch: 20 | Loss: 4.557 | Acc: 38.244,64.137,79.167,% | Adaptive Acc: 76.562% | clf_exit: 0.134 0.337 0.529
Batch: 40 | Loss: 4.615 | Acc: 38.072,63.472,78.087,% | Adaptive Acc: 75.572% | clf_exit: 0.135 0.339 0.525
Batch: 60 | Loss: 4.613 | Acc: 38.012,63.192,77.766,% | Adaptive Acc: 75.231% | clf_exit: 0.134 0.341 0.525
Batch: 80 | Loss: 4.585 | Acc: 38.455,63.272,78.067,% | Adaptive Acc: 75.473% | clf_exit: 0.134 0.343 0.522
Batch: 100 | Loss: 4.584 | Acc: 38.575,63.575,77.994,% | Adaptive Acc: 75.472% | clf_exit: 0.134 0.342 0.524
Batch: 120 | Loss: 4.578 | Acc: 38.662,63.682,77.964,% | Adaptive Acc: 75.478% | clf_exit: 0.132 0.342 0.526
Batch: 140 | Loss: 4.595 | Acc: 38.614,63.520,77.854,% | Adaptive Acc: 75.321% | clf_exit: 0.133 0.339 0.528
Batch: 160 | Loss: 4.589 | Acc: 38.703,63.553,77.863,% | Adaptive Acc: 75.408% | clf_exit: 0.133 0.341 0.526
Batch: 180 | Loss: 4.590 | Acc: 38.640,63.609,77.870,% | Adaptive Acc: 75.345% | clf_exit: 0.133 0.341 0.526
Batch: 200 | Loss: 4.587 | Acc: 38.740,63.705,77.795,% | Adaptive Acc: 75.272% | clf_exit: 0.133 0.343 0.524
Batch: 220 | Loss: 4.582 | Acc: 38.822,63.751,77.892,% | Adaptive Acc: 75.396% | clf_exit: 0.135 0.342 0.524
Batch: 240 | Loss: 4.575 | Acc: 38.829,63.780,78.044,% | Adaptive Acc: 75.535% | clf_exit: 0.135 0.343 0.523
Batch: 260 | Loss: 4.584 | Acc: 38.718,63.763,78.026,% | Adaptive Acc: 75.482% | clf_exit: 0.134 0.341 0.524
Batch: 280 | Loss: 4.584 | Acc: 38.698,63.732,78.039,% | Adaptive Acc: 75.534% | clf_exit: 0.133 0.342 0.525
Batch: 300 | Loss: 4.580 | Acc: 38.759,63.738,78.122,% | Adaptive Acc: 75.584% | clf_exit: 0.133 0.341 0.525
Batch: 320 | Loss: 4.583 | Acc: 38.729,63.612,78.035,% | Adaptive Acc: 75.545% | clf_exit: 0.132 0.341 0.526
Batch: 340 | Loss: 4.575 | Acc: 38.762,63.678,78.091,% | Adaptive Acc: 75.635% | clf_exit: 0.133 0.341 0.527
Batch: 360 | Loss: 4.573 | Acc: 38.870,63.714,78.082,% | Adaptive Acc: 75.636% | clf_exit: 0.133 0.342 0.525
Batch: 380 | Loss: 4.580 | Acc: 38.835,63.646,78.037,% | Adaptive Acc: 75.607% | clf_exit: 0.133 0.342 0.526
Batch: 0 | Loss: 5.134 | Acc: 38.281,60.156,69.531,% | Adaptive Acc: 64.062% | clf_exit: 0.227 0.422 0.352
Batch: 20 | Loss: 5.349 | Acc: 37.314,59.263,64.918,% | Adaptive Acc: 61.942% | clf_exit: 0.179 0.373 0.448
Batch: 40 | Loss: 5.373 | Acc: 37.309,58.556,63.815,% | Adaptive Acc: 61.604% | clf_exit: 0.178 0.371 0.451
Batch: 60 | Loss: 5.428 | Acc: 37.129,58.069,63.243,% | Adaptive Acc: 61.219% | clf_exit: 0.178 0.365 0.457
Train classifier parameters

Epoch: 44
Batch: 0 | Loss: 5.721 | Acc: 35.938,53.125,60.938,% | Adaptive Acc: 62.500% | clf_exit: 0.141 0.336 0.523
Batch: 20 | Loss: 4.584 | Acc: 38.542,64.286,78.720,% | Adaptive Acc: 76.190% | clf_exit: 0.123 0.358 0.519
Batch: 40 | Loss: 4.575 | Acc: 38.739,64.367,78.868,% | Adaptive Acc: 76.239% | clf_exit: 0.124 0.350 0.526
Batch: 60 | Loss: 4.558 | Acc: 38.678,64.037,78.893,% | Adaptive Acc: 76.306% | clf_exit: 0.130 0.342 0.528
Batch: 80 | Loss: 4.540 | Acc: 38.976,64.207,78.781,% | Adaptive Acc: 76.360% | clf_exit: 0.130 0.342 0.527
Batch: 100 | Loss: 4.543 | Acc: 38.877,64.209,78.558,% | Adaptive Acc: 76.153% | clf_exit: 0.131 0.341 0.528
Batch: 120 | Loss: 4.558 | Acc: 38.772,64.082,78.409,% | Adaptive Acc: 76.046% | clf_exit: 0.132 0.340 0.528
Batch: 140 | Loss: 4.558 | Acc: 38.774,64.190,78.302,% | Adaptive Acc: 75.947% | clf_exit: 0.133 0.341 0.527
Batch: 160 | Loss: 4.550 | Acc: 38.810,64.194,78.305,% | Adaptive Acc: 75.990% | clf_exit: 0.134 0.338 0.528
Batch: 180 | Loss: 4.548 | Acc: 38.855,64.412,78.211,% | Adaptive Acc: 75.906% | clf_exit: 0.133 0.339 0.528
Batch: 200 | Loss: 4.557 | Acc: 38.755,64.327,78.187,% | Adaptive Acc: 75.886% | clf_exit: 0.134 0.338 0.528
Batch: 220 | Loss: 4.562 | Acc: 38.776,64.250,78.242,% | Adaptive Acc: 75.933% | clf_exit: 0.133 0.339 0.529
Batch: 240 | Loss: 4.561 | Acc: 38.813,64.315,78.235,% | Adaptive Acc: 75.985% | clf_exit: 0.133 0.339 0.529
Batch: 260 | Loss: 4.572 | Acc: 38.667,64.188,78.248,% | Adaptive Acc: 75.970% | clf_exit: 0.132 0.339 0.529
Batch: 280 | Loss: 4.566 | Acc: 38.734,64.271,78.233,% | Adaptive Acc: 75.954% | clf_exit: 0.133 0.340 0.528
Batch: 300 | Loss: 4.572 | Acc: 38.767,64.197,78.224,% | Adaptive Acc: 75.924% | clf_exit: 0.133 0.340 0.528
Batch: 320 | Loss: 4.572 | Acc: 38.751,64.204,78.235,% | Adaptive Acc: 75.920% | clf_exit: 0.132 0.341 0.527
Batch: 340 | Loss: 4.574 | Acc: 38.629,64.095,78.207,% | Adaptive Acc: 75.845% | clf_exit: 0.132 0.341 0.527
Batch: 360 | Loss: 4.570 | Acc: 38.677,64.058,78.166,% | Adaptive Acc: 75.796% | clf_exit: 0.132 0.341 0.526
Batch: 380 | Loss: 4.564 | Acc: 38.771,64.036,78.228,% | Adaptive Acc: 75.843% | clf_exit: 0.133 0.342 0.526
Batch: 0 | Loss: 5.116 | Acc: 38.281,59.375,70.312,% | Adaptive Acc: 65.625% | clf_exit: 0.242 0.352 0.406
Batch: 20 | Loss: 5.361 | Acc: 37.277,58.222,65.067,% | Adaptive Acc: 62.351% | clf_exit: 0.186 0.369 0.445
Batch: 40 | Loss: 5.384 | Acc: 37.367,57.774,63.834,% | Adaptive Acc: 61.909% | clf_exit: 0.180 0.369 0.451
Batch: 60 | Loss: 5.434 | Acc: 37.257,57.403,63.179,% | Adaptive Acc: 61.322% | clf_exit: 0.181 0.364 0.455
Train classifier parameters

Epoch: 45
Batch: 0 | Loss: 4.340 | Acc: 37.500,66.406,76.562,% | Adaptive Acc: 77.344% | clf_exit: 0.141 0.352 0.508
Batch: 20 | Loss: 4.561 | Acc: 38.988,63.616,78.199,% | Adaptive Acc: 76.116% | clf_exit: 0.131 0.336 0.534
Batch: 40 | Loss: 4.549 | Acc: 39.539,64.177,78.201,% | Adaptive Acc: 76.315% | clf_exit: 0.131 0.341 0.529
Batch: 60 | Loss: 4.526 | Acc: 39.857,64.203,78.548,% | Adaptive Acc: 76.627% | clf_exit: 0.131 0.341 0.528
Batch: 80 | Loss: 4.531 | Acc: 39.361,64.043,78.540,% | Adaptive Acc: 76.331% | clf_exit: 0.131 0.343 0.526
Batch: 100 | Loss: 4.522 | Acc: 39.310,64.093,78.674,% | Adaptive Acc: 76.400% | clf_exit: 0.131 0.343 0.526
Batch: 120 | Loss: 4.526 | Acc: 39.043,64.037,78.532,% | Adaptive Acc: 76.298% | clf_exit: 0.132 0.342 0.526
Batch: 140 | Loss: 4.527 | Acc: 39.129,64.035,78.524,% | Adaptive Acc: 76.324% | clf_exit: 0.132 0.344 0.524
Batch: 160 | Loss: 4.524 | Acc: 39.087,64.043,78.445,% | Adaptive Acc: 76.169% | clf_exit: 0.132 0.346 0.522
Batch: 180 | Loss: 4.524 | Acc: 39.024,64.110,78.436,% | Adaptive Acc: 76.226% | clf_exit: 0.131 0.345 0.523
Batch: 200 | Loss: 4.532 | Acc: 38.926,63.973,78.284,% | Adaptive Acc: 76.131% | clf_exit: 0.132 0.343 0.525
Batch: 220 | Loss: 4.531 | Acc: 38.985,63.999,78.397,% | Adaptive Acc: 76.273% | clf_exit: 0.131 0.344 0.524
Batch: 240 | Loss: 4.536 | Acc: 38.952,63.959,78.339,% | Adaptive Acc: 76.131% | clf_exit: 0.132 0.343 0.525
Batch: 260 | Loss: 4.535 | Acc: 39.030,64.045,78.382,% | Adaptive Acc: 76.191% | clf_exit: 0.133 0.343 0.525
Batch: 280 | Loss: 4.537 | Acc: 39.079,63.999,78.292,% | Adaptive Acc: 76.073% | clf_exit: 0.134 0.343 0.523
Batch: 300 | Loss: 4.541 | Acc: 38.995,63.915,78.307,% | Adaptive Acc: 76.056% | clf_exit: 0.134 0.343 0.523
Batch: 320 | Loss: 4.545 | Acc: 38.946,63.921,78.249,% | Adaptive Acc: 75.944% | clf_exit: 0.133 0.344 0.523
Batch: 340 | Loss: 4.543 | Acc: 38.936,63.957,78.240,% | Adaptive Acc: 75.919% | clf_exit: 0.133 0.344 0.523
Batch: 360 | Loss: 4.545 | Acc: 38.935,63.898,78.188,% | Adaptive Acc: 75.859% | clf_exit: 0.133 0.343 0.524
Batch: 380 | Loss: 4.548 | Acc: 38.907,63.913,78.131,% | Adaptive Acc: 75.796% | clf_exit: 0.134 0.343 0.524
Batch: 0 | Loss: 5.107 | Acc: 36.719,60.938,71.094,% | Adaptive Acc: 65.625% | clf_exit: 0.242 0.359 0.398
Batch: 20 | Loss: 5.340 | Acc: 37.091,59.003,64.658,% | Adaptive Acc: 61.905% | clf_exit: 0.188 0.364 0.448
Batch: 40 | Loss: 5.365 | Acc: 37.424,58.575,63.872,% | Adaptive Acc: 61.700% | clf_exit: 0.185 0.363 0.452
Batch: 60 | Loss: 5.417 | Acc: 36.962,58.081,63.320,% | Adaptive Acc: 61.155% | clf_exit: 0.184 0.359 0.458
Train classifier parameters

Epoch: 46
Batch: 0 | Loss: 4.556 | Acc: 39.062,63.281,78.906,% | Adaptive Acc: 77.344% | clf_exit: 0.102 0.352 0.547
Batch: 20 | Loss: 4.452 | Acc: 40.179,65.997,78.757,% | Adaptive Acc: 76.823% | clf_exit: 0.137 0.338 0.525
Batch: 40 | Loss: 4.446 | Acc: 40.149,65.854,78.659,% | Adaptive Acc: 76.582% | clf_exit: 0.135 0.348 0.517
Batch: 60 | Loss: 4.450 | Acc: 39.818,65.266,78.881,% | Adaptive Acc: 76.844% | clf_exit: 0.135 0.342 0.523
Batch: 80 | Loss: 4.464 | Acc: 39.757,64.902,78.935,% | Adaptive Acc: 76.698% | clf_exit: 0.136 0.343 0.521
Batch: 100 | Loss: 4.485 | Acc: 39.604,64.821,78.813,% | Adaptive Acc: 76.555% | clf_exit: 0.135 0.342 0.523
Batch: 120 | Loss: 4.500 | Acc: 39.372,64.624,78.642,% | Adaptive Acc: 76.240% | clf_exit: 0.134 0.343 0.522
Batch: 140 | Loss: 4.504 | Acc: 39.450,64.556,78.485,% | Adaptive Acc: 76.114% | clf_exit: 0.133 0.345 0.522
Batch: 160 | Loss: 4.503 | Acc: 39.596,64.567,78.460,% | Adaptive Acc: 76.082% | clf_exit: 0.132 0.346 0.522
Batch: 180 | Loss: 4.508 | Acc: 39.615,64.468,78.406,% | Adaptive Acc: 76.023% | clf_exit: 0.133 0.346 0.521
Batch: 200 | Loss: 4.518 | Acc: 39.463,64.436,78.455,% | Adaptive Acc: 75.983% | clf_exit: 0.133 0.345 0.522
Batch: 220 | Loss: 4.524 | Acc: 39.381,64.324,78.450,% | Adaptive Acc: 75.933% | clf_exit: 0.133 0.345 0.522
Batch: 240 | Loss: 4.521 | Acc: 39.383,64.455,78.540,% | Adaptive Acc: 76.034% | clf_exit: 0.133 0.346 0.521
Batch: 260 | Loss: 4.520 | Acc: 39.389,64.335,78.499,% | Adaptive Acc: 76.021% | clf_exit: 0.133 0.345 0.522
Batch: 280 | Loss: 4.525 | Acc: 39.332,64.293,78.520,% | Adaptive Acc: 76.034% | clf_exit: 0.134 0.345 0.522
Batch: 300 | Loss: 4.515 | Acc: 39.304,64.345,78.590,% | Adaptive Acc: 76.113% | clf_exit: 0.134 0.346 0.520
Batch: 320 | Loss: 4.524 | Acc: 39.191,64.296,78.541,% | Adaptive Acc: 76.061% | clf_exit: 0.133 0.345 0.521
Batch: 340 | Loss: 4.521 | Acc: 39.232,64.248,78.540,% | Adaptive Acc: 76.068% | clf_exit: 0.134 0.345 0.521
Batch: 360 | Loss: 4.526 | Acc: 39.071,64.225,78.476,% | Adaptive Acc: 75.965% | clf_exit: 0.134 0.345 0.521
Batch: 380 | Loss: 4.532 | Acc: 39.032,64.124,78.431,% | Adaptive Acc: 75.908% | clf_exit: 0.134 0.345 0.521
Batch: 0 | Loss: 5.132 | Acc: 37.500,59.375,67.969,% | Adaptive Acc: 63.281% | clf_exit: 0.234 0.422 0.344
Batch: 20 | Loss: 5.340 | Acc: 37.091,58.966,64.955,% | Adaptive Acc: 61.905% | clf_exit: 0.180 0.378 0.443
Batch: 40 | Loss: 5.364 | Acc: 37.252,58.175,63.986,% | Adaptive Acc: 61.623% | clf_exit: 0.179 0.374 0.447
Batch: 60 | Loss: 5.414 | Acc: 36.988,57.877,63.371,% | Adaptive Acc: 61.130% | clf_exit: 0.180 0.370 0.450
Train classifier parameters

Epoch: 47
Batch: 0 | Loss: 4.355 | Acc: 39.844,67.188,72.656,% | Adaptive Acc: 70.312% | clf_exit: 0.133 0.352 0.516
Batch: 20 | Loss: 4.590 | Acc: 38.132,63.132,78.571,% | Adaptive Acc: 75.670% | clf_exit: 0.136 0.344 0.520
Batch: 40 | Loss: 4.548 | Acc: 38.643,63.586,78.639,% | Adaptive Acc: 76.181% | clf_exit: 0.137 0.332 0.531
Batch: 60 | Loss: 4.497 | Acc: 38.934,64.395,78.893,% | Adaptive Acc: 76.575% | clf_exit: 0.141 0.330 0.529
Batch: 80 | Loss: 4.510 | Acc: 39.072,64.313,78.549,% | Adaptive Acc: 76.215% | clf_exit: 0.140 0.335 0.525
Batch: 100 | Loss: 4.519 | Acc: 39.093,64.318,78.512,% | Adaptive Acc: 76.091% | clf_exit: 0.138 0.337 0.524
Batch: 120 | Loss: 4.509 | Acc: 39.224,64.682,78.790,% | Adaptive Acc: 76.311% | clf_exit: 0.139 0.339 0.523
Batch: 140 | Loss: 4.524 | Acc: 39.135,64.461,78.729,% | Adaptive Acc: 76.258% | clf_exit: 0.139 0.338 0.523
Batch: 160 | Loss: 4.528 | Acc: 39.198,64.451,78.659,% | Adaptive Acc: 76.233% | clf_exit: 0.139 0.339 0.522
Batch: 180 | Loss: 4.529 | Acc: 39.231,64.481,78.712,% | Adaptive Acc: 76.178% | clf_exit: 0.138 0.342 0.520
Batch: 200 | Loss: 4.523 | Acc: 39.241,64.436,78.898,% | Adaptive Acc: 76.376% | clf_exit: 0.138 0.343 0.520
Batch: 220 | Loss: 4.513 | Acc: 39.349,64.557,78.882,% | Adaptive Acc: 76.393% | clf_exit: 0.138 0.343 0.519
Batch: 240 | Loss: 4.519 | Acc: 39.276,64.533,78.806,% | Adaptive Acc: 76.280% | clf_exit: 0.138 0.342 0.520
Batch: 260 | Loss: 4.523 | Acc: 39.254,64.407,78.849,% | Adaptive Acc: 76.341% | clf_exit: 0.138 0.341 0.521
Batch: 280 | Loss: 4.524 | Acc: 39.227,64.335,78.767,% | Adaptive Acc: 76.271% | clf_exit: 0.138 0.342 0.520
Batch: 300 | Loss: 4.525 | Acc: 39.078,64.252,78.696,% | Adaptive Acc: 76.217% | clf_exit: 0.137 0.342 0.521
Batch: 320 | Loss: 4.521 | Acc: 39.114,64.257,78.733,% | Adaptive Acc: 76.253% | clf_exit: 0.138 0.341 0.521
Batch: 340 | Loss: 4.522 | Acc: 39.150,64.243,78.686,% | Adaptive Acc: 76.219% | clf_exit: 0.138 0.341 0.521
Batch: 360 | Loss: 4.530 | Acc: 39.136,64.147,78.590,% | Adaptive Acc: 76.125% | clf_exit: 0.138 0.341 0.521
Batch: 380 | Loss: 4.523 | Acc: 39.212,64.288,78.584,% | Adaptive Acc: 76.144% | clf_exit: 0.138 0.341 0.521
Batch: 0 | Loss: 5.088 | Acc: 39.062,58.594,70.312,% | Adaptive Acc: 66.406% | clf_exit: 0.250 0.375 0.375
Batch: 20 | Loss: 5.321 | Acc: 38.021,58.929,64.658,% | Adaptive Acc: 62.016% | clf_exit: 0.185 0.376 0.439
Batch: 40 | Loss: 5.344 | Acc: 38.224,58.498,63.967,% | Adaptive Acc: 61.795% | clf_exit: 0.181 0.375 0.444
Batch: 60 | Loss: 5.398 | Acc: 37.833,58.081,63.320,% | Adaptive Acc: 61.219% | clf_exit: 0.182 0.372 0.447
Train classifier parameters

Epoch: 48
Batch: 0 | Loss: 4.714 | Acc: 38.281,62.500,72.656,% | Adaptive Acc: 67.188% | clf_exit: 0.094 0.367 0.539
Batch: 20 | Loss: 4.529 | Acc: 38.765,64.360,78.943,% | Adaptive Acc: 76.042% | clf_exit: 0.124 0.357 0.519
Batch: 40 | Loss: 4.508 | Acc: 39.329,64.310,78.563,% | Adaptive Acc: 75.819% | clf_exit: 0.135 0.349 0.517
Batch: 60 | Loss: 4.511 | Acc: 39.716,64.460,78.765,% | Adaptive Acc: 75.832% | clf_exit: 0.138 0.341 0.520
Batch: 80 | Loss: 4.511 | Acc: 39.728,64.603,78.636,% | Adaptive Acc: 75.926% | clf_exit: 0.138 0.342 0.519
Batch: 100 | Loss: 4.511 | Acc: 39.588,64.465,78.643,% | Adaptive Acc: 75.975% | clf_exit: 0.136 0.343 0.520
Batch: 120 | Loss: 4.518 | Acc: 39.276,64.482,78.474,% | Adaptive Acc: 75.904% | clf_exit: 0.135 0.343 0.521
Batch: 140 | Loss: 4.520 | Acc: 39.279,64.373,78.541,% | Adaptive Acc: 75.997% | clf_exit: 0.133 0.346 0.521
Batch: 160 | Loss: 4.518 | Acc: 39.305,64.485,78.722,% | Adaptive Acc: 76.155% | clf_exit: 0.133 0.345 0.521
Batch: 180 | Loss: 4.519 | Acc: 39.192,64.490,78.721,% | Adaptive Acc: 76.144% | clf_exit: 0.135 0.345 0.520
Batch: 200 | Loss: 4.508 | Acc: 39.179,64.630,78.685,% | Adaptive Acc: 76.100% | clf_exit: 0.135 0.346 0.520
Batch: 220 | Loss: 4.509 | Acc: 39.073,64.536,78.733,% | Adaptive Acc: 76.198% | clf_exit: 0.134 0.345 0.521
Batch: 240 | Loss: 4.514 | Acc: 39.004,64.422,78.644,% | Adaptive Acc: 76.125% | clf_exit: 0.134 0.345 0.521
Batch: 260 | Loss: 4.514 | Acc: 38.988,64.362,78.691,% | Adaptive Acc: 76.179% | clf_exit: 0.135 0.344 0.521
Batch: 280 | Loss: 4.516 | Acc: 38.937,64.335,78.723,% | Adaptive Acc: 76.173% | clf_exit: 0.135 0.343 0.521
Batch: 300 | Loss: 4.518 | Acc: 38.964,64.283,78.657,% | Adaptive Acc: 76.088% | clf_exit: 0.136 0.342 0.522
Batch: 320 | Loss: 4.511 | Acc: 38.977,64.418,78.724,% | Adaptive Acc: 76.185% | clf_exit: 0.136 0.342 0.522
Batch: 340 | Loss: 4.507 | Acc: 39.046,64.447,78.666,% | Adaptive Acc: 76.104% | clf_exit: 0.136 0.343 0.520
Batch: 360 | Loss: 4.501 | Acc: 39.078,64.482,78.733,% | Adaptive Acc: 76.205% | clf_exit: 0.136 0.345 0.519
Batch: 380 | Loss: 4.502 | Acc: 39.138,64.440,78.683,% | Adaptive Acc: 76.185% | clf_exit: 0.137 0.344 0.519
Batch: 0 | Loss: 5.110 | Acc: 36.719,57.812,67.188,% | Adaptive Acc: 62.500% | clf_exit: 0.258 0.375 0.367
Batch: 20 | Loss: 5.321 | Acc: 37.463,59.003,64.881,% | Adaptive Acc: 61.830% | clf_exit: 0.183 0.379 0.438
Batch: 40 | Loss: 5.335 | Acc: 37.938,58.365,64.196,% | Adaptive Acc: 61.871% | clf_exit: 0.182 0.379 0.439
Batch: 60 | Loss: 5.391 | Acc: 37.628,57.877,63.601,% | Adaptive Acc: 61.347% | clf_exit: 0.184 0.370 0.446
Train classifier parameters

Epoch: 49
Batch: 0 | Loss: 4.713 | Acc: 38.281,64.062,75.000,% | Adaptive Acc: 73.438% | clf_exit: 0.125 0.344 0.531
Batch: 20 | Loss: 4.519 | Acc: 38.132,64.881,78.609,% | Adaptive Acc: 75.818% | clf_exit: 0.140 0.340 0.520
Batch: 40 | Loss: 4.571 | Acc: 37.786,63.910,78.201,% | Adaptive Acc: 75.476% | clf_exit: 0.137 0.338 0.525
Batch: 60 | Loss: 4.556 | Acc: 38.332,63.998,78.407,% | Adaptive Acc: 75.551% | clf_exit: 0.135 0.344 0.520
Batch: 80 | Loss: 4.521 | Acc: 38.812,64.304,78.424,% | Adaptive Acc: 75.733% | clf_exit: 0.137 0.344 0.518
Batch: 100 | Loss: 4.521 | Acc: 38.854,64.256,78.380,% | Adaptive Acc: 75.580% | clf_exit: 0.138 0.344 0.517
Batch: 120 | Loss: 4.533 | Acc: 38.765,64.179,78.312,% | Adaptive Acc: 75.646% | clf_exit: 0.138 0.342 0.520
Batch: 140 | Loss: 4.538 | Acc: 38.752,64.135,78.446,% | Adaptive Acc: 75.881% | clf_exit: 0.138 0.339 0.522
Batch: 160 | Loss: 4.531 | Acc: 38.810,64.286,78.465,% | Adaptive Acc: 75.995% | clf_exit: 0.138 0.340 0.522
Batch: 180 | Loss: 4.522 | Acc: 38.950,64.408,78.734,% | Adaptive Acc: 76.204% | clf_exit: 0.138 0.341 0.521
Batch: 200 | Loss: 4.517 | Acc: 39.078,64.471,78.755,% | Adaptive Acc: 76.279% | clf_exit: 0.138 0.342 0.520
Batch: 220 | Loss: 4.509 | Acc: 39.119,64.564,78.740,% | Adaptive Acc: 76.304% | clf_exit: 0.139 0.342 0.519
Batch: 240 | Loss: 4.502 | Acc: 39.131,64.627,78.819,% | Adaptive Acc: 76.319% | clf_exit: 0.139 0.343 0.518
Batch: 260 | Loss: 4.507 | Acc: 39.200,64.559,78.754,% | Adaptive Acc: 76.203% | clf_exit: 0.139 0.344 0.517
Batch: 280 | Loss: 4.502 | Acc: 39.346,64.510,78.823,% | Adaptive Acc: 76.268% | clf_exit: 0.139 0.345 0.516
Batch: 300 | Loss: 4.498 | Acc: 39.387,64.480,78.828,% | Adaptive Acc: 76.261% | clf_exit: 0.139 0.345 0.516
Batch: 320 | Loss: 4.497 | Acc: 39.386,64.503,78.872,% | Adaptive Acc: 76.285% | clf_exit: 0.139 0.345 0.516
Batch: 340 | Loss: 4.497 | Acc: 39.388,64.486,78.881,% | Adaptive Acc: 76.260% | clf_exit: 0.138 0.346 0.515
Batch: 360 | Loss: 4.495 | Acc: 39.413,64.446,78.882,% | Adaptive Acc: 76.281% | clf_exit: 0.139 0.345 0.516
Batch: 380 | Loss: 4.495 | Acc: 39.411,64.415,78.851,% | Adaptive Acc: 76.243% | clf_exit: 0.140 0.345 0.516
Batch: 0 | Loss: 5.102 | Acc: 37.500,60.156,69.531,% | Adaptive Acc: 64.062% | clf_exit: 0.250 0.367 0.383
Batch: 20 | Loss: 5.316 | Acc: 37.202,59.263,64.769,% | Adaptive Acc: 62.128% | clf_exit: 0.180 0.388 0.432
Batch: 40 | Loss: 5.338 | Acc: 37.633,58.403,64.215,% | Adaptive Acc: 62.214% | clf_exit: 0.178 0.382 0.440
Batch: 60 | Loss: 5.390 | Acc: 37.257,58.158,63.678,% | Adaptive Acc: 61.783% | clf_exit: 0.178 0.377 0.446
Train all parameters

Epoch: 50
Batch: 0 | Loss: 4.393 | Acc: 34.375,59.375,81.250,% | Adaptive Acc: 76.562% | clf_exit: 0.102 0.305 0.594
Batch: 20 | Loss: 4.997 | Acc: 36.421,60.900,71.317,% | Adaptive Acc: 69.494% | clf_exit: 0.127 0.356 0.517
Batch: 40 | Loss: 5.660 | Acc: 34.432,55.774,61.147,% | Adaptive Acc: 60.918% | clf_exit: 0.113 0.326 0.560
Batch: 60 | Loss: 5.922 | Acc: 33.568,53.573,57.351,% | Adaptive Acc: 57.441% | clf_exit: 0.113 0.304 0.583
Batch: 80 | Loss: 6.030 | Acc: 32.996,53.270,56.047,% | Adaptive Acc: 56.221% | clf_exit: 0.111 0.302 0.587
Batch: 100 | Loss: 6.033 | Acc: 33.037,53.187,56.018,% | Adaptive Acc: 56.196% | clf_exit: 0.112 0.300 0.589
Batch: 120 | Loss: 6.030 | Acc: 32.967,52.938,55.934,% | Adaptive Acc: 56.056% | clf_exit: 0.113 0.297 0.590
Batch: 140 | Loss: 6.001 | Acc: 33.322,53.142,56.078,% | Adaptive Acc: 56.217% | clf_exit: 0.112 0.298 0.590
Batch: 160 | Loss: 5.962 | Acc: 33.599,53.402,56.434,% | Adaptive Acc: 56.522% | clf_exit: 0.112 0.299 0.589
Batch: 180 | Loss: 5.935 | Acc: 33.883,53.673,56.802,% | Adaptive Acc: 56.872% | clf_exit: 0.113 0.301 0.585
Batch: 200 | Loss: 5.911 | Acc: 33.986,53.735,57.066,% | Adaptive Acc: 57.148% | clf_exit: 0.114 0.300 0.586
Batch: 220 | Loss: 5.877 | Acc: 34.219,53.963,57.424,% | Adaptive Acc: 57.427% | clf_exit: 0.116 0.299 0.585
Batch: 240 | Loss: 5.844 | Acc: 34.372,54.276,57.845,% | Adaptive Acc: 57.735% | clf_exit: 0.117 0.300 0.584
Batch: 260 | Loss: 5.818 | Acc: 34.438,54.496,58.154,% | Adaptive Acc: 57.959% | clf_exit: 0.118 0.300 0.582
Batch: 280 | Loss: 5.792 | Acc: 34.533,54.635,58.477,% | Adaptive Acc: 58.249% | clf_exit: 0.119 0.299 0.582
Batch: 300 | Loss: 5.770 | Acc: 34.572,54.765,58.814,% | Adaptive Acc: 58.487% | clf_exit: 0.120 0.299 0.581
Batch: 320 | Loss: 5.746 | Acc: 34.718,54.972,59.051,% | Adaptive Acc: 58.720% | clf_exit: 0.120 0.300 0.580
Batch: 340 | Loss: 5.742 | Acc: 34.769,54.912,59.107,% | Adaptive Acc: 58.782% | clf_exit: 0.119 0.300 0.580
Batch: 360 | Loss: 5.729 | Acc: 34.803,55.034,59.254,% | Adaptive Acc: 58.927% | clf_exit: 0.119 0.301 0.580
Batch: 380 | Loss: 5.713 | Acc: 34.884,55.159,59.369,% | Adaptive Acc: 59.033% | clf_exit: 0.120 0.302 0.579
Batch: 0 | Loss: 5.657 | Acc: 28.906,57.031,61.719,% | Adaptive Acc: 60.156% | clf_exit: 0.281 0.320 0.398
Batch: 20 | Loss: 6.110 | Acc: 32.180,52.604,57.329,% | Adaptive Acc: 55.618% | clf_exit: 0.196 0.343 0.462
Batch: 40 | Loss: 6.099 | Acc: 32.260,51.982,56.784,% | Adaptive Acc: 55.240% | clf_exit: 0.192 0.341 0.466
Batch: 60 | Loss: 6.133 | Acc: 31.852,52.062,56.493,% | Adaptive Acc: 54.854% | clf_exit: 0.194 0.338 0.467
Train all parameters

Epoch: 51
Batch: 0 | Loss: 5.173 | Acc: 39.844,56.250,71.094,% | Adaptive Acc: 68.750% | clf_exit: 0.172 0.297 0.531
Batch: 20 | Loss: 5.032 | Acc: 37.612,60.045,69.717,% | Adaptive Acc: 68.378% | clf_exit: 0.137 0.304 0.560
Batch: 40 | Loss: 5.081 | Acc: 37.424,59.394,68.960,% | Adaptive Acc: 67.302% | clf_exit: 0.137 0.314 0.549
Batch: 60 | Loss: 5.094 | Acc: 36.706,59.798,68.840,% | Adaptive Acc: 67.264% | clf_exit: 0.135 0.315 0.549
Batch: 80 | Loss: 5.117 | Acc: 37.037,59.693,68.277,% | Adaptive Acc: 66.850% | clf_exit: 0.134 0.317 0.549
Batch: 100 | Loss: 5.118 | Acc: 36.812,59.831,68.209,% | Adaptive Acc: 66.855% | clf_exit: 0.134 0.318 0.549
Batch: 120 | Loss: 5.098 | Acc: 36.996,59.892,68.311,% | Adaptive Acc: 66.974% | clf_exit: 0.133 0.320 0.547
Batch: 140 | Loss: 5.092 | Acc: 37.023,59.990,68.395,% | Adaptive Acc: 67.093% | clf_exit: 0.132 0.324 0.544
Batch: 160 | Loss: 5.077 | Acc: 37.286,60.117,68.541,% | Adaptive Acc: 67.129% | clf_exit: 0.133 0.326 0.541
Batch: 180 | Loss: 5.091 | Acc: 37.176,59.940,68.392,% | Adaptive Acc: 66.959% | clf_exit: 0.134 0.324 0.542
Batch: 200 | Loss: 5.091 | Acc: 37.100,59.900,68.276,% | Adaptive Acc: 66.869% | clf_exit: 0.134 0.324 0.542
Batch: 220 | Loss: 5.079 | Acc: 37.228,59.912,68.319,% | Adaptive Acc: 66.926% | clf_exit: 0.134 0.325 0.541
Batch: 240 | Loss: 5.074 | Acc: 37.286,60.033,68.338,% | Adaptive Acc: 66.957% | clf_exit: 0.135 0.326 0.539
Batch: 260 | Loss: 5.080 | Acc: 37.249,59.905,68.262,% | Adaptive Acc: 66.843% | clf_exit: 0.136 0.325 0.539
Batch: 280 | Loss: 5.086 | Acc: 37.236,59.773,68.247,% | Adaptive Acc: 66.832% | clf_exit: 0.136 0.324 0.541
Batch: 300 | Loss: 5.079 | Acc: 37.336,59.853,68.275,% | Adaptive Acc: 66.876% | clf_exit: 0.135 0.325 0.540
Batch: 320 | Loss: 5.076 | Acc: 37.405,59.952,68.322,% | Adaptive Acc: 66.908% | clf_exit: 0.136 0.326 0.539
Batch: 340 | Loss: 5.067 | Acc: 37.516,60.035,68.377,% | Adaptive Acc: 66.956% | clf_exit: 0.136 0.326 0.538
Batch: 360 | Loss: 5.073 | Acc: 37.522,59.953,68.328,% | Adaptive Acc: 66.913% | clf_exit: 0.136 0.326 0.538
Batch: 380 | Loss: 5.070 | Acc: 37.576,60.023,68.309,% | Adaptive Acc: 66.935% | clf_exit: 0.136 0.326 0.538
Batch: 0 | Loss: 5.623 | Acc: 36.719,54.688,60.938,% | Adaptive Acc: 57.812% | clf_exit: 0.266 0.328 0.406
Batch: 20 | Loss: 6.095 | Acc: 32.031,52.827,57.366,% | Adaptive Acc: 55.022% | clf_exit: 0.196 0.365 0.440
Batch: 40 | Loss: 6.071 | Acc: 31.955,52.268,57.622,% | Adaptive Acc: 55.030% | clf_exit: 0.193 0.360 0.448
Batch: 60 | Loss: 6.117 | Acc: 31.583,51.883,57.480,% | Adaptive Acc: 54.944% | clf_exit: 0.195 0.352 0.454
Train all parameters

Epoch: 52
Batch: 0 | Loss: 5.343 | Acc: 34.375,54.688,67.969,% | Adaptive Acc: 67.188% | clf_exit: 0.117 0.281 0.602
Batch: 20 | Loss: 4.813 | Acc: 37.500,62.165,72.321,% | Adaptive Acc: 69.606% | clf_exit: 0.147 0.327 0.525
Batch: 40 | Loss: 4.766 | Acc: 38.777,62.271,73.075,% | Adaptive Acc: 70.941% | clf_exit: 0.147 0.335 0.518
Batch: 60 | Loss: 4.783 | Acc: 38.678,62.269,72.925,% | Adaptive Acc: 70.722% | clf_exit: 0.144 0.334 0.521
Batch: 80 | Loss: 4.804 | Acc: 38.696,62.182,72.685,% | Adaptive Acc: 70.476% | clf_exit: 0.144 0.334 0.522
Batch: 100 | Loss: 4.794 | Acc: 38.583,62.229,72.834,% | Adaptive Acc: 70.707% | clf_exit: 0.143 0.336 0.521
Batch: 120 | Loss: 4.800 | Acc: 38.617,62.093,72.618,% | Adaptive Acc: 70.629% | clf_exit: 0.143 0.335 0.522
Batch: 140 | Loss: 4.814 | Acc: 38.514,61.863,72.584,% | Adaptive Acc: 70.612% | clf_exit: 0.145 0.331 0.524
Batch: 160 | Loss: 4.829 | Acc: 38.461,61.588,72.385,% | Adaptive Acc: 70.342% | clf_exit: 0.144 0.330 0.526
Batch: 180 | Loss: 4.825 | Acc: 38.704,61.723,72.384,% | Adaptive Acc: 70.356% | clf_exit: 0.146 0.331 0.523
Batch: 200 | Loss: 4.826 | Acc: 38.553,61.746,72.431,% | Adaptive Acc: 70.441% | clf_exit: 0.146 0.330 0.524
Batch: 220 | Loss: 4.819 | Acc: 38.649,61.874,72.317,% | Adaptive Acc: 70.450% | clf_exit: 0.146 0.332 0.522
Batch: 240 | Loss: 4.822 | Acc: 38.622,61.891,72.235,% | Adaptive Acc: 70.374% | clf_exit: 0.145 0.333 0.522
Batch: 260 | Loss: 4.817 | Acc: 38.628,62.000,72.216,% | Adaptive Acc: 70.363% | clf_exit: 0.144 0.335 0.521
Batch: 280 | Loss: 4.819 | Acc: 38.640,62.019,72.106,% | Adaptive Acc: 70.310% | clf_exit: 0.143 0.335 0.521
Batch: 300 | Loss: 4.826 | Acc: 38.546,61.903,71.976,% | Adaptive Acc: 70.167% | clf_exit: 0.143 0.335 0.521
Batch: 320 | Loss: 4.827 | Acc: 38.542,61.867,71.892,% | Adaptive Acc: 70.045% | clf_exit: 0.143 0.336 0.520
Batch: 340 | Loss: 4.831 | Acc: 38.609,61.801,71.774,% | Adaptive Acc: 69.946% | clf_exit: 0.143 0.336 0.521
Batch: 360 | Loss: 4.832 | Acc: 38.630,61.784,71.700,% | Adaptive Acc: 69.895% | clf_exit: 0.142 0.336 0.521
Batch: 380 | Loss: 4.823 | Acc: 38.630,61.856,71.824,% | Adaptive Acc: 69.991% | clf_exit: 0.143 0.337 0.521
Batch: 0 | Loss: 5.792 | Acc: 33.594,54.688,60.938,% | Adaptive Acc: 54.688% | clf_exit: 0.258 0.367 0.375
Batch: 20 | Loss: 6.017 | Acc: 31.994,53.720,60.528,% | Adaptive Acc: 54.874% | clf_exit: 0.259 0.331 0.409
Batch: 40 | Loss: 5.979 | Acc: 31.593,52.668,60.709,% | Adaptive Acc: 55.183% | clf_exit: 0.253 0.335 0.412
Batch: 60 | Loss: 5.956 | Acc: 31.519,52.754,60.848,% | Adaptive Acc: 55.635% | clf_exit: 0.250 0.339 0.411
Train all parameters

Epoch: 53
Batch: 0 | Loss: 4.653 | Acc: 34.375,57.031,72.656,% | Adaptive Acc: 67.969% | clf_exit: 0.125 0.312 0.562
Batch: 20 | Loss: 4.571 | Acc: 39.062,63.170,75.856,% | Adaptive Acc: 73.214% | clf_exit: 0.149 0.336 0.515
Batch: 40 | Loss: 4.552 | Acc: 39.310,63.872,76.467,% | Adaptive Acc: 74.028% | clf_exit: 0.148 0.342 0.510
Batch: 60 | Loss: 4.540 | Acc: 39.511,63.755,76.255,% | Adaptive Acc: 73.796% | clf_exit: 0.151 0.343 0.506
Batch: 80 | Loss: 4.523 | Acc: 39.767,64.043,76.302,% | Adaptive Acc: 73.881% | clf_exit: 0.149 0.347 0.505
Batch: 100 | Loss: 4.533 | Acc: 40.091,64.225,76.354,% | Adaptive Acc: 73.948% | clf_exit: 0.150 0.347 0.504
Batch: 120 | Loss: 4.549 | Acc: 39.986,64.004,76.098,% | Adaptive Acc: 73.586% | clf_exit: 0.150 0.346 0.504
Batch: 140 | Loss: 4.569 | Acc: 39.888,63.758,75.643,% | Adaptive Acc: 73.232% | clf_exit: 0.149 0.347 0.504
Batch: 160 | Loss: 4.594 | Acc: 39.625,63.635,75.432,% | Adaptive Acc: 73.030% | clf_exit: 0.148 0.346 0.506
Batch: 180 | Loss: 4.584 | Acc: 39.632,63.907,75.466,% | Adaptive Acc: 73.183% | clf_exit: 0.148 0.346 0.506
Batch: 200 | Loss: 4.593 | Acc: 39.747,63.841,75.373,% | Adaptive Acc: 73.142% | clf_exit: 0.148 0.346 0.506
Batch: 220 | Loss: 4.613 | Acc: 39.621,63.649,75.170,% | Adaptive Acc: 72.932% | clf_exit: 0.147 0.345 0.507
Batch: 240 | Loss: 4.617 | Acc: 39.610,63.605,75.006,% | Adaptive Acc: 72.799% | clf_exit: 0.146 0.346 0.508
Batch: 260 | Loss: 4.616 | Acc: 39.625,63.667,74.985,% | Adaptive Acc: 72.809% | clf_exit: 0.147 0.345 0.508
Batch: 280 | Loss: 4.622 | Acc: 39.560,63.620,74.814,% | Adaptive Acc: 72.637% | clf_exit: 0.147 0.345 0.508
Batch: 300 | Loss: 4.630 | Acc: 39.512,63.595,74.808,% | Adaptive Acc: 72.612% | clf_exit: 0.148 0.344 0.508
Batch: 320 | Loss: 4.633 | Acc: 39.471,63.500,74.749,% | Adaptive Acc: 72.535% | clf_exit: 0.148 0.343 0.508
Batch: 340 | Loss: 4.638 | Acc: 39.392,63.437,74.572,% | Adaptive Acc: 72.368% | clf_exit: 0.148 0.344 0.508
Batch: 360 | Loss: 4.644 | Acc: 39.381,63.359,74.459,% | Adaptive Acc: 72.262% | clf_exit: 0.148 0.343 0.509
Batch: 380 | Loss: 4.639 | Acc: 39.389,63.384,74.424,% | Adaptive Acc: 72.240% | clf_exit: 0.149 0.343 0.508
Batch: 0 | Loss: 5.612 | Acc: 32.812,54.688,68.750,% | Adaptive Acc: 65.625% | clf_exit: 0.242 0.391 0.367
Batch: 20 | Loss: 5.679 | Acc: 34.821,55.841,61.384,% | Adaptive Acc: 58.408% | clf_exit: 0.193 0.378 0.429
Batch: 40 | Loss: 5.651 | Acc: 34.889,55.107,61.014,% | Adaptive Acc: 58.365% | clf_exit: 0.188 0.377 0.436
Batch: 60 | Loss: 5.643 | Acc: 35.131,55.251,61.040,% | Adaptive Acc: 58.286% | clf_exit: 0.188 0.375 0.437
Train all parameters

Epoch: 54
Batch: 0 | Loss: 4.981 | Acc: 39.062,60.156,67.969,% | Adaptive Acc: 68.750% | clf_exit: 0.148 0.336 0.516
Batch: 20 | Loss: 4.560 | Acc: 39.435,63.356,76.414,% | Adaptive Acc: 74.368% | clf_exit: 0.143 0.354 0.503
Batch: 40 | Loss: 4.465 | Acc: 40.053,64.272,78.030,% | Adaptive Acc: 75.819% | clf_exit: 0.146 0.358 0.497
Batch: 60 | Loss: 4.432 | Acc: 39.985,65.087,77.715,% | Adaptive Acc: 75.525% | clf_exit: 0.148 0.356 0.496
Batch: 80 | Loss: 4.453 | Acc: 39.940,64.699,77.662,% | Adaptive Acc: 75.511% | clf_exit: 0.150 0.350 0.500
Batch: 100 | Loss: 4.459 | Acc: 39.728,64.581,77.645,% | Adaptive Acc: 75.186% | clf_exit: 0.150 0.349 0.501
Batch: 120 | Loss: 4.461 | Acc: 39.753,64.624,77.667,% | Adaptive Acc: 75.155% | clf_exit: 0.148 0.351 0.501
Batch: 140 | Loss: 4.455 | Acc: 39.860,64.777,77.610,% | Adaptive Acc: 74.922% | clf_exit: 0.152 0.349 0.499
Batch: 160 | Loss: 4.455 | Acc: 39.883,64.786,77.538,% | Adaptive Acc: 74.850% | clf_exit: 0.153 0.350 0.497
Batch: 180 | Loss: 4.466 | Acc: 39.753,64.529,77.430,% | Adaptive Acc: 74.767% | clf_exit: 0.153 0.349 0.498
Batch: 200 | Loss: 4.480 | Acc: 39.669,64.393,77.254,% | Adaptive Acc: 74.604% | clf_exit: 0.153 0.347 0.499
Batch: 220 | Loss: 4.491 | Acc: 39.674,64.367,77.040,% | Adaptive Acc: 74.431% | clf_exit: 0.154 0.346 0.500
Batch: 240 | Loss: 4.502 | Acc: 39.659,64.218,76.835,% | Adaptive Acc: 74.225% | clf_exit: 0.153 0.346 0.501
Batch: 260 | Loss: 4.520 | Acc: 39.619,64.110,76.646,% | Adaptive Acc: 74.063% | clf_exit: 0.152 0.346 0.502
Batch: 280 | Loss: 4.522 | Acc: 39.657,64.082,76.501,% | Adaptive Acc: 73.955% | clf_exit: 0.152 0.347 0.501
Batch: 300 | Loss: 4.534 | Acc: 39.569,64.008,76.404,% | Adaptive Acc: 73.918% | clf_exit: 0.152 0.347 0.501
Batch: 320 | Loss: 4.534 | Acc: 39.637,64.036,76.336,% | Adaptive Acc: 73.868% | clf_exit: 0.151 0.348 0.501
Batch: 340 | Loss: 4.536 | Acc: 39.709,64.056,76.258,% | Adaptive Acc: 73.827% | clf_exit: 0.151 0.348 0.501
Batch: 360 | Loss: 4.537 | Acc: 39.779,64.112,76.260,% | Adaptive Acc: 73.881% | clf_exit: 0.151 0.348 0.501
Batch: 380 | Loss: 4.537 | Acc: 39.741,64.120,76.239,% | Adaptive Acc: 73.841% | clf_exit: 0.151 0.349 0.500
Batch: 0 | Loss: 4.855 | Acc: 40.625,60.938,63.281,% | Adaptive Acc: 67.188% | clf_exit: 0.266 0.352 0.383
Batch: 20 | Loss: 5.349 | Acc: 35.603,58.408,64.472,% | Adaptive Acc: 61.756% | clf_exit: 0.219 0.347 0.433
Batch: 40 | Loss: 5.389 | Acc: 35.442,57.927,63.148,% | Adaptive Acc: 60.785% | clf_exit: 0.216 0.351 0.433
Batch: 60 | Loss: 5.400 | Acc: 35.272,57.812,63.512,% | Adaptive Acc: 60.976% | clf_exit: 0.216 0.350 0.434
Train all parameters

Epoch: 55
Batch: 0 | Loss: 3.599 | Acc: 49.219,71.094,79.688,% | Adaptive Acc: 75.781% | clf_exit: 0.188 0.469 0.344
Batch: 20 | Loss: 4.332 | Acc: 39.993,65.216,79.576,% | Adaptive Acc: 75.967% | clf_exit: 0.159 0.366 0.475
Batch: 40 | Loss: 4.322 | Acc: 39.425,65.587,79.688,% | Adaptive Acc: 76.562% | clf_exit: 0.156 0.363 0.482
Batch: 60 | Loss: 4.368 | Acc: 39.677,65.484,79.137,% | Adaptive Acc: 76.230% | clf_exit: 0.152 0.367 0.482
Batch: 80 | Loss: 4.349 | Acc: 39.921,65.673,79.263,% | Adaptive Acc: 76.466% | clf_exit: 0.153 0.364 0.483
Batch: 100 | Loss: 4.369 | Acc: 39.952,65.494,78.844,% | Adaptive Acc: 76.153% | clf_exit: 0.154 0.361 0.485
Batch: 120 | Loss: 4.390 | Acc: 39.637,65.322,78.680,% | Adaptive Acc: 75.975% | clf_exit: 0.153 0.361 0.486
Batch: 140 | Loss: 4.375 | Acc: 39.982,65.381,78.768,% | Adaptive Acc: 76.069% | clf_exit: 0.154 0.360 0.486
Batch: 160 | Loss: 4.372 | Acc: 40.261,65.387,78.804,% | Adaptive Acc: 76.131% | clf_exit: 0.157 0.358 0.485
Batch: 180 | Loss: 4.375 | Acc: 40.284,65.357,78.552,% | Adaptive Acc: 75.988% | clf_exit: 0.157 0.357 0.486
Batch: 200 | Loss: 4.381 | Acc: 40.178,65.396,78.432,% | Adaptive Acc: 75.894% | clf_exit: 0.158 0.356 0.486
Batch: 220 | Loss: 4.382 | Acc: 40.226,65.370,78.461,% | Adaptive Acc: 75.926% | clf_exit: 0.157 0.358 0.486
Batch: 240 | Loss: 4.389 | Acc: 40.210,65.382,78.339,% | Adaptive Acc: 75.772% | clf_exit: 0.157 0.356 0.487
Batch: 260 | Loss: 4.393 | Acc: 40.251,65.245,78.242,% | Adaptive Acc: 75.673% | clf_exit: 0.158 0.355 0.486
Batch: 280 | Loss: 4.396 | Acc: 40.275,65.264,78.200,% | Adaptive Acc: 75.576% | clf_exit: 0.157 0.357 0.486
Batch: 300 | Loss: 4.407 | Acc: 40.171,65.241,78.047,% | Adaptive Acc: 75.483% | clf_exit: 0.157 0.356 0.487
Batch: 320 | Loss: 4.423 | Acc: 40.094,65.009,77.848,% | Adaptive Acc: 75.307% | clf_exit: 0.156 0.355 0.488
Batch: 340 | Loss: 4.426 | Acc: 40.043,64.963,77.827,% | Adaptive Acc: 75.263% | clf_exit: 0.157 0.355 0.488
Batch: 360 | Loss: 4.431 | Acc: 40.034,64.917,77.740,% | Adaptive Acc: 75.208% | clf_exit: 0.157 0.355 0.489
Batch: 380 | Loss: 4.436 | Acc: 40.016,64.891,77.631,% | Adaptive Acc: 75.137% | clf_exit: 0.156 0.355 0.489
Batch: 0 | Loss: 5.297 | Acc: 36.719,60.156,66.406,% | Adaptive Acc: 67.188% | clf_exit: 0.227 0.312 0.461
Batch: 20 | Loss: 5.470 | Acc: 36.458,57.812,63.095,% | Adaptive Acc: 60.491% | clf_exit: 0.207 0.351 0.442
Batch: 40 | Loss: 5.444 | Acc: 37.005,57.298,63.529,% | Adaptive Acc: 61.147% | clf_exit: 0.198 0.359 0.443
Batch: 60 | Loss: 5.434 | Acc: 36.898,57.223,63.409,% | Adaptive Acc: 61.130% | clf_exit: 0.196 0.359 0.445
Train all parameters

Epoch: 56
Batch: 0 | Loss: 4.120 | Acc: 42.969,67.188,81.250,% | Adaptive Acc: 80.469% | clf_exit: 0.172 0.328 0.500
Batch: 20 | Loss: 4.224 | Acc: 39.249,66.741,80.357,% | Adaptive Acc: 78.237% | clf_exit: 0.157 0.362 0.481
Batch: 40 | Loss: 4.213 | Acc: 40.301,66.711,80.469,% | Adaptive Acc: 77.934% | clf_exit: 0.161 0.360 0.479
Batch: 60 | Loss: 4.231 | Acc: 40.394,66.406,80.392,% | Adaptive Acc: 77.959% | clf_exit: 0.156 0.364 0.480
Batch: 80 | Loss: 4.240 | Acc: 40.258,66.426,80.633,% | Adaptive Acc: 78.106% | clf_exit: 0.159 0.359 0.482
Batch: 100 | Loss: 4.266 | Acc: 40.169,66.476,80.314,% | Adaptive Acc: 77.723% | clf_exit: 0.158 0.359 0.483
Batch: 120 | Loss: 4.287 | Acc: 40.134,66.167,80.133,% | Adaptive Acc: 77.408% | clf_exit: 0.159 0.359 0.482
Batch: 140 | Loss: 4.304 | Acc: 40.243,66.118,79.815,% | Adaptive Acc: 77.155% | clf_exit: 0.158 0.357 0.484
Batch: 160 | Loss: 4.313 | Acc: 40.373,65.955,79.896,% | Adaptive Acc: 77.295% | clf_exit: 0.158 0.357 0.485
Batch: 180 | Loss: 4.315 | Acc: 40.396,65.996,79.860,% | Adaptive Acc: 77.219% | clf_exit: 0.158 0.358 0.484
Batch: 200 | Loss: 4.329 | Acc: 40.372,65.788,79.789,% | Adaptive Acc: 77.111% | clf_exit: 0.157 0.357 0.486
Batch: 220 | Loss: 4.333 | Acc: 40.487,65.770,79.726,% | Adaptive Acc: 77.064% | clf_exit: 0.157 0.358 0.485
Batch: 240 | Loss: 4.342 | Acc: 40.447,65.586,79.545,% | Adaptive Acc: 76.841% | clf_exit: 0.158 0.357 0.486
Batch: 260 | Loss: 4.347 | Acc: 40.418,65.406,79.439,% | Adaptive Acc: 76.652% | clf_exit: 0.159 0.355 0.487
Batch: 280 | Loss: 4.340 | Acc: 40.564,65.564,79.401,% | Adaptive Acc: 76.668% | clf_exit: 0.158 0.356 0.486
Batch: 300 | Loss: 4.351 | Acc: 40.425,65.482,79.259,% | Adaptive Acc: 76.495% | clf_exit: 0.158 0.356 0.487
Batch: 320 | Loss: 4.350 | Acc: 40.481,65.491,79.189,% | Adaptive Acc: 76.453% | clf_exit: 0.157 0.357 0.486
Batch: 340 | Loss: 4.357 | Acc: 40.375,65.403,79.108,% | Adaptive Acc: 76.359% | clf_exit: 0.157 0.356 0.487
Batch: 360 | Loss: 4.361 | Acc: 40.333,65.428,79.079,% | Adaptive Acc: 76.335% | clf_exit: 0.157 0.356 0.487
Batch: 380 | Loss: 4.370 | Acc: 40.287,65.397,78.931,% | Adaptive Acc: 76.220% | clf_exit: 0.157 0.356 0.487
Batch: 0 | Loss: 5.950 | Acc: 31.250,59.375,65.625,% | Adaptive Acc: 58.594% | clf_exit: 0.242 0.398 0.359
Batch: 20 | Loss: 6.060 | Acc: 31.920,54.092,61.049,% | Adaptive Acc: 56.548% | clf_exit: 0.244 0.365 0.392
Batch: 40 | Loss: 6.024 | Acc: 32.241,53.982,60.938,% | Adaptive Acc: 56.117% | clf_exit: 0.238 0.365 0.397
Batch: 60 | Loss: 6.015 | Acc: 32.121,53.868,61.283,% | Adaptive Acc: 56.199% | clf_exit: 0.238 0.364 0.398
Train all parameters

Epoch: 57
Batch: 0 | Loss: 4.122 | Acc: 48.438,70.312,78.125,% | Adaptive Acc: 73.438% | clf_exit: 0.227 0.312 0.461
Batch: 20 | Loss: 4.183 | Acc: 41.369,67.820,81.138,% | Adaptive Acc: 78.125% | clf_exit: 0.166 0.366 0.468
Batch: 40 | Loss: 4.215 | Acc: 40.568,66.959,81.079,% | Adaptive Acc: 78.163% | clf_exit: 0.163 0.360 0.477
Batch: 60 | Loss: 4.188 | Acc: 40.587,66.906,81.634,% | Adaptive Acc: 78.676% | clf_exit: 0.163 0.362 0.475
Batch: 80 | Loss: 4.193 | Acc: 40.760,66.686,81.578,% | Adaptive Acc: 78.559% | clf_exit: 0.164 0.363 0.473
Batch: 100 | Loss: 4.186 | Acc: 40.780,66.801,81.660,% | Adaptive Acc: 78.535% | clf_exit: 0.163 0.365 0.472
Batch: 120 | Loss: 4.222 | Acc: 40.573,66.548,81.379,% | Adaptive Acc: 78.273% | clf_exit: 0.161 0.365 0.474
Batch: 140 | Loss: 4.235 | Acc: 40.669,66.512,81.078,% | Adaptive Acc: 78.014% | clf_exit: 0.162 0.363 0.475
Batch: 160 | Loss: 4.238 | Acc: 40.644,66.547,80.983,% | Adaptive Acc: 77.955% | clf_exit: 0.161 0.365 0.474
Batch: 180 | Loss: 4.234 | Acc: 40.729,66.497,81.082,% | Adaptive Acc: 78.069% | clf_exit: 0.162 0.364 0.475
Batch: 200 | Loss: 4.240 | Acc: 40.644,66.418,81.176,% | Adaptive Acc: 78.106% | clf_exit: 0.161 0.364 0.475
Batch: 220 | Loss: 4.258 | Acc: 40.667,66.336,80.999,% | Adaptive Acc: 78.051% | clf_exit: 0.161 0.363 0.476
Batch: 240 | Loss: 4.274 | Acc: 40.589,66.111,80.654,% | Adaptive Acc: 77.749% | clf_exit: 0.160 0.362 0.478
Batch: 260 | Loss: 4.274 | Acc: 40.700,66.071,80.520,% | Adaptive Acc: 77.643% | clf_exit: 0.159 0.364 0.477
Batch: 280 | Loss: 4.283 | Acc: 40.628,66.000,80.360,% | Adaptive Acc: 77.491% | clf_exit: 0.159 0.364 0.477
Batch: 300 | Loss: 4.291 | Acc: 40.552,65.996,80.246,% | Adaptive Acc: 77.435% | clf_exit: 0.158 0.364 0.478
Batch: 320 | Loss: 4.295 | Acc: 40.598,65.941,80.169,% | Adaptive Acc: 77.361% | clf_exit: 0.159 0.363 0.479
Batch: 340 | Loss: 4.301 | Acc: 40.575,65.833,79.995,% | Adaptive Acc: 77.197% | clf_exit: 0.158 0.362 0.479
Batch: 360 | Loss: 4.301 | Acc: 40.575,65.785,79.943,% | Adaptive Acc: 77.151% | clf_exit: 0.159 0.362 0.479
Batch: 380 | Loss: 4.306 | Acc: 40.629,65.756,79.833,% | Adaptive Acc: 77.059% | clf_exit: 0.160 0.361 0.479
Batch: 0 | Loss: 5.052 | Acc: 41.406,60.156,67.969,% | Adaptive Acc: 64.062% | clf_exit: 0.258 0.406 0.336
Batch: 20 | Loss: 5.501 | Acc: 37.388,57.478,62.574,% | Adaptive Acc: 58.854% | clf_exit: 0.231 0.393 0.376
Batch: 40 | Loss: 5.479 | Acc: 38.091,56.936,62.386,% | Adaptive Acc: 59.489% | clf_exit: 0.230 0.385 0.385
Batch: 60 | Loss: 5.481 | Acc: 37.705,57.121,62.090,% | Adaptive Acc: 59.401% | clf_exit: 0.226 0.387 0.387
Train all parameters

Epoch: 58
Batch: 0 | Loss: 4.005 | Acc: 40.625,65.625,82.812,% | Adaptive Acc: 82.031% | clf_exit: 0.133 0.367 0.500
Batch: 20 | Loss: 4.098 | Acc: 40.848,68.341,83.519,% | Adaptive Acc: 80.394% | clf_exit: 0.161 0.367 0.472
Batch: 40 | Loss: 4.128 | Acc: 40.930,68.045,82.889,% | Adaptive Acc: 79.535% | clf_exit: 0.160 0.374 0.466
Batch: 60 | Loss: 4.150 | Acc: 40.894,67.738,82.531,% | Adaptive Acc: 79.265% | clf_exit: 0.162 0.371 0.468
Batch: 80 | Loss: 4.164 | Acc: 40.828,67.564,82.108,% | Adaptive Acc: 79.128% | clf_exit: 0.163 0.368 0.469
Batch: 100 | Loss: 4.160 | Acc: 40.911,67.512,82.039,% | Adaptive Acc: 79.038% | clf_exit: 0.163 0.370 0.467
Batch: 120 | Loss: 4.176 | Acc: 40.806,67.116,81.928,% | Adaptive Acc: 78.919% | clf_exit: 0.165 0.365 0.470
Batch: 140 | Loss: 4.176 | Acc: 40.969,66.938,81.904,% | Adaptive Acc: 78.879% | clf_exit: 0.166 0.364 0.470
Batch: 160 | Loss: 4.186 | Acc: 41.028,66.780,81.760,% | Adaptive Acc: 78.727% | clf_exit: 0.167 0.362 0.471
Batch: 180 | Loss: 4.189 | Acc: 41.083,66.816,81.634,% | Adaptive Acc: 78.634% | clf_exit: 0.166 0.363 0.471
Batch: 200 | Loss: 4.185 | Acc: 41.138,66.853,81.608,% | Adaptive Acc: 78.623% | clf_exit: 0.167 0.362 0.471
Batch: 220 | Loss: 4.197 | Acc: 41.180,66.756,81.437,% | Adaptive Acc: 78.447% | clf_exit: 0.167 0.361 0.473
Batch: 240 | Loss: 4.197 | Acc: 41.170,66.747,81.435,% | Adaptive Acc: 78.414% | clf_exit: 0.167 0.359 0.473
Batch: 260 | Loss: 4.211 | Acc: 41.086,66.667,81.364,% | Adaptive Acc: 78.388% | clf_exit: 0.166 0.360 0.474
Batch: 280 | Loss: 4.216 | Acc: 41.175,66.701,81.269,% | Adaptive Acc: 78.345% | clf_exit: 0.166 0.359 0.475
Batch: 300 | Loss: 4.220 | Acc: 41.160,66.744,81.208,% | Adaptive Acc: 78.270% | clf_exit: 0.166 0.359 0.475
Batch: 320 | Loss: 4.228 | Acc: 41.075,66.642,81.048,% | Adaptive Acc: 78.186% | clf_exit: 0.164 0.359 0.477
Batch: 340 | Loss: 4.238 | Acc: 41.042,66.606,80.927,% | Adaptive Acc: 78.068% | clf_exit: 0.163 0.359 0.477
Batch: 360 | Loss: 4.245 | Acc: 40.991,66.532,80.793,% | Adaptive Acc: 77.941% | clf_exit: 0.163 0.360 0.477
Batch: 380 | Loss: 4.252 | Acc: 40.957,66.503,80.678,% | Adaptive Acc: 77.865% | clf_exit: 0.163 0.359 0.478
Batch: 0 | Loss: 4.943 | Acc: 39.062,66.406,72.656,% | Adaptive Acc: 67.188% | clf_exit: 0.281 0.328 0.391
Batch: 20 | Loss: 5.244 | Acc: 37.277,58.891,65.699,% | Adaptive Acc: 61.421% | clf_exit: 0.232 0.377 0.391
Batch: 40 | Loss: 5.216 | Acc: 37.271,58.422,65.587,% | Adaptive Acc: 61.890% | clf_exit: 0.225 0.369 0.406
Batch: 60 | Loss: 5.215 | Acc: 37.526,58.504,65.574,% | Adaptive Acc: 61.898% | clf_exit: 0.225 0.369 0.405
Train all parameters

Epoch: 59
Batch: 0 | Loss: 4.006 | Acc: 47.656,69.531,81.250,% | Adaptive Acc: 78.906% | clf_exit: 0.219 0.352 0.430
Batch: 20 | Loss: 3.951 | Acc: 42.336,69.010,85.156,% | Adaptive Acc: 81.324% | clf_exit: 0.171 0.382 0.447
Batch: 40 | Loss: 3.981 | Acc: 42.245,69.188,84.870,% | Adaptive Acc: 81.421% | clf_exit: 0.165 0.380 0.454
Batch: 60 | Loss: 4.045 | Acc: 41.457,68.161,84.068,% | Adaptive Acc: 80.520% | clf_exit: 0.165 0.377 0.458
Batch: 80 | Loss: 4.058 | Acc: 41.426,68.364,83.980,% | Adaptive Acc: 80.546% | clf_exit: 0.163 0.375 0.462
Batch: 100 | Loss: 4.070 | Acc: 41.414,67.946,83.880,% | Adaptive Acc: 80.422% | clf_exit: 0.162 0.375 0.463
Batch: 120 | Loss: 4.078 | Acc: 41.471,67.833,83.775,% | Adaptive Acc: 80.327% | clf_exit: 0.163 0.372 0.465
Batch: 140 | Loss: 4.092 | Acc: 41.412,67.803,83.466,% | Adaptive Acc: 80.086% | clf_exit: 0.161 0.372 0.466
Batch: 160 | Loss: 4.105 | Acc: 41.426,67.615,83.385,% | Adaptive Acc: 79.964% | clf_exit: 0.162 0.371 0.467
Batch: 180 | Loss: 4.116 | Acc: 41.307,67.516,83.235,% | Adaptive Acc: 79.882% | clf_exit: 0.162 0.369 0.470
Batch: 200 | Loss: 4.136 | Acc: 41.239,67.199,82.894,% | Adaptive Acc: 79.559% | clf_exit: 0.162 0.367 0.471
Batch: 220 | Loss: 4.133 | Acc: 41.389,67.163,82.890,% | Adaptive Acc: 79.571% | clf_exit: 0.163 0.367 0.469
Batch: 240 | Loss: 4.144 | Acc: 41.296,67.025,82.709,% | Adaptive Acc: 79.418% | clf_exit: 0.163 0.368 0.470
Batch: 260 | Loss: 4.154 | Acc: 41.251,66.972,82.549,% | Adaptive Acc: 79.283% | clf_exit: 0.163 0.367 0.470
Batch: 280 | Loss: 4.160 | Acc: 41.317,66.934,82.393,% | Adaptive Acc: 79.145% | clf_exit: 0.162 0.368 0.470
Batch: 300 | Loss: 4.165 | Acc: 41.326,66.905,82.353,% | Adaptive Acc: 79.096% | clf_exit: 0.163 0.368 0.470
Batch: 320 | Loss: 4.164 | Acc: 41.379,66.961,82.353,% | Adaptive Acc: 79.103% | clf_exit: 0.164 0.368 0.469
Batch: 340 | Loss: 4.171 | Acc: 41.386,66.903,82.189,% | Adaptive Acc: 78.968% | clf_exit: 0.164 0.368 0.469
Batch: 360 | Loss: 4.173 | Acc: 41.400,66.973,82.111,% | Adaptive Acc: 78.917% | clf_exit: 0.164 0.367 0.469
Batch: 380 | Loss: 4.179 | Acc: 41.287,66.909,82.007,% | Adaptive Acc: 78.816% | clf_exit: 0.164 0.368 0.469
Batch: 0 | Loss: 5.324 | Acc: 37.500,58.594,65.625,% | Adaptive Acc: 63.281% | clf_exit: 0.258 0.414 0.328
Batch: 20 | Loss: 5.447 | Acc: 36.458,58.482,64.472,% | Adaptive Acc: 59.598% | clf_exit: 0.274 0.374 0.352
Batch: 40 | Loss: 5.472 | Acc: 36.585,57.889,63.777,% | Adaptive Acc: 59.299% | clf_exit: 0.271 0.363 0.366
Batch: 60 | Loss: 5.487 | Acc: 36.347,57.800,63.345,% | Adaptive Acc: 58.914% | clf_exit: 0.271 0.359 0.370
Train all parameters

Epoch: 60
Batch: 0 | Loss: 4.337 | Acc: 37.500,64.844,82.031,% | Adaptive Acc: 80.469% | clf_exit: 0.125 0.352 0.523
Batch: 20 | Loss: 4.089 | Acc: 41.927,66.704,83.594,% | Adaptive Acc: 79.799% | clf_exit: 0.164 0.360 0.476
Batch: 40 | Loss: 4.071 | Acc: 41.311,68.007,84.108,% | Adaptive Acc: 80.259% | clf_exit: 0.164 0.370 0.466
Batch: 60 | Loss: 4.074 | Acc: 41.048,67.789,84.183,% | Adaptive Acc: 80.533% | clf_exit: 0.164 0.372 0.464
Batch: 80 | Loss: 4.053 | Acc: 41.763,68.036,84.336,% | Adaptive Acc: 80.797% | clf_exit: 0.166 0.372 0.462
Batch: 100 | Loss: 4.039 | Acc: 41.979,68.216,84.213,% | Adaptive Acc: 80.832% | clf_exit: 0.170 0.371 0.459
Batch: 120 | Loss: 4.052 | Acc: 42.007,68.046,84.097,% | Adaptive Acc: 80.695% | clf_exit: 0.168 0.372 0.460
Batch: 140 | Loss: 4.076 | Acc: 41.872,67.719,83.743,% | Adaptive Acc: 80.308% | clf_exit: 0.170 0.370 0.460
Batch: 160 | Loss: 4.105 | Acc: 41.474,67.493,83.545,% | Adaptive Acc: 80.115% | clf_exit: 0.168 0.369 0.463
Batch: 180 | Loss: 4.106 | Acc: 41.497,67.472,83.365,% | Adaptive Acc: 79.955% | clf_exit: 0.167 0.369 0.464
Batch: 200 | Loss: 4.117 | Acc: 41.375,67.401,83.178,% | Adaptive Acc: 79.789% | clf_exit: 0.166 0.368 0.466
Batch: 220 | Loss: 4.122 | Acc: 41.336,67.361,83.085,% | Adaptive Acc: 79.726% | clf_exit: 0.166 0.368 0.466
Batch: 240 | Loss: 4.125 | Acc: 41.367,67.395,82.932,% | Adaptive Acc: 79.516% | clf_exit: 0.167 0.367 0.466
Batch: 260 | Loss: 4.135 | Acc: 41.406,67.319,82.597,% | Adaptive Acc: 79.236% | clf_exit: 0.167 0.367 0.466
Batch: 280 | Loss: 4.142 | Acc: 41.345,67.271,82.412,% | Adaptive Acc: 79.045% | clf_exit: 0.167 0.367 0.466
Batch: 300 | Loss: 4.146 | Acc: 41.334,67.180,82.325,% | Adaptive Acc: 78.979% | clf_exit: 0.166 0.367 0.467
Batch: 320 | Loss: 4.153 | Acc: 41.275,67.151,82.241,% | Adaptive Acc: 78.904% | clf_exit: 0.165 0.367 0.468
Batch: 340 | Loss: 4.155 | Acc: 41.360,67.178,82.155,% | Adaptive Acc: 78.883% | clf_exit: 0.165 0.366 0.468
Batch: 360 | Loss: 4.164 | Acc: 41.346,67.103,82.075,% | Adaptive Acc: 78.770% | clf_exit: 0.165 0.367 0.468
Batch: 380 | Loss: 4.165 | Acc: 41.322,67.163,82.085,% | Adaptive Acc: 78.773% | clf_exit: 0.166 0.366 0.468
Batch: 0 | Loss: 5.061 | Acc: 39.844,60.938,67.969,% | Adaptive Acc: 61.719% | clf_exit: 0.250 0.414 0.336
Batch: 20 | Loss: 5.263 | Acc: 38.058,59.115,65.216,% | Adaptive Acc: 61.310% | clf_exit: 0.238 0.377 0.385
Batch: 40 | Loss: 5.204 | Acc: 38.853,58.899,64.920,% | Adaptive Acc: 61.833% | clf_exit: 0.236 0.372 0.392
Batch: 60 | Loss: 5.187 | Acc: 38.755,59.426,65.177,% | Adaptive Acc: 62.141% | clf_exit: 0.234 0.372 0.394
Train all parameters

Epoch: 61
Batch: 0 | Loss: 3.432 | Acc: 45.312,80.469,87.500,% | Adaptive Acc: 86.719% | clf_exit: 0.211 0.430 0.359
Batch: 20 | Loss: 3.931 | Acc: 44.568,70.536,84.747,% | Adaptive Acc: 82.329% | clf_exit: 0.170 0.391 0.440
Batch: 40 | Loss: 3.972 | Acc: 43.026,69.455,84.470,% | Adaptive Acc: 81.269% | clf_exit: 0.171 0.390 0.438
Batch: 60 | Loss: 3.989 | Acc: 42.572,69.109,84.708,% | Adaptive Acc: 81.352% | clf_exit: 0.169 0.386 0.445
Batch: 80 | Loss: 4.013 | Acc: 42.052,68.596,84.423,% | Adaptive Acc: 81.096% | clf_exit: 0.168 0.385 0.447
Batch: 100 | Loss: 4.012 | Acc: 42.226,68.495,84.483,% | Adaptive Acc: 81.204% | clf_exit: 0.168 0.379 0.453
Batch: 120 | Loss: 4.007 | Acc: 42.349,68.530,84.427,% | Adaptive Acc: 81.153% | clf_exit: 0.169 0.376 0.455
Batch: 140 | Loss: 4.020 | Acc: 42.237,68.357,84.320,% | Adaptive Acc: 81.001% | clf_exit: 0.168 0.377 0.455
Batch: 160 | Loss: 4.027 | Acc: 42.202,68.202,84.312,% | Adaptive Acc: 81.022% | clf_exit: 0.168 0.375 0.457
Batch: 180 | Loss: 4.043 | Acc: 42.127,68.042,84.099,% | Adaptive Acc: 80.814% | clf_exit: 0.167 0.375 0.458
Batch: 200 | Loss: 4.041 | Acc: 42.040,68.066,84.068,% | Adaptive Acc: 80.784% | clf_exit: 0.167 0.375 0.458
Batch: 220 | Loss: 4.044 | Acc: 42.106,68.075,83.954,% | Adaptive Acc: 80.713% | clf_exit: 0.168 0.375 0.457
Batch: 240 | Loss: 4.063 | Acc: 41.964,67.839,83.704,% | Adaptive Acc: 80.524% | clf_exit: 0.167 0.374 0.459
Batch: 260 | Loss: 4.078 | Acc: 41.876,67.828,83.504,% | Adaptive Acc: 80.304% | clf_exit: 0.168 0.372 0.460
Batch: 280 | Loss: 4.084 | Acc: 41.857,67.821,83.363,% | Adaptive Acc: 80.082% | clf_exit: 0.168 0.372 0.460
Batch: 300 | Loss: 4.086 | Acc: 41.907,67.842,83.376,% | Adaptive Acc: 80.085% | clf_exit: 0.168 0.371 0.461
Batch: 320 | Loss: 4.092 | Acc: 41.793,67.757,83.304,% | Adaptive Acc: 80.001% | clf_exit: 0.168 0.371 0.461
Batch: 340 | Loss: 4.098 | Acc: 41.798,67.698,83.220,% | Adaptive Acc: 79.935% | clf_exit: 0.168 0.371 0.461
Batch: 360 | Loss: 4.100 | Acc: 41.826,67.692,83.183,% | Adaptive Acc: 79.906% | clf_exit: 0.168 0.370 0.462
Batch: 380 | Loss: 4.101 | Acc: 41.872,67.626,83.075,% | Adaptive Acc: 79.808% | clf_exit: 0.168 0.370 0.461
Batch: 0 | Loss: 4.732 | Acc: 40.625,63.281,68.750,% | Adaptive Acc: 65.625% | clf_exit: 0.250 0.359 0.391
Batch: 20 | Loss: 5.305 | Acc: 38.318,59.189,63.728,% | Adaptive Acc: 60.305% | clf_exit: 0.235 0.355 0.409
Batch: 40 | Loss: 5.294 | Acc: 38.700,58.689,63.624,% | Adaptive Acc: 60.899% | clf_exit: 0.225 0.356 0.419
Batch: 60 | Loss: 5.286 | Acc: 38.230,59.016,64.075,% | Adaptive Acc: 61.002% | clf_exit: 0.222 0.364 0.413
Train all parameters

Epoch: 62
Batch: 0 | Loss: 3.712 | Acc: 47.656,77.344,88.281,% | Adaptive Acc: 86.719% | clf_exit: 0.148 0.398 0.453
Batch: 20 | Loss: 3.976 | Acc: 41.853,69.382,85.751,% | Adaptive Acc: 82.068% | clf_exit: 0.158 0.388 0.455
Batch: 40 | Loss: 4.004 | Acc: 41.635,68.712,85.213,% | Adaptive Acc: 81.402% | clf_exit: 0.158 0.391 0.452
Batch: 60 | Loss: 3.988 | Acc: 41.240,68.942,85.476,% | Adaptive Acc: 81.673% | clf_exit: 0.161 0.386 0.453
Batch: 80 | Loss: 4.020 | Acc: 41.242,68.538,85.176,% | Adaptive Acc: 81.520% | clf_exit: 0.161 0.383 0.456
Batch: 100 | Loss: 3.999 | Acc: 41.259,68.564,85.280,% | Adaptive Acc: 81.614% | clf_exit: 0.164 0.380 0.457
Batch: 120 | Loss: 3.994 | Acc: 41.690,68.530,85.201,% | Adaptive Acc: 81.618% | clf_exit: 0.166 0.378 0.456
Batch: 140 | Loss: 3.981 | Acc: 41.777,68.595,85.234,% | Adaptive Acc: 81.754% | clf_exit: 0.167 0.377 0.456
Batch: 160 | Loss: 3.994 | Acc: 41.843,68.464,85.069,% | Adaptive Acc: 81.701% | clf_exit: 0.166 0.376 0.458
Batch: 180 | Loss: 4.007 | Acc: 41.834,68.305,84.932,% | Adaptive Acc: 81.457% | clf_exit: 0.167 0.374 0.459
Batch: 200 | Loss: 4.013 | Acc: 41.842,68.190,84.748,% | Adaptive Acc: 81.336% | clf_exit: 0.167 0.375 0.458
Batch: 220 | Loss: 4.033 | Acc: 41.654,68.039,84.601,% | Adaptive Acc: 81.144% | clf_exit: 0.167 0.374 0.459
Batch: 240 | Loss: 4.037 | Acc: 41.685,68.050,84.394,% | Adaptive Acc: 80.961% | clf_exit: 0.168 0.373 0.459
Batch: 260 | Loss: 4.051 | Acc: 41.700,67.825,84.130,% | Adaptive Acc: 80.672% | clf_exit: 0.168 0.372 0.459
Batch: 280 | Loss: 4.056 | Acc: 41.651,67.810,84.022,% | Adaptive Acc: 80.572% | clf_exit: 0.168 0.372 0.460
Batch: 300 | Loss: 4.060 | Acc: 41.642,67.795,83.913,% | Adaptive Acc: 80.469% | clf_exit: 0.168 0.373 0.460
Batch: 320 | Loss: 4.065 | Acc: 41.574,67.759,83.823,% | Adaptive Acc: 80.364% | clf_exit: 0.168 0.373 0.459
Batch: 340 | Loss: 4.066 | Acc: 41.608,67.708,83.717,% | Adaptive Acc: 80.304% | clf_exit: 0.168 0.372 0.459
Batch: 360 | Loss: 4.073 | Acc: 41.679,67.622,83.570,% | Adaptive Acc: 80.170% | clf_exit: 0.169 0.372 0.459
Batch: 380 | Loss: 4.075 | Acc: 41.738,67.600,83.555,% | Adaptive Acc: 80.145% | clf_exit: 0.169 0.373 0.459
Batch: 0 | Loss: 4.854 | Acc: 42.969,55.469,72.656,% | Adaptive Acc: 63.281% | clf_exit: 0.219 0.453 0.328
Batch: 20 | Loss: 5.165 | Acc: 39.062,59.784,66.332,% | Adaptive Acc: 62.314% | clf_exit: 0.234 0.369 0.397
Batch: 40 | Loss: 5.152 | Acc: 39.310,59.299,65.720,% | Adaptive Acc: 62.195% | clf_exit: 0.234 0.363 0.403
Batch: 60 | Loss: 5.175 | Acc: 39.101,59.631,65.279,% | Adaptive Acc: 62.129% | clf_exit: 0.235 0.364 0.400
Train all parameters

Epoch: 63
Batch: 0 | Loss: 4.373 | Acc: 42.188,62.500,82.031,% | Adaptive Acc: 82.031% | clf_exit: 0.141 0.336 0.523
Batch: 20 | Loss: 3.864 | Acc: 42.560,70.387,86.384,% | Adaptive Acc: 82.626% | clf_exit: 0.171 0.377 0.452
Batch: 40 | Loss: 3.912 | Acc: 42.111,69.207,86.357,% | Adaptive Acc: 82.641% | clf_exit: 0.171 0.373 0.456
Batch: 60 | Loss: 3.899 | Acc: 42.367,69.147,86.335,% | Adaptive Acc: 82.595% | clf_exit: 0.175 0.372 0.453
Batch: 80 | Loss: 3.916 | Acc: 42.265,68.953,85.918,% | Adaptive Acc: 82.166% | clf_exit: 0.173 0.372 0.455
Batch: 100 | Loss: 3.936 | Acc: 42.280,68.773,85.860,% | Adaptive Acc: 82.039% | clf_exit: 0.171 0.376 0.453
Batch: 120 | Loss: 3.951 | Acc: 42.194,68.705,85.744,% | Adaptive Acc: 81.934% | clf_exit: 0.173 0.375 0.452
Batch: 140 | Loss: 3.970 | Acc: 41.971,68.512,85.577,% | Adaptive Acc: 81.832% | clf_exit: 0.173 0.373 0.454
Batch: 160 | Loss: 3.978 | Acc: 41.935,68.401,85.506,% | Adaptive Acc: 81.857% | clf_exit: 0.173 0.372 0.455
Batch: 180 | Loss: 3.997 | Acc: 41.773,68.185,85.212,% | Adaptive Acc: 81.617% | clf_exit: 0.172 0.372 0.457
Batch: 200 | Loss: 4.006 | Acc: 41.772,68.140,84.970,% | Adaptive Acc: 81.472% | clf_exit: 0.172 0.370 0.458
Batch: 220 | Loss: 4.016 | Acc: 41.707,68.085,84.881,% | Adaptive Acc: 81.398% | clf_exit: 0.171 0.371 0.459
Batch: 240 | Loss: 4.015 | Acc: 41.753,68.063,84.813,% | Adaptive Acc: 81.341% | clf_exit: 0.171 0.371 0.458
Batch: 260 | Loss: 4.028 | Acc: 41.652,67.900,84.570,% | Adaptive Acc: 81.172% | clf_exit: 0.169 0.371 0.460
Batch: 280 | Loss: 4.033 | Acc: 41.629,67.835,84.383,% | Adaptive Acc: 80.991% | clf_exit: 0.169 0.371 0.460
Batch: 300 | Loss: 4.034 | Acc: 41.663,67.826,84.313,% | Adaptive Acc: 80.894% | clf_exit: 0.170 0.370 0.460
Batch: 320 | Loss: 4.036 | Acc: 41.667,67.854,84.222,% | Adaptive Acc: 80.802% | clf_exit: 0.170 0.371 0.460
Batch: 340 | Loss: 4.034 | Acc: 41.709,67.895,84.185,% | Adaptive Acc: 80.760% | clf_exit: 0.170 0.371 0.460
Batch: 360 | Loss: 4.035 | Acc: 41.785,67.865,84.057,% | Adaptive Acc: 80.631% | clf_exit: 0.169 0.372 0.459
Batch: 380 | Loss: 4.037 | Acc: 41.845,67.879,83.990,% | Adaptive Acc: 80.575% | clf_exit: 0.170 0.372 0.458
Batch: 0 | Loss: 5.265 | Acc: 35.938,60.156,69.531,% | Adaptive Acc: 65.625% | clf_exit: 0.234 0.422 0.344
Batch: 20 | Loss: 5.572 | Acc: 34.933,57.887,63.132,% | Adaptive Acc: 58.780% | clf_exit: 0.239 0.374 0.387
Batch: 40 | Loss: 5.641 | Acc: 35.023,56.212,62.881,% | Adaptive Acc: 58.784% | clf_exit: 0.230 0.375 0.396
Batch: 60 | Loss: 5.634 | Acc: 35.336,56.237,63.012,% | Adaptive Acc: 59.209% | clf_exit: 0.226 0.379 0.395
Train all parameters

Epoch: 64
Batch: 0 | Loss: 3.871 | Acc: 39.844,68.750,85.938,% | Adaptive Acc: 82.812% | clf_exit: 0.172 0.359 0.469
Batch: 20 | Loss: 3.889 | Acc: 42.262,70.126,85.417,% | Adaptive Acc: 82.403% | clf_exit: 0.170 0.382 0.448
Batch: 40 | Loss: 3.868 | Acc: 42.530,69.722,86.280,% | Adaptive Acc: 82.717% | clf_exit: 0.171 0.383 0.446
Batch: 60 | Loss: 3.869 | Acc: 42.444,69.557,86.411,% | Adaptive Acc: 82.672% | clf_exit: 0.173 0.383 0.444
Batch: 80 | Loss: 3.905 | Acc: 42.236,69.213,86.352,% | Adaptive Acc: 82.552% | clf_exit: 0.177 0.374 0.449
Batch: 100 | Loss: 3.895 | Acc: 42.334,69.152,86.510,% | Adaptive Acc: 82.689% | clf_exit: 0.177 0.375 0.448
Batch: 120 | Loss: 3.910 | Acc: 42.317,68.911,86.409,% | Adaptive Acc: 82.515% | clf_exit: 0.178 0.374 0.449
Batch: 140 | Loss: 3.923 | Acc: 42.104,68.833,86.270,% | Adaptive Acc: 82.447% | clf_exit: 0.176 0.373 0.451
Batch: 160 | Loss: 3.940 | Acc: 42.061,68.692,85.981,% | Adaptive Acc: 82.119% | clf_exit: 0.175 0.373 0.452
Batch: 180 | Loss: 3.955 | Acc: 42.062,68.551,85.666,% | Adaptive Acc: 81.854% | clf_exit: 0.174 0.375 0.451
Batch: 200 | Loss: 3.961 | Acc: 41.950,68.521,85.560,% | Adaptive Acc: 81.771% | clf_exit: 0.173 0.375 0.452
Batch: 220 | Loss: 3.967 | Acc: 41.979,68.464,85.443,% | Adaptive Acc: 81.642% | clf_exit: 0.174 0.374 0.452
Batch: 240 | Loss: 3.977 | Acc: 41.753,68.345,85.377,% | Adaptive Acc: 81.581% | clf_exit: 0.173 0.374 0.453
Batch: 260 | Loss: 3.979 | Acc: 41.771,68.241,85.228,% | Adaptive Acc: 81.457% | clf_exit: 0.172 0.374 0.453
Batch: 280 | Loss: 3.984 | Acc: 41.734,68.283,85.151,% | Adaptive Acc: 81.364% | clf_exit: 0.172 0.375 0.453
Batch: 300 | Loss: 3.987 | Acc: 41.710,68.226,85.071,% | Adaptive Acc: 81.354% | clf_exit: 0.172 0.375 0.453
Batch: 320 | Loss: 3.988 | Acc: 41.730,68.271,84.969,% | Adaptive Acc: 81.265% | clf_exit: 0.172 0.375 0.453
Batch: 340 | Loss: 3.989 | Acc: 41.766,68.285,84.854,% | Adaptive Acc: 81.145% | clf_exit: 0.172 0.375 0.452
Batch: 360 | Loss: 3.993 | Acc: 41.802,68.241,84.756,% | Adaptive Acc: 81.103% | clf_exit: 0.173 0.375 0.452
Batch: 380 | Loss: 4.005 | Acc: 41.788,68.149,84.576,% | Adaptive Acc: 80.916% | clf_exit: 0.173 0.374 0.453
Batch: 0 | Loss: 4.880 | Acc: 39.844,64.844,71.094,% | Adaptive Acc: 69.531% | clf_exit: 0.227 0.445 0.328
Batch: 20 | Loss: 5.319 | Acc: 37.835,60.045,64.918,% | Adaptive Acc: 61.719% | clf_exit: 0.245 0.386 0.369
Batch: 40 | Loss: 5.285 | Acc: 37.862,59.585,64.672,% | Adaptive Acc: 61.814% | clf_exit: 0.245 0.380 0.375
Batch: 60 | Loss: 5.291 | Acc: 37.500,59.183,64.588,% | Adaptive Acc: 61.847% | clf_exit: 0.243 0.379 0.378
Train all parameters

Epoch: 65
Batch: 0 | Loss: 3.908 | Acc: 47.656,73.438,87.500,% | Adaptive Acc: 84.375% | clf_exit: 0.180 0.398 0.422
Batch: 20 | Loss: 3.963 | Acc: 41.815,68.638,86.235,% | Adaptive Acc: 82.068% | clf_exit: 0.177 0.378 0.445
Batch: 40 | Loss: 3.968 | Acc: 41.597,68.960,86.433,% | Adaptive Acc: 82.470% | clf_exit: 0.170 0.384 0.446
Batch: 60 | Loss: 3.900 | Acc: 42.239,69.621,86.629,% | Adaptive Acc: 82.838% | clf_exit: 0.171 0.385 0.444
Batch: 80 | Loss: 3.872 | Acc: 42.515,69.753,86.651,% | Adaptive Acc: 82.938% | clf_exit: 0.178 0.384 0.438
Batch: 100 | Loss: 3.882 | Acc: 42.365,69.384,86.363,% | Adaptive Acc: 82.735% | clf_exit: 0.177 0.381 0.442
Batch: 120 | Loss: 3.896 | Acc: 42.291,69.415,86.125,% | Adaptive Acc: 82.587% | clf_exit: 0.177 0.379 0.444
Batch: 140 | Loss: 3.906 | Acc: 42.199,69.171,85.998,% | Adaptive Acc: 82.441% | clf_exit: 0.177 0.378 0.446
Batch: 160 | Loss: 3.917 | Acc: 42.158,69.124,85.802,% | Adaptive Acc: 82.284% | clf_exit: 0.177 0.378 0.445
Batch: 180 | Loss: 3.938 | Acc: 42.123,68.884,85.540,% | Adaptive Acc: 81.992% | clf_exit: 0.177 0.378 0.445
Batch: 200 | Loss: 3.942 | Acc: 42.153,68.948,85.366,% | Adaptive Acc: 81.841% | clf_exit: 0.177 0.378 0.445
Batch: 220 | Loss: 3.954 | Acc: 42.011,68.916,85.153,% | Adaptive Acc: 81.600% | clf_exit: 0.176 0.378 0.446
Batch: 240 | Loss: 3.968 | Acc: 41.999,68.792,84.965,% | Adaptive Acc: 81.428% | clf_exit: 0.175 0.378 0.447
Batch: 260 | Loss: 3.992 | Acc: 41.813,68.594,84.809,% | Adaptive Acc: 81.253% | clf_exit: 0.174 0.376 0.450
Batch: 280 | Loss: 3.997 | Acc: 41.862,68.469,84.761,% | Adaptive Acc: 81.189% | clf_exit: 0.174 0.375 0.451
Batch: 300 | Loss: 3.994 | Acc: 41.980,68.524,84.720,% | Adaptive Acc: 81.159% | clf_exit: 0.175 0.375 0.450
Batch: 320 | Loss: 3.999 | Acc: 41.968,68.524,84.589,% | Adaptive Acc: 81.089% | clf_exit: 0.174 0.375 0.451
Batch: 340 | Loss: 4.004 | Acc: 41.972,68.477,84.462,% | Adaptive Acc: 81.007% | clf_exit: 0.174 0.374 0.452
Batch: 360 | Loss: 3.997 | Acc: 42.049,68.573,84.455,% | Adaptive Acc: 81.005% | clf_exit: 0.174 0.376 0.451
Batch: 380 | Loss: 3.999 | Acc: 42.030,68.592,84.404,% | Adaptive Acc: 80.983% | clf_exit: 0.174 0.375 0.451
Batch: 0 | Loss: 5.093 | Acc: 36.719,62.500,70.312,% | Adaptive Acc: 64.844% | clf_exit: 0.195 0.469 0.336
Batch: 20 | Loss: 5.242 | Acc: 37.054,60.417,66.034,% | Adaptive Acc: 62.984% | clf_exit: 0.213 0.397 0.390
Batch: 40 | Loss: 5.271 | Acc: 37.386,59.242,65.282,% | Adaptive Acc: 62.538% | clf_exit: 0.213 0.395 0.392
Batch: 60 | Loss: 5.287 | Acc: 37.244,59.093,65.061,% | Adaptive Acc: 62.359% | clf_exit: 0.212 0.398 0.390
Train all parameters

Epoch: 66
Batch: 0 | Loss: 3.421 | Acc: 47.656,76.562,89.844,% | Adaptive Acc: 86.719% | clf_exit: 0.133 0.430 0.438
Batch: 20 | Loss: 3.905 | Acc: 42.671,68.564,86.644,% | Adaptive Acc: 83.185% | clf_exit: 0.177 0.375 0.448
Batch: 40 | Loss: 3.851 | Acc: 42.873,69.207,87.367,% | Adaptive Acc: 83.384% | clf_exit: 0.176 0.383 0.441
Batch: 60 | Loss: 3.838 | Acc: 43.199,69.237,87.308,% | Adaptive Acc: 83.402% | clf_exit: 0.175 0.386 0.439
Batch: 80 | Loss: 3.849 | Acc: 42.911,69.126,87.133,% | Adaptive Acc: 83.025% | clf_exit: 0.175 0.385 0.440
Batch: 100 | Loss: 3.846 | Acc: 42.860,68.928,87.237,% | Adaptive Acc: 83.029% | clf_exit: 0.176 0.383 0.440
Batch: 120 | Loss: 3.846 | Acc: 42.878,69.099,87.332,% | Adaptive Acc: 83.271% | clf_exit: 0.177 0.382 0.441
Batch: 140 | Loss: 3.843 | Acc: 42.875,69.099,87.245,% | Adaptive Acc: 83.167% | clf_exit: 0.178 0.383 0.440
Batch: 160 | Loss: 3.863 | Acc: 42.702,68.988,86.966,% | Adaptive Acc: 82.977% | clf_exit: 0.176 0.383 0.441
Batch: 180 | Loss: 3.880 | Acc: 42.511,68.858,86.758,% | Adaptive Acc: 82.769% | clf_exit: 0.176 0.382 0.442
Batch: 200 | Loss: 3.886 | Acc: 42.557,68.820,86.528,% | Adaptive Acc: 82.653% | clf_exit: 0.176 0.381 0.443
Batch: 220 | Loss: 3.901 | Acc: 42.414,68.616,86.252,% | Adaptive Acc: 82.293% | clf_exit: 0.176 0.380 0.444
Batch: 240 | Loss: 3.913 | Acc: 42.366,68.549,86.190,% | Adaptive Acc: 82.206% | clf_exit: 0.177 0.379 0.444
Batch: 260 | Loss: 3.912 | Acc: 42.544,68.645,86.108,% | Adaptive Acc: 82.196% | clf_exit: 0.177 0.379 0.444
Batch: 280 | Loss: 3.906 | Acc: 42.596,68.806,86.065,% | Adaptive Acc: 82.201% | clf_exit: 0.177 0.379 0.444
Batch: 300 | Loss: 3.912 | Acc: 42.548,68.747,85.930,% | Adaptive Acc: 82.073% | clf_exit: 0.177 0.379 0.444
Batch: 320 | Loss: 3.924 | Acc: 42.543,68.640,85.799,% | Adaptive Acc: 81.953% | clf_exit: 0.177 0.378 0.445
Batch: 340 | Loss: 3.929 | Acc: 42.554,68.594,85.768,% | Adaptive Acc: 81.930% | clf_exit: 0.177 0.377 0.446
Batch: 360 | Loss: 3.936 | Acc: 42.540,68.551,85.593,% | Adaptive Acc: 81.806% | clf_exit: 0.176 0.378 0.446
Batch: 380 | Loss: 3.943 | Acc: 42.530,68.465,85.478,% | Adaptive Acc: 81.707% | clf_exit: 0.176 0.377 0.447
Batch: 0 | Loss: 5.033 | Acc: 37.500,60.938,71.875,% | Adaptive Acc: 67.969% | clf_exit: 0.219 0.391 0.391
Batch: 20 | Loss: 5.276 | Acc: 38.579,59.189,65.476,% | Adaptive Acc: 62.909% | clf_exit: 0.182 0.416 0.402
Batch: 40 | Loss: 5.264 | Acc: 38.872,59.089,64.253,% | Adaptive Acc: 62.329% | clf_exit: 0.185 0.407 0.408
Batch: 60 | Loss: 5.253 | Acc: 38.409,59.183,64.319,% | Adaptive Acc: 62.167% | clf_exit: 0.184 0.406 0.411
Train all parameters

Epoch: 67
Batch: 0 | Loss: 3.858 | Acc: 42.188,69.531,91.406,% | Adaptive Acc: 85.938% | clf_exit: 0.164 0.414 0.422
Batch: 20 | Loss: 3.831 | Acc: 43.341,69.531,87.984,% | Adaptive Acc: 83.817% | clf_exit: 0.174 0.386 0.440
Batch: 40 | Loss: 3.838 | Acc: 43.236,69.322,87.957,% | Adaptive Acc: 83.937% | clf_exit: 0.174 0.388 0.438
Batch: 60 | Loss: 3.828 | Acc: 42.982,69.685,87.756,% | Adaptive Acc: 83.619% | clf_exit: 0.175 0.388 0.437
Batch: 80 | Loss: 3.835 | Acc: 42.747,69.888,87.876,% | Adaptive Acc: 83.681% | clf_exit: 0.173 0.389 0.438
Batch: 100 | Loss: 3.833 | Acc: 42.675,69.910,87.864,% | Adaptive Acc: 83.609% | clf_exit: 0.175 0.387 0.438
Batch: 120 | Loss: 3.850 | Acc: 42.575,69.641,87.649,% | Adaptive Acc: 83.335% | clf_exit: 0.176 0.383 0.441
Batch: 140 | Loss: 3.866 | Acc: 42.542,69.481,87.428,% | Adaptive Acc: 83.078% | clf_exit: 0.175 0.379 0.445
Batch: 160 | Loss: 3.869 | Acc: 42.629,69.439,87.282,% | Adaptive Acc: 82.934% | clf_exit: 0.176 0.379 0.445
Batch: 180 | Loss: 3.872 | Acc: 42.585,69.402,87.168,% | Adaptive Acc: 82.981% | clf_exit: 0.176 0.380 0.444
Batch: 200 | Loss: 3.878 | Acc: 42.638,69.384,86.987,% | Adaptive Acc: 82.859% | clf_exit: 0.175 0.380 0.445
Batch: 220 | Loss: 3.883 | Acc: 42.407,69.365,86.867,% | Adaptive Acc: 82.735% | clf_exit: 0.176 0.379 0.445
Batch: 240 | Loss: 3.894 | Acc: 42.379,69.301,86.719,% | Adaptive Acc: 82.612% | clf_exit: 0.176 0.378 0.446
Batch: 260 | Loss: 3.903 | Acc: 42.256,69.256,86.500,% | Adaptive Acc: 82.459% | clf_exit: 0.175 0.378 0.447
Batch: 280 | Loss: 3.906 | Acc: 42.310,69.206,86.366,% | Adaptive Acc: 82.382% | clf_exit: 0.176 0.378 0.446
Batch: 300 | Loss: 3.912 | Acc: 42.286,69.121,86.181,% | Adaptive Acc: 82.257% | clf_exit: 0.176 0.378 0.446
Batch: 320 | Loss: 3.912 | Acc: 42.385,69.113,86.030,% | Adaptive Acc: 82.187% | clf_exit: 0.176 0.379 0.445
Batch: 340 | Loss: 3.920 | Acc: 42.357,69.050,85.816,% | Adaptive Acc: 81.983% | clf_exit: 0.177 0.378 0.445
Batch: 360 | Loss: 3.925 | Acc: 42.341,68.971,85.678,% | Adaptive Acc: 81.880% | clf_exit: 0.177 0.377 0.446
Batch: 380 | Loss: 3.937 | Acc: 42.255,68.879,85.509,% | Adaptive Acc: 81.693% | clf_exit: 0.177 0.377 0.446
Batch: 0 | Loss: 4.852 | Acc: 44.531,61.719,65.625,% | Adaptive Acc: 67.188% | clf_exit: 0.266 0.375 0.359
Batch: 20 | Loss: 5.137 | Acc: 38.802,60.231,65.662,% | Adaptive Acc: 61.644% | clf_exit: 0.275 0.350 0.375
Batch: 40 | Loss: 5.142 | Acc: 39.463,59.947,65.415,% | Adaptive Acc: 61.986% | clf_exit: 0.266 0.346 0.388
Batch: 60 | Loss: 5.154 | Acc: 39.844,59.631,65.266,% | Adaptive Acc: 62.013% | clf_exit: 0.262 0.345 0.394
Train all parameters

Epoch: 68
Batch: 0 | Loss: 3.754 | Acc: 45.312,66.406,91.406,% | Adaptive Acc: 88.281% | clf_exit: 0.211 0.336 0.453
Batch: 20 | Loss: 3.861 | Acc: 43.527,69.717,87.202,% | Adaptive Acc: 83.445% | clf_exit: 0.187 0.375 0.438
Batch: 40 | Loss: 3.822 | Acc: 42.835,69.512,87.748,% | Adaptive Acc: 83.479% | clf_exit: 0.185 0.375 0.440
Batch: 60 | Loss: 3.796 | Acc: 43.097,69.890,87.897,% | Adaptive Acc: 83.811% | clf_exit: 0.184 0.381 0.435
Batch: 80 | Loss: 3.820 | Acc: 43.162,69.608,87.674,% | Adaptive Acc: 83.613% | clf_exit: 0.182 0.381 0.437
Batch: 100 | Loss: 3.830 | Acc: 42.907,69.431,87.717,% | Adaptive Acc: 83.571% | clf_exit: 0.182 0.380 0.438
Batch: 120 | Loss: 3.855 | Acc: 42.859,69.273,87.468,% | Adaptive Acc: 83.355% | clf_exit: 0.182 0.379 0.439
Batch: 140 | Loss: 3.854 | Acc: 42.692,69.376,87.356,% | Adaptive Acc: 83.339% | clf_exit: 0.181 0.379 0.441
Batch: 160 | Loss: 3.873 | Acc: 42.396,69.167,87.180,% | Adaptive Acc: 83.104% | clf_exit: 0.180 0.379 0.441
Batch: 180 | Loss: 3.876 | Acc: 42.408,69.117,87.077,% | Adaptive Acc: 83.115% | clf_exit: 0.180 0.377 0.443
Batch: 200 | Loss: 3.877 | Acc: 42.483,69.158,86.995,% | Adaptive Acc: 83.046% | clf_exit: 0.181 0.378 0.441
Batch: 220 | Loss: 3.882 | Acc: 42.421,69.153,86.917,% | Adaptive Acc: 82.968% | clf_exit: 0.181 0.377 0.442
Batch: 240 | Loss: 3.880 | Acc: 42.418,69.220,86.839,% | Adaptive Acc: 82.881% | clf_exit: 0.180 0.378 0.442
Batch: 260 | Loss: 3.879 | Acc: 42.466,69.265,86.830,% | Adaptive Acc: 82.848% | clf_exit: 0.180 0.380 0.441
Batch: 280 | Loss: 3.883 | Acc: 42.399,69.206,86.716,% | Adaptive Acc: 82.732% | clf_exit: 0.179 0.379 0.442
Batch: 300 | Loss: 3.879 | Acc: 42.522,69.233,86.646,% | Adaptive Acc: 82.716% | clf_exit: 0.181 0.378 0.441
Batch: 320 | Loss: 3.886 | Acc: 42.521,69.149,86.524,% | Adaptive Acc: 82.564% | clf_exit: 0.181 0.378 0.441
Batch: 340 | Loss: 3.886 | Acc: 42.495,69.089,86.513,% | Adaptive Acc: 82.528% | clf_exit: 0.181 0.378 0.442
Batch: 360 | Loss: 3.894 | Acc: 42.454,69.031,86.342,% | Adaptive Acc: 82.390% | clf_exit: 0.181 0.377 0.442
Batch: 380 | Loss: 3.903 | Acc: 42.372,68.943,86.167,% | Adaptive Acc: 82.271% | clf_exit: 0.180 0.377 0.443
Batch: 0 | Loss: 5.059 | Acc: 37.500,63.281,67.969,% | Adaptive Acc: 63.281% | clf_exit: 0.305 0.367 0.328
Batch: 20 | Loss: 5.323 | Acc: 38.170,59.933,64.621,% | Adaptive Acc: 61.756% | clf_exit: 0.258 0.365 0.378
Batch: 40 | Loss: 5.341 | Acc: 37.805,59.299,64.348,% | Adaptive Acc: 61.223% | clf_exit: 0.253 0.355 0.392
Batch: 60 | Loss: 5.340 | Acc: 38.076,59.439,64.357,% | Adaptive Acc: 61.399% | clf_exit: 0.252 0.351 0.397
Train all parameters

Epoch: 69
Batch: 0 | Loss: 3.889 | Acc: 42.969,75.781,85.938,% | Adaptive Acc: 82.812% | clf_exit: 0.164 0.375 0.461
Batch: 20 | Loss: 3.700 | Acc: 44.792,71.838,87.798,% | Adaptive Acc: 83.817% | clf_exit: 0.190 0.383 0.427
Batch: 40 | Loss: 3.751 | Acc: 43.045,70.503,87.748,% | Adaptive Acc: 83.670% | clf_exit: 0.186 0.380 0.434
Batch: 60 | Loss: 3.775 | Acc: 43.122,70.658,87.974,% | Adaptive Acc: 83.965% | clf_exit: 0.182 0.378 0.440
Batch: 80 | Loss: 3.793 | Acc: 42.930,70.785,87.606,% | Adaptive Acc: 83.796% | clf_exit: 0.182 0.378 0.440
Batch: 100 | Loss: 3.800 | Acc: 42.891,70.645,87.794,% | Adaptive Acc: 83.725% | clf_exit: 0.179 0.382 0.439
Batch: 120 | Loss: 3.806 | Acc: 42.846,70.448,87.584,% | Adaptive Acc: 83.574% | clf_exit: 0.181 0.382 0.438
Batch: 140 | Loss: 3.813 | Acc: 42.908,70.451,87.350,% | Adaptive Acc: 83.383% | clf_exit: 0.180 0.383 0.437
Batch: 160 | Loss: 3.806 | Acc: 43.105,70.424,87.320,% | Adaptive Acc: 83.448% | clf_exit: 0.184 0.382 0.435
Batch: 180 | Loss: 3.824 | Acc: 42.951,70.161,87.168,% | Adaptive Acc: 83.283% | clf_exit: 0.182 0.382 0.436
Batch: 200 | Loss: 3.831 | Acc: 42.984,69.951,87.041,% | Adaptive Acc: 83.193% | clf_exit: 0.182 0.380 0.438
Batch: 220 | Loss: 3.843 | Acc: 42.895,69.856,86.814,% | Adaptive Acc: 82.982% | clf_exit: 0.181 0.382 0.437
Batch: 240 | Loss: 3.844 | Acc: 42.933,69.878,86.654,% | Adaptive Acc: 82.855% | clf_exit: 0.181 0.383 0.437
Batch: 260 | Loss: 3.856 | Acc: 42.927,69.711,86.566,% | Adaptive Acc: 82.726% | clf_exit: 0.181 0.381 0.438
Batch: 280 | Loss: 3.860 | Acc: 42.960,69.629,86.421,% | Adaptive Acc: 82.596% | clf_exit: 0.181 0.380 0.439
Batch: 300 | Loss: 3.866 | Acc: 42.940,69.503,86.280,% | Adaptive Acc: 82.511% | clf_exit: 0.181 0.379 0.440
Batch: 320 | Loss: 3.874 | Acc: 42.971,69.322,86.127,% | Adaptive Acc: 82.360% | clf_exit: 0.181 0.378 0.440
Batch: 340 | Loss: 3.881 | Acc: 42.941,69.233,86.089,% | Adaptive Acc: 82.306% | clf_exit: 0.182 0.377 0.441
Batch: 360 | Loss: 3.888 | Acc: 42.854,69.178,86.000,% | Adaptive Acc: 82.252% | clf_exit: 0.181 0.377 0.442
Batch: 380 | Loss: 3.895 | Acc: 42.835,69.154,85.835,% | Adaptive Acc: 82.087% | clf_exit: 0.181 0.377 0.442
Batch: 0 | Loss: 5.003 | Acc: 40.625,62.500,67.969,% | Adaptive Acc: 67.188% | clf_exit: 0.273 0.391 0.336
Batch: 20 | Loss: 5.351 | Acc: 37.054,60.119,65.179,% | Adaptive Acc: 61.979% | clf_exit: 0.265 0.366 0.369
Batch: 40 | Loss: 5.387 | Acc: 36.795,59.642,64.806,% | Adaptive Acc: 61.528% | clf_exit: 0.257 0.361 0.382
Batch: 60 | Loss: 5.384 | Acc: 36.783,59.401,64.908,% | Adaptive Acc: 61.270% | clf_exit: 0.261 0.355 0.384
Train all parameters

Epoch: 70
Batch: 0 | Loss: 3.820 | Acc: 36.719,73.438,83.594,% | Adaptive Acc: 81.250% | clf_exit: 0.211 0.352 0.438
Batch: 20 | Loss: 3.723 | Acc: 44.345,71.391,88.281,% | Adaptive Acc: 84.933% | clf_exit: 0.191 0.372 0.438
Batch: 40 | Loss: 3.697 | Acc: 43.941,71.818,88.643,% | Adaptive Acc: 84.985% | clf_exit: 0.190 0.378 0.432
Batch: 60 | Loss: 3.717 | Acc: 43.673,71.145,88.525,% | Adaptive Acc: 84.606% | clf_exit: 0.188 0.382 0.430
Batch: 80 | Loss: 3.724 | Acc: 43.596,70.727,88.628,% | Adaptive Acc: 84.674% | clf_exit: 0.186 0.381 0.432
Batch: 100 | Loss: 3.740 | Acc: 43.750,70.583,88.382,% | Adaptive Acc: 84.491% | clf_exit: 0.183 0.385 0.432
Batch: 120 | Loss: 3.771 | Acc: 43.518,70.216,88.004,% | Adaptive Acc: 84.039% | clf_exit: 0.183 0.382 0.435
Batch: 140 | Loss: 3.781 | Acc: 43.551,70.257,87.893,% | Adaptive Acc: 83.937% | clf_exit: 0.183 0.382 0.436
Batch: 160 | Loss: 3.786 | Acc: 43.638,70.104,87.811,% | Adaptive Acc: 83.933% | clf_exit: 0.183 0.380 0.437
Batch: 180 | Loss: 3.797 | Acc: 43.456,69.877,87.733,% | Adaptive Acc: 83.771% | clf_exit: 0.183 0.380 0.437
Batch: 200 | Loss: 3.799 | Acc: 43.412,69.792,87.624,% | Adaptive Acc: 83.745% | clf_exit: 0.181 0.382 0.437
Batch: 220 | Loss: 3.815 | Acc: 43.156,69.602,87.486,% | Adaptive Acc: 83.523% | clf_exit: 0.180 0.383 0.437
Batch: 240 | Loss: 3.825 | Acc: 43.047,69.473,87.276,% | Adaptive Acc: 83.299% | clf_exit: 0.181 0.381 0.438
Batch: 260 | Loss: 3.834 | Acc: 43.011,69.322,87.162,% | Adaptive Acc: 83.142% | clf_exit: 0.181 0.379 0.440
Batch: 280 | Loss: 3.850 | Acc: 42.885,69.189,86.919,% | Adaptive Acc: 82.965% | clf_exit: 0.180 0.380 0.440
Batch: 300 | Loss: 3.857 | Acc: 42.888,69.170,86.849,% | Adaptive Acc: 82.906% | clf_exit: 0.181 0.379 0.441
Batch: 320 | Loss: 3.855 | Acc: 43.017,69.215,86.743,% | Adaptive Acc: 82.861% | clf_exit: 0.181 0.378 0.441
Batch: 340 | Loss: 3.859 | Acc: 42.996,69.254,86.627,% | Adaptive Acc: 82.769% | clf_exit: 0.181 0.379 0.440
Batch: 360 | Loss: 3.862 | Acc: 42.949,69.248,86.522,% | Adaptive Acc: 82.711% | clf_exit: 0.180 0.379 0.440
Batch: 380 | Loss: 3.867 | Acc: 42.924,69.191,86.430,% | Adaptive Acc: 82.624% | clf_exit: 0.180 0.379 0.440
Batch: 0 | Loss: 4.679 | Acc: 42.969,67.188,69.531,% | Adaptive Acc: 65.625% | clf_exit: 0.312 0.398 0.289
Batch: 20 | Loss: 5.170 | Acc: 39.360,61.049,65.885,% | Adaptive Acc: 62.388% | clf_exit: 0.286 0.369 0.345
Batch: 40 | Loss: 5.141 | Acc: 40.168,60.823,65.415,% | Adaptive Acc: 62.290% | clf_exit: 0.284 0.364 0.352
Batch: 60 | Loss: 5.159 | Acc: 39.485,60.707,64.985,% | Adaptive Acc: 61.898% | clf_exit: 0.284 0.363 0.352
Train all parameters

Epoch: 71
Batch: 0 | Loss: 3.735 | Acc: 39.844,73.438,89.062,% | Adaptive Acc: 85.938% | clf_exit: 0.172 0.406 0.422
Batch: 20 | Loss: 3.792 | Acc: 42.039,71.429,88.690,% | Adaptive Acc: 84.896% | clf_exit: 0.170 0.392 0.438
Batch: 40 | Loss: 3.769 | Acc: 42.950,70.865,88.377,% | Adaptive Acc: 84.394% | clf_exit: 0.178 0.390 0.432
Batch: 60 | Loss: 3.769 | Acc: 42.623,70.607,88.256,% | Adaptive Acc: 84.298% | clf_exit: 0.180 0.388 0.431
Batch: 80 | Loss: 3.765 | Acc: 42.612,70.476,88.368,% | Adaptive Acc: 84.414% | clf_exit: 0.179 0.391 0.431
Batch: 100 | Loss: 3.765 | Acc: 42.938,70.135,88.281,% | Adaptive Acc: 84.274% | clf_exit: 0.182 0.386 0.432
Batch: 120 | Loss: 3.794 | Acc: 42.652,69.983,87.997,% | Adaptive Acc: 84.143% | clf_exit: 0.180 0.387 0.433
Batch: 140 | Loss: 3.786 | Acc: 42.769,69.947,87.871,% | Adaptive Acc: 83.954% | clf_exit: 0.183 0.385 0.432
Batch: 160 | Loss: 3.791 | Acc: 42.658,69.900,87.922,% | Adaptive Acc: 84.021% | clf_exit: 0.181 0.385 0.434
Batch: 180 | Loss: 3.803 | Acc: 42.636,69.842,87.772,% | Adaptive Acc: 83.840% | clf_exit: 0.182 0.385 0.433
Batch: 200 | Loss: 3.810 | Acc: 42.701,69.745,87.632,% | Adaptive Acc: 83.734% | clf_exit: 0.182 0.383 0.435
Batch: 220 | Loss: 3.812 | Acc: 42.721,69.694,87.489,% | Adaptive Acc: 83.590% | clf_exit: 0.182 0.382 0.436
Batch: 240 | Loss: 3.813 | Acc: 42.803,69.671,87.409,% | Adaptive Acc: 83.555% | clf_exit: 0.181 0.383 0.436
Batch: 260 | Loss: 3.822 | Acc: 42.867,69.612,87.201,% | Adaptive Acc: 83.348% | clf_exit: 0.181 0.383 0.436
Batch: 280 | Loss: 3.831 | Acc: 42.771,69.537,87.064,% | Adaptive Acc: 83.243% | clf_exit: 0.182 0.382 0.436
Batch: 300 | Loss: 3.840 | Acc: 42.730,69.500,86.971,% | Adaptive Acc: 83.155% | clf_exit: 0.181 0.381 0.437
Batch: 320 | Loss: 3.843 | Acc: 42.725,69.463,86.909,% | Adaptive Acc: 83.088% | clf_exit: 0.181 0.381 0.438
Batch: 340 | Loss: 3.845 | Acc: 42.808,69.453,86.838,% | Adaptive Acc: 83.021% | clf_exit: 0.182 0.381 0.437
Batch: 360 | Loss: 3.846 | Acc: 42.850,69.520,86.779,% | Adaptive Acc: 82.977% | clf_exit: 0.183 0.380 0.437
Batch: 380 | Loss: 3.852 | Acc: 42.797,69.429,86.612,% | Adaptive Acc: 82.794% | clf_exit: 0.183 0.379 0.438
Batch: 0 | Loss: 5.159 | Acc: 36.719,55.469,66.406,% | Adaptive Acc: 65.625% | clf_exit: 0.195 0.406 0.398
Batch: 20 | Loss: 5.237 | Acc: 38.467,59.896,66.257,% | Adaptive Acc: 63.281% | clf_exit: 0.193 0.416 0.392
Batch: 40 | Loss: 5.242 | Acc: 39.329,59.604,65.187,% | Adaptive Acc: 62.824% | clf_exit: 0.193 0.410 0.397
Batch: 60 | Loss: 5.226 | Acc: 39.280,59.734,65.369,% | Adaptive Acc: 63.089% | clf_exit: 0.192 0.411 0.397
Train all parameters

Epoch: 72
Batch: 0 | Loss: 3.912 | Acc: 39.844,68.750,92.188,% | Adaptive Acc: 85.938% | clf_exit: 0.180 0.398 0.422
Batch: 20 | Loss: 3.844 | Acc: 42.634,69.308,88.207,% | Adaptive Acc: 84.189% | clf_exit: 0.166 0.388 0.446
Batch: 40 | Loss: 3.812 | Acc: 42.454,69.550,88.643,% | Adaptive Acc: 84.546% | clf_exit: 0.179 0.378 0.443
Batch: 60 | Loss: 3.784 | Acc: 43.033,69.723,88.883,% | Adaptive Acc: 84.682% | clf_exit: 0.182 0.381 0.438
Batch: 80 | Loss: 3.807 | Acc: 42.998,69.579,88.329,% | Adaptive Acc: 84.028% | clf_exit: 0.184 0.378 0.438
Batch: 100 | Loss: 3.778 | Acc: 43.162,69.794,88.390,% | Adaptive Acc: 83.950% | clf_exit: 0.185 0.382 0.433
Batch: 120 | Loss: 3.777 | Acc: 43.221,69.899,88.320,% | Adaptive Acc: 83.955% | clf_exit: 0.188 0.380 0.432
Batch: 140 | Loss: 3.763 | Acc: 43.384,70.069,88.287,% | Adaptive Acc: 83.971% | clf_exit: 0.188 0.381 0.431
Batch: 160 | Loss: 3.762 | Acc: 43.488,70.065,88.228,% | Adaptive Acc: 83.972% | clf_exit: 0.187 0.381 0.432
Batch: 180 | Loss: 3.760 | Acc: 43.694,70.054,88.217,% | Adaptive Acc: 83.952% | clf_exit: 0.188 0.380 0.432
Batch: 200 | Loss: 3.760 | Acc: 43.641,70.099,88.215,% | Adaptive Acc: 83.874% | clf_exit: 0.188 0.380 0.431
Batch: 220 | Loss: 3.767 | Acc: 43.718,70.090,88.101,% | Adaptive Acc: 83.792% | clf_exit: 0.189 0.380 0.431
Batch: 240 | Loss: 3.777 | Acc: 43.653,70.024,87.983,% | Adaptive Acc: 83.717% | clf_exit: 0.188 0.379 0.432
Batch: 260 | Loss: 3.785 | Acc: 43.621,69.962,87.790,% | Adaptive Acc: 83.522% | clf_exit: 0.188 0.379 0.433
Batch: 280 | Loss: 3.796 | Acc: 43.478,69.876,87.653,% | Adaptive Acc: 83.419% | clf_exit: 0.187 0.380 0.433
Batch: 300 | Loss: 3.809 | Acc: 43.488,69.741,87.513,% | Adaptive Acc: 83.365% | clf_exit: 0.186 0.380 0.434
Batch: 320 | Loss: 3.818 | Acc: 43.421,69.680,87.361,% | Adaptive Acc: 83.224% | clf_exit: 0.186 0.378 0.436
Batch: 340 | Loss: 3.831 | Acc: 43.333,69.531,87.163,% | Adaptive Acc: 83.081% | clf_exit: 0.185 0.378 0.437
Batch: 360 | Loss: 3.832 | Acc: 43.267,69.531,87.035,% | Adaptive Acc: 82.983% | clf_exit: 0.185 0.377 0.437
Batch: 380 | Loss: 3.837 | Acc: 43.209,69.474,86.893,% | Adaptive Acc: 82.878% | clf_exit: 0.185 0.378 0.437
Batch: 0 | Loss: 5.712 | Acc: 35.938,57.812,64.844,% | Adaptive Acc: 58.594% | clf_exit: 0.266 0.391 0.344
Batch: 20 | Loss: 5.791 | Acc: 33.445,56.808,62.128,% | Adaptive Acc: 57.626% | clf_exit: 0.236 0.392 0.372
Batch: 40 | Loss: 5.720 | Acc: 33.746,57.107,62.176,% | Adaptive Acc: 58.384% | clf_exit: 0.235 0.385 0.380
Batch: 60 | Loss: 5.724 | Acc: 33.952,57.262,62.398,% | Adaptive Acc: 58.683% | clf_exit: 0.233 0.390 0.377
Train all parameters

Epoch: 73
Batch: 0 | Loss: 3.781 | Acc: 43.750,75.781,85.938,% | Adaptive Acc: 81.250% | clf_exit: 0.156 0.398 0.445
Batch: 20 | Loss: 3.742 | Acc: 43.564,70.833,88.058,% | Adaptive Acc: 84.115% | clf_exit: 0.195 0.374 0.432
Batch: 40 | Loss: 3.763 | Acc: 44.169,70.370,88.148,% | Adaptive Acc: 84.165% | clf_exit: 0.196 0.371 0.433
Batch: 60 | Loss: 3.780 | Acc: 43.558,70.133,88.204,% | Adaptive Acc: 84.144% | clf_exit: 0.191 0.377 0.432
Batch: 80 | Loss: 3.786 | Acc: 43.277,69.743,88.387,% | Adaptive Acc: 84.066% | clf_exit: 0.188 0.381 0.431
Batch: 100 | Loss: 3.783 | Acc: 43.170,69.825,88.374,% | Adaptive Acc: 84.066% | clf_exit: 0.187 0.384 0.429
Batch: 120 | Loss: 3.794 | Acc: 43.337,69.589,88.255,% | Adaptive Acc: 83.929% | clf_exit: 0.187 0.382 0.431
Batch: 140 | Loss: 3.779 | Acc: 43.456,69.753,88.331,% | Adaptive Acc: 83.993% | clf_exit: 0.187 0.382 0.430
Batch: 160 | Loss: 3.786 | Acc: 43.493,69.861,88.257,% | Adaptive Acc: 83.987% | clf_exit: 0.186 0.383 0.431
Batch: 180 | Loss: 3.783 | Acc: 43.621,69.894,88.113,% | Adaptive Acc: 83.926% | clf_exit: 0.188 0.382 0.430
Batch: 200 | Loss: 3.787 | Acc: 43.521,69.862,87.998,% | Adaptive Acc: 83.901% | clf_exit: 0.186 0.382 0.432
Batch: 220 | Loss: 3.785 | Acc: 43.503,69.793,88.020,% | Adaptive Acc: 83.848% | clf_exit: 0.186 0.382 0.432
Batch: 240 | Loss: 3.790 | Acc: 43.312,69.842,87.847,% | Adaptive Acc: 83.801% | clf_exit: 0.186 0.382 0.432
Batch: 260 | Loss: 3.797 | Acc: 43.268,69.825,87.602,% | Adaptive Acc: 83.615% | clf_exit: 0.185 0.383 0.432
Batch: 280 | Loss: 3.803 | Acc: 43.252,69.787,87.444,% | Adaptive Acc: 83.410% | clf_exit: 0.185 0.384 0.432
Batch: 300 | Loss: 3.807 | Acc: 43.132,69.705,87.404,% | Adaptive Acc: 83.337% | clf_exit: 0.185 0.383 0.432
Batch: 320 | Loss: 3.814 | Acc: 43.100,69.677,87.325,% | Adaptive Acc: 83.321% | clf_exit: 0.185 0.382 0.433
Batch: 340 | Loss: 3.819 | Acc: 43.131,69.664,87.225,% | Adaptive Acc: 83.243% | clf_exit: 0.184 0.382 0.434
Batch: 360 | Loss: 3.821 | Acc: 43.177,69.676,87.154,% | Adaptive Acc: 83.183% | clf_exit: 0.185 0.381 0.434
Batch: 380 | Loss: 3.826 | Acc: 43.168,69.613,87.039,% | Adaptive Acc: 83.124% | clf_exit: 0.185 0.380 0.435
Batch: 0 | Loss: 4.755 | Acc: 38.281,60.156,64.844,% | Adaptive Acc: 63.281% | clf_exit: 0.289 0.438 0.273
Batch: 20 | Loss: 5.302 | Acc: 36.942,59.784,65.774,% | Adaptive Acc: 62.091% | clf_exit: 0.245 0.395 0.359
Batch: 40 | Loss: 5.253 | Acc: 37.271,59.851,65.415,% | Adaptive Acc: 62.062% | clf_exit: 0.246 0.384 0.371
Batch: 60 | Loss: 5.261 | Acc: 37.308,59.862,65.061,% | Adaptive Acc: 62.116% | clf_exit: 0.246 0.383 0.372
Train all parameters

Epoch: 74
Batch: 0 | Loss: 3.764 | Acc: 39.844,72.656,91.406,% | Adaptive Acc: 88.281% | clf_exit: 0.148 0.438 0.414
Batch: 20 | Loss: 3.743 | Acc: 42.113,70.424,89.732,% | Adaptive Acc: 85.045% | clf_exit: 0.182 0.378 0.439
Batch: 40 | Loss: 3.697 | Acc: 43.216,70.884,89.691,% | Adaptive Acc: 85.099% | clf_exit: 0.186 0.388 0.426
Batch: 60 | Loss: 3.701 | Acc: 43.199,70.966,89.165,% | Adaptive Acc: 85.131% | clf_exit: 0.187 0.387 0.426
Batch: 80 | Loss: 3.705 | Acc: 43.345,70.689,89.169,% | Adaptive Acc: 85.041% | clf_exit: 0.188 0.386 0.426
Batch: 100 | Loss: 3.727 | Acc: 42.930,70.614,89.372,% | Adaptive Acc: 85.063% | clf_exit: 0.185 0.385 0.430
Batch: 120 | Loss: 3.730 | Acc: 43.169,70.564,89.140,% | Adaptive Acc: 84.846% | clf_exit: 0.186 0.387 0.427
Batch: 140 | Loss: 3.712 | Acc: 43.440,70.662,89.118,% | Adaptive Acc: 84.951% | clf_exit: 0.189 0.385 0.426
Batch: 160 | Loss: 3.716 | Acc: 43.469,70.633,88.922,% | Adaptive Acc: 84.783% | clf_exit: 0.190 0.385 0.425
Batch: 180 | Loss: 3.716 | Acc: 43.539,70.632,88.808,% | Adaptive Acc: 84.733% | clf_exit: 0.189 0.384 0.427
Batch: 200 | Loss: 3.731 | Acc: 43.427,70.429,88.635,% | Adaptive Acc: 84.577% | clf_exit: 0.188 0.383 0.429
Batch: 220 | Loss: 3.739 | Acc: 43.290,70.366,88.543,% | Adaptive Acc: 84.495% | clf_exit: 0.186 0.384 0.430
Batch: 240 | Loss: 3.748 | Acc: 43.218,70.319,88.485,% | Adaptive Acc: 84.417% | clf_exit: 0.185 0.384 0.431
Batch: 260 | Loss: 3.759 | Acc: 43.178,70.226,88.332,% | Adaptive Acc: 84.276% | clf_exit: 0.186 0.383 0.431
Batch: 280 | Loss: 3.764 | Acc: 43.202,70.182,88.134,% | Adaptive Acc: 84.139% | clf_exit: 0.186 0.383 0.430
Batch: 300 | Loss: 3.778 | Acc: 43.150,69.996,87.959,% | Adaptive Acc: 83.942% | clf_exit: 0.186 0.383 0.431
Batch: 320 | Loss: 3.784 | Acc: 43.161,69.945,87.911,% | Adaptive Acc: 83.871% | clf_exit: 0.185 0.383 0.432
Batch: 340 | Loss: 3.787 | Acc: 43.159,69.882,87.812,% | Adaptive Acc: 83.791% | clf_exit: 0.185 0.383 0.432
Batch: 360 | Loss: 3.801 | Acc: 43.116,69.774,87.626,% | Adaptive Acc: 83.592% | clf_exit: 0.185 0.383 0.432
Batch: 380 | Loss: 3.805 | Acc: 43.176,69.706,87.494,% | Adaptive Acc: 83.475% | clf_exit: 0.185 0.383 0.432
Batch: 0 | Loss: 5.157 | Acc: 42.188,60.938,64.062,% | Adaptive Acc: 61.719% | clf_exit: 0.281 0.438 0.281
Batch: 20 | Loss: 5.214 | Acc: 39.844,59.896,65.253,% | Adaptive Acc: 61.756% | clf_exit: 0.271 0.390 0.339
Batch: 40 | Loss: 5.171 | Acc: 40.492,60.137,65.225,% | Adaptive Acc: 62.100% | clf_exit: 0.264 0.389 0.347
Batch: 60 | Loss: 5.177 | Acc: 40.433,59.823,65.138,% | Adaptive Acc: 62.065% | clf_exit: 0.264 0.388 0.348
Train all parameters

Epoch: 75
Batch: 0 | Loss: 3.864 | Acc: 44.531,71.875,85.938,% | Adaptive Acc: 83.594% | clf_exit: 0.227 0.375 0.398
Batch: 20 | Loss: 3.584 | Acc: 44.754,71.354,88.170,% | Adaptive Acc: 84.263% | clf_exit: 0.200 0.398 0.402
Batch: 40 | Loss: 3.676 | Acc: 43.426,70.808,88.129,% | Adaptive Acc: 83.899% | clf_exit: 0.191 0.385 0.424
Batch: 60 | Loss: 3.665 | Acc: 43.916,70.889,88.499,% | Adaptive Acc: 84.209% | clf_exit: 0.189 0.389 0.422
Batch: 80 | Loss: 3.695 | Acc: 43.345,70.341,88.686,% | Adaptive Acc: 84.095% | clf_exit: 0.186 0.389 0.425
Batch: 100 | Loss: 3.707 | Acc: 43.417,70.227,88.560,% | Adaptive Acc: 84.035% | clf_exit: 0.186 0.389 0.424
Batch: 120 | Loss: 3.725 | Acc: 43.530,70.158,88.333,% | Adaptive Acc: 83.988% | clf_exit: 0.186 0.387 0.427
Batch: 140 | Loss: 3.737 | Acc: 43.606,70.041,88.276,% | Adaptive Acc: 83.876% | clf_exit: 0.185 0.387 0.428
Batch: 160 | Loss: 3.732 | Acc: 43.677,70.070,88.393,% | Adaptive Acc: 84.050% | clf_exit: 0.185 0.386 0.428
Batch: 180 | Loss: 3.738 | Acc: 43.564,70.101,88.311,% | Adaptive Acc: 83.909% | clf_exit: 0.185 0.387 0.428
Batch: 200 | Loss: 3.746 | Acc: 43.505,70.099,88.262,% | Adaptive Acc: 83.897% | clf_exit: 0.184 0.387 0.429
Batch: 220 | Loss: 3.757 | Acc: 43.414,70.065,88.246,% | Adaptive Acc: 83.880% | clf_exit: 0.184 0.386 0.430
Batch: 240 | Loss: 3.765 | Acc: 43.371,70.056,88.203,% | Adaptive Acc: 83.892% | clf_exit: 0.184 0.386 0.430
Batch: 260 | Loss: 3.769 | Acc: 43.376,70.040,88.132,% | Adaptive Acc: 83.845% | clf_exit: 0.185 0.385 0.431
Batch: 280 | Loss: 3.768 | Acc: 43.472,70.096,88.037,% | Adaptive Acc: 83.833% | clf_exit: 0.185 0.385 0.430
Batch: 300 | Loss: 3.771 | Acc: 43.446,70.024,88.009,% | Adaptive Acc: 83.768% | clf_exit: 0.185 0.385 0.430
Batch: 320 | Loss: 3.778 | Acc: 43.458,69.964,87.904,% | Adaptive Acc: 83.645% | clf_exit: 0.185 0.385 0.430
Batch: 340 | Loss: 3.782 | Acc: 43.535,69.909,87.777,% | Adaptive Acc: 83.511% | clf_exit: 0.186 0.384 0.430
Batch: 360 | Loss: 3.789 | Acc: 43.473,69.843,87.610,% | Adaptive Acc: 83.371% | clf_exit: 0.186 0.384 0.430
Batch: 380 | Loss: 3.794 | Acc: 43.488,69.761,87.494,% | Adaptive Acc: 83.241% | clf_exit: 0.186 0.384 0.430
Batch: 0 | Loss: 4.899 | Acc: 40.625,60.156,67.969,% | Adaptive Acc: 63.281% | clf_exit: 0.242 0.453 0.305
Batch: 20 | Loss: 5.099 | Acc: 39.323,61.682,66.481,% | Adaptive Acc: 63.616% | clf_exit: 0.237 0.400 0.363
Batch: 40 | Loss: 5.084 | Acc: 39.348,61.395,66.273,% | Adaptive Acc: 64.310% | clf_exit: 0.231 0.397 0.372
Batch: 60 | Loss: 5.088 | Acc: 39.242,61.527,65.868,% | Adaptive Acc: 64.011% | clf_exit: 0.232 0.393 0.374
Train all parameters

Epoch: 76
Batch: 0 | Loss: 3.301 | Acc: 45.312,76.562,95.312,% | Adaptive Acc: 89.844% | clf_exit: 0.156 0.453 0.391
Batch: 20 | Loss: 3.609 | Acc: 44.234,71.057,89.732,% | Adaptive Acc: 85.342% | clf_exit: 0.198 0.383 0.420
Batch: 40 | Loss: 3.665 | Acc: 43.483,70.941,89.615,% | Adaptive Acc: 85.118% | clf_exit: 0.195 0.389 0.416
Batch: 60 | Loss: 3.655 | Acc: 43.827,71.363,89.793,% | Adaptive Acc: 85.259% | clf_exit: 0.194 0.393 0.413
Batch: 80 | Loss: 3.669 | Acc: 43.625,71.113,89.776,% | Adaptive Acc: 85.079% | clf_exit: 0.191 0.393 0.416
Batch: 100 | Loss: 3.683 | Acc: 43.371,70.947,89.867,% | Adaptive Acc: 85.071% | clf_exit: 0.190 0.392 0.417
Batch: 120 | Loss: 3.700 | Acc: 43.279,70.519,89.502,% | Adaptive Acc: 84.872% | clf_exit: 0.189 0.390 0.420
Batch: 140 | Loss: 3.702 | Acc: 43.285,70.556,89.373,% | Adaptive Acc: 84.713% | clf_exit: 0.189 0.392 0.420
Batch: 160 | Loss: 3.704 | Acc: 43.274,70.579,89.223,% | Adaptive Acc: 84.608% | clf_exit: 0.189 0.390 0.421
Batch: 180 | Loss: 3.712 | Acc: 43.331,70.589,88.963,% | Adaptive Acc: 84.410% | clf_exit: 0.190 0.389 0.421
Batch: 200 | Loss: 3.722 | Acc: 43.400,70.550,88.775,% | Adaptive Acc: 84.235% | clf_exit: 0.189 0.388 0.423
Batch: 220 | Loss: 3.727 | Acc: 43.365,70.567,88.674,% | Adaptive Acc: 84.188% | clf_exit: 0.188 0.387 0.425
Batch: 240 | Loss: 3.735 | Acc: 43.261,70.501,88.631,% | Adaptive Acc: 84.142% | clf_exit: 0.187 0.387 0.426
Batch: 260 | Loss: 3.728 | Acc: 43.478,70.597,88.551,% | Adaptive Acc: 84.049% | clf_exit: 0.189 0.386 0.425
Batch: 280 | Loss: 3.736 | Acc: 43.436,70.493,88.431,% | Adaptive Acc: 83.900% | clf_exit: 0.190 0.386 0.425
Batch: 300 | Loss: 3.743 | Acc: 43.454,70.450,88.346,% | Adaptive Acc: 83.833% | clf_exit: 0.189 0.386 0.425
Batch: 320 | Loss: 3.757 | Acc: 43.290,70.330,88.164,% | Adaptive Acc: 83.667% | clf_exit: 0.189 0.386 0.425
Batch: 340 | Loss: 3.760 | Acc: 43.292,70.322,88.130,% | Adaptive Acc: 83.642% | clf_exit: 0.188 0.387 0.425
Batch: 360 | Loss: 3.770 | Acc: 43.157,70.193,87.970,% | Adaptive Acc: 83.509% | clf_exit: 0.189 0.385 0.426
Batch: 380 | Loss: 3.775 | Acc: 43.174,70.140,87.873,% | Adaptive Acc: 83.469% | clf_exit: 0.188 0.385 0.426
Batch: 0 | Loss: 4.833 | Acc: 40.625,68.750,71.094,% | Adaptive Acc: 66.406% | clf_exit: 0.242 0.461 0.297
Batch: 20 | Loss: 5.248 | Acc: 37.314,61.458,66.629,% | Adaptive Acc: 62.760% | clf_exit: 0.250 0.388 0.362
Batch: 40 | Loss: 5.215 | Acc: 38.148,60.976,66.025,% | Adaptive Acc: 62.691% | clf_exit: 0.248 0.386 0.366
Batch: 60 | Loss: 5.234 | Acc: 38.230,60.694,65.907,% | Adaptive Acc: 62.487% | clf_exit: 0.244 0.386 0.370
Train all parameters

Epoch: 77
Batch: 0 | Loss: 3.585 | Acc: 37.500,74.219,92.188,% | Adaptive Acc: 84.375% | clf_exit: 0.188 0.406 0.406
Batch: 20 | Loss: 3.723 | Acc: 43.527,70.833,89.658,% | Adaptive Acc: 85.603% | clf_exit: 0.188 0.372 0.441
Batch: 40 | Loss: 3.688 | Acc: 43.921,71.075,89.596,% | Adaptive Acc: 85.671% | clf_exit: 0.190 0.375 0.435
Batch: 60 | Loss: 3.692 | Acc: 43.686,71.171,89.216,% | Adaptive Acc: 85.118% | clf_exit: 0.188 0.379 0.432
Batch: 80 | Loss: 3.694 | Acc: 43.509,71.267,89.284,% | Adaptive Acc: 84.973% | clf_exit: 0.191 0.380 0.429
Batch: 100 | Loss: 3.692 | Acc: 43.719,71.179,89.202,% | Adaptive Acc: 84.940% | clf_exit: 0.190 0.383 0.427
Batch: 120 | Loss: 3.697 | Acc: 43.660,71.087,89.069,% | Adaptive Acc: 84.756% | clf_exit: 0.190 0.383 0.427
Batch: 140 | Loss: 3.706 | Acc: 43.672,70.828,88.907,% | Adaptive Acc: 84.613% | clf_exit: 0.188 0.384 0.428
Batch: 160 | Loss: 3.721 | Acc: 43.430,70.618,88.859,% | Adaptive Acc: 84.550% | clf_exit: 0.186 0.385 0.430
Batch: 180 | Loss: 3.736 | Acc: 43.189,70.554,88.791,% | Adaptive Acc: 84.578% | clf_exit: 0.184 0.386 0.430
Batch: 200 | Loss: 3.733 | Acc: 43.202,70.581,88.752,% | Adaptive Acc: 84.690% | clf_exit: 0.184 0.386 0.431
Batch: 220 | Loss: 3.731 | Acc: 43.382,70.631,88.652,% | Adaptive Acc: 84.591% | clf_exit: 0.185 0.386 0.429
Batch: 240 | Loss: 3.739 | Acc: 43.436,70.549,88.489,% | Adaptive Acc: 84.433% | clf_exit: 0.185 0.386 0.429
Batch: 260 | Loss: 3.743 | Acc: 43.343,70.474,88.389,% | Adaptive Acc: 84.285% | clf_exit: 0.184 0.387 0.429
Batch: 280 | Loss: 3.738 | Acc: 43.447,70.546,88.340,% | Adaptive Acc: 84.217% | clf_exit: 0.185 0.386 0.429
Batch: 300 | Loss: 3.737 | Acc: 43.426,70.637,88.320,% | Adaptive Acc: 84.232% | clf_exit: 0.185 0.387 0.429
Batch: 320 | Loss: 3.740 | Acc: 43.475,70.519,88.235,% | Adaptive Acc: 84.166% | clf_exit: 0.184 0.387 0.429
Batch: 340 | Loss: 3.746 | Acc: 43.427,70.425,88.146,% | Adaptive Acc: 84.024% | clf_exit: 0.184 0.388 0.428
Batch: 360 | Loss: 3.749 | Acc: 43.443,70.386,88.050,% | Adaptive Acc: 83.983% | clf_exit: 0.185 0.387 0.428
Batch: 380 | Loss: 3.755 | Acc: 43.438,70.292,87.953,% | Adaptive Acc: 83.879% | clf_exit: 0.185 0.387 0.428
Batch: 0 | Loss: 4.647 | Acc: 39.844,59.375,70.312,% | Adaptive Acc: 64.844% | clf_exit: 0.234 0.438 0.328
Batch: 20 | Loss: 5.204 | Acc: 39.583,60.900,66.034,% | Adaptive Acc: 62.984% | clf_exit: 0.242 0.398 0.360
Batch: 40 | Loss: 5.241 | Acc: 39.653,60.137,64.977,% | Adaptive Acc: 62.443% | clf_exit: 0.239 0.389 0.372
Batch: 60 | Loss: 5.263 | Acc: 39.088,59.772,64.741,% | Adaptive Acc: 62.257% | clf_exit: 0.235 0.392 0.374
Train all parameters

Epoch: 78
Batch: 0 | Loss: 4.032 | Acc: 39.844,75.000,85.156,% | Adaptive Acc: 82.812% | clf_exit: 0.156 0.398 0.445
Batch: 20 | Loss: 3.741 | Acc: 43.006,71.094,88.132,% | Adaptive Acc: 84.115% | clf_exit: 0.190 0.372 0.438
Batch: 40 | Loss: 3.695 | Acc: 43.045,71.418,88.758,% | Adaptive Acc: 84.985% | clf_exit: 0.187 0.386 0.428
Batch: 60 | Loss: 3.672 | Acc: 43.263,71.324,89.588,% | Adaptive Acc: 85.476% | clf_exit: 0.186 0.389 0.425
Batch: 80 | Loss: 3.695 | Acc: 43.596,71.345,89.448,% | Adaptive Acc: 85.272% | clf_exit: 0.186 0.393 0.421
Batch: 100 | Loss: 3.669 | Acc: 44.152,71.658,89.325,% | Adaptive Acc: 85.249% | clf_exit: 0.188 0.391 0.421
Batch: 120 | Loss: 3.668 | Acc: 44.299,71.449,89.172,% | Adaptive Acc: 84.963% | clf_exit: 0.191 0.390 0.420
Batch: 140 | Loss: 3.664 | Acc: 44.271,71.443,89.024,% | Adaptive Acc: 84.824% | clf_exit: 0.191 0.390 0.418
Batch: 160 | Loss: 3.678 | Acc: 44.211,71.167,88.859,% | Adaptive Acc: 84.627% | clf_exit: 0.190 0.389 0.420
Batch: 180 | Loss: 3.682 | Acc: 44.138,71.206,88.799,% | Adaptive Acc: 84.578% | clf_exit: 0.190 0.389 0.422
Batch: 200 | Loss: 3.688 | Acc: 44.045,71.051,88.596,% | Adaptive Acc: 84.289% | clf_exit: 0.190 0.387 0.423
Batch: 220 | Loss: 3.699 | Acc: 43.983,70.963,88.486,% | Adaptive Acc: 84.188% | clf_exit: 0.190 0.386 0.423
Batch: 240 | Loss: 3.706 | Acc: 43.850,70.841,88.391,% | Adaptive Acc: 84.057% | clf_exit: 0.190 0.386 0.424
Batch: 260 | Loss: 3.714 | Acc: 43.843,70.726,88.209,% | Adaptive Acc: 83.890% | clf_exit: 0.190 0.387 0.423
Batch: 280 | Loss: 3.724 | Acc: 43.731,70.635,88.114,% | Adaptive Acc: 83.794% | clf_exit: 0.189 0.387 0.424
Batch: 300 | Loss: 3.730 | Acc: 43.670,70.486,87.991,% | Adaptive Acc: 83.781% | clf_exit: 0.188 0.386 0.425
Batch: 320 | Loss: 3.733 | Acc: 43.577,70.551,87.962,% | Adaptive Acc: 83.715% | clf_exit: 0.189 0.386 0.425
Batch: 340 | Loss: 3.741 | Acc: 43.585,70.519,87.825,% | Adaptive Acc: 83.571% | clf_exit: 0.190 0.386 0.424
Batch: 360 | Loss: 3.751 | Acc: 43.473,70.373,87.738,% | Adaptive Acc: 83.468% | clf_exit: 0.189 0.386 0.425
Batch: 380 | Loss: 3.758 | Acc: 43.506,70.343,87.639,% | Adaptive Acc: 83.370% | clf_exit: 0.189 0.385 0.425
Batch: 0 | Loss: 4.769 | Acc: 37.500,59.375,70.312,% | Adaptive Acc: 68.750% | clf_exit: 0.219 0.391 0.391
Batch: 20 | Loss: 5.220 | Acc: 39.360,60.528,65.290,% | Adaptive Acc: 62.463% | clf_exit: 0.225 0.390 0.385
Batch: 40 | Loss: 5.188 | Acc: 40.072,60.252,64.863,% | Adaptive Acc: 62.348% | clf_exit: 0.220 0.390 0.390
Batch: 60 | Loss: 5.189 | Acc: 39.575,60.323,65.061,% | Adaptive Acc: 62.782% | clf_exit: 0.217 0.393 0.390
Train all parameters

Epoch: 79
Batch: 0 | Loss: 3.867 | Acc: 44.531,67.188,91.406,% | Adaptive Acc: 87.500% | clf_exit: 0.141 0.430 0.430
Batch: 20 | Loss: 3.589 | Acc: 44.829,72.210,89.955,% | Adaptive Acc: 85.640% | clf_exit: 0.173 0.410 0.417
Batch: 40 | Loss: 3.618 | Acc: 43.769,71.532,90.187,% | Adaptive Acc: 86.223% | clf_exit: 0.178 0.405 0.417
Batch: 60 | Loss: 3.594 | Acc: 43.750,71.926,90.279,% | Adaptive Acc: 86.053% | clf_exit: 0.183 0.403 0.414
Batch: 80 | Loss: 3.606 | Acc: 43.962,71.634,90.490,% | Adaptive Acc: 86.121% | clf_exit: 0.184 0.402 0.414
Batch: 100 | Loss: 3.627 | Acc: 43.750,71.465,90.269,% | Adaptive Acc: 85.837% | clf_exit: 0.184 0.400 0.416
Batch: 120 | Loss: 3.639 | Acc: 43.582,71.268,90.089,% | Adaptive Acc: 85.660% | clf_exit: 0.185 0.397 0.417
Batch: 140 | Loss: 3.644 | Acc: 43.772,71.315,89.977,% | Adaptive Acc: 85.627% | clf_exit: 0.185 0.397 0.418
Batch: 160 | Loss: 3.639 | Acc: 43.832,71.288,89.931,% | Adaptive Acc: 85.510% | clf_exit: 0.187 0.396 0.418
Batch: 180 | Loss: 3.634 | Acc: 43.910,71.301,89.865,% | Adaptive Acc: 85.407% | clf_exit: 0.189 0.395 0.416
Batch: 200 | Loss: 3.642 | Acc: 43.886,71.234,89.731,% | Adaptive Acc: 85.382% | clf_exit: 0.190 0.394 0.417
Batch: 220 | Loss: 3.651 | Acc: 43.842,71.129,89.526,% | Adaptive Acc: 85.142% | clf_exit: 0.190 0.393 0.418
Batch: 240 | Loss: 3.671 | Acc: 43.744,70.932,89.315,% | Adaptive Acc: 84.939% | clf_exit: 0.188 0.393 0.419
Batch: 260 | Loss: 3.684 | Acc: 43.690,70.788,89.116,% | Adaptive Acc: 84.722% | clf_exit: 0.188 0.394 0.419
Batch: 280 | Loss: 3.699 | Acc: 43.639,70.588,88.821,% | Adaptive Acc: 84.514% | clf_exit: 0.187 0.392 0.421
Batch: 300 | Loss: 3.705 | Acc: 43.607,70.564,88.634,% | Adaptive Acc: 84.300% | clf_exit: 0.188 0.391 0.421
Batch: 320 | Loss: 3.712 | Acc: 43.597,70.461,88.464,% | Adaptive Acc: 84.156% | clf_exit: 0.189 0.391 0.420
Batch: 340 | Loss: 3.723 | Acc: 43.645,70.459,88.281,% | Adaptive Acc: 84.013% | clf_exit: 0.189 0.390 0.421
Batch: 360 | Loss: 3.731 | Acc: 43.644,70.367,88.134,% | Adaptive Acc: 83.858% | clf_exit: 0.189 0.390 0.421
Batch: 380 | Loss: 3.738 | Acc: 43.641,70.257,88.037,% | Adaptive Acc: 83.772% | clf_exit: 0.189 0.389 0.422
Batch: 0 | Loss: 4.775 | Acc: 43.750,64.844,71.094,% | Adaptive Acc: 67.188% | clf_exit: 0.234 0.422 0.344
Batch: 20 | Loss: 5.123 | Acc: 40.848,61.310,63.914,% | Adaptive Acc: 61.830% | clf_exit: 0.230 0.417 0.353
Batch: 40 | Loss: 5.077 | Acc: 41.311,61.261,64.062,% | Adaptive Acc: 62.157% | clf_exit: 0.225 0.406 0.369
Batch: 60 | Loss: 5.113 | Acc: 40.932,61.117,64.191,% | Adaptive Acc: 62.218% | clf_exit: 0.229 0.399 0.372
Train all parameters

Epoch: 80
Batch: 0 | Loss: 3.459 | Acc: 43.750,73.438,88.281,% | Adaptive Acc: 87.500% | clf_exit: 0.219 0.359 0.422
Batch: 20 | Loss: 3.620 | Acc: 43.341,72.545,90.030,% | Adaptive Acc: 85.751% | clf_exit: 0.198 0.389 0.414
Batch: 40 | Loss: 3.578 | Acc: 45.027,72.370,89.920,% | Adaptive Acc: 85.690% | clf_exit: 0.198 0.392 0.410
Batch: 60 | Loss: 3.615 | Acc: 44.557,71.555,89.511,% | Adaptive Acc: 85.169% | clf_exit: 0.196 0.397 0.406
Batch: 80 | Loss: 3.624 | Acc: 44.493,71.441,89.583,% | Adaptive Acc: 85.301% | clf_exit: 0.194 0.394 0.412
Batch: 100 | Loss: 3.628 | Acc: 44.199,71.349,89.728,% | Adaptive Acc: 85.342% | clf_exit: 0.194 0.395 0.411
Batch: 120 | Loss: 3.633 | Acc: 44.073,71.223,89.786,% | Adaptive Acc: 85.389% | clf_exit: 0.192 0.395 0.413
Batch: 140 | Loss: 3.646 | Acc: 44.099,71.016,89.661,% | Adaptive Acc: 85.250% | clf_exit: 0.192 0.395 0.413
Batch: 160 | Loss: 3.655 | Acc: 44.017,71.002,89.475,% | Adaptive Acc: 85.117% | clf_exit: 0.192 0.394 0.414
Batch: 180 | Loss: 3.664 | Acc: 43.888,70.731,89.317,% | Adaptive Acc: 84.893% | clf_exit: 0.192 0.393 0.415
Batch: 200 | Loss: 3.663 | Acc: 44.003,70.818,89.307,% | Adaptive Acc: 84.985% | clf_exit: 0.193 0.392 0.415
Batch: 220 | Loss: 3.660 | Acc: 44.192,70.807,89.179,% | Adaptive Acc: 84.919% | clf_exit: 0.193 0.393 0.414
Batch: 240 | Loss: 3.674 | Acc: 44.035,70.708,89.092,% | Adaptive Acc: 84.767% | clf_exit: 0.193 0.392 0.415
Batch: 260 | Loss: 3.681 | Acc: 44.040,70.735,88.913,% | Adaptive Acc: 84.671% | clf_exit: 0.192 0.391 0.416
Batch: 280 | Loss: 3.688 | Acc: 43.936,70.668,88.851,% | Adaptive Acc: 84.609% | clf_exit: 0.193 0.390 0.416
Batch: 300 | Loss: 3.694 | Acc: 43.976,70.608,88.738,% | Adaptive Acc: 84.539% | clf_exit: 0.193 0.389 0.418
Batch: 320 | Loss: 3.703 | Acc: 43.913,70.490,88.615,% | Adaptive Acc: 84.426% | clf_exit: 0.193 0.389 0.418
Batch: 340 | Loss: 3.707 | Acc: 43.926,70.484,88.542,% | Adaptive Acc: 84.357% | clf_exit: 0.193 0.390 0.418
Batch: 360 | Loss: 3.715 | Acc: 43.893,70.373,88.398,% | Adaptive Acc: 84.198% | clf_exit: 0.192 0.389 0.418
Batch: 380 | Loss: 3.722 | Acc: 43.840,70.319,88.312,% | Adaptive Acc: 84.125% | clf_exit: 0.192 0.390 0.418
Batch: 0 | Loss: 4.465 | Acc: 43.750,67.969,75.000,% | Adaptive Acc: 71.094% | clf_exit: 0.234 0.461 0.305
Batch: 20 | Loss: 5.252 | Acc: 39.807,60.305,65.179,% | Adaptive Acc: 62.314% | clf_exit: 0.238 0.384 0.378
Batch: 40 | Loss: 5.214 | Acc: 40.168,60.194,64.844,% | Adaptive Acc: 62.157% | clf_exit: 0.239 0.383 0.378
Batch: 60 | Loss: 5.216 | Acc: 40.138,60.323,64.985,% | Adaptive Acc: 62.282% | clf_exit: 0.237 0.385 0.377
Train all parameters

Epoch: 81
Batch: 0 | Loss: 3.359 | Acc: 50.781,74.219,91.406,% | Adaptive Acc: 88.281% | clf_exit: 0.258 0.367 0.375
Batch: 20 | Loss: 3.613 | Acc: 43.787,71.131,90.774,% | Adaptive Acc: 86.310% | clf_exit: 0.193 0.398 0.409
Batch: 40 | Loss: 3.609 | Acc: 44.017,71.437,90.492,% | Adaptive Acc: 85.804% | clf_exit: 0.198 0.387 0.415
Batch: 60 | Loss: 3.664 | Acc: 43.904,70.863,90.049,% | Adaptive Acc: 85.374% | clf_exit: 0.196 0.385 0.420
Batch: 80 | Loss: 3.634 | Acc: 44.309,71.094,89.911,% | Adaptive Acc: 85.359% | clf_exit: 0.198 0.387 0.415
Batch: 100 | Loss: 3.616 | Acc: 44.438,71.643,89.944,% | Adaptive Acc: 85.535% | clf_exit: 0.197 0.391 0.411
Batch: 120 | Loss: 3.626 | Acc: 44.228,71.455,90.044,% | Adaptive Acc: 85.479% | clf_exit: 0.196 0.388 0.415
Batch: 140 | Loss: 3.635 | Acc: 44.132,71.277,89.960,% | Adaptive Acc: 85.361% | clf_exit: 0.195 0.389 0.416
Batch: 160 | Loss: 3.656 | Acc: 43.881,71.137,89.853,% | Adaptive Acc: 85.214% | clf_exit: 0.195 0.388 0.417
Batch: 180 | Loss: 3.660 | Acc: 43.923,71.141,89.710,% | Adaptive Acc: 85.079% | clf_exit: 0.195 0.389 0.416
Batch: 200 | Loss: 3.655 | Acc: 44.045,71.230,89.560,% | Adaptive Acc: 85.005% | clf_exit: 0.195 0.390 0.415
Batch: 220 | Loss: 3.666 | Acc: 43.973,71.051,89.359,% | Adaptive Acc: 84.785% | clf_exit: 0.195 0.392 0.414
Batch: 240 | Loss: 3.666 | Acc: 43.957,71.045,89.319,% | Adaptive Acc: 84.706% | clf_exit: 0.195 0.391 0.414
Batch: 260 | Loss: 3.679 | Acc: 43.930,70.860,89.170,% | Adaptive Acc: 84.588% | clf_exit: 0.194 0.390 0.416
Batch: 280 | Loss: 3.686 | Acc: 43.928,70.746,89.082,% | Adaptive Acc: 84.500% | clf_exit: 0.194 0.389 0.417
Batch: 300 | Loss: 3.695 | Acc: 43.903,70.720,88.961,% | Adaptive Acc: 84.409% | clf_exit: 0.194 0.388 0.418
Batch: 320 | Loss: 3.698 | Acc: 43.967,70.724,88.824,% | Adaptive Acc: 84.326% | clf_exit: 0.194 0.388 0.418
Batch: 340 | Loss: 3.698 | Acc: 43.977,70.729,88.767,% | Adaptive Acc: 84.281% | clf_exit: 0.193 0.389 0.418
Batch: 360 | Loss: 3.710 | Acc: 43.863,70.592,88.645,% | Adaptive Acc: 84.180% | clf_exit: 0.193 0.388 0.419
Batch: 380 | Loss: 3.718 | Acc: 43.812,70.585,88.552,% | Adaptive Acc: 84.108% | clf_exit: 0.193 0.388 0.419
Batch: 0 | Loss: 4.772 | Acc: 41.406,66.406,70.312,% | Adaptive Acc: 67.969% | clf_exit: 0.266 0.406 0.328
Batch: 20 | Loss: 5.266 | Acc: 38.876,60.677,66.815,% | Adaptive Acc: 62.723% | clf_exit: 0.266 0.383 0.351
Batch: 40 | Loss: 5.257 | Acc: 39.024,60.518,65.606,% | Adaptive Acc: 61.909% | clf_exit: 0.266 0.376 0.358
Batch: 60 | Loss: 5.262 | Acc: 38.806,60.233,65.318,% | Adaptive Acc: 62.154% | clf_exit: 0.266 0.369 0.365
Train all parameters

Epoch: 82
Batch: 0 | Loss: 3.697 | Acc: 45.312,74.219,90.625,% | Adaptive Acc: 86.719% | clf_exit: 0.141 0.391 0.469
Batch: 20 | Loss: 3.627 | Acc: 45.015,71.577,89.546,% | Adaptive Acc: 84.784% | clf_exit: 0.190 0.398 0.413
Batch: 40 | Loss: 3.581 | Acc: 45.141,71.742,89.920,% | Adaptive Acc: 85.423% | clf_exit: 0.195 0.395 0.410
Batch: 60 | Loss: 3.638 | Acc: 44.198,71.350,89.844,% | Adaptive Acc: 85.579% | clf_exit: 0.194 0.390 0.416
Batch: 80 | Loss: 3.651 | Acc: 43.904,71.200,89.709,% | Adaptive Acc: 85.484% | clf_exit: 0.192 0.391 0.417
Batch: 100 | Loss: 3.657 | Acc: 43.758,71.047,89.821,% | Adaptive Acc: 85.404% | clf_exit: 0.193 0.390 0.417
Batch: 120 | Loss: 3.658 | Acc: 43.847,71.055,89.799,% | Adaptive Acc: 85.363% | clf_exit: 0.196 0.387 0.417
Batch: 140 | Loss: 3.663 | Acc: 43.778,71.149,89.761,% | Adaptive Acc: 85.339% | clf_exit: 0.194 0.388 0.417
Batch: 160 | Loss: 3.672 | Acc: 43.866,71.196,89.596,% | Adaptive Acc: 85.045% | clf_exit: 0.195 0.387 0.417
Batch: 180 | Loss: 3.666 | Acc: 44.035,71.197,89.451,% | Adaptive Acc: 84.975% | clf_exit: 0.197 0.385 0.418
Batch: 200 | Loss: 3.677 | Acc: 43.836,71.012,89.346,% | Adaptive Acc: 84.826% | clf_exit: 0.197 0.384 0.420
Batch: 220 | Loss: 3.685 | Acc: 43.754,71.027,89.257,% | Adaptive Acc: 84.838% | clf_exit: 0.195 0.384 0.421
Batch: 240 | Loss: 3.688 | Acc: 43.773,70.915,89.137,% | Adaptive Acc: 84.813% | clf_exit: 0.194 0.385 0.421
Batch: 260 | Loss: 3.692 | Acc: 43.771,70.866,89.071,% | Adaptive Acc: 84.779% | clf_exit: 0.193 0.385 0.422
Batch: 280 | Loss: 3.705 | Acc: 43.675,70.760,88.896,% | Adaptive Acc: 84.656% | clf_exit: 0.193 0.384 0.423
Batch: 300 | Loss: 3.702 | Acc: 43.773,70.668,88.850,% | Adaptive Acc: 84.611% | clf_exit: 0.193 0.385 0.422
Batch: 320 | Loss: 3.702 | Acc: 43.823,70.629,88.739,% | Adaptive Acc: 84.526% | clf_exit: 0.193 0.386 0.421
Batch: 340 | Loss: 3.702 | Acc: 43.821,70.622,88.666,% | Adaptive Acc: 84.444% | clf_exit: 0.194 0.385 0.421
Batch: 360 | Loss: 3.708 | Acc: 43.718,70.540,88.543,% | Adaptive Acc: 84.319% | clf_exit: 0.194 0.385 0.421
Batch: 380 | Loss: 3.710 | Acc: 43.789,70.567,88.443,% | Adaptive Acc: 84.238% | clf_exit: 0.193 0.386 0.421
Batch: 0 | Loss: 4.803 | Acc: 39.844,63.281,67.969,% | Adaptive Acc: 68.750% | clf_exit: 0.219 0.461 0.320
Batch: 20 | Loss: 5.169 | Acc: 39.807,61.310,65.476,% | Adaptive Acc: 63.207% | clf_exit: 0.243 0.392 0.365
Batch: 40 | Loss: 5.168 | Acc: 39.996,60.652,65.091,% | Adaptive Acc: 62.862% | clf_exit: 0.237 0.388 0.375
Batch: 60 | Loss: 5.174 | Acc: 39.921,60.400,64.959,% | Adaptive Acc: 62.577% | clf_exit: 0.236 0.387 0.377
Train all parameters

Epoch: 83
Batch: 0 | Loss: 3.682 | Acc: 43.750,67.188,92.188,% | Adaptive Acc: 85.156% | clf_exit: 0.180 0.383 0.438
Batch: 20 | Loss: 3.521 | Acc: 44.606,72.805,91.555,% | Adaptive Acc: 86.942% | clf_exit: 0.188 0.407 0.405
Batch: 40 | Loss: 3.529 | Acc: 44.703,72.980,91.178,% | Adaptive Acc: 86.719% | clf_exit: 0.193 0.401 0.407
Batch: 60 | Loss: 3.556 | Acc: 44.224,72.310,91.176,% | Adaptive Acc: 86.565% | clf_exit: 0.195 0.393 0.413
Batch: 80 | Loss: 3.583 | Acc: 43.933,72.068,91.030,% | Adaptive Acc: 86.188% | clf_exit: 0.193 0.394 0.413
Batch: 100 | Loss: 3.574 | Acc: 44.384,72.192,90.803,% | Adaptive Acc: 86.061% | clf_exit: 0.195 0.394 0.411
Batch: 120 | Loss: 3.576 | Acc: 44.396,72.256,90.767,% | Adaptive Acc: 86.144% | clf_exit: 0.196 0.393 0.411
Batch: 140 | Loss: 3.573 | Acc: 44.470,72.174,90.592,% | Adaptive Acc: 85.960% | clf_exit: 0.198 0.393 0.409
Batch: 160 | Loss: 3.595 | Acc: 44.230,71.865,90.334,% | Adaptive Acc: 85.569% | clf_exit: 0.198 0.392 0.411
Batch: 180 | Loss: 3.611 | Acc: 44.164,71.633,90.111,% | Adaptive Acc: 85.411% | clf_exit: 0.196 0.393 0.411
Batch: 200 | Loss: 3.631 | Acc: 43.890,71.393,89.918,% | Adaptive Acc: 85.261% | clf_exit: 0.196 0.390 0.414
Batch: 220 | Loss: 3.640 | Acc: 43.860,71.355,89.727,% | Adaptive Acc: 85.163% | clf_exit: 0.196 0.390 0.414
Batch: 240 | Loss: 3.645 | Acc: 43.899,71.220,89.597,% | Adaptive Acc: 85.059% | clf_exit: 0.197 0.389 0.414
Batch: 260 | Loss: 3.653 | Acc: 43.846,71.100,89.482,% | Adaptive Acc: 84.959% | clf_exit: 0.196 0.390 0.414
Batch: 280 | Loss: 3.660 | Acc: 43.853,71.066,89.329,% | Adaptive Acc: 84.812% | clf_exit: 0.195 0.390 0.415
Batch: 300 | Loss: 3.670 | Acc: 43.820,71.003,89.244,% | Adaptive Acc: 84.764% | clf_exit: 0.194 0.390 0.416
Batch: 320 | Loss: 3.674 | Acc: 43.813,71.018,89.209,% | Adaptive Acc: 84.755% | clf_exit: 0.195 0.390 0.416
Batch: 340 | Loss: 3.681 | Acc: 43.761,70.883,89.083,% | Adaptive Acc: 84.680% | clf_exit: 0.194 0.389 0.417
Batch: 360 | Loss: 3.684 | Acc: 43.772,70.849,88.909,% | Adaptive Acc: 84.561% | clf_exit: 0.195 0.388 0.417
Batch: 380 | Loss: 3.687 | Acc: 43.865,70.794,88.761,% | Adaptive Acc: 84.449% | clf_exit: 0.195 0.387 0.418
Batch: 0 | Loss: 4.946 | Acc: 39.844,65.625,67.188,% | Adaptive Acc: 66.406% | clf_exit: 0.281 0.406 0.312
Batch: 20 | Loss: 5.126 | Acc: 40.030,61.310,65.551,% | Adaptive Acc: 63.318% | clf_exit: 0.235 0.429 0.336
Batch: 40 | Loss: 5.131 | Acc: 40.644,60.938,65.263,% | Adaptive Acc: 62.862% | clf_exit: 0.237 0.419 0.344
Batch: 60 | Loss: 5.133 | Acc: 40.587,60.989,65.113,% | Adaptive Acc: 62.833% | clf_exit: 0.238 0.418 0.345
Train all parameters

Epoch: 84
Batch: 0 | Loss: 3.877 | Acc: 41.406,73.438,85.938,% | Adaptive Acc: 84.375% | clf_exit: 0.188 0.375 0.438
Batch: 20 | Loss: 3.557 | Acc: 44.903,73.103,90.476,% | Adaptive Acc: 85.826% | clf_exit: 0.196 0.395 0.409
Batch: 40 | Loss: 3.568 | Acc: 44.569,72.332,90.282,% | Adaptive Acc: 85.938% | clf_exit: 0.197 0.394 0.409
Batch: 60 | Loss: 3.548 | Acc: 44.557,72.221,90.484,% | Adaptive Acc: 85.963% | clf_exit: 0.200 0.397 0.403
Batch: 80 | Loss: 3.551 | Acc: 44.772,71.701,90.519,% | Adaptive Acc: 85.899% | clf_exit: 0.199 0.397 0.405
Batch: 100 | Loss: 3.561 | Acc: 44.810,71.821,90.532,% | Adaptive Acc: 85.992% | clf_exit: 0.197 0.394 0.409
Batch: 120 | Loss: 3.573 | Acc: 44.848,71.726,90.386,% | Adaptive Acc: 85.757% | clf_exit: 0.198 0.396 0.407
Batch: 140 | Loss: 3.591 | Acc: 44.603,71.559,90.137,% | Adaptive Acc: 85.483% | clf_exit: 0.196 0.395 0.409
Batch: 160 | Loss: 3.599 | Acc: 44.371,71.497,90.193,% | Adaptive Acc: 85.477% | clf_exit: 0.196 0.396 0.409
Batch: 180 | Loss: 3.600 | Acc: 44.484,71.465,90.090,% | Adaptive Acc: 85.394% | clf_exit: 0.197 0.395 0.408
Batch: 200 | Loss: 3.614 | Acc: 44.360,71.273,90.003,% | Adaptive Acc: 85.397% | clf_exit: 0.197 0.393 0.410
Batch: 220 | Loss: 3.629 | Acc: 44.400,71.154,89.812,% | Adaptive Acc: 85.276% | clf_exit: 0.197 0.391 0.412
Batch: 240 | Loss: 3.626 | Acc: 44.486,71.188,89.688,% | Adaptive Acc: 85.215% | clf_exit: 0.198 0.391 0.411
Batch: 260 | Loss: 3.638 | Acc: 44.376,71.106,89.604,% | Adaptive Acc: 85.114% | clf_exit: 0.198 0.390 0.412
Batch: 280 | Loss: 3.646 | Acc: 44.256,70.983,89.518,% | Adaptive Acc: 85.003% | clf_exit: 0.198 0.391 0.411
Batch: 300 | Loss: 3.658 | Acc: 44.186,70.938,89.390,% | Adaptive Acc: 84.923% | clf_exit: 0.197 0.390 0.413
Batch: 320 | Loss: 3.659 | Acc: 44.191,70.928,89.320,% | Adaptive Acc: 84.835% | clf_exit: 0.197 0.390 0.413
Batch: 340 | Loss: 3.661 | Acc: 44.169,70.917,89.221,% | Adaptive Acc: 84.776% | clf_exit: 0.197 0.389 0.414
Batch: 360 | Loss: 3.668 | Acc: 44.109,70.869,89.123,% | Adaptive Acc: 84.676% | clf_exit: 0.196 0.390 0.414
Batch: 380 | Loss: 3.678 | Acc: 44.012,70.749,89.015,% | Adaptive Acc: 84.568% | clf_exit: 0.195 0.389 0.415
Batch: 0 | Loss: 4.934 | Acc: 35.156,58.594,67.969,% | Adaptive Acc: 64.062% | clf_exit: 0.273 0.398 0.328
Batch: 20 | Loss: 5.147 | Acc: 39.286,61.161,65.625,% | Adaptive Acc: 63.244% | clf_exit: 0.268 0.391 0.341
Batch: 40 | Loss: 5.183 | Acc: 39.710,60.156,65.111,% | Adaptive Acc: 62.405% | clf_exit: 0.268 0.388 0.345
Batch: 60 | Loss: 5.194 | Acc: 39.793,60.310,65.049,% | Adaptive Acc: 61.962% | clf_exit: 0.272 0.383 0.345
Train classifier parameters

Epoch: 85
Batch: 0 | Loss: 3.452 | Acc: 51.562,71.875,87.500,% | Adaptive Acc: 82.812% | clf_exit: 0.156 0.477 0.367
Batch: 20 | Loss: 4.014 | Acc: 41.257,67.448,84.970,% | Adaptive Acc: 81.213% | clf_exit: 0.168 0.380 0.452
Batch: 40 | Loss: 4.255 | Acc: 40.282,66.425,81.231,% | Adaptive Acc: 78.716% | clf_exit: 0.161 0.364 0.475
Batch: 60 | Loss: 4.395 | Acc: 39.652,65.599,79.457,% | Adaptive Acc: 77.088% | clf_exit: 0.154 0.362 0.484
Batch: 80 | Loss: 4.432 | Acc: 39.873,65.422,78.848,% | Adaptive Acc: 76.524% | clf_exit: 0.154 0.362 0.485
Batch: 100 | Loss: 4.461 | Acc: 39.728,65.300,78.504,% | Adaptive Acc: 76.214% | clf_exit: 0.153 0.357 0.490
Batch: 120 | Loss: 4.479 | Acc: 39.889,65.192,78.286,% | Adaptive Acc: 75.981% | clf_exit: 0.150 0.357 0.493
Batch: 140 | Loss: 4.493 | Acc: 39.727,64.938,78.142,% | Adaptive Acc: 75.803% | clf_exit: 0.151 0.355 0.494
Batch: 160 | Loss: 4.495 | Acc: 39.786,64.931,78.091,% | Adaptive Acc: 75.869% | clf_exit: 0.149 0.356 0.495
Batch: 180 | Loss: 4.497 | Acc: 39.904,64.770,77.935,% | Adaptive Acc: 75.742% | clf_exit: 0.148 0.355 0.497
Batch: 200 | Loss: 4.477 | Acc: 40.112,64.984,78.067,% | Adaptive Acc: 75.890% | clf_exit: 0.148 0.357 0.495
Batch: 220 | Loss: 4.479 | Acc: 40.119,64.975,78.136,% | Adaptive Acc: 75.947% | clf_exit: 0.148 0.356 0.497
Batch: 240 | Loss: 4.473 | Acc: 40.029,65.067,78.261,% | Adaptive Acc: 76.105% | clf_exit: 0.148 0.355 0.497
Batch: 260 | Loss: 4.475 | Acc: 40.005,65.068,78.254,% | Adaptive Acc: 76.039% | clf_exit: 0.148 0.354 0.498
Batch: 280 | Loss: 4.472 | Acc: 40.052,65.133,78.386,% | Adaptive Acc: 76.148% | clf_exit: 0.147 0.355 0.498
Batch: 300 | Loss: 4.471 | Acc: 40.046,65.147,78.426,% | Adaptive Acc: 76.191% | clf_exit: 0.147 0.355 0.498
Batch: 320 | Loss: 4.469 | Acc: 40.063,65.085,78.398,% | Adaptive Acc: 76.156% | clf_exit: 0.146 0.355 0.499
Batch: 340 | Loss: 4.463 | Acc: 40.006,65.187,78.482,% | Adaptive Acc: 76.210% | clf_exit: 0.146 0.356 0.498
Batch: 360 | Loss: 4.462 | Acc: 39.950,65.194,78.532,% | Adaptive Acc: 76.290% | clf_exit: 0.146 0.355 0.499
Batch: 380 | Loss: 4.461 | Acc: 39.961,65.184,78.599,% | Adaptive Acc: 76.403% | clf_exit: 0.146 0.354 0.499
Batch: 0 | Loss: 5.299 | Acc: 33.594,59.375,66.406,% | Adaptive Acc: 67.188% | clf_exit: 0.203 0.422 0.375
Batch: 20 | Loss: 5.529 | Acc: 39.397,58.222,62.314,% | Adaptive Acc: 61.347% | clf_exit: 0.205 0.377 0.417
Batch: 40 | Loss: 5.564 | Acc: 39.367,57.774,60.880,% | Adaptive Acc: 59.813% | clf_exit: 0.202 0.373 0.425
Batch: 60 | Loss: 5.561 | Acc: 39.280,58.197,60.233,% | Adaptive Acc: 59.682% | clf_exit: 0.201 0.372 0.427
Train classifier parameters

Epoch: 86
Batch: 0 | Loss: 4.212 | Acc: 45.312,69.531,82.812,% | Adaptive Acc: 81.250% | clf_exit: 0.148 0.344 0.508
Batch: 20 | Loss: 4.345 | Acc: 41.034,66.369,79.129,% | Adaptive Acc: 77.158% | clf_exit: 0.153 0.349 0.498
Batch: 40 | Loss: 4.328 | Acc: 40.758,66.502,79.497,% | Adaptive Acc: 77.363% | clf_exit: 0.146 0.361 0.493
Batch: 60 | Loss: 4.349 | Acc: 40.561,66.457,79.572,% | Adaptive Acc: 77.497% | clf_exit: 0.144 0.364 0.491
Batch: 80 | Loss: 4.347 | Acc: 40.384,66.464,79.620,% | Adaptive Acc: 77.566% | clf_exit: 0.146 0.357 0.497
Batch: 100 | Loss: 4.358 | Acc: 40.463,66.298,79.440,% | Adaptive Acc: 77.205% | clf_exit: 0.145 0.359 0.495
Batch: 120 | Loss: 4.358 | Acc: 40.651,66.348,79.500,% | Adaptive Acc: 77.311% | clf_exit: 0.146 0.358 0.495
Batch: 140 | Loss: 4.356 | Acc: 40.797,66.273,79.710,% | Adaptive Acc: 77.383% | clf_exit: 0.147 0.357 0.496
Batch: 160 | Loss: 4.336 | Acc: 40.911,66.382,79.945,% | Adaptive Acc: 77.582% | clf_exit: 0.149 0.356 0.495
Batch: 180 | Loss: 4.331 | Acc: 41.070,66.467,79.977,% | Adaptive Acc: 77.646% | clf_exit: 0.149 0.355 0.495
Batch: 200 | Loss: 4.326 | Acc: 41.126,66.367,80.103,% | Adaptive Acc: 77.795% | clf_exit: 0.150 0.354 0.496
Batch: 220 | Loss: 4.321 | Acc: 41.283,66.445,80.267,% | Adaptive Acc: 77.991% | clf_exit: 0.150 0.354 0.496
Batch: 240 | Loss: 4.318 | Acc: 41.374,66.458,80.252,% | Adaptive Acc: 77.953% | clf_exit: 0.150 0.354 0.495
Batch: 260 | Loss: 4.312 | Acc: 41.499,66.562,80.244,% | Adaptive Acc: 77.960% | clf_exit: 0.152 0.354 0.495
Batch: 280 | Loss: 4.309 | Acc: 41.520,66.640,80.230,% | Adaptive Acc: 77.947% | clf_exit: 0.152 0.355 0.494
Batch: 300 | Loss: 4.305 | Acc: 41.471,66.684,80.269,% | Adaptive Acc: 78.039% | clf_exit: 0.151 0.355 0.494
Batch: 320 | Loss: 4.303 | Acc: 41.426,66.625,80.247,% | Adaptive Acc: 78.023% | clf_exit: 0.151 0.355 0.493
Batch: 340 | Loss: 4.299 | Acc: 41.413,66.690,80.407,% | Adaptive Acc: 78.134% | clf_exit: 0.152 0.355 0.493
Batch: 360 | Loss: 4.293 | Acc: 41.441,66.791,80.570,% | Adaptive Acc: 78.257% | clf_exit: 0.152 0.356 0.492
Batch: 380 | Loss: 4.291 | Acc: 41.464,66.779,80.612,% | Adaptive Acc: 78.289% | clf_exit: 0.152 0.356 0.492
Batch: 0 | Loss: 5.176 | Acc: 35.938,58.594,66.406,% | Adaptive Acc: 64.844% | clf_exit: 0.227 0.391 0.383
Batch: 20 | Loss: 5.387 | Acc: 41.146,59.375,63.058,% | Adaptive Acc: 61.905% | clf_exit: 0.211 0.381 0.408
Batch: 40 | Loss: 5.423 | Acc: 40.301,58.994,61.585,% | Adaptive Acc: 60.728% | clf_exit: 0.208 0.371 0.421
Batch: 60 | Loss: 5.420 | Acc: 40.113,59.298,61.168,% | Adaptive Acc: 60.605% | clf_exit: 0.208 0.370 0.422
Train classifier parameters

Epoch: 87
Batch: 0 | Loss: 4.489 | Acc: 32.812,67.969,77.344,% | Adaptive Acc: 75.781% | clf_exit: 0.164 0.320 0.516
Batch: 20 | Loss: 4.182 | Acc: 42.708,67.039,81.250,% | Adaptive Acc: 79.241% | clf_exit: 0.158 0.371 0.471
Batch: 40 | Loss: 4.177 | Acc: 42.778,67.321,81.250,% | Adaptive Acc: 79.135% | clf_exit: 0.161 0.362 0.478
Batch: 60 | Loss: 4.231 | Acc: 41.586,66.983,81.160,% | Adaptive Acc: 79.009% | clf_exit: 0.156 0.358 0.486
Batch: 80 | Loss: 4.234 | Acc: 41.889,67.043,81.356,% | Adaptive Acc: 79.070% | clf_exit: 0.156 0.359 0.485
Batch: 100 | Loss: 4.221 | Acc: 42.095,67.203,81.343,% | Adaptive Acc: 79.022% | clf_exit: 0.156 0.360 0.484
Batch: 120 | Loss: 4.216 | Acc: 42.259,67.271,81.418,% | Adaptive Acc: 79.068% | clf_exit: 0.157 0.360 0.483
Batch: 140 | Loss: 4.211 | Acc: 42.043,67.188,81.582,% | Adaptive Acc: 79.261% | clf_exit: 0.156 0.360 0.484
Batch: 160 | Loss: 4.219 | Acc: 42.129,67.304,81.541,% | Adaptive Acc: 79.188% | clf_exit: 0.155 0.361 0.484
Batch: 180 | Loss: 4.209 | Acc: 42.205,67.446,81.733,% | Adaptive Acc: 79.355% | clf_exit: 0.155 0.362 0.483
Batch: 200 | Loss: 4.208 | Acc: 42.009,67.343,81.954,% | Adaptive Acc: 79.361% | clf_exit: 0.155 0.361 0.483
Batch: 220 | Loss: 4.206 | Acc: 42.018,67.421,82.070,% | Adaptive Acc: 79.461% | clf_exit: 0.155 0.362 0.483
Batch: 240 | Loss: 4.197 | Acc: 42.058,67.573,82.135,% | Adaptive Acc: 79.542% | clf_exit: 0.156 0.363 0.482
Batch: 260 | Loss: 4.210 | Acc: 41.873,67.418,82.094,% | Adaptive Acc: 79.472% | clf_exit: 0.156 0.361 0.483
Batch: 280 | Loss: 4.203 | Acc: 41.932,67.566,82.173,% | Adaptive Acc: 79.579% | clf_exit: 0.156 0.360 0.484
Batch: 300 | Loss: 4.201 | Acc: 41.988,67.582,82.148,% | Adaptive Acc: 79.553% | clf_exit: 0.156 0.360 0.484
Batch: 320 | Loss: 4.202 | Acc: 42.010,67.589,82.116,% | Adaptive Acc: 79.490% | clf_exit: 0.156 0.360 0.484
Batch: 340 | Loss: 4.195 | Acc: 42.050,67.616,82.187,% | Adaptive Acc: 79.573% | clf_exit: 0.155 0.360 0.485
Batch: 360 | Loss: 4.200 | Acc: 42.019,67.538,82.109,% | Adaptive Acc: 79.478% | clf_exit: 0.155 0.360 0.484
Batch: 380 | Loss: 4.200 | Acc: 41.995,67.557,82.050,% | Adaptive Acc: 79.427% | clf_exit: 0.156 0.360 0.484
Batch: 0 | Loss: 5.097 | Acc: 36.719,61.719,67.188,% | Adaptive Acc: 65.625% | clf_exit: 0.258 0.359 0.383
Batch: 20 | Loss: 5.307 | Acc: 40.588,59.859,64.100,% | Adaptive Acc: 62.798% | clf_exit: 0.218 0.372 0.410
Batch: 40 | Loss: 5.337 | Acc: 40.644,59.413,62.538,% | Adaptive Acc: 61.357% | clf_exit: 0.211 0.369 0.420
Batch: 60 | Loss: 5.340 | Acc: 40.535,59.785,62.141,% | Adaptive Acc: 61.309% | clf_exit: 0.211 0.372 0.417
Train classifier parameters

Epoch: 88
Batch: 0 | Loss: 3.816 | Acc: 47.656,71.875,85.938,% | Adaptive Acc: 85.156% | clf_exit: 0.125 0.469 0.406
Batch: 20 | Loss: 4.051 | Acc: 44.978,69.568,83.296,% | Adaptive Acc: 80.618% | clf_exit: 0.156 0.374 0.471
Batch: 40 | Loss: 4.084 | Acc: 43.921,68.617,82.812,% | Adaptive Acc: 80.450% | clf_exit: 0.157 0.371 0.473
Batch: 60 | Loss: 4.129 | Acc: 43.071,68.186,82.505,% | Adaptive Acc: 79.956% | clf_exit: 0.162 0.363 0.476
Batch: 80 | Loss: 4.113 | Acc: 43.104,68.432,82.976,% | Adaptive Acc: 80.401% | clf_exit: 0.164 0.364 0.472
Batch: 100 | Loss: 4.151 | Acc: 42.342,67.961,82.882,% | Adaptive Acc: 80.237% | clf_exit: 0.163 0.360 0.478
Batch: 120 | Loss: 4.160 | Acc: 42.168,67.723,82.800,% | Adaptive Acc: 80.120% | clf_exit: 0.162 0.360 0.478
Batch: 140 | Loss: 4.144 | Acc: 42.176,67.936,82.979,% | Adaptive Acc: 80.275% | clf_exit: 0.161 0.361 0.478
Batch: 160 | Loss: 4.141 | Acc: 42.241,67.940,82.929,% | Adaptive Acc: 80.309% | clf_exit: 0.161 0.360 0.478
Batch: 180 | Loss: 4.138 | Acc: 42.270,67.947,82.933,% | Adaptive Acc: 80.339% | clf_exit: 0.162 0.359 0.479
Batch: 200 | Loss: 4.136 | Acc: 42.320,68.148,82.809,% | Adaptive Acc: 80.224% | clf_exit: 0.163 0.359 0.478
Batch: 220 | Loss: 4.130 | Acc: 42.410,68.209,82.827,% | Adaptive Acc: 80.175% | clf_exit: 0.163 0.360 0.477
Batch: 240 | Loss: 4.126 | Acc: 42.453,68.176,82.757,% | Adaptive Acc: 80.141% | clf_exit: 0.162 0.361 0.476
Batch: 260 | Loss: 4.123 | Acc: 42.544,68.133,82.750,% | Adaptive Acc: 80.086% | clf_exit: 0.163 0.361 0.476
Batch: 280 | Loss: 4.125 | Acc: 42.507,68.127,82.718,% | Adaptive Acc: 80.054% | clf_exit: 0.162 0.362 0.476
Batch: 300 | Loss: 4.132 | Acc: 42.442,68.044,82.716,% | Adaptive Acc: 80.033% | clf_exit: 0.162 0.361 0.477
Batch: 320 | Loss: 4.132 | Acc: 42.441,68.044,82.798,% | Adaptive Acc: 80.079% | clf_exit: 0.162 0.362 0.477
Batch: 340 | Loss: 4.131 | Acc: 42.375,68.076,82.792,% | Adaptive Acc: 80.059% | clf_exit: 0.162 0.362 0.476
Batch: 360 | Loss: 4.126 | Acc: 42.397,68.107,82.858,% | Adaptive Acc: 80.101% | clf_exit: 0.162 0.362 0.476
Batch: 380 | Loss: 4.127 | Acc: 42.339,68.102,82.802,% | Adaptive Acc: 80.065% | clf_exit: 0.162 0.362 0.476
Batch: 0 | Loss: 5.049 | Acc: 36.719,63.281,67.969,% | Adaptive Acc: 66.406% | clf_exit: 0.242 0.391 0.367
Batch: 20 | Loss: 5.263 | Acc: 40.960,60.231,64.249,% | Adaptive Acc: 62.909% | clf_exit: 0.224 0.381 0.395
Batch: 40 | Loss: 5.291 | Acc: 40.835,59.566,63.167,% | Adaptive Acc: 61.928% | clf_exit: 0.218 0.373 0.409
Batch: 60 | Loss: 5.292 | Acc: 40.638,59.721,62.692,% | Adaptive Acc: 61.885% | clf_exit: 0.217 0.375 0.409
Train classifier parameters

Epoch: 89
Batch: 0 | Loss: 3.843 | Acc: 46.875,70.312,88.281,% | Adaptive Acc: 83.594% | clf_exit: 0.195 0.391 0.414
Batch: 20 | Loss: 4.025 | Acc: 43.043,68.676,83.891,% | Adaptive Acc: 81.362% | clf_exit: 0.163 0.362 0.475
Batch: 40 | Loss: 4.001 | Acc: 42.873,69.245,84.432,% | Adaptive Acc: 81.421% | clf_exit: 0.167 0.372 0.462
Batch: 60 | Loss: 4.008 | Acc: 43.058,69.006,84.554,% | Adaptive Acc: 81.263% | clf_exit: 0.166 0.368 0.466
Batch: 80 | Loss: 4.027 | Acc: 42.892,68.837,84.365,% | Adaptive Acc: 80.922% | clf_exit: 0.165 0.368 0.467
Batch: 100 | Loss: 4.052 | Acc: 42.907,68.735,83.926,% | Adaptive Acc: 80.585% | clf_exit: 0.165 0.368 0.467
Batch: 120 | Loss: 4.045 | Acc: 43.098,68.666,83.858,% | Adaptive Acc: 80.520% | clf_exit: 0.165 0.367 0.468
Batch: 140 | Loss: 4.062 | Acc: 43.019,68.440,83.583,% | Adaptive Acc: 80.397% | clf_exit: 0.163 0.368 0.469
Batch: 160 | Loss: 4.068 | Acc: 42.867,68.459,83.560,% | Adaptive Acc: 80.406% | clf_exit: 0.163 0.368 0.469
Batch: 180 | Loss: 4.065 | Acc: 42.809,68.547,83.564,% | Adaptive Acc: 80.499% | clf_exit: 0.164 0.367 0.469
Batch: 200 | Loss: 4.075 | Acc: 42.739,68.474,83.551,% | Adaptive Acc: 80.449% | clf_exit: 0.163 0.368 0.469
Batch: 220 | Loss: 4.070 | Acc: 42.686,68.570,83.573,% | Adaptive Acc: 80.490% | clf_exit: 0.163 0.368 0.469
Batch: 240 | Loss: 4.069 | Acc: 42.706,68.633,83.545,% | Adaptive Acc: 80.491% | clf_exit: 0.164 0.368 0.468
Batch: 260 | Loss: 4.070 | Acc: 42.672,68.711,83.543,% | Adaptive Acc: 80.469% | clf_exit: 0.164 0.367 0.468
Batch: 280 | Loss: 4.076 | Acc: 42.624,68.586,83.521,% | Adaptive Acc: 80.435% | clf_exit: 0.164 0.368 0.468
Batch: 300 | Loss: 4.080 | Acc: 42.525,68.537,83.500,% | Adaptive Acc: 80.422% | clf_exit: 0.164 0.369 0.468
Batch: 320 | Loss: 4.086 | Acc: 42.436,68.429,83.487,% | Adaptive Acc: 80.432% | clf_exit: 0.163 0.367 0.469
Batch: 340 | Loss: 4.086 | Acc: 42.527,68.399,83.378,% | Adaptive Acc: 80.398% | clf_exit: 0.164 0.366 0.470
Batch: 360 | Loss: 4.087 | Acc: 42.605,68.354,83.345,% | Adaptive Acc: 80.350% | clf_exit: 0.164 0.367 0.469
Batch: 380 | Loss: 4.090 | Acc: 42.505,68.248,83.327,% | Adaptive Acc: 80.282% | clf_exit: 0.164 0.367 0.469
Batch: 0 | Loss: 5.049 | Acc: 37.500,64.844,71.094,% | Adaptive Acc: 69.531% | clf_exit: 0.250 0.367 0.383
Batch: 20 | Loss: 5.254 | Acc: 41.332,60.454,64.881,% | Adaptive Acc: 63.281% | clf_exit: 0.227 0.381 0.391
Batch: 40 | Loss: 5.271 | Acc: 40.835,59.928,63.338,% | Adaptive Acc: 62.138% | clf_exit: 0.222 0.371 0.407
Batch: 60 | Loss: 5.267 | Acc: 40.587,60.143,62.923,% | Adaptive Acc: 61.975% | clf_exit: 0.221 0.372 0.407
Train classifier parameters

Epoch: 90
Batch: 0 | Loss: 3.954 | Acc: 43.750,68.750,85.156,% | Adaptive Acc: 82.031% | clf_exit: 0.219 0.344 0.438
Batch: 20 | Loss: 4.047 | Acc: 41.853,68.043,83.743,% | Adaptive Acc: 80.357% | clf_exit: 0.178 0.362 0.460
Batch: 40 | Loss: 4.067 | Acc: 42.778,68.502,83.479,% | Adaptive Acc: 80.659% | clf_exit: 0.172 0.367 0.461
Batch: 60 | Loss: 4.054 | Acc: 42.943,68.660,83.735,% | Adaptive Acc: 81.096% | clf_exit: 0.173 0.360 0.467
Batch: 80 | Loss: 4.080 | Acc: 42.631,68.461,83.652,% | Adaptive Acc: 80.932% | clf_exit: 0.172 0.359 0.469
Batch: 100 | Loss: 4.091 | Acc: 42.597,68.386,83.617,% | Adaptive Acc: 80.848% | clf_exit: 0.168 0.360 0.472
Batch: 120 | Loss: 4.082 | Acc: 42.672,68.543,83.897,% | Adaptive Acc: 81.089% | clf_exit: 0.170 0.359 0.471
Batch: 140 | Loss: 4.092 | Acc: 42.581,68.340,83.843,% | Adaptive Acc: 80.962% | clf_exit: 0.167 0.362 0.471
Batch: 160 | Loss: 4.077 | Acc: 42.678,68.536,83.938,% | Adaptive Acc: 81.032% | clf_exit: 0.168 0.362 0.470
Batch: 180 | Loss: 4.061 | Acc: 42.947,68.720,83.987,% | Adaptive Acc: 81.103% | clf_exit: 0.169 0.364 0.467
Batch: 200 | Loss: 4.066 | Acc: 42.945,68.758,83.889,% | Adaptive Acc: 81.005% | clf_exit: 0.169 0.364 0.468
Batch: 220 | Loss: 4.066 | Acc: 42.870,68.817,83.926,% | Adaptive Acc: 81.034% | clf_exit: 0.168 0.364 0.468
Batch: 240 | Loss: 4.063 | Acc: 42.865,68.860,83.892,% | Adaptive Acc: 80.971% | clf_exit: 0.167 0.364 0.468
Batch: 260 | Loss: 4.069 | Acc: 42.732,68.720,83.848,% | Adaptive Acc: 80.915% | clf_exit: 0.166 0.365 0.469
Batch: 280 | Loss: 4.065 | Acc: 42.802,68.708,83.841,% | Adaptive Acc: 80.877% | clf_exit: 0.167 0.365 0.468
Batch: 300 | Loss: 4.067 | Acc: 42.735,68.703,83.840,% | Adaptive Acc: 80.866% | clf_exit: 0.167 0.365 0.468
Batch: 320 | Loss: 4.060 | Acc: 42.818,68.779,83.832,% | Adaptive Acc: 80.868% | clf_exit: 0.167 0.366 0.467
Batch: 340 | Loss: 4.060 | Acc: 42.829,68.729,83.889,% | Adaptive Acc: 80.897% | clf_exit: 0.167 0.366 0.467
Batch: 360 | Loss: 4.055 | Acc: 42.917,68.748,83.864,% | Adaptive Acc: 80.869% | clf_exit: 0.167 0.366 0.467
Batch: 380 | Loss: 4.057 | Acc: 42.780,68.707,83.875,% | Adaptive Acc: 80.811% | clf_exit: 0.167 0.366 0.467
Batch: 0 | Loss: 5.077 | Acc: 35.156,64.062,67.969,% | Adaptive Acc: 66.406% | clf_exit: 0.250 0.375 0.375
Batch: 20 | Loss: 5.221 | Acc: 40.811,60.863,64.732,% | Adaptive Acc: 63.244% | clf_exit: 0.225 0.385 0.390
Batch: 40 | Loss: 5.259 | Acc: 40.777,60.385,63.243,% | Adaptive Acc: 62.195% | clf_exit: 0.221 0.373 0.405
Batch: 60 | Loss: 5.255 | Acc: 40.676,60.515,62.961,% | Adaptive Acc: 62.103% | clf_exit: 0.221 0.373 0.405
Train classifier parameters

Epoch: 91
Batch: 0 | Loss: 3.145 | Acc: 55.469,78.125,89.844,% | Adaptive Acc: 85.938% | clf_exit: 0.234 0.391 0.375
Batch: 20 | Loss: 3.909 | Acc: 44.308,69.829,84.784,% | Adaptive Acc: 82.068% | clf_exit: 0.171 0.378 0.451
Batch: 40 | Loss: 4.002 | Acc: 43.064,68.826,83.975,% | Adaptive Acc: 81.155% | clf_exit: 0.168 0.376 0.456
Batch: 60 | Loss: 3.978 | Acc: 43.391,69.173,84.426,% | Adaptive Acc: 81.416% | clf_exit: 0.171 0.375 0.455
Batch: 80 | Loss: 3.997 | Acc: 43.229,69.377,84.288,% | Adaptive Acc: 81.385% | clf_exit: 0.168 0.370 0.462
Batch: 100 | Loss: 3.997 | Acc: 43.255,69.175,84.259,% | Adaptive Acc: 81.149% | clf_exit: 0.168 0.372 0.460
Batch: 120 | Loss: 4.000 | Acc: 43.085,69.060,84.394,% | Adaptive Acc: 81.321% | clf_exit: 0.167 0.372 0.461
Batch: 140 | Loss: 3.997 | Acc: 43.124,69.121,84.430,% | Adaptive Acc: 81.267% | clf_exit: 0.167 0.373 0.460
Batch: 160 | Loss: 4.002 | Acc: 43.032,69.167,84.496,% | Adaptive Acc: 81.332% | clf_exit: 0.168 0.371 0.462
Batch: 180 | Loss: 4.005 | Acc: 43.159,69.169,84.401,% | Adaptive Acc: 81.224% | clf_exit: 0.168 0.372 0.460
Batch: 200 | Loss: 4.013 | Acc: 42.949,69.123,84.301,% | Adaptive Acc: 81.130% | clf_exit: 0.168 0.371 0.461
Batch: 220 | Loss: 4.018 | Acc: 42.813,69.100,84.202,% | Adaptive Acc: 81.056% | clf_exit: 0.168 0.370 0.462
Batch: 240 | Loss: 4.026 | Acc: 42.690,69.032,84.109,% | Adaptive Acc: 80.965% | clf_exit: 0.167 0.370 0.463
Batch: 260 | Loss: 4.022 | Acc: 42.699,69.091,84.198,% | Adaptive Acc: 81.064% | clf_exit: 0.167 0.370 0.463
Batch: 280 | Loss: 4.030 | Acc: 42.605,69.067,84.052,% | Adaptive Acc: 80.925% | clf_exit: 0.167 0.369 0.464
Batch: 300 | Loss: 4.026 | Acc: 42.618,69.041,84.064,% | Adaptive Acc: 80.941% | clf_exit: 0.167 0.369 0.465
Batch: 320 | Loss: 4.025 | Acc: 42.662,69.052,83.993,% | Adaptive Acc: 80.912% | clf_exit: 0.167 0.369 0.464
Batch: 340 | Loss: 4.025 | Acc: 42.616,69.018,84.061,% | Adaptive Acc: 80.968% | clf_exit: 0.167 0.369 0.464
Batch: 360 | Loss: 4.028 | Acc: 42.711,68.945,84.057,% | Adaptive Acc: 80.954% | clf_exit: 0.167 0.369 0.463
Batch: 380 | Loss: 4.030 | Acc: 42.682,68.906,84.067,% | Adaptive Acc: 80.940% | clf_exit: 0.167 0.369 0.464
Batch: 0 | Loss: 4.960 | Acc: 38.281,63.281,68.750,% | Adaptive Acc: 67.969% | clf_exit: 0.242 0.383 0.375
Batch: 20 | Loss: 5.193 | Acc: 41.406,60.528,64.955,% | Adaptive Acc: 63.318% | clf_exit: 0.236 0.379 0.384
Batch: 40 | Loss: 5.219 | Acc: 41.387,60.099,63.529,% | Adaptive Acc: 62.329% | clf_exit: 0.227 0.377 0.396
Batch: 60 | Loss: 5.219 | Acc: 40.907,60.374,63.204,% | Adaptive Acc: 62.269% | clf_exit: 0.226 0.376 0.398
Train classifier parameters

Epoch: 92
Batch: 0 | Loss: 4.246 | Acc: 46.094,67.188,83.594,% | Adaptive Acc: 83.594% | clf_exit: 0.156 0.367 0.477
Batch: 20 | Loss: 3.963 | Acc: 43.118,69.048,84.635,% | Adaptive Acc: 80.878% | clf_exit: 0.176 0.369 0.455
Batch: 40 | Loss: 3.978 | Acc: 42.893,68.712,84.737,% | Adaptive Acc: 81.650% | clf_exit: 0.169 0.370 0.461
Batch: 60 | Loss: 3.987 | Acc: 42.918,68.724,84.734,% | Adaptive Acc: 81.852% | clf_exit: 0.165 0.374 0.460
Batch: 80 | Loss: 3.976 | Acc: 42.814,68.740,84.905,% | Adaptive Acc: 81.935% | clf_exit: 0.166 0.372 0.462
Batch: 100 | Loss: 3.975 | Acc: 42.876,68.665,84.839,% | Adaptive Acc: 81.737% | clf_exit: 0.169 0.371 0.460
Batch: 120 | Loss: 3.983 | Acc: 42.898,68.582,84.801,% | Adaptive Acc: 81.721% | clf_exit: 0.169 0.370 0.461
Batch: 140 | Loss: 4.008 | Acc: 42.670,68.495,84.763,% | Adaptive Acc: 81.577% | clf_exit: 0.169 0.368 0.463
Batch: 160 | Loss: 4.004 | Acc: 42.619,68.692,84.749,% | Adaptive Acc: 81.570% | clf_exit: 0.169 0.368 0.463
Batch: 180 | Loss: 4.006 | Acc: 42.714,68.750,84.681,% | Adaptive Acc: 81.561% | clf_exit: 0.168 0.368 0.463
Batch: 200 | Loss: 4.021 | Acc: 42.704,68.544,84.530,% | Adaptive Acc: 81.273% | clf_exit: 0.168 0.369 0.463
Batch: 220 | Loss: 4.022 | Acc: 42.753,68.478,84.435,% | Adaptive Acc: 81.176% | clf_exit: 0.169 0.369 0.463
Batch: 240 | Loss: 4.021 | Acc: 42.739,68.604,84.443,% | Adaptive Acc: 81.244% | clf_exit: 0.169 0.369 0.462
Batch: 260 | Loss: 4.017 | Acc: 42.762,68.699,84.375,% | Adaptive Acc: 81.169% | clf_exit: 0.169 0.370 0.461
Batch: 280 | Loss: 4.017 | Acc: 42.813,68.692,84.367,% | Adaptive Acc: 81.186% | clf_exit: 0.170 0.369 0.461
Batch: 300 | Loss: 4.009 | Acc: 42.906,68.781,84.411,% | Adaptive Acc: 81.234% | clf_exit: 0.170 0.368 0.462
Batch: 320 | Loss: 4.005 | Acc: 42.901,68.891,84.453,% | Adaptive Acc: 81.267% | clf_exit: 0.170 0.368 0.461
Batch: 340 | Loss: 4.003 | Acc: 42.973,68.917,84.464,% | Adaptive Acc: 81.310% | clf_exit: 0.170 0.369 0.461
Batch: 360 | Loss: 4.006 | Acc: 42.915,68.897,84.418,% | Adaptive Acc: 81.267% | clf_exit: 0.171 0.368 0.461
Batch: 380 | Loss: 4.011 | Acc: 42.862,68.853,84.393,% | Adaptive Acc: 81.246% | clf_exit: 0.171 0.367 0.462
Batch: 0 | Loss: 5.027 | Acc: 38.281,61.719,69.531,% | Adaptive Acc: 67.188% | clf_exit: 0.250 0.383 0.367
Batch: 20 | Loss: 5.198 | Acc: 40.699,60.565,64.769,% | Adaptive Acc: 63.281% | clf_exit: 0.228 0.379 0.393
Batch: 40 | Loss: 5.220 | Acc: 40.835,60.252,63.700,% | Adaptive Acc: 62.367% | clf_exit: 0.225 0.372 0.404
Batch: 60 | Loss: 5.218 | Acc: 40.753,60.553,63.422,% | Adaptive Acc: 62.129% | clf_exit: 0.225 0.377 0.399
Train classifier parameters

Epoch: 93
Batch: 0 | Loss: 4.183 | Acc: 42.188,73.438,76.562,% | Adaptive Acc: 79.688% | clf_exit: 0.141 0.406 0.453
Batch: 20 | Loss: 4.001 | Acc: 42.783,69.271,83.891,% | Adaptive Acc: 80.878% | clf_exit: 0.168 0.383 0.449
Batch: 40 | Loss: 4.012 | Acc: 42.854,69.665,83.822,% | Adaptive Acc: 81.155% | clf_exit: 0.172 0.374 0.454
Batch: 60 | Loss: 4.031 | Acc: 42.905,69.416,83.952,% | Adaptive Acc: 80.879% | clf_exit: 0.172 0.371 0.457
Batch: 80 | Loss: 4.016 | Acc: 42.670,69.454,84.182,% | Adaptive Acc: 80.980% | clf_exit: 0.171 0.373 0.456
Batch: 100 | Loss: 4.031 | Acc: 42.659,69.183,84.112,% | Adaptive Acc: 81.064% | clf_exit: 0.167 0.371 0.461
Batch: 120 | Loss: 4.016 | Acc: 42.872,69.234,84.175,% | Adaptive Acc: 81.089% | clf_exit: 0.169 0.370 0.460
Batch: 140 | Loss: 4.011 | Acc: 42.908,69.221,84.198,% | Adaptive Acc: 81.039% | clf_exit: 0.170 0.369 0.461
Batch: 160 | Loss: 4.006 | Acc: 42.993,69.148,84.210,% | Adaptive Acc: 81.032% | clf_exit: 0.169 0.368 0.462
Batch: 180 | Loss: 3.994 | Acc: 43.236,69.203,84.310,% | Adaptive Acc: 81.146% | clf_exit: 0.170 0.370 0.460
Batch: 200 | Loss: 3.990 | Acc: 43.202,69.271,84.414,% | Adaptive Acc: 81.269% | clf_exit: 0.172 0.370 0.459
Batch: 220 | Loss: 3.993 | Acc: 43.315,69.174,84.421,% | Adaptive Acc: 81.299% | clf_exit: 0.171 0.370 0.459
Batch: 240 | Loss: 3.991 | Acc: 43.374,69.168,84.466,% | Adaptive Acc: 81.347% | clf_exit: 0.172 0.370 0.458
Batch: 260 | Loss: 4.000 | Acc: 43.277,69.052,84.438,% | Adaptive Acc: 81.316% | clf_exit: 0.172 0.369 0.459
Batch: 280 | Loss: 3.999 | Acc: 43.227,69.028,84.467,% | Adaptive Acc: 81.308% | clf_exit: 0.171 0.370 0.459
Batch: 300 | Loss: 3.996 | Acc: 43.293,69.103,84.463,% | Adaptive Acc: 81.359% | clf_exit: 0.171 0.370 0.460
Batch: 320 | Loss: 3.998 | Acc: 43.178,69.035,84.453,% | Adaptive Acc: 81.311% | clf_exit: 0.171 0.369 0.460
Batch: 340 | Loss: 3.992 | Acc: 43.228,69.114,84.494,% | Adaptive Acc: 81.346% | clf_exit: 0.171 0.369 0.460
Batch: 360 | Loss: 3.995 | Acc: 43.209,69.070,84.529,% | Adaptive Acc: 81.345% | clf_exit: 0.171 0.370 0.459
Batch: 380 | Loss: 3.997 | Acc: 43.176,69.076,84.533,% | Adaptive Acc: 81.336% | clf_exit: 0.171 0.369 0.460
Batch: 0 | Loss: 4.997 | Acc: 40.625,64.062,67.969,% | Adaptive Acc: 67.188% | clf_exit: 0.242 0.375 0.383
Batch: 20 | Loss: 5.195 | Acc: 41.257,60.975,65.179,% | Adaptive Acc: 63.393% | clf_exit: 0.239 0.374 0.387
Batch: 40 | Loss: 5.219 | Acc: 40.816,60.328,63.662,% | Adaptive Acc: 62.405% | clf_exit: 0.233 0.370 0.397
Batch: 60 | Loss: 5.213 | Acc: 40.740,60.553,63.473,% | Adaptive Acc: 62.257% | clf_exit: 0.232 0.372 0.396
Train classifier parameters

Epoch: 94
Batch: 0 | Loss: 3.749 | Acc: 42.188,70.312,86.719,% | Adaptive Acc: 81.250% | clf_exit: 0.234 0.344 0.422
Batch: 20 | Loss: 3.968 | Acc: 43.192,69.457,84.189,% | Adaptive Acc: 81.213% | clf_exit: 0.174 0.371 0.455
Batch: 40 | Loss: 4.000 | Acc: 42.569,69.017,84.280,% | Adaptive Acc: 81.117% | clf_exit: 0.174 0.371 0.455
Batch: 60 | Loss: 4.007 | Acc: 42.572,68.827,84.298,% | Adaptive Acc: 81.084% | clf_exit: 0.169 0.374 0.458
Batch: 80 | Loss: 3.982 | Acc: 42.882,69.367,84.558,% | Adaptive Acc: 81.375% | clf_exit: 0.170 0.376 0.454
Batch: 100 | Loss: 3.975 | Acc: 43.046,69.407,84.506,% | Adaptive Acc: 81.374% | clf_exit: 0.170 0.375 0.454
Batch: 120 | Loss: 3.965 | Acc: 43.253,69.596,84.607,% | Adaptive Acc: 81.444% | clf_exit: 0.172 0.375 0.453
Batch: 140 | Loss: 3.971 | Acc: 43.296,69.620,84.563,% | Adaptive Acc: 81.383% | clf_exit: 0.171 0.375 0.454
Batch: 160 | Loss: 3.980 | Acc: 43.333,69.449,84.545,% | Adaptive Acc: 81.323% | clf_exit: 0.173 0.373 0.454
Batch: 180 | Loss: 3.974 | Acc: 43.284,69.462,84.681,% | Adaptive Acc: 81.436% | clf_exit: 0.173 0.371 0.456
Batch: 200 | Loss: 3.975 | Acc: 43.291,69.341,84.725,% | Adaptive Acc: 81.479% | clf_exit: 0.175 0.369 0.456
Batch: 220 | Loss: 3.978 | Acc: 43.209,69.210,84.782,% | Adaptive Acc: 81.448% | clf_exit: 0.174 0.368 0.457
Batch: 240 | Loss: 3.976 | Acc: 43.134,69.256,84.796,% | Adaptive Acc: 81.477% | clf_exit: 0.174 0.369 0.457
Batch: 260 | Loss: 3.980 | Acc: 43.026,69.163,84.791,% | Adaptive Acc: 81.454% | clf_exit: 0.174 0.368 0.458
Batch: 280 | Loss: 3.974 | Acc: 43.060,69.270,84.786,% | Adaptive Acc: 81.495% | clf_exit: 0.173 0.369 0.458
Batch: 300 | Loss: 3.977 | Acc: 43.018,69.196,84.795,% | Adaptive Acc: 81.502% | clf_exit: 0.173 0.368 0.458
Batch: 320 | Loss: 3.976 | Acc: 43.013,69.191,84.791,% | Adaptive Acc: 81.518% | clf_exit: 0.173 0.368 0.459
Batch: 340 | Loss: 3.981 | Acc: 43.044,69.156,84.746,% | Adaptive Acc: 81.433% | clf_exit: 0.173 0.368 0.458
Batch: 360 | Loss: 3.976 | Acc: 43.125,69.207,84.784,% | Adaptive Acc: 81.471% | clf_exit: 0.173 0.369 0.457
Batch: 380 | Loss: 3.972 | Acc: 43.139,69.218,84.804,% | Adaptive Acc: 81.496% | clf_exit: 0.174 0.369 0.457
Batch: 0 | Loss: 5.012 | Acc: 39.062,63.281,69.531,% | Adaptive Acc: 67.969% | clf_exit: 0.258 0.367 0.375
Batch: 20 | Loss: 5.173 | Acc: 40.923,61.161,65.327,% | Adaptive Acc: 63.318% | clf_exit: 0.235 0.379 0.385
Batch: 40 | Loss: 5.206 | Acc: 41.006,60.518,63.986,% | Adaptive Acc: 62.290% | clf_exit: 0.231 0.376 0.393
Batch: 60 | Loss: 5.201 | Acc: 40.920,60.707,63.665,% | Adaptive Acc: 62.077% | clf_exit: 0.228 0.376 0.395
Train classifier parameters

Epoch: 95
Batch: 0 | Loss: 3.563 | Acc: 47.656,73.438,86.719,% | Adaptive Acc: 83.594% | clf_exit: 0.188 0.453 0.359
Batch: 20 | Loss: 3.930 | Acc: 44.680,70.052,84.598,% | Adaptive Acc: 81.882% | clf_exit: 0.180 0.368 0.452
Batch: 40 | Loss: 4.021 | Acc: 42.797,69.264,84.089,% | Adaptive Acc: 81.117% | clf_exit: 0.176 0.367 0.457
Batch: 60 | Loss: 4.019 | Acc: 42.559,68.878,84.503,% | Adaptive Acc: 81.673% | clf_exit: 0.176 0.364 0.460
Batch: 80 | Loss: 4.016 | Acc: 42.448,68.846,84.423,% | Adaptive Acc: 81.375% | clf_exit: 0.175 0.367 0.458
Batch: 100 | Loss: 3.998 | Acc: 42.659,69.075,84.476,% | Adaptive Acc: 81.459% | clf_exit: 0.175 0.367 0.458
Batch: 120 | Loss: 3.991 | Acc: 42.794,69.196,84.446,% | Adaptive Acc: 81.411% | clf_exit: 0.174 0.368 0.458
Batch: 140 | Loss: 3.981 | Acc: 42.919,69.354,84.597,% | Adaptive Acc: 81.566% | clf_exit: 0.175 0.368 0.457
Batch: 160 | Loss: 3.976 | Acc: 42.925,69.337,84.705,% | Adaptive Acc: 81.701% | clf_exit: 0.175 0.367 0.459
Batch: 180 | Loss: 3.969 | Acc: 42.951,69.531,84.833,% | Adaptive Acc: 81.794% | clf_exit: 0.175 0.369 0.456
Batch: 200 | Loss: 3.968 | Acc: 43.012,69.578,84.834,% | Adaptive Acc: 81.732% | clf_exit: 0.175 0.370 0.456
Batch: 220 | Loss: 3.970 | Acc: 43.039,69.521,84.845,% | Adaptive Acc: 81.741% | clf_exit: 0.174 0.369 0.457
Batch: 240 | Loss: 3.969 | Acc: 43.166,69.622,84.793,% | Adaptive Acc: 81.714% | clf_exit: 0.174 0.369 0.457
Batch: 260 | Loss: 3.962 | Acc: 43.364,69.618,84.860,% | Adaptive Acc: 81.720% | clf_exit: 0.175 0.369 0.457
Batch: 280 | Loss: 3.962 | Acc: 43.355,69.553,84.873,% | Adaptive Acc: 81.728% | clf_exit: 0.174 0.368 0.457
Batch: 300 | Loss: 3.959 | Acc: 43.278,69.614,85.001,% | Adaptive Acc: 81.779% | clf_exit: 0.175 0.368 0.457
Batch: 320 | Loss: 3.963 | Acc: 43.232,69.473,84.927,% | Adaptive Acc: 81.708% | clf_exit: 0.174 0.368 0.458
Batch: 340 | Loss: 3.968 | Acc: 43.152,69.474,84.916,% | Adaptive Acc: 81.708% | clf_exit: 0.174 0.368 0.458
Batch: 360 | Loss: 3.969 | Acc: 43.211,69.577,84.847,% | Adaptive Acc: 81.627% | clf_exit: 0.174 0.369 0.457
Batch: 380 | Loss: 3.967 | Acc: 43.217,69.611,84.912,% | Adaptive Acc: 81.679% | clf_exit: 0.174 0.369 0.457
Batch: 0 | Loss: 4.982 | Acc: 41.406,62.500,68.750,% | Adaptive Acc: 65.625% | clf_exit: 0.250 0.375 0.375
Batch: 20 | Loss: 5.179 | Acc: 41.704,60.975,65.141,% | Adaptive Acc: 63.058% | clf_exit: 0.244 0.380 0.376
Batch: 40 | Loss: 5.210 | Acc: 41.292,60.728,63.910,% | Adaptive Acc: 62.290% | clf_exit: 0.236 0.376 0.388
Batch: 60 | Loss: 5.202 | Acc: 41.073,60.873,63.730,% | Adaptive Acc: 62.244% | clf_exit: 0.234 0.376 0.390
Train classifier parameters

Epoch: 96
Batch: 0 | Loss: 4.060 | Acc: 44.531,64.844,80.469,% | Adaptive Acc: 78.125% | clf_exit: 0.180 0.336 0.484
Batch: 20 | Loss: 3.991 | Acc: 42.671,69.010,85.528,% | Adaptive Acc: 82.180% | clf_exit: 0.169 0.358 0.474
Batch: 40 | Loss: 3.925 | Acc: 43.369,70.103,85.556,% | Adaptive Acc: 82.470% | clf_exit: 0.167 0.369 0.465
Batch: 60 | Loss: 3.895 | Acc: 43.519,70.364,85.707,% | Adaptive Acc: 82.480% | clf_exit: 0.168 0.376 0.456
Batch: 80 | Loss: 3.880 | Acc: 43.818,70.226,85.754,% | Adaptive Acc: 82.591% | clf_exit: 0.172 0.375 0.453
Batch: 100 | Loss: 3.912 | Acc: 43.325,69.864,85.226,% | Adaptive Acc: 82.000% | clf_exit: 0.171 0.374 0.456
Batch: 120 | Loss: 3.932 | Acc: 43.253,69.576,85.285,% | Adaptive Acc: 81.980% | clf_exit: 0.170 0.374 0.456
Batch: 140 | Loss: 3.939 | Acc: 43.390,69.792,85.134,% | Adaptive Acc: 81.909% | clf_exit: 0.170 0.374 0.456
Batch: 160 | Loss: 3.948 | Acc: 43.279,69.803,85.049,% | Adaptive Acc: 81.891% | clf_exit: 0.171 0.373 0.456
Batch: 180 | Loss: 3.953 | Acc: 43.362,69.665,85.070,% | Adaptive Acc: 81.785% | clf_exit: 0.171 0.373 0.456
Batch: 200 | Loss: 3.956 | Acc: 43.404,69.617,84.981,% | Adaptive Acc: 81.728% | clf_exit: 0.171 0.372 0.457
Batch: 220 | Loss: 3.949 | Acc: 43.492,69.719,85.064,% | Adaptive Acc: 81.851% | clf_exit: 0.171 0.373 0.456
Batch: 240 | Loss: 3.954 | Acc: 43.335,69.616,85.088,% | Adaptive Acc: 81.811% | clf_exit: 0.172 0.373 0.455
Batch: 260 | Loss: 3.951 | Acc: 43.394,69.657,85.213,% | Adaptive Acc: 81.959% | clf_exit: 0.172 0.373 0.455
Batch: 280 | Loss: 3.948 | Acc: 43.364,69.698,85.240,% | Adaptive Acc: 81.976% | clf_exit: 0.171 0.375 0.454
Batch: 300 | Loss: 3.949 | Acc: 43.353,69.651,85.174,% | Adaptive Acc: 81.863% | clf_exit: 0.172 0.374 0.454
Batch: 320 | Loss: 3.946 | Acc: 43.409,69.697,85.198,% | Adaptive Acc: 81.893% | clf_exit: 0.172 0.373 0.455
Batch: 340 | Loss: 3.945 | Acc: 43.429,69.715,85.204,% | Adaptive Acc: 81.903% | clf_exit: 0.173 0.373 0.454
Batch: 360 | Loss: 3.948 | Acc: 43.358,69.730,85.165,% | Adaptive Acc: 81.869% | clf_exit: 0.173 0.372 0.454
Batch: 380 | Loss: 3.950 | Acc: 43.323,69.724,85.140,% | Adaptive Acc: 81.832% | clf_exit: 0.173 0.372 0.455
Batch: 0 | Loss: 4.986 | Acc: 39.844,62.500,67.969,% | Adaptive Acc: 65.625% | clf_exit: 0.266 0.375 0.359
Batch: 20 | Loss: 5.162 | Acc: 40.625,61.049,65.253,% | Adaptive Acc: 63.170% | clf_exit: 0.236 0.381 0.383
Batch: 40 | Loss: 5.187 | Acc: 40.873,60.728,63.872,% | Adaptive Acc: 62.424% | clf_exit: 0.231 0.374 0.396
Batch: 60 | Loss: 5.183 | Acc: 40.715,60.925,63.665,% | Adaptive Acc: 62.295% | clf_exit: 0.230 0.376 0.394
Train classifier parameters

Epoch: 97
Batch: 0 | Loss: 3.740 | Acc: 50.000,65.625,84.375,% | Adaptive Acc: 78.906% | clf_exit: 0.250 0.320 0.430
Batch: 20 | Loss: 3.945 | Acc: 43.192,69.680,85.417,% | Adaptive Acc: 82.292% | clf_exit: 0.192 0.347 0.461
Batch: 40 | Loss: 3.938 | Acc: 43.178,69.474,85.290,% | Adaptive Acc: 82.165% | clf_exit: 0.184 0.351 0.464
Batch: 60 | Loss: 3.980 | Acc: 42.802,68.814,84.849,% | Adaptive Acc: 81.596% | clf_exit: 0.176 0.360 0.464
Batch: 80 | Loss: 3.984 | Acc: 43.027,68.953,84.674,% | Adaptive Acc: 81.491% | clf_exit: 0.176 0.361 0.463
Batch: 100 | Loss: 3.957 | Acc: 43.479,69.322,84.955,% | Adaptive Acc: 81.791% | clf_exit: 0.177 0.364 0.459
Batch: 120 | Loss: 3.957 | Acc: 43.543,69.473,85.066,% | Adaptive Acc: 81.831% | clf_exit: 0.176 0.366 0.458
Batch: 140 | Loss: 3.952 | Acc: 43.528,69.504,85.117,% | Adaptive Acc: 81.865% | clf_exit: 0.176 0.366 0.458
Batch: 160 | Loss: 3.940 | Acc: 43.619,69.788,85.059,% | Adaptive Acc: 81.910% | clf_exit: 0.176 0.368 0.457
Batch: 180 | Loss: 3.945 | Acc: 43.504,69.738,85.156,% | Adaptive Acc: 81.954% | clf_exit: 0.175 0.368 0.456
Batch: 200 | Loss: 3.947 | Acc: 43.369,69.652,85.102,% | Adaptive Acc: 81.915% | clf_exit: 0.174 0.370 0.456
Batch: 220 | Loss: 3.956 | Acc: 43.319,69.602,85.057,% | Adaptive Acc: 81.869% | clf_exit: 0.173 0.369 0.458
Batch: 240 | Loss: 3.955 | Acc: 43.432,69.609,85.059,% | Adaptive Acc: 81.866% | clf_exit: 0.172 0.371 0.456
Batch: 260 | Loss: 3.955 | Acc: 43.412,69.573,84.977,% | Adaptive Acc: 81.792% | clf_exit: 0.172 0.371 0.456
Batch: 280 | Loss: 3.949 | Acc: 43.439,69.601,84.962,% | Adaptive Acc: 81.806% | clf_exit: 0.173 0.372 0.456
Batch: 300 | Loss: 3.942 | Acc: 43.444,69.609,84.985,% | Adaptive Acc: 81.824% | clf_exit: 0.173 0.372 0.455
Batch: 320 | Loss: 3.944 | Acc: 43.407,69.558,84.993,% | Adaptive Acc: 81.793% | clf_exit: 0.174 0.371 0.456
Batch: 340 | Loss: 3.941 | Acc: 43.425,69.623,85.005,% | Adaptive Acc: 81.816% | clf_exit: 0.173 0.371 0.456
Batch: 360 | Loss: 3.946 | Acc: 43.343,69.590,85.057,% | Adaptive Acc: 81.871% | clf_exit: 0.173 0.372 0.456
Batch: 380 | Loss: 3.945 | Acc: 43.338,69.597,85.076,% | Adaptive Acc: 81.916% | clf_exit: 0.173 0.371 0.456
Batch: 0 | Loss: 4.994 | Acc: 40.625,62.500,67.969,% | Adaptive Acc: 64.844% | clf_exit: 0.266 0.359 0.375
Batch: 20 | Loss: 5.163 | Acc: 40.848,61.012,65.253,% | Adaptive Acc: 63.356% | clf_exit: 0.246 0.376 0.378
Batch: 40 | Loss: 5.186 | Acc: 40.949,60.728,63.910,% | Adaptive Acc: 62.367% | clf_exit: 0.239 0.373 0.388
Batch: 60 | Loss: 5.179 | Acc: 40.958,61.002,63.614,% | Adaptive Acc: 62.372% | clf_exit: 0.238 0.374 0.388
Train classifier parameters

Epoch: 98
Batch: 0 | Loss: 3.508 | Acc: 45.312,72.656,89.844,% | Adaptive Acc: 85.156% | clf_exit: 0.203 0.383 0.414
Batch: 20 | Loss: 3.913 | Acc: 43.713,69.420,84.970,% | Adaptive Acc: 81.548% | clf_exit: 0.178 0.363 0.459
Batch: 40 | Loss: 3.915 | Acc: 43.255,69.055,85.271,% | Adaptive Acc: 82.069% | clf_exit: 0.180 0.364 0.456
Batch: 60 | Loss: 3.897 | Acc: 43.571,69.800,85.528,% | Adaptive Acc: 82.275% | clf_exit: 0.180 0.366 0.454
Batch: 80 | Loss: 3.924 | Acc: 43.441,69.483,85.262,% | Adaptive Acc: 82.051% | clf_exit: 0.179 0.370 0.451
Batch: 100 | Loss: 3.930 | Acc: 43.402,69.500,85.195,% | Adaptive Acc: 82.186% | clf_exit: 0.177 0.372 0.452
Batch: 120 | Loss: 3.929 | Acc: 43.285,69.447,85.544,% | Adaptive Acc: 82.238% | clf_exit: 0.177 0.373 0.450
Batch: 140 | Loss: 3.930 | Acc: 43.268,69.520,85.483,% | Adaptive Acc: 82.142% | clf_exit: 0.176 0.374 0.450
Batch: 160 | Loss: 3.931 | Acc: 43.405,69.468,85.428,% | Adaptive Acc: 82.225% | clf_exit: 0.175 0.376 0.449
Batch: 180 | Loss: 3.932 | Acc: 43.452,69.436,85.394,% | Adaptive Acc: 82.230% | clf_exit: 0.174 0.374 0.451
Batch: 200 | Loss: 3.937 | Acc: 43.198,69.372,85.358,% | Adaptive Acc: 82.090% | clf_exit: 0.177 0.372 0.451
Batch: 220 | Loss: 3.934 | Acc: 43.241,69.425,85.464,% | Adaptive Acc: 82.144% | clf_exit: 0.177 0.373 0.450
Batch: 240 | Loss: 3.934 | Acc: 43.147,69.437,85.529,% | Adaptive Acc: 82.265% | clf_exit: 0.176 0.374 0.451
Batch: 260 | Loss: 3.932 | Acc: 43.160,69.552,85.465,% | Adaptive Acc: 82.280% | clf_exit: 0.175 0.374 0.451
Batch: 280 | Loss: 3.937 | Acc: 43.130,69.503,85.426,% | Adaptive Acc: 82.231% | clf_exit: 0.175 0.374 0.451
Batch: 300 | Loss: 3.943 | Acc: 43.005,69.521,85.356,% | Adaptive Acc: 82.171% | clf_exit: 0.174 0.373 0.453
Batch: 320 | Loss: 3.941 | Acc: 43.051,69.529,85.417,% | Adaptive Acc: 82.199% | clf_exit: 0.174 0.373 0.452
Batch: 340 | Loss: 3.941 | Acc: 43.074,69.561,85.381,% | Adaptive Acc: 82.212% | clf_exit: 0.174 0.373 0.453
Batch: 360 | Loss: 3.942 | Acc: 43.099,69.494,85.282,% | Adaptive Acc: 82.133% | clf_exit: 0.175 0.373 0.453
Batch: 380 | Loss: 3.938 | Acc: 43.100,69.568,85.263,% | Adaptive Acc: 82.087% | clf_exit: 0.175 0.373 0.452
Batch: 0 | Loss: 4.919 | Acc: 40.625,62.500,66.406,% | Adaptive Acc: 66.406% | clf_exit: 0.250 0.359 0.391
Batch: 20 | Loss: 5.145 | Acc: 41.592,60.938,65.625,% | Adaptive Acc: 63.876% | clf_exit: 0.243 0.373 0.384
Batch: 40 | Loss: 5.169 | Acc: 41.349,60.690,64.215,% | Adaptive Acc: 62.671% | clf_exit: 0.236 0.376 0.388
Batch: 60 | Loss: 5.164 | Acc: 41.265,60.899,63.794,% | Adaptive Acc: 62.423% | clf_exit: 0.236 0.376 0.388
Train classifier parameters

Epoch: 99
Batch: 0 | Loss: 4.170 | Acc: 42.188,68.750,87.500,% | Adaptive Acc: 85.156% | clf_exit: 0.164 0.352 0.484
Batch: 20 | Loss: 3.909 | Acc: 42.857,69.866,84.970,% | Adaptive Acc: 82.515% | clf_exit: 0.179 0.369 0.452
Batch: 40 | Loss: 3.909 | Acc: 43.197,70.046,84.604,% | Adaptive Acc: 81.879% | clf_exit: 0.173 0.375 0.451
Batch: 60 | Loss: 3.927 | Acc: 43.174,70.095,84.862,% | Adaptive Acc: 81.929% | clf_exit: 0.175 0.372 0.453
Batch: 80 | Loss: 3.910 | Acc: 43.403,70.351,85.224,% | Adaptive Acc: 82.253% | clf_exit: 0.176 0.375 0.449
Batch: 100 | Loss: 3.919 | Acc: 43.278,70.026,85.381,% | Adaptive Acc: 82.225% | clf_exit: 0.173 0.374 0.452
Batch: 120 | Loss: 3.918 | Acc: 43.363,69.977,85.498,% | Adaptive Acc: 82.367% | clf_exit: 0.173 0.376 0.451
Batch: 140 | Loss: 3.925 | Acc: 43.479,69.914,85.361,% | Adaptive Acc: 82.225% | clf_exit: 0.174 0.374 0.453
Batch: 160 | Loss: 3.919 | Acc: 43.546,69.881,85.409,% | Adaptive Acc: 82.216% | clf_exit: 0.174 0.374 0.452
Batch: 180 | Loss: 3.926 | Acc: 43.482,69.829,85.372,% | Adaptive Acc: 82.256% | clf_exit: 0.174 0.373 0.453
Batch: 200 | Loss: 3.929 | Acc: 43.408,69.706,85.452,% | Adaptive Acc: 82.253% | clf_exit: 0.175 0.372 0.453
Batch: 220 | Loss: 3.926 | Acc: 43.506,69.697,85.428,% | Adaptive Acc: 82.201% | clf_exit: 0.175 0.372 0.453
Batch: 240 | Loss: 3.930 | Acc: 43.468,69.680,85.458,% | Adaptive Acc: 82.193% | clf_exit: 0.176 0.372 0.453
Batch: 260 | Loss: 3.924 | Acc: 43.603,69.669,85.554,% | Adaptive Acc: 82.271% | clf_exit: 0.176 0.371 0.452
Batch: 280 | Loss: 3.918 | Acc: 43.558,69.720,85.551,% | Adaptive Acc: 82.284% | clf_exit: 0.177 0.371 0.452
Batch: 300 | Loss: 3.920 | Acc: 43.540,69.653,85.569,% | Adaptive Acc: 82.265% | clf_exit: 0.177 0.372 0.452
Batch: 320 | Loss: 3.922 | Acc: 43.538,69.641,85.580,% | Adaptive Acc: 82.311% | clf_exit: 0.176 0.372 0.452
Batch: 340 | Loss: 3.918 | Acc: 43.477,69.664,85.573,% | Adaptive Acc: 82.283% | clf_exit: 0.176 0.373 0.451
Batch: 360 | Loss: 3.915 | Acc: 43.475,69.676,85.585,% | Adaptive Acc: 82.332% | clf_exit: 0.176 0.374 0.451
Batch: 380 | Loss: 3.918 | Acc: 43.444,69.640,85.577,% | Adaptive Acc: 82.310% | clf_exit: 0.175 0.373 0.452
Batch: 0 | Loss: 4.983 | Acc: 37.500,62.500,67.969,% | Adaptive Acc: 66.406% | clf_exit: 0.266 0.359 0.375
Batch: 20 | Loss: 5.154 | Acc: 41.443,61.272,65.290,% | Adaptive Acc: 63.690% | clf_exit: 0.248 0.369 0.383
Batch: 40 | Loss: 5.186 | Acc: 41.254,60.804,63.929,% | Adaptive Acc: 62.691% | clf_exit: 0.240 0.372 0.388
Batch: 60 | Loss: 5.175 | Acc: 41.163,61.002,63.665,% | Adaptive Acc: 62.602% | clf_exit: 0.239 0.371 0.390
Train all parameters

Epoch: 100
Batch: 0 | Loss: 3.728 | Acc: 42.969,67.188,88.281,% | Adaptive Acc: 82.031% | clf_exit: 0.227 0.375 0.398
Batch: 20 | Loss: 4.415 | Acc: 39.621,65.551,77.158,% | Adaptive Acc: 74.628% | clf_exit: 0.186 0.360 0.455
Batch: 40 | Loss: 5.225 | Acc: 36.604,60.023,64.577,% | Adaptive Acc: 64.291% | clf_exit: 0.162 0.336 0.502
Batch: 60 | Loss: 5.559 | Acc: 35.579,57.057,59.337,% | Adaptive Acc: 59.964% | clf_exit: 0.152 0.320 0.528
Batch: 80 | Loss: 5.637 | Acc: 35.639,56.173,57.976,% | Adaptive Acc: 58.835% | clf_exit: 0.149 0.317 0.534
Batch: 100 | Loss: 5.674 | Acc: 35.528,55.786,57.611,% | Adaptive Acc: 58.338% | clf_exit: 0.146 0.312 0.542
Batch: 120 | Loss: 5.674 | Acc: 35.757,55.811,57.638,% | Adaptive Acc: 58.135% | clf_exit: 0.147 0.309 0.544
Batch: 140 | Loss: 5.648 | Acc: 35.954,56.012,58.062,% | Adaptive Acc: 58.378% | clf_exit: 0.148 0.308 0.543
Batch: 160 | Loss: 5.615 | Acc: 36.166,56.294,58.468,% | Adaptive Acc: 58.778% | clf_exit: 0.148 0.309 0.543
Batch: 180 | Loss: 5.580 | Acc: 36.412,56.617,58.745,% | Adaptive Acc: 59.047% | clf_exit: 0.151 0.308 0.541
Batch: 200 | Loss: 5.538 | Acc: 36.738,56.899,59.142,% | Adaptive Acc: 59.363% | clf_exit: 0.151 0.310 0.539
Batch: 220 | Loss: 5.513 | Acc: 36.860,57.102,59.608,% | Adaptive Acc: 59.668% | clf_exit: 0.152 0.311 0.537
Batch: 240 | Loss: 5.500 | Acc: 36.920,57.180,59.819,% | Adaptive Acc: 59.806% | clf_exit: 0.152 0.312 0.536
Batch: 260 | Loss: 5.480 | Acc: 37.042,57.352,60.243,% | Adaptive Acc: 60.111% | clf_exit: 0.153 0.312 0.535
Batch: 280 | Loss: 5.460 | Acc: 37.027,57.537,60.626,% | Adaptive Acc: 60.345% | clf_exit: 0.153 0.313 0.535
Batch: 300 | Loss: 5.437 | Acc: 37.160,57.602,60.935,% | Adaptive Acc: 60.605% | clf_exit: 0.153 0.314 0.533
Batch: 320 | Loss: 5.417 | Acc: 37.356,57.883,61.203,% | Adaptive Acc: 60.860% | clf_exit: 0.153 0.315 0.532
Batch: 340 | Loss: 5.402 | Acc: 37.385,57.948,61.423,% | Adaptive Acc: 61.018% | clf_exit: 0.155 0.314 0.530
Batch: 360 | Loss: 5.380 | Acc: 37.455,58.053,61.665,% | Adaptive Acc: 61.240% | clf_exit: 0.156 0.316 0.528
Batch: 380 | Loss: 5.364 | Acc: 37.504,58.153,61.877,% | Adaptive Acc: 61.434% | clf_exit: 0.157 0.316 0.528
Batch: 0 | Loss: 5.547 | Acc: 34.375,56.250,64.062,% | Adaptive Acc: 62.500% | clf_exit: 0.188 0.398 0.414
Batch: 20 | Loss: 6.095 | Acc: 34.487,52.158,58.668,% | Adaptive Acc: 56.510% | clf_exit: 0.175 0.385 0.440
Batch: 40 | Loss: 6.049 | Acc: 34.489,52.229,58.537,% | Adaptive Acc: 56.460% | clf_exit: 0.173 0.377 0.450
Batch: 60 | Loss: 6.121 | Acc: 33.632,52.049,57.710,% | Adaptive Acc: 55.789% | clf_exit: 0.172 0.378 0.450
Train all parameters

Epoch: 101
Batch: 0 | Loss: 4.660 | Acc: 35.938,64.062,73.438,% | Adaptive Acc: 71.094% | clf_exit: 0.188 0.359 0.453
Batch: 20 | Loss: 4.650 | Acc: 40.513,63.170,73.438,% | Adaptive Acc: 71.503% | clf_exit: 0.169 0.334 0.497
Batch: 40 | Loss: 4.628 | Acc: 40.549,63.510,73.685,% | Adaptive Acc: 71.551% | clf_exit: 0.171 0.336 0.493
Batch: 60 | Loss: 4.633 | Acc: 40.625,63.525,73.284,% | Adaptive Acc: 71.235% | clf_exit: 0.172 0.337 0.491
Batch: 80 | Loss: 4.623 | Acc: 40.529,63.976,72.926,% | Adaptive Acc: 71.132% | clf_exit: 0.168 0.345 0.487
Batch: 100 | Loss: 4.634 | Acc: 40.610,63.869,72.540,% | Adaptive Acc: 70.893% | clf_exit: 0.167 0.344 0.489
Batch: 120 | Loss: 4.628 | Acc: 40.754,63.811,72.669,% | Adaptive Acc: 70.990% | clf_exit: 0.169 0.343 0.488
Batch: 140 | Loss: 4.648 | Acc: 40.464,63.486,72.418,% | Adaptive Acc: 70.739% | clf_exit: 0.168 0.343 0.489
Batch: 160 | Loss: 4.663 | Acc: 40.586,63.456,72.079,% | Adaptive Acc: 70.477% | clf_exit: 0.168 0.341 0.491
Batch: 180 | Loss: 4.671 | Acc: 40.375,63.376,72.074,% | Adaptive Acc: 70.412% | clf_exit: 0.167 0.341 0.492
Batch: 200 | Loss: 4.690 | Acc: 40.120,63.207,71.859,% | Adaptive Acc: 70.153% | clf_exit: 0.167 0.341 0.492
Batch: 220 | Loss: 4.695 | Acc: 40.119,63.066,71.677,% | Adaptive Acc: 69.970% | clf_exit: 0.166 0.341 0.493
Batch: 240 | Loss: 4.689 | Acc: 40.207,63.061,71.719,% | Adaptive Acc: 70.069% | clf_exit: 0.167 0.343 0.490
Batch: 260 | Loss: 4.680 | Acc: 40.296,63.179,71.752,% | Adaptive Acc: 70.130% | clf_exit: 0.168 0.343 0.488
Batch: 280 | Loss: 4.682 | Acc: 40.280,63.148,71.717,% | Adaptive Acc: 70.087% | clf_exit: 0.169 0.343 0.488
Batch: 300 | Loss: 4.675 | Acc: 40.308,63.211,71.844,% | Adaptive Acc: 70.203% | clf_exit: 0.168 0.344 0.488
Batch: 320 | Loss: 4.678 | Acc: 40.272,63.264,71.819,% | Adaptive Acc: 70.181% | clf_exit: 0.168 0.344 0.488
Batch: 340 | Loss: 4.675 | Acc: 40.274,63.313,71.822,% | Adaptive Acc: 70.198% | clf_exit: 0.167 0.345 0.488
Batch: 360 | Loss: 4.686 | Acc: 40.138,63.286,71.734,% | Adaptive Acc: 70.079% | clf_exit: 0.167 0.344 0.488
Batch: 380 | Loss: 4.681 | Acc: 40.221,63.337,71.781,% | Adaptive Acc: 70.099% | clf_exit: 0.168 0.345 0.487
Batch: 0 | Loss: 4.974 | Acc: 42.188,59.375,63.281,% | Adaptive Acc: 60.938% | clf_exit: 0.281 0.375 0.344
Batch: 20 | Loss: 5.415 | Acc: 38.616,58.482,64.025,% | Adaptive Acc: 59.673% | clf_exit: 0.269 0.347 0.384
Batch: 40 | Loss: 5.385 | Acc: 38.472,57.908,63.224,% | Adaptive Acc: 59.623% | clf_exit: 0.262 0.348 0.390
Batch: 60 | Loss: 5.378 | Acc: 38.307,57.877,63.025,% | Adaptive Acc: 59.734% | clf_exit: 0.265 0.344 0.391
Train all parameters

Epoch: 102
Batch: 0 | Loss: 4.469 | Acc: 44.531,70.312,78.125,% | Adaptive Acc: 78.906% | clf_exit: 0.141 0.391 0.469
Batch: 20 | Loss: 4.439 | Acc: 41.555,64.844,77.158,% | Adaptive Acc: 74.554% | clf_exit: 0.164 0.365 0.471
Batch: 40 | Loss: 4.451 | Acc: 41.540,65.244,76.505,% | Adaptive Acc: 73.800% | clf_exit: 0.169 0.362 0.469
Batch: 60 | Loss: 4.434 | Acc: 41.457,65.292,76.767,% | Adaptive Acc: 73.988% | clf_exit: 0.169 0.358 0.472
Batch: 80 | Loss: 4.411 | Acc: 41.512,65.529,76.775,% | Adaptive Acc: 74.055% | clf_exit: 0.172 0.356 0.471
Batch: 100 | Loss: 4.381 | Acc: 41.662,65.702,76.965,% | Adaptive Acc: 74.180% | clf_exit: 0.176 0.355 0.469
Batch: 120 | Loss: 4.383 | Acc: 41.626,65.657,76.601,% | Adaptive Acc: 73.915% | clf_exit: 0.174 0.356 0.470
Batch: 140 | Loss: 4.386 | Acc: 41.467,65.719,76.762,% | Adaptive Acc: 74.102% | clf_exit: 0.174 0.357 0.469
Batch: 160 | Loss: 4.378 | Acc: 41.460,65.775,76.698,% | Adaptive Acc: 74.020% | clf_exit: 0.176 0.356 0.468
Batch: 180 | Loss: 4.370 | Acc: 41.592,65.806,76.705,% | Adaptive Acc: 74.012% | clf_exit: 0.176 0.356 0.468
Batch: 200 | Loss: 4.379 | Acc: 41.484,65.508,76.539,% | Adaptive Acc: 73.818% | clf_exit: 0.177 0.356 0.467
Batch: 220 | Loss: 4.388 | Acc: 41.438,65.544,76.428,% | Adaptive Acc: 73.805% | clf_exit: 0.176 0.356 0.469
Batch: 240 | Loss: 4.397 | Acc: 41.413,65.518,76.297,% | Adaptive Acc: 73.674% | clf_exit: 0.176 0.356 0.468
Batch: 260 | Loss: 4.392 | Acc: 41.379,65.601,76.299,% | Adaptive Acc: 73.713% | clf_exit: 0.176 0.356 0.469
Batch: 280 | Loss: 4.393 | Acc: 41.334,65.611,76.262,% | Adaptive Acc: 73.691% | clf_exit: 0.174 0.357 0.468
Batch: 300 | Loss: 4.392 | Acc: 41.383,65.628,76.254,% | Adaptive Acc: 73.689% | clf_exit: 0.175 0.357 0.468
Batch: 320 | Loss: 4.391 | Acc: 41.414,65.708,76.229,% | Adaptive Acc: 73.722% | clf_exit: 0.175 0.358 0.467
Batch: 340 | Loss: 4.386 | Acc: 41.491,65.669,76.221,% | Adaptive Acc: 73.754% | clf_exit: 0.176 0.358 0.467
Batch: 360 | Loss: 4.392 | Acc: 41.428,65.573,76.169,% | Adaptive Acc: 73.704% | clf_exit: 0.176 0.357 0.467
Batch: 380 | Loss: 4.394 | Acc: 41.458,65.572,76.152,% | Adaptive Acc: 73.747% | clf_exit: 0.176 0.357 0.467
Batch: 0 | Loss: 5.347 | Acc: 36.719,57.812,66.406,% | Adaptive Acc: 61.719% | clf_exit: 0.242 0.422 0.336
Batch: 20 | Loss: 5.623 | Acc: 34.375,56.287,62.649,% | Adaptive Acc: 58.445% | clf_exit: 0.237 0.346 0.417
Batch: 40 | Loss: 5.686 | Acc: 34.184,55.583,61.928,% | Adaptive Acc: 58.441% | clf_exit: 0.237 0.342 0.421
Batch: 60 | Loss: 5.745 | Acc: 34.273,55.405,61.501,% | Adaptive Acc: 58.056% | clf_exit: 0.237 0.340 0.423
Train all parameters

Epoch: 103
Batch: 0 | Loss: 3.869 | Acc: 44.531,73.438,78.906,% | Adaptive Acc: 78.906% | clf_exit: 0.133 0.359 0.508
Batch: 20 | Loss: 4.209 | Acc: 42.150,66.369,80.134,% | Adaptive Acc: 76.749% | clf_exit: 0.165 0.378 0.457
Batch: 40 | Loss: 4.169 | Acc: 42.435,66.997,80.716,% | Adaptive Acc: 77.306% | clf_exit: 0.171 0.373 0.456
Batch: 60 | Loss: 4.160 | Acc: 42.200,67.277,80.622,% | Adaptive Acc: 76.895% | clf_exit: 0.177 0.369 0.454
Batch: 80 | Loss: 4.171 | Acc: 42.004,67.188,80.382,% | Adaptive Acc: 76.852% | clf_exit: 0.177 0.373 0.450
Batch: 100 | Loss: 4.180 | Acc: 41.824,67.010,80.384,% | Adaptive Acc: 77.058% | clf_exit: 0.176 0.373 0.451
Batch: 120 | Loss: 4.179 | Acc: 41.949,67.162,80.230,% | Adaptive Acc: 77.008% | clf_exit: 0.176 0.374 0.450
Batch: 140 | Loss: 4.179 | Acc: 42.199,67.171,80.208,% | Adaptive Acc: 77.100% | clf_exit: 0.177 0.373 0.450
Batch: 160 | Loss: 4.189 | Acc: 41.911,67.120,80.144,% | Adaptive Acc: 76.994% | clf_exit: 0.178 0.370 0.453
Batch: 180 | Loss: 4.183 | Acc: 42.153,67.118,80.059,% | Adaptive Acc: 76.873% | clf_exit: 0.179 0.370 0.451
Batch: 200 | Loss: 4.177 | Acc: 42.160,67.094,80.057,% | Adaptive Acc: 76.835% | clf_exit: 0.180 0.370 0.450
Batch: 220 | Loss: 4.181 | Acc: 42.223,67.195,80.009,% | Adaptive Acc: 76.856% | clf_exit: 0.181 0.370 0.450
Batch: 240 | Loss: 4.182 | Acc: 42.200,67.223,79.934,% | Adaptive Acc: 76.838% | clf_exit: 0.179 0.371 0.450
Batch: 260 | Loss: 4.187 | Acc: 42.289,67.232,79.897,% | Adaptive Acc: 76.880% | clf_exit: 0.179 0.370 0.451
Batch: 280 | Loss: 4.192 | Acc: 42.338,67.165,79.740,% | Adaptive Acc: 76.810% | clf_exit: 0.179 0.369 0.451
Batch: 300 | Loss: 4.202 | Acc: 42.229,67.058,79.628,% | Adaptive Acc: 76.677% | clf_exit: 0.179 0.369 0.452
Batch: 320 | Loss: 4.200 | Acc: 42.280,67.102,79.544,% | Adaptive Acc: 76.616% | clf_exit: 0.181 0.368 0.451
Batch: 340 | Loss: 4.204 | Acc: 42.297,67.151,79.488,% | Adaptive Acc: 76.556% | clf_exit: 0.181 0.368 0.451
Batch: 360 | Loss: 4.214 | Acc: 42.192,67.036,79.337,% | Adaptive Acc: 76.402% | clf_exit: 0.181 0.368 0.451
Batch: 380 | Loss: 4.219 | Acc: 42.126,66.976,79.277,% | Adaptive Acc: 76.372% | clf_exit: 0.181 0.367 0.452
Batch: 0 | Loss: 5.035 | Acc: 39.844,60.156,66.406,% | Adaptive Acc: 64.062% | clf_exit: 0.297 0.336 0.367
Batch: 20 | Loss: 5.347 | Acc: 38.318,57.999,63.207,% | Adaptive Acc: 60.007% | clf_exit: 0.267 0.358 0.375
Batch: 40 | Loss: 5.331 | Acc: 38.415,58.537,63.491,% | Adaptive Acc: 60.213% | clf_exit: 0.259 0.363 0.378
Batch: 60 | Loss: 5.375 | Acc: 38.435,58.325,63.140,% | Adaptive Acc: 60.028% | clf_exit: 0.261 0.362 0.377
Train all parameters

Epoch: 104
Batch: 0 | Loss: 3.866 | Acc: 37.500,72.656,85.156,% | Adaptive Acc: 81.250% | clf_exit: 0.203 0.375 0.422
Batch: 20 | Loss: 4.006 | Acc: 43.378,70.015,82.589,% | Adaptive Acc: 79.501% | clf_exit: 0.189 0.374 0.437
Batch: 40 | Loss: 4.013 | Acc: 43.559,69.131,82.965,% | Adaptive Acc: 79.592% | clf_exit: 0.188 0.380 0.432
Batch: 60 | Loss: 4.022 | Acc: 43.212,68.904,82.877,% | Adaptive Acc: 79.329% | clf_exit: 0.186 0.378 0.436
Batch: 80 | Loss: 4.026 | Acc: 42.988,68.538,82.957,% | Adaptive Acc: 79.340% | clf_exit: 0.185 0.376 0.439
Batch: 100 | Loss: 4.046 | Acc: 42.837,68.286,82.782,% | Adaptive Acc: 79.316% | clf_exit: 0.181 0.375 0.443
Batch: 120 | Loss: 4.046 | Acc: 42.730,68.343,82.606,% | Adaptive Acc: 79.300% | clf_exit: 0.182 0.377 0.441
Batch: 140 | Loss: 4.026 | Acc: 42.936,68.573,82.691,% | Adaptive Acc: 79.438% | clf_exit: 0.184 0.376 0.440
Batch: 160 | Loss: 4.025 | Acc: 42.969,68.507,82.531,% | Adaptive Acc: 79.304% | clf_exit: 0.185 0.378 0.437
Batch: 180 | Loss: 4.038 | Acc: 42.921,68.452,82.251,% | Adaptive Acc: 79.131% | clf_exit: 0.186 0.378 0.437
Batch: 200 | Loss: 4.044 | Acc: 42.844,68.346,82.128,% | Adaptive Acc: 79.062% | clf_exit: 0.184 0.378 0.438
Batch: 220 | Loss: 4.050 | Acc: 42.902,68.347,82.031,% | Adaptive Acc: 78.963% | clf_exit: 0.185 0.376 0.439
Batch: 240 | Loss: 4.053 | Acc: 42.904,68.342,82.034,% | Adaptive Acc: 78.971% | clf_exit: 0.185 0.376 0.440
Batch: 260 | Loss: 4.046 | Acc: 42.876,68.451,81.971,% | Adaptive Acc: 78.975% | clf_exit: 0.185 0.376 0.439
Batch: 280 | Loss: 4.056 | Acc: 42.699,68.339,81.831,% | Adaptive Acc: 78.812% | clf_exit: 0.184 0.376 0.440
Batch: 300 | Loss: 4.064 | Acc: 42.655,68.239,81.735,% | Adaptive Acc: 78.714% | clf_exit: 0.184 0.376 0.440
Batch: 320 | Loss: 4.066 | Acc: 42.711,68.222,81.698,% | Adaptive Acc: 78.641% | clf_exit: 0.185 0.375 0.440
Batch: 340 | Loss: 4.073 | Acc: 42.689,68.145,81.497,% | Adaptive Acc: 78.437% | clf_exit: 0.185 0.374 0.441
Batch: 360 | Loss: 4.083 | Acc: 42.629,68.047,81.380,% | Adaptive Acc: 78.309% | clf_exit: 0.185 0.373 0.442
Batch: 380 | Loss: 4.086 | Acc: 42.579,67.983,81.322,% | Adaptive Acc: 78.246% | clf_exit: 0.185 0.372 0.442
Batch: 0 | Loss: 5.011 | Acc: 38.281,61.719,72.656,% | Adaptive Acc: 67.188% | clf_exit: 0.242 0.383 0.375
Batch: 20 | Loss: 5.238 | Acc: 38.393,59.710,65.290,% | Adaptive Acc: 61.868% | clf_exit: 0.237 0.370 0.393
Batch: 40 | Loss: 5.247 | Acc: 38.434,59.680,64.405,% | Adaptive Acc: 61.623% | clf_exit: 0.234 0.365 0.401
Batch: 60 | Loss: 5.240 | Acc: 37.935,59.734,64.280,% | Adaptive Acc: 61.527% | clf_exit: 0.235 0.365 0.400
Train all parameters

Epoch: 105
Batch: 0 | Loss: 3.691 | Acc: 46.094,67.188,89.062,% | Adaptive Acc: 87.500% | clf_exit: 0.203 0.305 0.492
Batch: 20 | Loss: 3.837 | Acc: 43.304,69.606,84.673,% | Adaptive Acc: 81.436% | clf_exit: 0.187 0.375 0.439
Batch: 40 | Loss: 3.883 | Acc: 43.102,69.207,84.775,% | Adaptive Acc: 81.079% | clf_exit: 0.189 0.376 0.435
Batch: 60 | Loss: 3.900 | Acc: 43.404,69.390,85.015,% | Adaptive Acc: 81.352% | clf_exit: 0.192 0.376 0.432
Batch: 80 | Loss: 3.889 | Acc: 43.104,69.618,85.195,% | Adaptive Acc: 81.742% | clf_exit: 0.187 0.375 0.438
Batch: 100 | Loss: 3.914 | Acc: 42.992,69.261,84.638,% | Adaptive Acc: 81.235% | clf_exit: 0.186 0.376 0.438
Batch: 120 | Loss: 3.941 | Acc: 42.794,69.124,84.272,% | Adaptive Acc: 80.863% | clf_exit: 0.187 0.374 0.439
Batch: 140 | Loss: 3.940 | Acc: 42.791,69.099,84.148,% | Adaptive Acc: 80.768% | clf_exit: 0.186 0.375 0.439
Batch: 160 | Loss: 3.938 | Acc: 42.906,69.133,84.157,% | Adaptive Acc: 80.770% | clf_exit: 0.185 0.377 0.438
Batch: 180 | Loss: 3.948 | Acc: 42.939,69.031,84.120,% | Adaptive Acc: 80.728% | clf_exit: 0.185 0.374 0.441
Batch: 200 | Loss: 3.947 | Acc: 43.085,69.010,83.979,% | Adaptive Acc: 80.632% | clf_exit: 0.186 0.373 0.441
Batch: 220 | Loss: 3.953 | Acc: 42.958,69.050,83.891,% | Adaptive Acc: 80.539% | clf_exit: 0.187 0.374 0.440
Batch: 240 | Loss: 3.957 | Acc: 42.975,68.951,83.756,% | Adaptive Acc: 80.384% | clf_exit: 0.187 0.374 0.439
Batch: 260 | Loss: 3.961 | Acc: 42.939,68.933,83.675,% | Adaptive Acc: 80.316% | clf_exit: 0.186 0.375 0.439
Batch: 280 | Loss: 3.972 | Acc: 42.877,68.803,83.552,% | Adaptive Acc: 80.113% | clf_exit: 0.186 0.375 0.439
Batch: 300 | Loss: 3.981 | Acc: 42.826,68.701,83.365,% | Adaptive Acc: 79.996% | clf_exit: 0.185 0.375 0.440
Batch: 320 | Loss: 3.995 | Acc: 42.820,68.494,83.139,% | Adaptive Acc: 79.770% | clf_exit: 0.185 0.374 0.440
Batch: 340 | Loss: 4.004 | Acc: 42.769,68.436,82.929,% | Adaptive Acc: 79.568% | clf_exit: 0.185 0.373 0.441
Batch: 360 | Loss: 4.008 | Acc: 42.804,68.417,82.787,% | Adaptive Acc: 79.493% | clf_exit: 0.185 0.373 0.441
Batch: 380 | Loss: 4.012 | Acc: 42.868,68.350,82.687,% | Adaptive Acc: 79.394% | clf_exit: 0.185 0.374 0.441
Batch: 0 | Loss: 4.790 | Acc: 40.625,64.062,66.406,% | Adaptive Acc: 64.062% | clf_exit: 0.266 0.375 0.359
Batch: 20 | Loss: 5.126 | Acc: 39.211,60.603,65.365,% | Adaptive Acc: 61.570% | clf_exit: 0.274 0.359 0.368
Batch: 40 | Loss: 5.117 | Acc: 39.558,60.175,65.568,% | Adaptive Acc: 61.947% | clf_exit: 0.267 0.357 0.376
Batch: 60 | Loss: 5.148 | Acc: 38.998,59.900,65.036,% | Adaptive Acc: 61.693% | clf_exit: 0.269 0.351 0.381
Train all parameters

Epoch: 106
Batch: 0 | Loss: 3.907 | Acc: 46.094,66.406,82.031,% | Adaptive Acc: 81.250% | clf_exit: 0.180 0.336 0.484
Batch: 20 | Loss: 3.869 | Acc: 42.448,68.638,85.305,% | Adaptive Acc: 81.138% | clf_exit: 0.190 0.380 0.430
Batch: 40 | Loss: 3.853 | Acc: 42.816,69.379,85.804,% | Adaptive Acc: 82.031% | clf_exit: 0.189 0.381 0.429
Batch: 60 | Loss: 3.868 | Acc: 43.199,69.467,85.361,% | Adaptive Acc: 81.711% | clf_exit: 0.192 0.381 0.426
Batch: 80 | Loss: 3.871 | Acc: 43.345,69.502,85.214,% | Adaptive Acc: 81.443% | clf_exit: 0.194 0.381 0.425
Batch: 100 | Loss: 3.901 | Acc: 43.100,69.191,84.839,% | Adaptive Acc: 81.111% | clf_exit: 0.191 0.378 0.430
Batch: 120 | Loss: 3.890 | Acc: 43.330,69.312,84.859,% | Adaptive Acc: 81.192% | clf_exit: 0.192 0.379 0.429
Batch: 140 | Loss: 3.906 | Acc: 43.146,69.199,84.774,% | Adaptive Acc: 80.990% | clf_exit: 0.193 0.375 0.432
Batch: 160 | Loss: 3.921 | Acc: 42.954,69.109,84.584,% | Adaptive Acc: 80.891% | clf_exit: 0.191 0.374 0.434
Batch: 180 | Loss: 3.922 | Acc: 43.029,69.087,84.461,% | Adaptive Acc: 80.849% | clf_exit: 0.189 0.377 0.434
Batch: 200 | Loss: 3.923 | Acc: 43.012,69.007,84.406,% | Adaptive Acc: 80.869% | clf_exit: 0.189 0.375 0.436
Batch: 220 | Loss: 3.928 | Acc: 43.174,69.079,84.234,% | Adaptive Acc: 80.790% | clf_exit: 0.189 0.375 0.436
Batch: 240 | Loss: 3.935 | Acc: 43.251,69.084,84.090,% | Adaptive Acc: 80.757% | clf_exit: 0.188 0.374 0.437
Batch: 260 | Loss: 3.947 | Acc: 43.268,68.924,83.884,% | Adaptive Acc: 80.538% | clf_exit: 0.188 0.374 0.438
Batch: 280 | Loss: 3.943 | Acc: 43.286,68.922,83.863,% | Adaptive Acc: 80.474% | clf_exit: 0.188 0.376 0.437
Batch: 300 | Loss: 3.941 | Acc: 43.254,68.934,83.760,% | Adaptive Acc: 80.386% | clf_exit: 0.189 0.376 0.436
Batch: 320 | Loss: 3.949 | Acc: 43.154,68.828,83.628,% | Adaptive Acc: 80.228% | clf_exit: 0.188 0.376 0.436
Batch: 340 | Loss: 3.953 | Acc: 43.186,68.773,83.614,% | Adaptive Acc: 80.201% | clf_exit: 0.188 0.376 0.437
Batch: 360 | Loss: 3.953 | Acc: 43.246,68.776,83.644,% | Adaptive Acc: 80.255% | clf_exit: 0.188 0.375 0.437
Batch: 380 | Loss: 3.953 | Acc: 43.186,68.830,83.698,% | Adaptive Acc: 80.294% | clf_exit: 0.188 0.375 0.437
Batch: 0 | Loss: 4.710 | Acc: 45.312,67.969,74.219,% | Adaptive Acc: 74.219% | clf_exit: 0.266 0.414 0.320
Batch: 20 | Loss: 5.189 | Acc: 39.472,60.677,65.290,% | Adaptive Acc: 62.760% | clf_exit: 0.277 0.383 0.340
Batch: 40 | Loss: 5.176 | Acc: 38.872,60.614,65.568,% | Adaptive Acc: 62.767% | clf_exit: 0.269 0.381 0.351
Batch: 60 | Loss: 5.204 | Acc: 38.665,60.297,65.241,% | Adaptive Acc: 62.295% | clf_exit: 0.269 0.382 0.349
Train all parameters

Epoch: 107
Batch: 0 | Loss: 3.967 | Acc: 40.625,67.969,85.938,% | Adaptive Acc: 81.250% | clf_exit: 0.172 0.422 0.406
Batch: 20 | Loss: 3.757 | Acc: 42.039,70.573,87.686,% | Adaptive Acc: 83.594% | clf_exit: 0.180 0.390 0.430
Batch: 40 | Loss: 3.780 | Acc: 43.159,69.950,87.119,% | Adaptive Acc: 83.060% | clf_exit: 0.183 0.384 0.433
Batch: 60 | Loss: 3.792 | Acc: 43.327,69.864,86.821,% | Adaptive Acc: 82.518% | clf_exit: 0.188 0.382 0.430
Batch: 80 | Loss: 3.773 | Acc: 43.972,70.033,86.921,% | Adaptive Acc: 82.803% | clf_exit: 0.191 0.379 0.430
Batch: 100 | Loss: 3.767 | Acc: 44.044,70.251,86.819,% | Adaptive Acc: 82.542% | clf_exit: 0.196 0.378 0.426
Batch: 120 | Loss: 3.777 | Acc: 43.802,70.145,86.861,% | Adaptive Acc: 82.593% | clf_exit: 0.194 0.380 0.426
Batch: 140 | Loss: 3.788 | Acc: 43.800,70.019,86.652,% | Adaptive Acc: 82.380% | clf_exit: 0.195 0.381 0.424
Batch: 160 | Loss: 3.800 | Acc: 43.546,69.944,86.607,% | Adaptive Acc: 82.313% | clf_exit: 0.194 0.378 0.428
Batch: 180 | Loss: 3.805 | Acc: 43.629,69.868,86.404,% | Adaptive Acc: 82.195% | clf_exit: 0.192 0.379 0.428
Batch: 200 | Loss: 3.816 | Acc: 43.517,69.792,86.338,% | Adaptive Acc: 82.109% | clf_exit: 0.192 0.379 0.428
Batch: 220 | Loss: 3.828 | Acc: 43.464,69.701,86.118,% | Adaptive Acc: 81.950% | clf_exit: 0.191 0.380 0.429
Batch: 240 | Loss: 3.836 | Acc: 43.458,69.671,85.960,% | Adaptive Acc: 81.846% | clf_exit: 0.191 0.380 0.428
Batch: 260 | Loss: 3.841 | Acc: 43.436,69.594,85.725,% | Adaptive Acc: 81.636% | clf_exit: 0.192 0.380 0.428
Batch: 280 | Loss: 3.848 | Acc: 43.480,69.453,85.615,% | Adaptive Acc: 81.553% | clf_exit: 0.192 0.379 0.429
Batch: 300 | Loss: 3.855 | Acc: 43.597,69.433,85.463,% | Adaptive Acc: 81.447% | clf_exit: 0.193 0.378 0.429
Batch: 320 | Loss: 3.863 | Acc: 43.558,69.344,85.370,% | Adaptive Acc: 81.423% | clf_exit: 0.192 0.377 0.430
Batch: 340 | Loss: 3.864 | Acc: 43.551,69.382,85.252,% | Adaptive Acc: 81.362% | clf_exit: 0.193 0.377 0.430
Batch: 360 | Loss: 3.870 | Acc: 43.605,69.375,85.100,% | Adaptive Acc: 81.269% | clf_exit: 0.192 0.378 0.431
Batch: 380 | Loss: 3.878 | Acc: 43.545,69.369,85.009,% | Adaptive Acc: 81.172% | clf_exit: 0.191 0.378 0.431
Batch: 0 | Loss: 4.957 | Acc: 33.594,62.500,71.094,% | Adaptive Acc: 65.625% | clf_exit: 0.266 0.422 0.312
Batch: 20 | Loss: 5.271 | Acc: 37.202,59.152,64.695,% | Adaptive Acc: 61.644% | clf_exit: 0.259 0.378 0.363
Batch: 40 | Loss: 5.240 | Acc: 38.072,59.737,64.520,% | Adaptive Acc: 61.338% | clf_exit: 0.254 0.378 0.368
Batch: 60 | Loss: 5.252 | Acc: 38.307,59.337,64.728,% | Adaptive Acc: 61.399% | clf_exit: 0.255 0.378 0.367
Train all parameters

Epoch: 108
Batch: 0 | Loss: 3.639 | Acc: 43.750,71.094,89.844,% | Adaptive Acc: 85.938% | clf_exit: 0.180 0.406 0.414
Batch: 20 | Loss: 3.778 | Acc: 44.010,70.312,86.421,% | Adaptive Acc: 81.920% | clf_exit: 0.202 0.381 0.417
Batch: 40 | Loss: 3.794 | Acc: 43.369,70.046,86.776,% | Adaptive Acc: 82.050% | clf_exit: 0.199 0.379 0.422
Batch: 60 | Loss: 3.773 | Acc: 43.712,70.543,86.821,% | Adaptive Acc: 82.441% | clf_exit: 0.197 0.385 0.418
Batch: 80 | Loss: 3.779 | Acc: 43.422,70.679,86.777,% | Adaptive Acc: 82.456% | clf_exit: 0.193 0.390 0.417
Batch: 100 | Loss: 3.778 | Acc: 43.796,70.661,86.781,% | Adaptive Acc: 82.379% | clf_exit: 0.195 0.390 0.415
Batch: 120 | Loss: 3.790 | Acc: 43.808,70.526,86.693,% | Adaptive Acc: 82.406% | clf_exit: 0.196 0.388 0.417
Batch: 140 | Loss: 3.779 | Acc: 43.944,70.623,86.818,% | Adaptive Acc: 82.480% | clf_exit: 0.197 0.387 0.416
Batch: 160 | Loss: 3.794 | Acc: 43.701,70.439,86.593,% | Adaptive Acc: 82.284% | clf_exit: 0.198 0.383 0.419
Batch: 180 | Loss: 3.807 | Acc: 43.720,70.248,86.382,% | Adaptive Acc: 82.234% | clf_exit: 0.197 0.382 0.421
Batch: 200 | Loss: 3.798 | Acc: 43.820,70.468,86.400,% | Adaptive Acc: 82.299% | clf_exit: 0.198 0.382 0.420
Batch: 220 | Loss: 3.803 | Acc: 43.704,70.358,86.348,% | Adaptive Acc: 82.279% | clf_exit: 0.198 0.381 0.421
Batch: 240 | Loss: 3.803 | Acc: 43.753,70.283,86.333,% | Adaptive Acc: 82.323% | clf_exit: 0.197 0.381 0.422
Batch: 260 | Loss: 3.808 | Acc: 43.768,70.208,86.177,% | Adaptive Acc: 82.190% | clf_exit: 0.197 0.381 0.422
Batch: 280 | Loss: 3.818 | Acc: 43.772,70.087,86.038,% | Adaptive Acc: 82.020% | clf_exit: 0.196 0.381 0.423
Batch: 300 | Loss: 3.823 | Acc: 43.830,70.126,85.938,% | Adaptive Acc: 81.959% | clf_exit: 0.196 0.381 0.423
Batch: 320 | Loss: 3.830 | Acc: 43.718,70.103,85.818,% | Adaptive Acc: 81.856% | clf_exit: 0.195 0.381 0.424
Batch: 340 | Loss: 3.833 | Acc: 43.711,70.079,85.713,% | Adaptive Acc: 81.802% | clf_exit: 0.195 0.382 0.423
Batch: 360 | Loss: 3.841 | Acc: 43.661,69.955,85.587,% | Adaptive Acc: 81.672% | clf_exit: 0.195 0.382 0.423
Batch: 380 | Loss: 3.843 | Acc: 43.639,69.933,85.501,% | Adaptive Acc: 81.588% | clf_exit: 0.195 0.382 0.423
Batch: 0 | Loss: 4.917 | Acc: 39.062,58.594,67.188,% | Adaptive Acc: 65.625% | clf_exit: 0.250 0.430 0.320
Batch: 20 | Loss: 5.185 | Acc: 38.951,60.193,66.332,% | Adaptive Acc: 63.058% | clf_exit: 0.233 0.409 0.358
Batch: 40 | Loss: 5.136 | Acc: 39.787,60.004,65.663,% | Adaptive Acc: 62.767% | clf_exit: 0.228 0.406 0.366
Batch: 60 | Loss: 5.170 | Acc: 39.408,59.785,65.164,% | Adaptive Acc: 62.359% | clf_exit: 0.231 0.403 0.366
Train all parameters

Epoch: 109
Batch: 0 | Loss: 3.817 | Acc: 43.750,71.094,89.844,% | Adaptive Acc: 82.812% | clf_exit: 0.188 0.422 0.391
Batch: 20 | Loss: 3.679 | Acc: 44.978,71.801,88.728,% | Adaptive Acc: 84.003% | clf_exit: 0.201 0.395 0.405
Batch: 40 | Loss: 3.684 | Acc: 44.455,71.151,88.891,% | Adaptive Acc: 84.242% | clf_exit: 0.200 0.386 0.414
Batch: 60 | Loss: 3.677 | Acc: 44.198,71.273,88.627,% | Adaptive Acc: 84.170% | clf_exit: 0.200 0.384 0.415
Batch: 80 | Loss: 3.716 | Acc: 44.059,70.621,88.011,% | Adaptive Acc: 83.864% | clf_exit: 0.196 0.381 0.423
Batch: 100 | Loss: 3.708 | Acc: 44.098,70.722,88.188,% | Adaptive Acc: 83.864% | clf_exit: 0.196 0.385 0.419
Batch: 120 | Loss: 3.731 | Acc: 43.673,70.487,88.036,% | Adaptive Acc: 83.561% | clf_exit: 0.196 0.385 0.419
Batch: 140 | Loss: 3.725 | Acc: 43.756,70.484,87.949,% | Adaptive Acc: 83.644% | clf_exit: 0.195 0.386 0.419
Batch: 160 | Loss: 3.756 | Acc: 43.483,70.196,87.636,% | Adaptive Acc: 83.332% | clf_exit: 0.193 0.383 0.424
Batch: 180 | Loss: 3.761 | Acc: 43.491,70.187,87.457,% | Adaptive Acc: 83.188% | clf_exit: 0.194 0.382 0.423
Batch: 200 | Loss: 3.764 | Acc: 43.493,70.157,87.372,% | Adaptive Acc: 83.050% | clf_exit: 0.195 0.383 0.422
Batch: 220 | Loss: 3.774 | Acc: 43.488,70.001,87.136,% | Adaptive Acc: 82.844% | clf_exit: 0.196 0.381 0.422
Batch: 240 | Loss: 3.777 | Acc: 43.513,70.102,87.046,% | Adaptive Acc: 82.796% | clf_exit: 0.195 0.382 0.423
Batch: 260 | Loss: 3.784 | Acc: 43.576,69.929,86.865,% | Adaptive Acc: 82.663% | clf_exit: 0.195 0.382 0.423
Batch: 280 | Loss: 3.791 | Acc: 43.547,69.848,86.688,% | Adaptive Acc: 82.498% | clf_exit: 0.195 0.381 0.424
Batch: 300 | Loss: 3.792 | Acc: 43.612,69.806,86.576,% | Adaptive Acc: 82.389% | clf_exit: 0.195 0.382 0.423
Batch: 320 | Loss: 3.798 | Acc: 43.599,69.758,86.490,% | Adaptive Acc: 82.328% | clf_exit: 0.195 0.380 0.424
Batch: 340 | Loss: 3.798 | Acc: 43.723,69.795,86.425,% | Adaptive Acc: 82.325% | clf_exit: 0.195 0.381 0.424
Batch: 360 | Loss: 3.800 | Acc: 43.791,69.830,86.327,% | Adaptive Acc: 82.280% | clf_exit: 0.194 0.382 0.424
Batch: 380 | Loss: 3.810 | Acc: 43.711,69.765,86.188,% | Adaptive Acc: 82.124% | clf_exit: 0.194 0.382 0.424
Batch: 0 | Loss: 5.205 | Acc: 36.719,60.156,66.406,% | Adaptive Acc: 65.625% | clf_exit: 0.320 0.328 0.352
Batch: 20 | Loss: 5.374 | Acc: 36.421,60.045,64.658,% | Adaptive Acc: 61.682% | clf_exit: 0.275 0.350 0.375
Batch: 40 | Loss: 5.428 | Acc: 36.566,58.803,63.872,% | Adaptive Acc: 61.014% | clf_exit: 0.273 0.353 0.374
Batch: 60 | Loss: 5.459 | Acc: 36.488,58.504,63.614,% | Adaptive Acc: 60.720% | clf_exit: 0.270 0.358 0.372
Train all parameters

Epoch: 110
Batch: 0 | Loss: 3.445 | Acc: 46.094,72.656,85.156,% | Adaptive Acc: 78.125% | clf_exit: 0.227 0.367 0.406
Batch: 20 | Loss: 3.815 | Acc: 43.787,70.275,87.426,% | Adaptive Acc: 83.631% | clf_exit: 0.187 0.400 0.413
Batch: 40 | Loss: 3.733 | Acc: 44.722,71.475,87.748,% | Adaptive Acc: 83.880% | clf_exit: 0.190 0.397 0.413
Batch: 60 | Loss: 3.734 | Acc: 44.109,71.183,88.051,% | Adaptive Acc: 83.850% | clf_exit: 0.190 0.394 0.416
Batch: 80 | Loss: 3.711 | Acc: 44.174,71.412,88.281,% | Adaptive Acc: 83.941% | clf_exit: 0.190 0.395 0.414
Batch: 100 | Loss: 3.725 | Acc: 43.998,71.326,88.366,% | Adaptive Acc: 84.066% | clf_exit: 0.189 0.394 0.418
Batch: 120 | Loss: 3.711 | Acc: 44.112,71.300,88.475,% | Adaptive Acc: 84.059% | clf_exit: 0.191 0.395 0.414
Batch: 140 | Loss: 3.718 | Acc: 43.961,71.072,88.436,% | Adaptive Acc: 83.993% | clf_exit: 0.190 0.394 0.416
Batch: 160 | Loss: 3.713 | Acc: 44.090,71.133,88.500,% | Adaptive Acc: 84.123% | clf_exit: 0.191 0.393 0.416
Batch: 180 | Loss: 3.707 | Acc: 44.143,71.158,88.389,% | Adaptive Acc: 84.060% | clf_exit: 0.192 0.393 0.416
Batch: 200 | Loss: 3.716 | Acc: 44.205,71.020,88.204,% | Adaptive Acc: 83.947% | clf_exit: 0.192 0.391 0.417
Batch: 220 | Loss: 3.725 | Acc: 44.086,70.875,87.981,% | Adaptive Acc: 83.693% | clf_exit: 0.193 0.389 0.418
Batch: 240 | Loss: 3.726 | Acc: 44.068,70.877,87.879,% | Adaptive Acc: 83.623% | clf_exit: 0.194 0.388 0.418
Batch: 260 | Loss: 3.733 | Acc: 43.992,70.696,87.736,% | Adaptive Acc: 83.495% | clf_exit: 0.195 0.386 0.419
Batch: 280 | Loss: 3.737 | Acc: 43.900,70.674,87.653,% | Adaptive Acc: 83.455% | clf_exit: 0.194 0.386 0.420
Batch: 300 | Loss: 3.740 | Acc: 43.877,70.647,87.560,% | Adaptive Acc: 83.386% | clf_exit: 0.194 0.387 0.419
Batch: 320 | Loss: 3.749 | Acc: 43.787,70.563,87.417,% | Adaptive Acc: 83.282% | clf_exit: 0.193 0.387 0.420
Batch: 340 | Loss: 3.759 | Acc: 43.759,70.484,87.259,% | Adaptive Acc: 83.168% | clf_exit: 0.192 0.387 0.421
Batch: 360 | Loss: 3.764 | Acc: 43.711,70.399,87.132,% | Adaptive Acc: 83.048% | clf_exit: 0.193 0.385 0.422
Batch: 380 | Loss: 3.767 | Acc: 43.760,70.347,87.045,% | Adaptive Acc: 82.977% | clf_exit: 0.193 0.384 0.422
Batch: 0 | Loss: 5.187 | Acc: 35.938,55.469,70.312,% | Adaptive Acc: 64.844% | clf_exit: 0.289 0.406 0.305
Batch: 20 | Loss: 5.241 | Acc: 38.653,58.966,64.435,% | Adaptive Acc: 61.124% | clf_exit: 0.264 0.383 0.353
Batch: 40 | Loss: 5.245 | Acc: 38.853,58.727,64.691,% | Adaptive Acc: 61.357% | clf_exit: 0.258 0.381 0.361
Batch: 60 | Loss: 5.285 | Acc: 38.832,58.696,64.152,% | Adaptive Acc: 61.335% | clf_exit: 0.257 0.381 0.362
Train all parameters

Epoch: 111
Batch: 0 | Loss: 3.660 | Acc: 41.406,71.875,89.062,% | Adaptive Acc: 86.719% | clf_exit: 0.172 0.430 0.398
Batch: 20 | Loss: 3.678 | Acc: 45.238,71.317,88.653,% | Adaptive Acc: 84.077% | clf_exit: 0.185 0.403 0.412
Batch: 40 | Loss: 3.637 | Acc: 45.408,71.437,88.929,% | Adaptive Acc: 84.337% | clf_exit: 0.196 0.396 0.408
Batch: 60 | Loss: 3.646 | Acc: 45.287,71.580,88.755,% | Adaptive Acc: 84.273% | clf_exit: 0.198 0.392 0.410
Batch: 80 | Loss: 3.669 | Acc: 44.724,71.373,88.600,% | Adaptive Acc: 84.211% | clf_exit: 0.198 0.393 0.409
Batch: 100 | Loss: 3.686 | Acc: 44.670,71.233,88.413,% | Adaptive Acc: 83.996% | clf_exit: 0.196 0.392 0.412
Batch: 120 | Loss: 3.690 | Acc: 44.667,71.204,88.346,% | Adaptive Acc: 84.039% | clf_exit: 0.194 0.394 0.412
Batch: 140 | Loss: 3.694 | Acc: 44.587,71.105,88.198,% | Adaptive Acc: 83.887% | clf_exit: 0.195 0.391 0.414
Batch: 160 | Loss: 3.699 | Acc: 44.488,70.972,88.184,% | Adaptive Acc: 83.895% | clf_exit: 0.195 0.389 0.416
Batch: 180 | Loss: 3.699 | Acc: 44.380,71.085,88.113,% | Adaptive Acc: 83.788% | clf_exit: 0.196 0.389 0.415
Batch: 200 | Loss: 3.711 | Acc: 44.259,71.059,87.931,% | Adaptive Acc: 83.668% | clf_exit: 0.195 0.389 0.415
Batch: 220 | Loss: 3.709 | Acc: 44.432,71.058,87.786,% | Adaptive Acc: 83.580% | clf_exit: 0.196 0.389 0.415
Batch: 240 | Loss: 3.723 | Acc: 44.308,70.838,87.652,% | Adaptive Acc: 83.471% | clf_exit: 0.196 0.388 0.416
Batch: 260 | Loss: 3.735 | Acc: 44.217,70.666,87.476,% | Adaptive Acc: 83.294% | clf_exit: 0.195 0.387 0.418
Batch: 280 | Loss: 3.739 | Acc: 44.148,70.641,87.469,% | Adaptive Acc: 83.338% | clf_exit: 0.195 0.387 0.418
Batch: 300 | Loss: 3.746 | Acc: 44.145,70.608,87.238,% | Adaptive Acc: 83.116% | clf_exit: 0.195 0.387 0.418
Batch: 320 | Loss: 3.749 | Acc: 44.105,70.505,87.077,% | Adaptive Acc: 83.007% | clf_exit: 0.195 0.387 0.418
Batch: 340 | Loss: 3.751 | Acc: 44.110,70.475,87.012,% | Adaptive Acc: 82.934% | clf_exit: 0.196 0.386 0.418
Batch: 360 | Loss: 3.754 | Acc: 44.072,70.419,86.888,% | Adaptive Acc: 82.856% | clf_exit: 0.195 0.386 0.419
Batch: 380 | Loss: 3.759 | Acc: 44.043,70.323,86.772,% | Adaptive Acc: 82.776% | clf_exit: 0.195 0.386 0.420
Batch: 0 | Loss: 5.105 | Acc: 42.188,64.844,70.312,% | Adaptive Acc: 65.625% | clf_exit: 0.242 0.422 0.336
Batch: 20 | Loss: 5.176 | Acc: 39.509,61.235,66.109,% | Adaptive Acc: 62.835% | clf_exit: 0.231 0.400 0.369
Batch: 40 | Loss: 5.170 | Acc: 39.463,60.175,65.168,% | Adaptive Acc: 62.119% | clf_exit: 0.231 0.392 0.377
Batch: 60 | Loss: 5.186 | Acc: 39.306,60.041,65.010,% | Adaptive Acc: 61.808% | clf_exit: 0.231 0.391 0.378
Train all parameters

Epoch: 112
Batch: 0 | Loss: 3.580 | Acc: 44.531,77.344,85.938,% | Adaptive Acc: 82.031% | clf_exit: 0.234 0.414 0.352
Batch: 20 | Loss: 3.645 | Acc: 43.601,72.284,89.025,% | Adaptive Acc: 84.487% | clf_exit: 0.207 0.389 0.404
Batch: 40 | Loss: 3.657 | Acc: 43.693,71.475,89.215,% | Adaptive Acc: 84.413% | clf_exit: 0.204 0.384 0.413
Batch: 60 | Loss: 3.653 | Acc: 43.955,71.516,89.101,% | Adaptive Acc: 84.554% | clf_exit: 0.201 0.386 0.413
Batch: 80 | Loss: 3.660 | Acc: 44.001,71.267,88.744,% | Adaptive Acc: 84.298% | clf_exit: 0.201 0.383 0.416
Batch: 100 | Loss: 3.645 | Acc: 44.516,71.380,88.753,% | Adaptive Acc: 84.282% | clf_exit: 0.203 0.385 0.412
Batch: 120 | Loss: 3.639 | Acc: 44.680,71.513,88.701,% | Adaptive Acc: 84.285% | clf_exit: 0.205 0.384 0.411
Batch: 140 | Loss: 3.655 | Acc: 44.592,71.338,88.531,% | Adaptive Acc: 84.159% | clf_exit: 0.204 0.385 0.411
Batch: 160 | Loss: 3.664 | Acc: 44.580,71.249,88.441,% | Adaptive Acc: 84.123% | clf_exit: 0.203 0.383 0.413
Batch: 180 | Loss: 3.666 | Acc: 44.531,71.171,88.372,% | Adaptive Acc: 84.056% | clf_exit: 0.203 0.384 0.414
Batch: 200 | Loss: 3.674 | Acc: 44.636,71.086,88.153,% | Adaptive Acc: 83.889% | clf_exit: 0.202 0.384 0.414
Batch: 220 | Loss: 3.683 | Acc: 44.574,71.002,88.030,% | Adaptive Acc: 83.778% | clf_exit: 0.202 0.383 0.415
Batch: 240 | Loss: 3.684 | Acc: 44.577,71.113,88.035,% | Adaptive Acc: 83.814% | clf_exit: 0.201 0.385 0.414
Batch: 260 | Loss: 3.684 | Acc: 44.624,71.100,88.027,% | Adaptive Acc: 83.809% | clf_exit: 0.201 0.384 0.414
Batch: 280 | Loss: 3.691 | Acc: 44.528,70.932,87.895,% | Adaptive Acc: 83.680% | clf_exit: 0.201 0.384 0.415
Batch: 300 | Loss: 3.697 | Acc: 44.459,70.941,87.773,% | Adaptive Acc: 83.617% | clf_exit: 0.201 0.384 0.415
Batch: 320 | Loss: 3.704 | Acc: 44.451,70.809,87.656,% | Adaptive Acc: 83.494% | clf_exit: 0.200 0.384 0.416
Batch: 340 | Loss: 3.710 | Acc: 44.387,70.734,87.608,% | Adaptive Acc: 83.452% | clf_exit: 0.199 0.384 0.416
Batch: 360 | Loss: 3.712 | Acc: 44.298,70.782,87.580,% | Adaptive Acc: 83.442% | clf_exit: 0.198 0.385 0.416
Batch: 380 | Loss: 3.717 | Acc: 44.246,70.751,87.512,% | Adaptive Acc: 83.380% | clf_exit: 0.198 0.385 0.416
Batch: 0 | Loss: 5.407 | Acc: 36.719,57.031,68.750,% | Adaptive Acc: 64.062% | clf_exit: 0.266 0.414 0.320
Batch: 20 | Loss: 5.554 | Acc: 35.193,57.440,64.211,% | Adaptive Acc: 59.598% | clf_exit: 0.243 0.398 0.359
Batch: 40 | Loss: 5.546 | Acc: 35.118,57.546,63.796,% | Adaptive Acc: 59.794% | clf_exit: 0.238 0.390 0.372
Batch: 60 | Loss: 5.555 | Acc: 34.913,57.825,63.397,% | Adaptive Acc: 59.708% | clf_exit: 0.238 0.384 0.378
Train all parameters

Epoch: 113
Batch: 0 | Loss: 3.510 | Acc: 46.875,73.438,89.062,% | Adaptive Acc: 84.375% | clf_exit: 0.164 0.430 0.406
Batch: 20 | Loss: 3.626 | Acc: 43.973,72.359,89.137,% | Adaptive Acc: 85.379% | clf_exit: 0.197 0.393 0.410
Batch: 40 | Loss: 3.685 | Acc: 43.483,71.380,88.777,% | Adaptive Acc: 84.775% | clf_exit: 0.194 0.388 0.418
Batch: 60 | Loss: 3.643 | Acc: 44.237,72.003,89.088,% | Adaptive Acc: 85.067% | clf_exit: 0.192 0.394 0.413
Batch: 80 | Loss: 3.620 | Acc: 44.300,72.049,89.246,% | Adaptive Acc: 85.089% | clf_exit: 0.195 0.395 0.410
Batch: 100 | Loss: 3.637 | Acc: 43.998,71.697,89.055,% | Adaptive Acc: 84.700% | clf_exit: 0.194 0.395 0.412
Batch: 120 | Loss: 3.646 | Acc: 44.086,71.333,88.927,% | Adaptive Acc: 84.433% | clf_exit: 0.195 0.393 0.412
Batch: 140 | Loss: 3.658 | Acc: 43.999,71.121,88.830,% | Adaptive Acc: 84.347% | clf_exit: 0.195 0.391 0.414
Batch: 160 | Loss: 3.666 | Acc: 43.852,70.997,88.766,% | Adaptive Acc: 84.341% | clf_exit: 0.194 0.391 0.414
Batch: 180 | Loss: 3.657 | Acc: 43.944,71.072,88.704,% | Adaptive Acc: 84.310% | clf_exit: 0.196 0.391 0.413
Batch: 200 | Loss: 3.664 | Acc: 43.940,71.024,88.623,% | Adaptive Acc: 84.332% | clf_exit: 0.196 0.391 0.413
Batch: 220 | Loss: 3.662 | Acc: 44.019,70.963,88.564,% | Adaptive Acc: 84.280% | clf_exit: 0.196 0.391 0.413
Batch: 240 | Loss: 3.668 | Acc: 44.081,70.954,88.440,% | Adaptive Acc: 84.190% | clf_exit: 0.196 0.391 0.412
Batch: 260 | Loss: 3.672 | Acc: 44.202,70.896,88.416,% | Adaptive Acc: 84.192% | clf_exit: 0.197 0.390 0.413
Batch: 280 | Loss: 3.674 | Acc: 44.214,70.882,88.298,% | Adaptive Acc: 84.108% | clf_exit: 0.196 0.391 0.413
Batch: 300 | Loss: 3.683 | Acc: 44.129,70.762,88.235,% | Adaptive Acc: 84.064% | clf_exit: 0.197 0.390 0.413
Batch: 320 | Loss: 3.688 | Acc: 44.139,70.738,88.106,% | Adaptive Acc: 83.927% | clf_exit: 0.197 0.389 0.414
Batch: 340 | Loss: 3.692 | Acc: 44.137,70.647,87.983,% | Adaptive Acc: 83.784% | clf_exit: 0.197 0.390 0.413
Batch: 360 | Loss: 3.695 | Acc: 44.150,70.648,87.892,% | Adaptive Acc: 83.672% | clf_exit: 0.197 0.390 0.413
Batch: 380 | Loss: 3.699 | Acc: 44.197,70.614,87.803,% | Adaptive Acc: 83.598% | clf_exit: 0.197 0.391 0.413
Batch: 0 | Loss: 4.934 | Acc: 41.406,60.156,67.969,% | Adaptive Acc: 65.625% | clf_exit: 0.250 0.406 0.344
Batch: 20 | Loss: 5.325 | Acc: 38.876,58.333,63.802,% | Adaptive Acc: 60.082% | clf_exit: 0.236 0.384 0.379
Batch: 40 | Loss: 5.324 | Acc: 39.234,58.594,63.700,% | Adaptive Acc: 60.347% | clf_exit: 0.229 0.390 0.381
Batch: 60 | Loss: 5.347 | Acc: 38.973,58.555,63.627,% | Adaptive Acc: 60.438% | clf_exit: 0.227 0.392 0.381
Train all parameters

Epoch: 114
Batch: 0 | Loss: 3.136 | Acc: 47.656,75.781,91.406,% | Adaptive Acc: 89.844% | clf_exit: 0.219 0.430 0.352
Batch: 20 | Loss: 3.551 | Acc: 45.089,72.359,90.067,% | Adaptive Acc: 85.640% | clf_exit: 0.206 0.394 0.400
Batch: 40 | Loss: 3.587 | Acc: 44.741,72.485,89.920,% | Adaptive Acc: 85.537% | clf_exit: 0.201 0.397 0.402
Batch: 60 | Loss: 3.605 | Acc: 44.634,72.131,89.793,% | Adaptive Acc: 85.246% | clf_exit: 0.201 0.399 0.400
Batch: 80 | Loss: 3.600 | Acc: 44.724,72.029,89.892,% | Adaptive Acc: 85.494% | clf_exit: 0.201 0.394 0.406
Batch: 100 | Loss: 3.634 | Acc: 44.361,71.620,89.588,% | Adaptive Acc: 85.203% | clf_exit: 0.197 0.394 0.409
Batch: 120 | Loss: 3.629 | Acc: 44.370,71.746,89.502,% | Adaptive Acc: 85.111% | clf_exit: 0.199 0.396 0.405
Batch: 140 | Loss: 3.624 | Acc: 44.409,71.748,89.400,% | Adaptive Acc: 84.957% | clf_exit: 0.200 0.395 0.404
Batch: 160 | Loss: 3.630 | Acc: 44.255,71.661,89.446,% | Adaptive Acc: 84.860% | clf_exit: 0.200 0.395 0.405
Batch: 180 | Loss: 3.645 | Acc: 44.039,71.487,89.304,% | Adaptive Acc: 84.751% | clf_exit: 0.198 0.395 0.407
Batch: 200 | Loss: 3.645 | Acc: 44.069,71.327,89.167,% | Adaptive Acc: 84.651% | clf_exit: 0.198 0.394 0.408
Batch: 220 | Loss: 3.663 | Acc: 43.944,71.214,89.013,% | Adaptive Acc: 84.502% | clf_exit: 0.198 0.392 0.410
Batch: 240 | Loss: 3.657 | Acc: 44.003,71.259,88.930,% | Adaptive Acc: 84.472% | clf_exit: 0.199 0.392 0.409
Batch: 260 | Loss: 3.666 | Acc: 44.022,71.190,88.730,% | Adaptive Acc: 84.360% | clf_exit: 0.198 0.392 0.410
Batch: 280 | Loss: 3.667 | Acc: 44.059,71.208,88.648,% | Adaptive Acc: 84.275% | clf_exit: 0.199 0.392 0.410
Batch: 300 | Loss: 3.667 | Acc: 44.124,71.208,88.580,% | Adaptive Acc: 84.240% | clf_exit: 0.199 0.392 0.409
Batch: 320 | Loss: 3.675 | Acc: 44.125,71.094,88.456,% | Adaptive Acc: 84.122% | clf_exit: 0.198 0.392 0.410
Batch: 340 | Loss: 3.681 | Acc: 44.091,70.988,88.325,% | Adaptive Acc: 83.965% | clf_exit: 0.198 0.391 0.411
Batch: 360 | Loss: 3.683 | Acc: 44.127,71.053,88.210,% | Adaptive Acc: 83.871% | clf_exit: 0.198 0.391 0.410
Batch: 380 | Loss: 3.683 | Acc: 44.242,71.073,88.148,% | Adaptive Acc: 83.786% | clf_exit: 0.199 0.392 0.410
Batch: 0 | Loss: 4.981 | Acc: 41.406,65.625,67.188,% | Adaptive Acc: 62.500% | clf_exit: 0.328 0.352 0.320
Batch: 20 | Loss: 5.254 | Acc: 40.476,60.789,64.360,% | Adaptive Acc: 61.793% | clf_exit: 0.283 0.385 0.331
Batch: 40 | Loss: 5.205 | Acc: 40.473,60.575,64.272,% | Adaptive Acc: 61.814% | clf_exit: 0.275 0.383 0.342
Batch: 60 | Loss: 5.227 | Acc: 40.254,60.348,64.050,% | Adaptive Acc: 61.373% | clf_exit: 0.279 0.372 0.350
Train all parameters

Epoch: 115
Batch: 0 | Loss: 3.550 | Acc: 46.094,69.531,89.062,% | Adaptive Acc: 82.031% | clf_exit: 0.242 0.391 0.367
Batch: 20 | Loss: 3.586 | Acc: 44.829,71.466,89.286,% | Adaptive Acc: 85.193% | clf_exit: 0.199 0.389 0.412
Batch: 40 | Loss: 3.564 | Acc: 45.274,72.123,89.653,% | Adaptive Acc: 85.232% | clf_exit: 0.211 0.383 0.406
Batch: 60 | Loss: 3.573 | Acc: 44.582,71.670,89.780,% | Adaptive Acc: 85.400% | clf_exit: 0.207 0.386 0.407
Batch: 80 | Loss: 3.557 | Acc: 45.071,72.058,89.950,% | Adaptive Acc: 85.783% | clf_exit: 0.203 0.390 0.406
Batch: 100 | Loss: 3.557 | Acc: 45.266,72.130,89.937,% | Adaptive Acc: 85.705% | clf_exit: 0.204 0.392 0.404
Batch: 120 | Loss: 3.565 | Acc: 45.125,72.101,89.934,% | Adaptive Acc: 85.557% | clf_exit: 0.205 0.391 0.404
Batch: 140 | Loss: 3.592 | Acc: 44.825,71.825,89.799,% | Adaptive Acc: 85.428% | clf_exit: 0.204 0.389 0.407
Batch: 160 | Loss: 3.592 | Acc: 44.861,71.744,89.722,% | Adaptive Acc: 85.326% | clf_exit: 0.204 0.388 0.407
Batch: 180 | Loss: 3.591 | Acc: 44.695,71.853,89.693,% | Adaptive Acc: 85.221% | clf_exit: 0.204 0.390 0.406
Batch: 200 | Loss: 3.597 | Acc: 44.834,71.751,89.572,% | Adaptive Acc: 85.047% | clf_exit: 0.204 0.390 0.406
Batch: 220 | Loss: 3.603 | Acc: 44.793,71.751,89.391,% | Adaptive Acc: 84.919% | clf_exit: 0.204 0.390 0.407
Batch: 240 | Loss: 3.611 | Acc: 44.774,71.625,89.257,% | Adaptive Acc: 84.800% | clf_exit: 0.203 0.391 0.407
Batch: 260 | Loss: 3.622 | Acc: 44.612,71.411,89.083,% | Adaptive Acc: 84.653% | clf_exit: 0.202 0.390 0.408
Batch: 280 | Loss: 3.627 | Acc: 44.620,71.422,88.971,% | Adaptive Acc: 84.564% | clf_exit: 0.202 0.389 0.408
Batch: 300 | Loss: 3.633 | Acc: 44.609,71.301,88.883,% | Adaptive Acc: 84.448% | clf_exit: 0.203 0.389 0.409
Batch: 320 | Loss: 3.638 | Acc: 44.546,71.228,88.758,% | Adaptive Acc: 84.336% | clf_exit: 0.202 0.389 0.409
Batch: 340 | Loss: 3.647 | Acc: 44.421,71.181,88.597,% | Adaptive Acc: 84.194% | clf_exit: 0.201 0.389 0.410
Batch: 360 | Loss: 3.652 | Acc: 44.367,71.128,88.485,% | Adaptive Acc: 84.081% | clf_exit: 0.201 0.389 0.410
Batch: 380 | Loss: 3.664 | Acc: 44.318,71.014,88.298,% | Adaptive Acc: 83.875% | clf_exit: 0.201 0.389 0.410
Batch: 0 | Loss: 4.489 | Acc: 41.406,63.281,68.750,% | Adaptive Acc: 67.188% | clf_exit: 0.297 0.383 0.320
Batch: 20 | Loss: 5.075 | Acc: 39.769,62.128,66.071,% | Adaptive Acc: 62.872% | clf_exit: 0.275 0.359 0.366
Batch: 40 | Loss: 5.050 | Acc: 40.130,61.814,66.368,% | Adaptive Acc: 63.396% | clf_exit: 0.264 0.356 0.380
Batch: 60 | Loss: 5.066 | Acc: 39.908,61.463,66.112,% | Adaptive Acc: 63.217% | clf_exit: 0.258 0.355 0.387
Train all parameters

Epoch: 116
Batch: 0 | Loss: 3.968 | Acc: 35.938,67.188,89.844,% | Adaptive Acc: 85.938% | clf_exit: 0.117 0.430 0.453
Batch: 20 | Loss: 3.523 | Acc: 44.345,73.214,90.104,% | Adaptive Acc: 85.826% | clf_exit: 0.199 0.390 0.412
Batch: 40 | Loss: 3.553 | Acc: 45.027,72.085,89.596,% | Adaptive Acc: 85.213% | clf_exit: 0.194 0.400 0.406
Batch: 60 | Loss: 3.555 | Acc: 45.287,72.054,89.857,% | Adaptive Acc: 85.476% | clf_exit: 0.196 0.398 0.406
Batch: 80 | Loss: 3.581 | Acc: 44.753,71.971,90.056,% | Adaptive Acc: 85.725% | clf_exit: 0.193 0.396 0.412
Batch: 100 | Loss: 3.584 | Acc: 44.647,71.790,89.937,% | Adaptive Acc: 85.528% | clf_exit: 0.191 0.400 0.409
Batch: 120 | Loss: 3.571 | Acc: 44.809,72.043,90.050,% | Adaptive Acc: 85.582% | clf_exit: 0.194 0.400 0.406
Batch: 140 | Loss: 3.569 | Acc: 44.858,71.969,89.955,% | Adaptive Acc: 85.372% | clf_exit: 0.198 0.399 0.404
Batch: 160 | Loss: 3.588 | Acc: 44.585,71.734,89.718,% | Adaptive Acc: 85.195% | clf_exit: 0.197 0.397 0.406
Batch: 180 | Loss: 3.594 | Acc: 44.492,71.733,89.568,% | Adaptive Acc: 85.074% | clf_exit: 0.198 0.395 0.407
Batch: 200 | Loss: 3.610 | Acc: 44.426,71.529,89.350,% | Adaptive Acc: 84.900% | clf_exit: 0.199 0.392 0.409
Batch: 220 | Loss: 3.609 | Acc: 44.453,71.628,89.292,% | Adaptive Acc: 84.873% | clf_exit: 0.197 0.394 0.409
Batch: 240 | Loss: 3.620 | Acc: 44.311,71.518,89.205,% | Adaptive Acc: 84.754% | clf_exit: 0.196 0.394 0.409
Batch: 260 | Loss: 3.625 | Acc: 44.328,71.414,89.164,% | Adaptive Acc: 84.767% | clf_exit: 0.197 0.394 0.409
Batch: 280 | Loss: 3.627 | Acc: 44.356,71.394,89.110,% | Adaptive Acc: 84.706% | clf_exit: 0.198 0.392 0.410
Batch: 300 | Loss: 3.627 | Acc: 44.464,71.429,89.039,% | Adaptive Acc: 84.655% | clf_exit: 0.199 0.392 0.409
Batch: 320 | Loss: 3.627 | Acc: 44.526,71.398,88.970,% | Adaptive Acc: 84.592% | clf_exit: 0.199 0.392 0.409
Batch: 340 | Loss: 3.634 | Acc: 44.529,71.366,88.863,% | Adaptive Acc: 84.538% | clf_exit: 0.199 0.392 0.409
Batch: 360 | Loss: 3.641 | Acc: 44.488,71.319,88.744,% | Adaptive Acc: 84.397% | clf_exit: 0.199 0.393 0.408
Batch: 380 | Loss: 3.647 | Acc: 44.447,71.274,88.695,% | Adaptive Acc: 84.393% | clf_exit: 0.198 0.392 0.409
Batch: 0 | Loss: 4.415 | Acc: 42.969,62.500,68.750,% | Adaptive Acc: 65.625% | clf_exit: 0.266 0.469 0.266
Batch: 20 | Loss: 5.001 | Acc: 41.034,62.351,67.225,% | Adaptive Acc: 62.872% | clf_exit: 0.282 0.383 0.335
Batch: 40 | Loss: 4.982 | Acc: 41.349,61.776,66.654,% | Adaptive Acc: 62.767% | clf_exit: 0.283 0.376 0.341
Batch: 60 | Loss: 5.027 | Acc: 41.240,61.347,66.176,% | Adaptive Acc: 62.551% | clf_exit: 0.283 0.372 0.346
Train all parameters

Epoch: 117
Batch: 0 | Loss: 3.033 | Acc: 52.344,78.125,89.844,% | Adaptive Acc: 89.062% | clf_exit: 0.266 0.391 0.344
Batch: 20 | Loss: 3.688 | Acc: 42.522,69.568,90.290,% | Adaptive Acc: 85.082% | clf_exit: 0.192 0.390 0.417
Batch: 40 | Loss: 3.666 | Acc: 42.473,69.836,90.111,% | Adaptive Acc: 85.271% | clf_exit: 0.192 0.390 0.418
Batch: 60 | Loss: 3.657 | Acc: 43.161,70.223,90.061,% | Adaptive Acc: 85.323% | clf_exit: 0.192 0.391 0.417
Batch: 80 | Loss: 3.643 | Acc: 43.355,70.486,89.998,% | Adaptive Acc: 85.349% | clf_exit: 0.192 0.393 0.415
Batch: 100 | Loss: 3.630 | Acc: 43.649,70.854,90.045,% | Adaptive Acc: 85.535% | clf_exit: 0.192 0.395 0.413
Batch: 120 | Loss: 3.611 | Acc: 43.924,71.223,90.128,% | Adaptive Acc: 85.795% | clf_exit: 0.193 0.398 0.409
Batch: 140 | Loss: 3.618 | Acc: 43.994,71.282,90.032,% | Adaptive Acc: 85.583% | clf_exit: 0.196 0.395 0.409
Batch: 160 | Loss: 3.610 | Acc: 44.167,71.249,90.038,% | Adaptive Acc: 85.510% | clf_exit: 0.198 0.396 0.406
Batch: 180 | Loss: 3.617 | Acc: 44.138,71.206,89.900,% | Adaptive Acc: 85.320% | clf_exit: 0.197 0.397 0.406
Batch: 200 | Loss: 3.613 | Acc: 44.119,71.273,89.879,% | Adaptive Acc: 85.378% | clf_exit: 0.197 0.396 0.406
Batch: 220 | Loss: 3.620 | Acc: 44.118,71.200,89.791,% | Adaptive Acc: 85.259% | clf_exit: 0.199 0.394 0.407
Batch: 240 | Loss: 3.626 | Acc: 44.123,71.139,89.617,% | Adaptive Acc: 85.134% | clf_exit: 0.199 0.393 0.408
Batch: 260 | Loss: 3.627 | Acc: 44.178,71.127,89.532,% | Adaptive Acc: 85.084% | clf_exit: 0.199 0.393 0.408
Batch: 280 | Loss: 3.628 | Acc: 44.256,71.197,89.421,% | Adaptive Acc: 85.009% | clf_exit: 0.200 0.393 0.407
Batch: 300 | Loss: 3.629 | Acc: 44.313,71.205,89.255,% | Adaptive Acc: 84.907% | clf_exit: 0.200 0.392 0.407
Batch: 320 | Loss: 3.638 | Acc: 44.344,71.106,89.099,% | Adaptive Acc: 84.769% | clf_exit: 0.200 0.392 0.408
Batch: 340 | Loss: 3.632 | Acc: 44.380,71.238,89.049,% | Adaptive Acc: 84.755% | clf_exit: 0.200 0.393 0.407
Batch: 360 | Loss: 3.637 | Acc: 44.386,71.230,88.982,% | Adaptive Acc: 84.684% | clf_exit: 0.199 0.393 0.407
Batch: 380 | Loss: 3.644 | Acc: 44.400,71.118,88.814,% | Adaptive Acc: 84.539% | clf_exit: 0.199 0.393 0.408
Batch: 0 | Loss: 4.799 | Acc: 42.188,64.062,71.094,% | Adaptive Acc: 64.062% | clf_exit: 0.273 0.391 0.336
Batch: 20 | Loss: 5.007 | Acc: 40.997,61.756,66.257,% | Adaptive Acc: 63.244% | clf_exit: 0.268 0.379 0.353
Batch: 40 | Loss: 4.999 | Acc: 41.463,61.528,66.178,% | Adaptive Acc: 63.357% | clf_exit: 0.269 0.372 0.359
Batch: 60 | Loss: 5.019 | Acc: 41.522,61.796,65.779,% | Adaptive Acc: 63.345% | clf_exit: 0.269 0.370 0.361
Train all parameters

Epoch: 118
Batch: 0 | Loss: 3.752 | Acc: 45.312,72.656,88.281,% | Adaptive Acc: 85.938% | clf_exit: 0.242 0.320 0.438
Batch: 20 | Loss: 3.638 | Acc: 44.234,72.135,90.365,% | Adaptive Acc: 86.161% | clf_exit: 0.195 0.382 0.423
Batch: 40 | Loss: 3.606 | Acc: 44.760,72.104,90.777,% | Adaptive Acc: 86.204% | clf_exit: 0.194 0.394 0.412
Batch: 60 | Loss: 3.593 | Acc: 44.416,72.272,90.907,% | Adaptive Acc: 86.155% | clf_exit: 0.197 0.396 0.407
Batch: 80 | Loss: 3.580 | Acc: 44.522,72.280,90.885,% | Adaptive Acc: 85.966% | clf_exit: 0.196 0.400 0.404
Batch: 100 | Loss: 3.578 | Acc: 44.616,72.045,90.772,% | Adaptive Acc: 85.814% | clf_exit: 0.197 0.398 0.405
Batch: 120 | Loss: 3.581 | Acc: 44.880,71.907,90.670,% | Adaptive Acc: 85.757% | clf_exit: 0.198 0.399 0.404
Batch: 140 | Loss: 3.589 | Acc: 44.625,71.809,90.631,% | Adaptive Acc: 85.710% | clf_exit: 0.198 0.397 0.405
Batch: 160 | Loss: 3.589 | Acc: 44.614,71.822,90.630,% | Adaptive Acc: 85.637% | clf_exit: 0.199 0.397 0.404
Batch: 180 | Loss: 3.592 | Acc: 44.764,71.741,90.470,% | Adaptive Acc: 85.527% | clf_exit: 0.200 0.396 0.404
Batch: 200 | Loss: 3.588 | Acc: 44.846,71.731,90.299,% | Adaptive Acc: 85.440% | clf_exit: 0.202 0.394 0.404
Batch: 220 | Loss: 3.589 | Acc: 45.005,71.688,90.180,% | Adaptive Acc: 85.372% | clf_exit: 0.202 0.393 0.404
Batch: 240 | Loss: 3.588 | Acc: 44.943,71.781,90.064,% | Adaptive Acc: 85.305% | clf_exit: 0.202 0.395 0.403
Batch: 260 | Loss: 3.591 | Acc: 44.935,71.746,89.952,% | Adaptive Acc: 85.252% | clf_exit: 0.203 0.395 0.402
Batch: 280 | Loss: 3.591 | Acc: 44.990,71.747,89.830,% | Adaptive Acc: 85.148% | clf_exit: 0.204 0.395 0.402
Batch: 300 | Loss: 3.600 | Acc: 44.978,71.662,89.756,% | Adaptive Acc: 85.068% | clf_exit: 0.205 0.394 0.402
Batch: 320 | Loss: 3.601 | Acc: 44.938,71.632,89.678,% | Adaptive Acc: 85.030% | clf_exit: 0.205 0.394 0.402
Batch: 340 | Loss: 3.608 | Acc: 44.934,71.495,89.596,% | Adaptive Acc: 85.003% | clf_exit: 0.204 0.393 0.403
Batch: 360 | Loss: 3.605 | Acc: 44.940,71.501,89.543,% | Adaptive Acc: 84.938% | clf_exit: 0.204 0.393 0.403
Batch: 380 | Loss: 3.613 | Acc: 44.851,71.479,89.384,% | Adaptive Acc: 84.804% | clf_exit: 0.204 0.392 0.404
Batch: 0 | Loss: 5.058 | Acc: 44.531,63.281,65.625,% | Adaptive Acc: 64.062% | clf_exit: 0.297 0.375 0.328
Batch: 20 | Loss: 5.290 | Acc: 40.923,60.119,64.918,% | Adaptive Acc: 61.161% | clf_exit: 0.267 0.392 0.341
Batch: 40 | Loss: 5.226 | Acc: 41.749,60.023,64.329,% | Adaptive Acc: 61.719% | clf_exit: 0.265 0.388 0.347
Batch: 60 | Loss: 5.229 | Acc: 41.086,59.913,64.575,% | Adaptive Acc: 62.244% | clf_exit: 0.261 0.389 0.350
Train all parameters

Epoch: 119
Batch: 0 | Loss: 3.776 | Acc: 36.719,68.750,90.625,% | Adaptive Acc: 86.719% | clf_exit: 0.156 0.398 0.445
Batch: 20 | Loss: 3.567 | Acc: 43.824,71.652,90.067,% | Adaptive Acc: 85.491% | clf_exit: 0.204 0.401 0.394
Batch: 40 | Loss: 3.546 | Acc: 43.826,71.894,90.816,% | Adaptive Acc: 85.595% | clf_exit: 0.206 0.394 0.401
Batch: 60 | Loss: 3.550 | Acc: 44.121,71.798,90.868,% | Adaptive Acc: 85.758% | clf_exit: 0.208 0.390 0.402
Batch: 80 | Loss: 3.542 | Acc: 44.473,72.155,90.818,% | Adaptive Acc: 85.880% | clf_exit: 0.207 0.394 0.399
Batch: 100 | Loss: 3.543 | Acc: 44.593,72.300,90.795,% | Adaptive Acc: 85.798% | clf_exit: 0.208 0.392 0.399
Batch: 120 | Loss: 3.562 | Acc: 44.428,72.095,90.631,% | Adaptive Acc: 85.692% | clf_exit: 0.208 0.390 0.402
Batch: 140 | Loss: 3.554 | Acc: 44.492,72.285,90.459,% | Adaptive Acc: 85.611% | clf_exit: 0.211 0.388 0.402
Batch: 160 | Loss: 3.566 | Acc: 44.463,72.113,90.329,% | Adaptive Acc: 85.510% | clf_exit: 0.210 0.388 0.402
Batch: 180 | Loss: 3.577 | Acc: 44.436,72.043,90.254,% | Adaptive Acc: 85.441% | clf_exit: 0.209 0.388 0.403
Batch: 200 | Loss: 3.578 | Acc: 44.570,72.077,90.093,% | Adaptive Acc: 85.386% | clf_exit: 0.207 0.389 0.403
Batch: 220 | Loss: 3.584 | Acc: 44.514,72.144,89.950,% | Adaptive Acc: 85.294% | clf_exit: 0.206 0.390 0.404
Batch: 240 | Loss: 3.581 | Acc: 44.609,72.115,89.941,% | Adaptive Acc: 85.302% | clf_exit: 0.206 0.391 0.403
Batch: 260 | Loss: 3.584 | Acc: 44.567,72.046,89.811,% | Adaptive Acc: 85.189% | clf_exit: 0.205 0.392 0.403
Batch: 280 | Loss: 3.591 | Acc: 44.556,71.917,89.671,% | Adaptive Acc: 85.087% | clf_exit: 0.204 0.392 0.403
Batch: 300 | Loss: 3.596 | Acc: 44.498,71.808,89.553,% | Adaptive Acc: 84.985% | clf_exit: 0.204 0.393 0.403
Batch: 320 | Loss: 3.608 | Acc: 44.461,71.734,89.411,% | Adaptive Acc: 84.927% | clf_exit: 0.203 0.393 0.403
Batch: 340 | Loss: 3.616 | Acc: 44.410,71.657,89.305,% | Adaptive Acc: 84.836% | clf_exit: 0.202 0.392 0.405
Batch: 360 | Loss: 3.618 | Acc: 44.430,71.626,89.186,% | Adaptive Acc: 84.710% | clf_exit: 0.203 0.392 0.405
Batch: 380 | Loss: 3.617 | Acc: 44.482,71.686,89.132,% | Adaptive Acc: 84.713% | clf_exit: 0.203 0.393 0.405
Batch: 0 | Loss: 4.705 | Acc: 44.531,65.625,71.875,% | Adaptive Acc: 71.875% | clf_exit: 0.266 0.375 0.359
Batch: 20 | Loss: 5.048 | Acc: 41.295,62.723,66.853,% | Adaptive Acc: 63.467% | clf_exit: 0.267 0.373 0.360
Batch: 40 | Loss: 5.071 | Acc: 40.568,61.471,65.892,% | Adaptive Acc: 63.224% | clf_exit: 0.262 0.374 0.364
Batch: 60 | Loss: 5.082 | Acc: 40.791,61.181,65.779,% | Adaptive Acc: 62.999% | clf_exit: 0.260 0.374 0.367
Train all parameters

Epoch: 120
Batch: 0 | Loss: 3.693 | Acc: 34.375,71.094,91.406,% | Adaptive Acc: 83.594% | clf_exit: 0.164 0.383 0.453
Batch: 20 | Loss: 3.618 | Acc: 42.783,72.284,90.774,% | Adaptive Acc: 86.384% | clf_exit: 0.206 0.377 0.417
Batch: 40 | Loss: 3.549 | Acc: 44.055,72.199,91.197,% | Adaptive Acc: 86.357% | clf_exit: 0.207 0.381 0.412
Batch: 60 | Loss: 3.544 | Acc: 44.531,72.144,91.253,% | Adaptive Acc: 86.514% | clf_exit: 0.206 0.386 0.408
Batch: 80 | Loss: 3.539 | Acc: 44.599,72.328,91.397,% | Adaptive Acc: 86.564% | clf_exit: 0.203 0.390 0.407
Batch: 100 | Loss: 3.540 | Acc: 44.129,72.262,91.414,% | Adaptive Acc: 86.371% | clf_exit: 0.202 0.393 0.405
Batch: 120 | Loss: 3.533 | Acc: 44.344,72.321,91.393,% | Adaptive Acc: 86.448% | clf_exit: 0.202 0.394 0.404
Batch: 140 | Loss: 3.526 | Acc: 44.498,72.496,91.362,% | Adaptive Acc: 86.436% | clf_exit: 0.202 0.395 0.403
Batch: 160 | Loss: 3.531 | Acc: 44.541,72.389,91.115,% | Adaptive Acc: 86.190% | clf_exit: 0.205 0.394 0.401
Batch: 180 | Loss: 3.538 | Acc: 44.531,72.289,90.936,% | Adaptive Acc: 86.011% | clf_exit: 0.205 0.395 0.400
Batch: 200 | Loss: 3.555 | Acc: 44.364,72.054,90.644,% | Adaptive Acc: 85.731% | clf_exit: 0.205 0.394 0.401
Batch: 220 | Loss: 3.560 | Acc: 44.379,71.907,90.519,% | Adaptive Acc: 85.609% | clf_exit: 0.205 0.394 0.401
Batch: 240 | Loss: 3.567 | Acc: 44.434,71.856,90.340,% | Adaptive Acc: 85.461% | clf_exit: 0.205 0.394 0.401
Batch: 260 | Loss: 3.575 | Acc: 44.513,71.785,90.125,% | Adaptive Acc: 85.306% | clf_exit: 0.204 0.394 0.401
Batch: 280 | Loss: 3.573 | Acc: 44.534,71.836,90.063,% | Adaptive Acc: 85.309% | clf_exit: 0.205 0.394 0.401
Batch: 300 | Loss: 3.578 | Acc: 44.490,71.841,89.992,% | Adaptive Acc: 85.281% | clf_exit: 0.205 0.393 0.402
Batch: 320 | Loss: 3.580 | Acc: 44.463,71.802,89.931,% | Adaptive Acc: 85.220% | clf_exit: 0.204 0.394 0.402
Batch: 340 | Loss: 3.584 | Acc: 44.476,71.774,89.786,% | Adaptive Acc: 85.099% | clf_exit: 0.205 0.394 0.402
Batch: 360 | Loss: 3.587 | Acc: 44.551,71.758,89.664,% | Adaptive Acc: 84.940% | clf_exit: 0.206 0.394 0.401
Batch: 380 | Loss: 3.587 | Acc: 44.515,71.750,89.567,% | Adaptive Acc: 84.843% | clf_exit: 0.205 0.395 0.400
Batch: 0 | Loss: 4.623 | Acc: 41.406,65.625,71.875,% | Adaptive Acc: 67.969% | clf_exit: 0.219 0.531 0.250
Batch: 20 | Loss: 4.961 | Acc: 41.741,62.686,65.848,% | Adaptive Acc: 64.174% | clf_exit: 0.244 0.410 0.346
Batch: 40 | Loss: 4.960 | Acc: 42.168,62.271,65.663,% | Adaptive Acc: 64.043% | clf_exit: 0.246 0.401 0.353
Batch: 60 | Loss: 4.979 | Acc: 41.944,62.129,65.971,% | Adaptive Acc: 63.832% | clf_exit: 0.247 0.397 0.356
Train all parameters

Epoch: 121
Batch: 0 | Loss: 3.607 | Acc: 39.844,69.531,95.312,% | Adaptive Acc: 85.938% | clf_exit: 0.125 0.492 0.383
Batch: 20 | Loss: 3.597 | Acc: 43.080,71.726,91.629,% | Adaptive Acc: 86.607% | clf_exit: 0.199 0.402 0.398
Batch: 40 | Loss: 3.526 | Acc: 45.236,72.618,91.463,% | Adaptive Acc: 86.795% | clf_exit: 0.208 0.402 0.389
Batch: 60 | Loss: 3.532 | Acc: 45.402,72.490,91.176,% | Adaptive Acc: 86.463% | clf_exit: 0.209 0.400 0.391
Batch: 80 | Loss: 3.507 | Acc: 45.583,73.052,91.339,% | Adaptive Acc: 86.719% | clf_exit: 0.208 0.402 0.390
Batch: 100 | Loss: 3.479 | Acc: 45.893,73.167,91.391,% | Adaptive Acc: 86.897% | clf_exit: 0.210 0.401 0.389
Batch: 120 | Loss: 3.482 | Acc: 45.655,73.011,91.329,% | Adaptive Acc: 86.719% | clf_exit: 0.210 0.399 0.391
Batch: 140 | Loss: 3.483 | Acc: 45.490,73.005,91.362,% | Adaptive Acc: 86.713% | clf_exit: 0.208 0.401 0.390
Batch: 160 | Loss: 3.492 | Acc: 45.259,72.938,91.232,% | Adaptive Acc: 86.568% | clf_exit: 0.208 0.401 0.391
Batch: 180 | Loss: 3.503 | Acc: 45.041,72.799,91.126,% | Adaptive Acc: 86.266% | clf_exit: 0.209 0.400 0.391
Batch: 200 | Loss: 3.510 | Acc: 44.970,72.734,90.959,% | Adaptive Acc: 86.132% | clf_exit: 0.208 0.401 0.391
Batch: 220 | Loss: 3.509 | Acc: 45.005,72.730,90.858,% | Adaptive Acc: 86.008% | clf_exit: 0.207 0.402 0.391
Batch: 240 | Loss: 3.525 | Acc: 44.878,72.517,90.751,% | Adaptive Acc: 85.808% | clf_exit: 0.206 0.401 0.393
Batch: 260 | Loss: 3.525 | Acc: 44.896,72.560,90.622,% | Adaptive Acc: 85.761% | clf_exit: 0.206 0.400 0.394
Batch: 280 | Loss: 3.528 | Acc: 44.929,72.531,90.455,% | Adaptive Acc: 85.637% | clf_exit: 0.207 0.399 0.394
Batch: 300 | Loss: 3.541 | Acc: 44.967,72.337,90.241,% | Adaptive Acc: 85.447% | clf_exit: 0.205 0.399 0.396
Batch: 320 | Loss: 3.550 | Acc: 44.838,72.250,90.143,% | Adaptive Acc: 85.383% | clf_exit: 0.205 0.398 0.397
Batch: 340 | Loss: 3.557 | Acc: 44.818,72.223,90.018,% | Adaptive Acc: 85.335% | clf_exit: 0.205 0.398 0.397
Batch: 360 | Loss: 3.561 | Acc: 44.806,72.182,89.948,% | Adaptive Acc: 85.299% | clf_exit: 0.204 0.399 0.397
Batch: 380 | Loss: 3.567 | Acc: 44.818,72.084,89.838,% | Adaptive Acc: 85.203% | clf_exit: 0.205 0.398 0.398
Batch: 0 | Loss: 5.081 | Acc: 37.500,60.938,69.531,% | Adaptive Acc: 66.406% | clf_exit: 0.289 0.367 0.344
Batch: 20 | Loss: 5.175 | Acc: 39.472,60.528,65.179,% | Adaptive Acc: 62.835% | clf_exit: 0.250 0.380 0.370
Batch: 40 | Loss: 5.208 | Acc: 39.787,60.290,64.615,% | Adaptive Acc: 62.443% | clf_exit: 0.248 0.381 0.371
Batch: 60 | Loss: 5.290 | Acc: 39.101,59.580,64.203,% | Adaptive Acc: 61.783% | clf_exit: 0.248 0.381 0.371
Train all parameters

Epoch: 122
Batch: 0 | Loss: 3.739 | Acc: 44.531,69.531,86.719,% | Adaptive Acc: 82.812% | clf_exit: 0.305 0.289 0.406
Batch: 20 | Loss: 3.522 | Acc: 44.792,73.661,91.406,% | Adaptive Acc: 86.124% | clf_exit: 0.210 0.388 0.402
Batch: 40 | Loss: 3.448 | Acc: 45.274,73.723,91.902,% | Adaptive Acc: 87.062% | clf_exit: 0.204 0.400 0.396
Batch: 60 | Loss: 3.487 | Acc: 45.146,73.156,91.944,% | Adaptive Acc: 86.962% | clf_exit: 0.208 0.398 0.394
Batch: 80 | Loss: 3.504 | Acc: 44.734,72.936,91.763,% | Adaptive Acc: 86.989% | clf_exit: 0.207 0.396 0.397
Batch: 100 | Loss: 3.509 | Acc: 44.825,72.881,91.545,% | Adaptive Acc: 86.912% | clf_exit: 0.206 0.393 0.401
Batch: 120 | Loss: 3.519 | Acc: 44.718,72.927,91.400,% | Adaptive Acc: 86.790% | clf_exit: 0.204 0.395 0.401
Batch: 140 | Loss: 3.509 | Acc: 44.758,73.022,91.323,% | Adaptive Acc: 86.586% | clf_exit: 0.205 0.396 0.399
Batch: 160 | Loss: 3.523 | Acc: 44.623,72.807,91.144,% | Adaptive Acc: 86.413% | clf_exit: 0.205 0.395 0.400
Batch: 180 | Loss: 3.520 | Acc: 44.738,72.760,91.065,% | Adaptive Acc: 86.283% | clf_exit: 0.205 0.395 0.401
Batch: 200 | Loss: 3.524 | Acc: 44.924,72.812,90.843,% | Adaptive Acc: 86.144% | clf_exit: 0.205 0.395 0.400
Batch: 220 | Loss: 3.526 | Acc: 44.959,72.794,90.728,% | Adaptive Acc: 86.022% | clf_exit: 0.206 0.396 0.399
Batch: 240 | Loss: 3.535 | Acc: 44.885,72.689,90.528,% | Adaptive Acc: 85.824% | clf_exit: 0.206 0.395 0.399
Batch: 260 | Loss: 3.543 | Acc: 44.872,72.551,90.353,% | Adaptive Acc: 85.635% | clf_exit: 0.206 0.395 0.399
Batch: 280 | Loss: 3.555 | Acc: 44.718,72.406,90.200,% | Adaptive Acc: 85.451% | clf_exit: 0.206 0.395 0.399
Batch: 300 | Loss: 3.555 | Acc: 44.856,72.358,90.176,% | Adaptive Acc: 85.431% | clf_exit: 0.206 0.396 0.398
Batch: 320 | Loss: 3.557 | Acc: 44.879,72.345,90.102,% | Adaptive Acc: 85.412% | clf_exit: 0.205 0.396 0.399
Batch: 340 | Loss: 3.556 | Acc: 44.854,72.313,90.036,% | Adaptive Acc: 85.356% | clf_exit: 0.206 0.395 0.398
Batch: 360 | Loss: 3.567 | Acc: 44.782,72.128,89.900,% | Adaptive Acc: 85.187% | clf_exit: 0.206 0.394 0.399
Batch: 380 | Loss: 3.575 | Acc: 44.786,72.047,89.768,% | Adaptive Acc: 85.113% | clf_exit: 0.206 0.394 0.400
Batch: 0 | Loss: 5.482 | Acc: 39.062,57.812,63.281,% | Adaptive Acc: 60.156% | clf_exit: 0.258 0.492 0.250
Batch: 20 | Loss: 5.383 | Acc: 38.765,59.375,63.653,% | Adaptive Acc: 60.677% | clf_exit: 0.246 0.410 0.343
Batch: 40 | Loss: 5.373 | Acc: 39.291,59.184,63.453,% | Adaptive Acc: 60.442% | clf_exit: 0.246 0.402 0.352
Batch: 60 | Loss: 5.374 | Acc: 39.075,59.439,63.320,% | Adaptive Acc: 60.592% | clf_exit: 0.244 0.402 0.354
Train all parameters

Epoch: 123
Batch: 0 | Loss: 3.281 | Acc: 40.625,78.125,95.312,% | Adaptive Acc: 91.406% | clf_exit: 0.133 0.383 0.484
Batch: 20 | Loss: 3.373 | Acc: 45.908,75.632,91.890,% | Adaptive Acc: 86.868% | clf_exit: 0.202 0.408 0.390
Batch: 40 | Loss: 3.416 | Acc: 45.770,74.943,91.635,% | Adaptive Acc: 86.604% | clf_exit: 0.206 0.403 0.392
Batch: 60 | Loss: 3.458 | Acc: 45.735,74.232,91.598,% | Adaptive Acc: 86.680% | clf_exit: 0.202 0.408 0.390
Batch: 80 | Loss: 3.477 | Acc: 45.660,73.823,91.136,% | Adaptive Acc: 86.256% | clf_exit: 0.205 0.400 0.395
Batch: 100 | Loss: 3.478 | Acc: 45.606,73.755,91.019,% | Adaptive Acc: 86.015% | clf_exit: 0.206 0.401 0.394
Batch: 120 | Loss: 3.480 | Acc: 45.558,73.625,91.083,% | Adaptive Acc: 86.067% | clf_exit: 0.206 0.401 0.393
Batch: 140 | Loss: 3.494 | Acc: 45.656,73.305,90.935,% | Adaptive Acc: 85.827% | clf_exit: 0.208 0.400 0.393
Batch: 160 | Loss: 3.509 | Acc: 45.458,73.044,90.746,% | Adaptive Acc: 85.690% | clf_exit: 0.207 0.398 0.394
Batch: 180 | Loss: 3.525 | Acc: 45.338,72.876,90.565,% | Adaptive Acc: 85.597% | clf_exit: 0.206 0.398 0.397
Batch: 200 | Loss: 3.537 | Acc: 45.309,72.707,90.435,% | Adaptive Acc: 85.491% | clf_exit: 0.205 0.397 0.398
Batch: 220 | Loss: 3.535 | Acc: 45.401,72.762,90.445,% | Adaptive Acc: 85.595% | clf_exit: 0.206 0.396 0.397
Batch: 240 | Loss: 3.534 | Acc: 45.387,72.685,90.434,% | Adaptive Acc: 85.568% | clf_exit: 0.207 0.397 0.396
Batch: 260 | Loss: 3.548 | Acc: 45.265,72.474,90.308,% | Adaptive Acc: 85.441% | clf_exit: 0.206 0.397 0.397
Batch: 280 | Loss: 3.548 | Acc: 45.262,72.501,90.186,% | Adaptive Acc: 85.368% | clf_exit: 0.205 0.397 0.398
Batch: 300 | Loss: 3.552 | Acc: 45.211,72.469,90.127,% | Adaptive Acc: 85.296% | clf_exit: 0.206 0.397 0.397
Batch: 320 | Loss: 3.566 | Acc: 45.128,72.291,89.951,% | Adaptive Acc: 85.139% | clf_exit: 0.205 0.397 0.398
Batch: 340 | Loss: 3.569 | Acc: 45.161,72.219,89.828,% | Adaptive Acc: 85.060% | clf_exit: 0.204 0.397 0.399
Batch: 360 | Loss: 3.576 | Acc: 45.053,72.146,89.740,% | Adaptive Acc: 85.005% | clf_exit: 0.205 0.395 0.400
Batch: 380 | Loss: 3.578 | Acc: 45.075,72.113,89.680,% | Adaptive Acc: 85.041% | clf_exit: 0.204 0.395 0.401
Batch: 0 | Loss: 5.074 | Acc: 38.281,60.938,71.094,% | Adaptive Acc: 67.188% | clf_exit: 0.266 0.398 0.336
Batch: 20 | Loss: 5.272 | Acc: 39.397,60.119,65.774,% | Adaptive Acc: 62.277% | clf_exit: 0.242 0.399 0.359
Batch: 40 | Loss: 5.206 | Acc: 40.301,60.175,65.492,% | Adaptive Acc: 62.576% | clf_exit: 0.239 0.397 0.364
Batch: 60 | Loss: 5.201 | Acc: 39.741,60.297,65.638,% | Adaptive Acc: 62.628% | clf_exit: 0.238 0.398 0.364
Train all parameters

Epoch: 124
Batch: 0 | Loss: 3.684 | Acc: 44.531,65.625,91.406,% | Adaptive Acc: 82.812% | clf_exit: 0.148 0.391 0.461
Batch: 20 | Loss: 3.475 | Acc: 45.796,73.735,91.592,% | Adaptive Acc: 86.198% | clf_exit: 0.205 0.394 0.400
Batch: 40 | Loss: 3.464 | Acc: 45.675,73.399,91.997,% | Adaptive Acc: 86.585% | clf_exit: 0.209 0.399 0.392
Batch: 60 | Loss: 3.468 | Acc: 45.825,73.591,91.816,% | Adaptive Acc: 86.578% | clf_exit: 0.213 0.397 0.390
Batch: 80 | Loss: 3.463 | Acc: 46.007,73.428,91.966,% | Adaptive Acc: 86.709% | clf_exit: 0.212 0.397 0.391
Batch: 100 | Loss: 3.467 | Acc: 45.939,73.252,92.064,% | Adaptive Acc: 86.866% | clf_exit: 0.210 0.398 0.392
Batch: 120 | Loss: 3.474 | Acc: 45.939,73.140,91.852,% | Adaptive Acc: 86.615% | clf_exit: 0.211 0.399 0.391
Batch: 140 | Loss: 3.481 | Acc: 45.745,72.955,91.805,% | Adaptive Acc: 86.514% | clf_exit: 0.210 0.399 0.390
Batch: 160 | Loss: 3.500 | Acc: 45.560,72.690,91.440,% | Adaptive Acc: 86.301% | clf_exit: 0.208 0.399 0.393
Batch: 180 | Loss: 3.509 | Acc: 45.511,72.579,91.359,% | Adaptive Acc: 86.188% | clf_exit: 0.208 0.397 0.395
Batch: 200 | Loss: 3.521 | Acc: 45.204,72.380,91.181,% | Adaptive Acc: 86.039% | clf_exit: 0.207 0.397 0.396
Batch: 220 | Loss: 3.526 | Acc: 45.178,72.274,91.014,% | Adaptive Acc: 85.909% | clf_exit: 0.207 0.396 0.397
Batch: 240 | Loss: 3.526 | Acc: 45.150,72.348,90.888,% | Adaptive Acc: 85.856% | clf_exit: 0.207 0.397 0.396
Batch: 260 | Loss: 3.526 | Acc: 45.205,72.306,90.727,% | Adaptive Acc: 85.767% | clf_exit: 0.208 0.398 0.394
Batch: 280 | Loss: 3.531 | Acc: 45.132,72.273,90.625,% | Adaptive Acc: 85.757% | clf_exit: 0.208 0.397 0.395
Batch: 300 | Loss: 3.533 | Acc: 45.237,72.228,90.519,% | Adaptive Acc: 85.696% | clf_exit: 0.209 0.397 0.395
Batch: 320 | Loss: 3.539 | Acc: 45.213,72.162,90.394,% | Adaptive Acc: 85.570% | clf_exit: 0.208 0.397 0.395
Batch: 340 | Loss: 3.551 | Acc: 45.095,72.095,90.277,% | Adaptive Acc: 85.500% | clf_exit: 0.207 0.396 0.397
Batch: 360 | Loss: 3.559 | Acc: 45.046,72.089,90.129,% | Adaptive Acc: 85.386% | clf_exit: 0.206 0.397 0.397
Batch: 380 | Loss: 3.565 | Acc: 44.943,72.004,89.967,% | Adaptive Acc: 85.224% | clf_exit: 0.206 0.396 0.397
Batch: 0 | Loss: 4.707 | Acc: 42.969,64.844,64.844,% | Adaptive Acc: 63.281% | clf_exit: 0.227 0.383 0.391
Batch: 20 | Loss: 5.031 | Acc: 39.769,62.649,65.923,% | Adaptive Acc: 63.542% | clf_exit: 0.244 0.375 0.381
Batch: 40 | Loss: 5.056 | Acc: 40.091,61.814,65.454,% | Adaptive Acc: 63.357% | clf_exit: 0.248 0.367 0.385
Batch: 60 | Loss: 5.079 | Acc: 40.036,61.655,65.727,% | Adaptive Acc: 63.397% | clf_exit: 0.248 0.364 0.388
Train all parameters

Epoch: 125
Batch: 0 | Loss: 4.762 | Acc: 32.812,64.844,84.375,% | Adaptive Acc: 77.344% | clf_exit: 0.203 0.312 0.484
Batch: 20 | Loss: 3.510 | Acc: 44.903,73.065,90.774,% | Adaptive Acc: 86.086% | clf_exit: 0.221 0.398 0.381
Batch: 40 | Loss: 3.530 | Acc: 45.255,73.037,90.892,% | Adaptive Acc: 86.300% | clf_exit: 0.212 0.392 0.395
Batch: 60 | Loss: 3.490 | Acc: 45.671,73.873,91.189,% | Adaptive Acc: 86.373% | clf_exit: 0.211 0.399 0.390
Batch: 80 | Loss: 3.496 | Acc: 45.438,73.679,91.329,% | Adaptive Acc: 86.535% | clf_exit: 0.207 0.404 0.389
Batch: 100 | Loss: 3.502 | Acc: 45.297,73.492,91.267,% | Adaptive Acc: 86.510% | clf_exit: 0.206 0.401 0.393
Batch: 120 | Loss: 3.494 | Acc: 45.229,73.360,91.322,% | Adaptive Acc: 86.577% | clf_exit: 0.206 0.400 0.394
Batch: 140 | Loss: 3.502 | Acc: 45.213,73.116,91.279,% | Adaptive Acc: 86.558% | clf_exit: 0.207 0.397 0.396
Batch: 160 | Loss: 3.503 | Acc: 45.249,73.122,91.159,% | Adaptive Acc: 86.525% | clf_exit: 0.207 0.397 0.396
Batch: 180 | Loss: 3.491 | Acc: 45.325,73.131,91.212,% | Adaptive Acc: 86.563% | clf_exit: 0.207 0.399 0.394
Batch: 200 | Loss: 3.502 | Acc: 45.239,72.924,91.076,% | Adaptive Acc: 86.466% | clf_exit: 0.207 0.398 0.395
Batch: 220 | Loss: 3.508 | Acc: 45.231,72.875,90.915,% | Adaptive Acc: 86.252% | clf_exit: 0.207 0.398 0.395
Batch: 240 | Loss: 3.520 | Acc: 45.193,72.721,90.742,% | Adaptive Acc: 86.103% | clf_exit: 0.207 0.398 0.395
Batch: 260 | Loss: 3.522 | Acc: 45.154,72.626,90.688,% | Adaptive Acc: 85.946% | clf_exit: 0.208 0.398 0.394
Batch: 280 | Loss: 3.536 | Acc: 45.012,72.412,90.505,% | Adaptive Acc: 85.815% | clf_exit: 0.207 0.397 0.396
Batch: 300 | Loss: 3.543 | Acc: 45.061,72.350,90.301,% | Adaptive Acc: 85.621% | clf_exit: 0.207 0.397 0.396
Batch: 320 | Loss: 3.546 | Acc: 45.086,72.301,90.184,% | Adaptive Acc: 85.572% | clf_exit: 0.207 0.397 0.396
Batch: 340 | Loss: 3.548 | Acc: 45.047,72.255,90.130,% | Adaptive Acc: 85.514% | clf_exit: 0.207 0.396 0.396
Batch: 360 | Loss: 3.548 | Acc: 45.053,72.258,90.043,% | Adaptive Acc: 85.461% | clf_exit: 0.207 0.396 0.396
Batch: 380 | Loss: 3.554 | Acc: 45.091,72.232,89.930,% | Adaptive Acc: 85.392% | clf_exit: 0.207 0.396 0.396
Batch: 0 | Loss: 4.908 | Acc: 39.062,58.594,70.312,% | Adaptive Acc: 67.969% | clf_exit: 0.258 0.469 0.273
Batch: 20 | Loss: 5.166 | Acc: 40.960,61.049,66.109,% | Adaptive Acc: 63.467% | clf_exit: 0.239 0.381 0.381
Batch: 40 | Loss: 5.147 | Acc: 40.949,61.300,65.225,% | Adaptive Acc: 62.995% | clf_exit: 0.240 0.375 0.385
Batch: 60 | Loss: 5.161 | Acc: 40.433,61.027,64.959,% | Adaptive Acc: 62.551% | clf_exit: 0.242 0.369 0.388
Train all parameters

Epoch: 126
Batch: 0 | Loss: 3.058 | Acc: 50.000,75.781,96.875,% | Adaptive Acc: 91.406% | clf_exit: 0.234 0.414 0.352
Batch: 20 | Loss: 3.378 | Acc: 46.168,74.107,92.001,% | Adaptive Acc: 86.384% | clf_exit: 0.222 0.392 0.386
Batch: 40 | Loss: 3.386 | Acc: 45.998,73.590,91.921,% | Adaptive Acc: 86.700% | clf_exit: 0.213 0.407 0.381
Batch: 60 | Loss: 3.396 | Acc: 46.183,73.732,91.995,% | Adaptive Acc: 87.180% | clf_exit: 0.217 0.401 0.383
Batch: 80 | Loss: 3.398 | Acc: 46.152,73.688,91.696,% | Adaptive Acc: 86.883% | clf_exit: 0.215 0.403 0.382
Batch: 100 | Loss: 3.415 | Acc: 45.831,73.577,91.399,% | Adaptive Acc: 86.649% | clf_exit: 0.216 0.402 0.383
Batch: 120 | Loss: 3.412 | Acc: 45.945,73.605,91.445,% | Adaptive Acc: 86.757% | clf_exit: 0.216 0.403 0.382
Batch: 140 | Loss: 3.425 | Acc: 45.761,73.521,91.484,% | Adaptive Acc: 86.791% | clf_exit: 0.212 0.404 0.383
Batch: 160 | Loss: 3.436 | Acc: 45.575,73.389,91.411,% | Adaptive Acc: 86.738% | clf_exit: 0.211 0.403 0.386
Batch: 180 | Loss: 3.443 | Acc: 45.705,73.265,91.307,% | Adaptive Acc: 86.637% | clf_exit: 0.211 0.402 0.387
Batch: 200 | Loss: 3.450 | Acc: 45.565,73.080,91.216,% | Adaptive Acc: 86.482% | clf_exit: 0.212 0.400 0.389
Batch: 220 | Loss: 3.468 | Acc: 45.383,72.946,91.081,% | Adaptive Acc: 86.394% | clf_exit: 0.210 0.400 0.390
Batch: 240 | Loss: 3.470 | Acc: 45.410,72.958,90.995,% | Adaptive Acc: 86.369% | clf_exit: 0.209 0.401 0.389
Batch: 260 | Loss: 3.480 | Acc: 45.318,72.776,90.852,% | Adaptive Acc: 86.183% | clf_exit: 0.209 0.401 0.390
Batch: 280 | Loss: 3.494 | Acc: 45.257,72.665,90.745,% | Adaptive Acc: 85.999% | clf_exit: 0.209 0.399 0.391
Batch: 300 | Loss: 3.502 | Acc: 45.201,72.602,90.680,% | Adaptive Acc: 85.943% | clf_exit: 0.209 0.398 0.393
Batch: 320 | Loss: 3.515 | Acc: 45.089,72.442,90.542,% | Adaptive Acc: 85.845% | clf_exit: 0.208 0.398 0.395
Batch: 340 | Loss: 3.524 | Acc: 45.044,72.397,90.433,% | Adaptive Acc: 85.750% | clf_exit: 0.207 0.397 0.396
Batch: 360 | Loss: 3.533 | Acc: 44.988,72.310,90.322,% | Adaptive Acc: 85.624% | clf_exit: 0.207 0.396 0.396
Batch: 380 | Loss: 3.535 | Acc: 45.044,72.281,90.264,% | Adaptive Acc: 85.589% | clf_exit: 0.207 0.396 0.397
Batch: 0 | Loss: 4.554 | Acc: 42.188,63.281,69.531,% | Adaptive Acc: 64.062% | clf_exit: 0.297 0.359 0.344
Batch: 20 | Loss: 5.051 | Acc: 41.220,61.793,67.076,% | Adaptive Acc: 63.244% | clf_exit: 0.282 0.376 0.342
Batch: 40 | Loss: 5.095 | Acc: 41.806,60.880,65.758,% | Adaptive Acc: 62.557% | clf_exit: 0.281 0.370 0.349
Batch: 60 | Loss: 5.072 | Acc: 41.662,60.886,65.868,% | Adaptive Acc: 62.782% | clf_exit: 0.278 0.370 0.352
Train all parameters

Epoch: 127
Batch: 0 | Loss: 3.585 | Acc: 43.750,71.875,96.094,% | Adaptive Acc: 92.969% | clf_exit: 0.141 0.414 0.445
Batch: 20 | Loss: 3.413 | Acc: 46.466,74.740,92.522,% | Adaptive Acc: 88.281% | clf_exit: 0.219 0.386 0.395
Batch: 40 | Loss: 3.429 | Acc: 46.380,73.647,91.959,% | Adaptive Acc: 87.576% | clf_exit: 0.208 0.394 0.398
Batch: 60 | Loss: 3.438 | Acc: 45.543,73.297,92.085,% | Adaptive Acc: 87.321% | clf_exit: 0.210 0.399 0.391
Batch: 80 | Loss: 3.456 | Acc: 45.438,73.148,91.850,% | Adaptive Acc: 87.201% | clf_exit: 0.210 0.399 0.391
Batch: 100 | Loss: 3.468 | Acc: 44.964,72.912,91.801,% | Adaptive Acc: 86.912% | clf_exit: 0.210 0.398 0.392
Batch: 120 | Loss: 3.458 | Acc: 45.216,73.108,91.671,% | Adaptive Acc: 86.757% | clf_exit: 0.210 0.399 0.391
Batch: 140 | Loss: 3.464 | Acc: 45.423,72.839,91.462,% | Adaptive Acc: 86.553% | clf_exit: 0.212 0.398 0.390
Batch: 160 | Loss: 3.471 | Acc: 45.245,72.676,91.479,% | Adaptive Acc: 86.593% | clf_exit: 0.211 0.399 0.390
Batch: 180 | Loss: 3.471 | Acc: 45.351,72.695,91.333,% | Adaptive Acc: 86.537% | clf_exit: 0.211 0.400 0.389
Batch: 200 | Loss: 3.467 | Acc: 45.468,72.750,91.278,% | Adaptive Acc: 86.478% | clf_exit: 0.212 0.400 0.388
Batch: 220 | Loss: 3.473 | Acc: 45.419,72.706,91.194,% | Adaptive Acc: 86.379% | clf_exit: 0.212 0.400 0.388
Batch: 240 | Loss: 3.488 | Acc: 45.267,72.672,91.017,% | Adaptive Acc: 86.229% | clf_exit: 0.210 0.401 0.388
Batch: 260 | Loss: 3.497 | Acc: 45.301,72.596,90.844,% | Adaptive Acc: 86.063% | clf_exit: 0.209 0.401 0.389
Batch: 280 | Loss: 3.500 | Acc: 45.363,72.565,90.742,% | Adaptive Acc: 85.918% | clf_exit: 0.210 0.400 0.390
Batch: 300 | Loss: 3.512 | Acc: 45.250,72.428,90.604,% | Adaptive Acc: 85.751% | clf_exit: 0.210 0.400 0.390
Batch: 320 | Loss: 3.517 | Acc: 45.222,72.384,90.540,% | Adaptive Acc: 85.789% | clf_exit: 0.208 0.400 0.392
Batch: 340 | Loss: 3.516 | Acc: 45.278,72.432,90.476,% | Adaptive Acc: 85.766% | clf_exit: 0.208 0.400 0.392
Batch: 360 | Loss: 3.520 | Acc: 45.256,72.394,90.385,% | Adaptive Acc: 85.669% | clf_exit: 0.208 0.400 0.392
Batch: 380 | Loss: 3.533 | Acc: 45.095,72.213,90.244,% | Adaptive Acc: 85.542% | clf_exit: 0.207 0.399 0.393
Batch: 0 | Loss: 4.954 | Acc: 41.406,63.281,63.281,% | Adaptive Acc: 60.156% | clf_exit: 0.250 0.414 0.336
Batch: 20 | Loss: 5.253 | Acc: 40.365,60.900,64.062,% | Adaptive Acc: 60.640% | clf_exit: 0.270 0.389 0.341
Batch: 40 | Loss: 5.268 | Acc: 40.873,60.232,63.815,% | Adaptive Acc: 61.319% | clf_exit: 0.257 0.394 0.348
Batch: 60 | Loss: 5.276 | Acc: 40.202,59.887,63.691,% | Adaptive Acc: 61.488% | clf_exit: 0.256 0.387 0.357
Train all parameters

Epoch: 128
Batch: 0 | Loss: 3.560 | Acc: 41.406,70.312,86.719,% | Adaptive Acc: 84.375% | clf_exit: 0.219 0.336 0.445
Batch: 20 | Loss: 3.465 | Acc: 45.350,73.326,90.923,% | Adaptive Acc: 86.570% | clf_exit: 0.213 0.388 0.399
Batch: 40 | Loss: 3.479 | Acc: 45.236,72.923,90.930,% | Adaptive Acc: 86.280% | clf_exit: 0.211 0.394 0.394
Batch: 60 | Loss: 3.466 | Acc: 45.120,73.066,91.227,% | Adaptive Acc: 86.475% | clf_exit: 0.209 0.399 0.393
Batch: 80 | Loss: 3.486 | Acc: 44.715,73.119,91.339,% | Adaptive Acc: 86.728% | clf_exit: 0.207 0.397 0.396
Batch: 100 | Loss: 3.480 | Acc: 44.709,73.082,91.538,% | Adaptive Acc: 86.835% | clf_exit: 0.209 0.397 0.393
Batch: 120 | Loss: 3.493 | Acc: 44.686,72.818,91.419,% | Adaptive Acc: 86.712% | clf_exit: 0.208 0.398 0.394
Batch: 140 | Loss: 3.500 | Acc: 44.725,72.689,91.301,% | Adaptive Acc: 86.519% | clf_exit: 0.208 0.398 0.394
Batch: 160 | Loss: 3.511 | Acc: 44.653,72.404,91.139,% | Adaptive Acc: 86.331% | clf_exit: 0.207 0.397 0.396
Batch: 180 | Loss: 3.508 | Acc: 44.846,72.423,91.013,% | Adaptive Acc: 86.192% | clf_exit: 0.207 0.398 0.396
Batch: 200 | Loss: 3.513 | Acc: 44.807,72.458,90.951,% | Adaptive Acc: 86.144% | clf_exit: 0.206 0.398 0.396
Batch: 220 | Loss: 3.507 | Acc: 44.931,72.483,90.887,% | Adaptive Acc: 86.093% | clf_exit: 0.207 0.398 0.395
Batch: 240 | Loss: 3.515 | Acc: 44.888,72.407,90.729,% | Adaptive Acc: 85.876% | clf_exit: 0.206 0.399 0.395
Batch: 260 | Loss: 3.521 | Acc: 44.852,72.384,90.634,% | Adaptive Acc: 85.794% | clf_exit: 0.207 0.399 0.395
Batch: 280 | Loss: 3.522 | Acc: 44.873,72.370,90.594,% | Adaptive Acc: 85.760% | clf_exit: 0.207 0.399 0.394
Batch: 300 | Loss: 3.529 | Acc: 44.918,72.192,90.467,% | Adaptive Acc: 85.639% | clf_exit: 0.208 0.398 0.395
Batch: 320 | Loss: 3.529 | Acc: 44.945,72.148,90.438,% | Adaptive Acc: 85.577% | clf_exit: 0.207 0.398 0.395
Batch: 340 | Loss: 3.526 | Acc: 45.044,72.120,90.398,% | Adaptive Acc: 85.596% | clf_exit: 0.208 0.398 0.394
Batch: 360 | Loss: 3.534 | Acc: 45.025,71.998,90.261,% | Adaptive Acc: 85.472% | clf_exit: 0.208 0.398 0.394
Batch: 380 | Loss: 3.538 | Acc: 45.079,72.021,90.162,% | Adaptive Acc: 85.392% | clf_exit: 0.207 0.398 0.394
Batch: 0 | Loss: 4.574 | Acc: 41.406,67.188,75.000,% | Adaptive Acc: 66.406% | clf_exit: 0.266 0.484 0.250
Batch: 20 | Loss: 5.164 | Acc: 41.183,61.235,64.881,% | Adaptive Acc: 62.240% | clf_exit: 0.259 0.400 0.340
Batch: 40 | Loss: 5.175 | Acc: 41.311,60.518,64.043,% | Adaptive Acc: 61.509% | clf_exit: 0.254 0.395 0.351
Batch: 60 | Loss: 5.180 | Acc: 41.137,60.131,64.165,% | Adaptive Acc: 61.283% | clf_exit: 0.249 0.394 0.357
Train all parameters

Epoch: 129
Batch: 0 | Loss: 3.240 | Acc: 43.750,75.781,91.406,% | Adaptive Acc: 86.719% | clf_exit: 0.234 0.406 0.359
Batch: 20 | Loss: 3.493 | Acc: 44.494,73.921,91.220,% | Adaptive Acc: 86.570% | clf_exit: 0.199 0.410 0.391
Batch: 40 | Loss: 3.476 | Acc: 44.703,73.685,90.873,% | Adaptive Acc: 85.861% | clf_exit: 0.208 0.402 0.390
Batch: 60 | Loss: 3.462 | Acc: 45.095,73.655,91.534,% | Adaptive Acc: 86.296% | clf_exit: 0.208 0.406 0.386
Batch: 80 | Loss: 3.454 | Acc: 45.409,73.515,91.561,% | Adaptive Acc: 86.391% | clf_exit: 0.205 0.409 0.387
Batch: 100 | Loss: 3.439 | Acc: 45.699,73.670,91.654,% | Adaptive Acc: 86.448% | clf_exit: 0.209 0.409 0.383
Batch: 120 | Loss: 3.448 | Acc: 45.558,73.354,91.516,% | Adaptive Acc: 86.344% | clf_exit: 0.208 0.407 0.384
Batch: 140 | Loss: 3.458 | Acc: 45.385,73.177,91.556,% | Adaptive Acc: 86.525% | clf_exit: 0.209 0.403 0.387
Batch: 160 | Loss: 3.459 | Acc: 45.507,73.200,91.518,% | Adaptive Acc: 86.544% | clf_exit: 0.208 0.403 0.389
Batch: 180 | Loss: 3.465 | Acc: 45.502,73.118,91.359,% | Adaptive Acc: 86.460% | clf_exit: 0.208 0.402 0.390
Batch: 200 | Loss: 3.474 | Acc: 45.565,73.053,91.185,% | Adaptive Acc: 86.326% | clf_exit: 0.208 0.402 0.389
Batch: 220 | Loss: 3.477 | Acc: 45.666,73.070,91.134,% | Adaptive Acc: 86.312% | clf_exit: 0.209 0.402 0.390
Batch: 240 | Loss: 3.481 | Acc: 45.565,72.942,91.063,% | Adaptive Acc: 86.220% | clf_exit: 0.209 0.401 0.390
Batch: 260 | Loss: 3.483 | Acc: 45.582,72.899,91.035,% | Adaptive Acc: 86.150% | clf_exit: 0.209 0.401 0.390
Batch: 280 | Loss: 3.489 | Acc: 45.524,72.790,90.978,% | Adaptive Acc: 86.071% | clf_exit: 0.209 0.400 0.390
Batch: 300 | Loss: 3.496 | Acc: 45.486,72.742,90.864,% | Adaptive Acc: 86.057% | clf_exit: 0.209 0.400 0.391
Batch: 320 | Loss: 3.495 | Acc: 45.449,72.766,90.829,% | Adaptive Acc: 86.045% | clf_exit: 0.209 0.401 0.390
Batch: 340 | Loss: 3.503 | Acc: 45.443,72.590,90.646,% | Adaptive Acc: 85.830% | clf_exit: 0.209 0.400 0.391
Batch: 360 | Loss: 3.506 | Acc: 45.438,72.552,90.538,% | Adaptive Acc: 85.738% | clf_exit: 0.209 0.400 0.391
Batch: 380 | Loss: 3.513 | Acc: 45.397,72.472,90.395,% | Adaptive Acc: 85.597% | clf_exit: 0.209 0.400 0.391
Batch: 0 | Loss: 4.726 | Acc: 44.531,61.719,67.969,% | Adaptive Acc: 65.625% | clf_exit: 0.234 0.453 0.312
Batch: 20 | Loss: 5.248 | Acc: 39.695,60.379,64.658,% | Adaptive Acc: 61.979% | clf_exit: 0.269 0.381 0.350
Batch: 40 | Loss: 5.243 | Acc: 40.111,59.718,64.482,% | Adaptive Acc: 61.623% | clf_exit: 0.261 0.388 0.351
Batch: 60 | Loss: 5.238 | Acc: 40.049,59.734,64.165,% | Adaptive Acc: 61.322% | clf_exit: 0.258 0.388 0.355
Train all parameters

Epoch: 130
Batch: 0 | Loss: 3.350 | Acc: 51.562,76.562,87.500,% | Adaptive Acc: 82.812% | clf_exit: 0.180 0.492 0.328
Batch: 20 | Loss: 3.486 | Acc: 45.796,72.991,90.737,% | Adaptive Acc: 86.905% | clf_exit: 0.204 0.407 0.389
Batch: 40 | Loss: 3.495 | Acc: 45.922,73.095,91.006,% | Adaptive Acc: 86.833% | clf_exit: 0.205 0.406 0.389
Batch: 60 | Loss: 3.453 | Acc: 46.593,73.040,91.304,% | Adaptive Acc: 86.885% | clf_exit: 0.206 0.410 0.384
Batch: 80 | Loss: 3.462 | Acc: 46.258,72.936,91.561,% | Adaptive Acc: 86.815% | clf_exit: 0.210 0.403 0.386
Batch: 100 | Loss: 3.438 | Acc: 46.156,73.461,91.716,% | Adaptive Acc: 87.167% | clf_exit: 0.209 0.403 0.388
Batch: 120 | Loss: 3.451 | Acc: 45.842,73.121,91.568,% | Adaptive Acc: 86.848% | clf_exit: 0.209 0.403 0.388
Batch: 140 | Loss: 3.467 | Acc: 45.518,72.850,91.312,% | Adaptive Acc: 86.442% | clf_exit: 0.210 0.403 0.388
Batch: 160 | Loss: 3.479 | Acc: 45.327,72.666,91.256,% | Adaptive Acc: 86.306% | clf_exit: 0.209 0.402 0.389
Batch: 180 | Loss: 3.478 | Acc: 45.304,72.682,91.165,% | Adaptive Acc: 86.317% | clf_exit: 0.209 0.402 0.389
Batch: 200 | Loss: 3.488 | Acc: 45.180,72.567,91.060,% | Adaptive Acc: 86.233% | clf_exit: 0.210 0.400 0.390
Batch: 220 | Loss: 3.492 | Acc: 45.249,72.402,90.929,% | Adaptive Acc: 86.093% | clf_exit: 0.210 0.399 0.391
Batch: 240 | Loss: 3.494 | Acc: 45.309,72.394,90.842,% | Adaptive Acc: 86.093% | clf_exit: 0.210 0.399 0.391
Batch: 260 | Loss: 3.501 | Acc: 45.253,72.309,90.778,% | Adaptive Acc: 86.003% | clf_exit: 0.210 0.399 0.391
Batch: 280 | Loss: 3.504 | Acc: 45.279,72.325,90.722,% | Adaptive Acc: 85.968% | clf_exit: 0.210 0.399 0.391
Batch: 300 | Loss: 3.511 | Acc: 45.245,72.288,90.633,% | Adaptive Acc: 85.852% | clf_exit: 0.210 0.399 0.391
Batch: 320 | Loss: 3.516 | Acc: 45.225,72.255,90.596,% | Adaptive Acc: 85.828% | clf_exit: 0.210 0.398 0.392
Batch: 340 | Loss: 3.521 | Acc: 45.223,72.182,90.446,% | Adaptive Acc: 85.704% | clf_exit: 0.210 0.399 0.391
Batch: 360 | Loss: 3.524 | Acc: 45.215,72.169,90.383,% | Adaptive Acc: 85.598% | clf_exit: 0.210 0.398 0.392
Batch: 380 | Loss: 3.528 | Acc: 45.228,72.144,90.252,% | Adaptive Acc: 85.484% | clf_exit: 0.210 0.398 0.392
Batch: 0 | Loss: 4.621 | Acc: 46.875,67.969,69.531,% | Adaptive Acc: 65.625% | clf_exit: 0.273 0.438 0.289
Batch: 20 | Loss: 5.008 | Acc: 42.560,62.723,66.295,% | Adaptive Acc: 64.918% | clf_exit: 0.269 0.398 0.333
Batch: 40 | Loss: 4.978 | Acc: 42.816,62.691,66.387,% | Adaptive Acc: 64.710% | clf_exit: 0.271 0.390 0.338
Batch: 60 | Loss: 5.007 | Acc: 42.008,62.295,65.932,% | Adaptive Acc: 64.088% | clf_exit: 0.274 0.381 0.345
Train all parameters

Epoch: 131
Batch: 0 | Loss: 3.331 | Acc: 46.094,72.656,93.750,% | Adaptive Acc: 82.812% | clf_exit: 0.219 0.422 0.359
Batch: 20 | Loss: 3.439 | Acc: 45.461,74.033,91.778,% | Adaptive Acc: 86.830% | clf_exit: 0.192 0.420 0.388
Batch: 40 | Loss: 3.440 | Acc: 45.789,73.819,92.226,% | Adaptive Acc: 87.729% | clf_exit: 0.204 0.405 0.392
Batch: 60 | Loss: 3.446 | Acc: 45.735,73.322,92.303,% | Adaptive Acc: 87.513% | clf_exit: 0.205 0.405 0.390
Batch: 80 | Loss: 3.442 | Acc: 45.544,73.187,92.120,% | Adaptive Acc: 87.297% | clf_exit: 0.207 0.405 0.389
Batch: 100 | Loss: 3.440 | Acc: 45.777,73.136,92.064,% | Adaptive Acc: 87.160% | clf_exit: 0.207 0.405 0.387
Batch: 120 | Loss: 3.450 | Acc: 45.668,73.024,91.955,% | Adaptive Acc: 86.906% | clf_exit: 0.209 0.402 0.389
Batch: 140 | Loss: 3.442 | Acc: 45.894,73.083,91.916,% | Adaptive Acc: 86.785% | clf_exit: 0.210 0.402 0.388
Batch: 160 | Loss: 3.446 | Acc: 45.730,73.064,91.794,% | Adaptive Acc: 86.665% | clf_exit: 0.210 0.401 0.389
Batch: 180 | Loss: 3.443 | Acc: 45.688,73.109,91.786,% | Adaptive Acc: 86.615% | clf_exit: 0.212 0.401 0.388
Batch: 200 | Loss: 3.444 | Acc: 45.674,73.080,91.725,% | Adaptive Acc: 86.567% | clf_exit: 0.213 0.401 0.386
Batch: 220 | Loss: 3.451 | Acc: 45.521,72.953,91.572,% | Adaptive Acc: 86.524% | clf_exit: 0.212 0.401 0.388
Batch: 240 | Loss: 3.455 | Acc: 45.543,72.870,91.523,% | Adaptive Acc: 86.437% | clf_exit: 0.212 0.400 0.388
Batch: 260 | Loss: 3.457 | Acc: 45.582,72.812,91.397,% | Adaptive Acc: 86.327% | clf_exit: 0.213 0.399 0.388
Batch: 280 | Loss: 3.464 | Acc: 45.524,72.801,91.234,% | Adaptive Acc: 86.202% | clf_exit: 0.213 0.399 0.388
Batch: 300 | Loss: 3.470 | Acc: 45.502,72.706,91.142,% | Adaptive Acc: 86.101% | clf_exit: 0.212 0.399 0.389
Batch: 320 | Loss: 3.478 | Acc: 45.468,72.649,90.993,% | Adaptive Acc: 86.008% | clf_exit: 0.211 0.400 0.389
Batch: 340 | Loss: 3.482 | Acc: 45.489,72.652,90.845,% | Adaptive Acc: 85.931% | clf_exit: 0.212 0.398 0.390
Batch: 360 | Loss: 3.486 | Acc: 45.462,72.648,90.714,% | Adaptive Acc: 85.799% | clf_exit: 0.212 0.398 0.390
Batch: 380 | Loss: 3.494 | Acc: 45.358,72.574,90.582,% | Adaptive Acc: 85.714% | clf_exit: 0.210 0.398 0.391
Batch: 0 | Loss: 4.915 | Acc: 43.750,63.281,67.969,% | Adaptive Acc: 67.188% | clf_exit: 0.312 0.375 0.312
Batch: 20 | Loss: 5.202 | Acc: 40.885,62.128,65.588,% | Adaptive Acc: 62.574% | clf_exit: 0.290 0.385 0.324
Batch: 40 | Loss: 5.157 | Acc: 41.063,61.795,65.568,% | Adaptive Acc: 62.233% | clf_exit: 0.287 0.380 0.333
Batch: 60 | Loss: 5.180 | Acc: 40.548,61.014,65.126,% | Adaptive Acc: 61.847% | clf_exit: 0.285 0.378 0.337
Train all parameters

Epoch: 132
Batch: 0 | Loss: 3.525 | Acc: 46.094,75.000,92.188,% | Adaptive Acc: 89.844% | clf_exit: 0.195 0.391 0.414
Batch: 20 | Loss: 3.432 | Acc: 45.908,74.702,92.113,% | Adaptive Acc: 88.132% | clf_exit: 0.215 0.392 0.393
Batch: 40 | Loss: 3.428 | Acc: 45.884,74.276,91.883,% | Adaptive Acc: 87.424% | clf_exit: 0.214 0.397 0.389
Batch: 60 | Loss: 3.443 | Acc: 45.479,73.809,91.714,% | Adaptive Acc: 86.949% | clf_exit: 0.211 0.403 0.386
Batch: 80 | Loss: 3.450 | Acc: 45.361,73.640,91.416,% | Adaptive Acc: 86.603% | clf_exit: 0.210 0.404 0.386
Batch: 100 | Loss: 3.454 | Acc: 45.111,73.422,91.267,% | Adaptive Acc: 86.471% | clf_exit: 0.211 0.401 0.388
Batch: 120 | Loss: 3.455 | Acc: 45.338,73.341,91.245,% | Adaptive Acc: 86.454% | clf_exit: 0.210 0.402 0.388
Batch: 140 | Loss: 3.450 | Acc: 45.523,73.354,91.240,% | Adaptive Acc: 86.464% | clf_exit: 0.210 0.401 0.388
Batch: 160 | Loss: 3.452 | Acc: 45.633,73.243,91.193,% | Adaptive Acc: 86.335% | clf_exit: 0.212 0.399 0.389
Batch: 180 | Loss: 3.458 | Acc: 45.489,73.235,91.130,% | Adaptive Acc: 86.322% | clf_exit: 0.212 0.398 0.390
Batch: 200 | Loss: 3.463 | Acc: 45.344,73.150,91.006,% | Adaptive Acc: 86.171% | clf_exit: 0.211 0.398 0.391
Batch: 220 | Loss: 3.467 | Acc: 45.203,73.091,90.954,% | Adaptive Acc: 86.128% | clf_exit: 0.211 0.399 0.391
Batch: 240 | Loss: 3.476 | Acc: 45.215,72.945,90.829,% | Adaptive Acc: 85.993% | clf_exit: 0.209 0.399 0.392
Batch: 260 | Loss: 3.486 | Acc: 45.142,72.740,90.715,% | Adaptive Acc: 85.890% | clf_exit: 0.210 0.398 0.393
Batch: 280 | Loss: 3.485 | Acc: 45.173,72.759,90.697,% | Adaptive Acc: 85.918% | clf_exit: 0.210 0.398 0.393
Batch: 300 | Loss: 3.493 | Acc: 45.131,72.674,90.646,% | Adaptive Acc: 85.867% | clf_exit: 0.209 0.397 0.393
Batch: 320 | Loss: 3.498 | Acc: 45.091,72.656,90.574,% | Adaptive Acc: 85.787% | clf_exit: 0.210 0.398 0.393
Batch: 340 | Loss: 3.500 | Acc: 45.106,72.652,90.453,% | Adaptive Acc: 85.708% | clf_exit: 0.210 0.397 0.393
Batch: 360 | Loss: 3.506 | Acc: 45.165,72.641,90.333,% | Adaptive Acc: 85.600% | clf_exit: 0.210 0.397 0.393
Batch: 380 | Loss: 3.516 | Acc: 45.151,72.531,90.143,% | Adaptive Acc: 85.449% | clf_exit: 0.210 0.397 0.393
Batch: 0 | Loss: 4.654 | Acc: 45.312,62.500,68.750,% | Adaptive Acc: 66.406% | clf_exit: 0.336 0.398 0.266
Batch: 20 | Loss: 5.121 | Acc: 41.853,61.830,64.881,% | Adaptive Acc: 62.202% | clf_exit: 0.296 0.392 0.312
Batch: 40 | Loss: 5.035 | Acc: 42.168,62.024,65.625,% | Adaptive Acc: 62.767% | clf_exit: 0.295 0.384 0.321
Batch: 60 | Loss: 5.046 | Acc: 41.470,61.680,65.702,% | Adaptive Acc: 62.526% | clf_exit: 0.296 0.379 0.324
Train all parameters

Epoch: 133
Batch: 0 | Loss: 3.070 | Acc: 44.531,79.688,93.750,% | Adaptive Acc: 91.406% | clf_exit: 0.227 0.398 0.375
Batch: 20 | Loss: 3.433 | Acc: 45.722,73.772,91.741,% | Adaptive Acc: 86.942% | clf_exit: 0.191 0.417 0.392
Batch: 40 | Loss: 3.418 | Acc: 45.446,73.685,92.226,% | Adaptive Acc: 87.100% | clf_exit: 0.212 0.401 0.387
Batch: 60 | Loss: 3.402 | Acc: 45.748,73.745,91.931,% | Adaptive Acc: 86.847% | clf_exit: 0.211 0.405 0.384
Batch: 80 | Loss: 3.424 | Acc: 45.611,73.505,92.052,% | Adaptive Acc: 86.960% | clf_exit: 0.209 0.407 0.384
Batch: 100 | Loss: 3.429 | Acc: 45.545,73.453,91.955,% | Adaptive Acc: 86.889% | clf_exit: 0.208 0.407 0.385
Batch: 120 | Loss: 3.435 | Acc: 45.500,73.360,91.897,% | Adaptive Acc: 86.848% | clf_exit: 0.209 0.406 0.385
Batch: 140 | Loss: 3.444 | Acc: 45.512,73.465,91.711,% | Adaptive Acc: 86.674% | clf_exit: 0.208 0.405 0.387
Batch: 160 | Loss: 3.444 | Acc: 45.497,73.457,91.600,% | Adaptive Acc: 86.505% | clf_exit: 0.209 0.404 0.387
Batch: 180 | Loss: 3.458 | Acc: 45.420,73.304,91.454,% | Adaptive Acc: 86.352% | clf_exit: 0.208 0.403 0.388
Batch: 200 | Loss: 3.470 | Acc: 45.452,73.263,91.305,% | Adaptive Acc: 86.381% | clf_exit: 0.208 0.403 0.389
Batch: 220 | Loss: 3.474 | Acc: 45.503,73.162,91.251,% | Adaptive Acc: 86.295% | clf_exit: 0.208 0.402 0.390
Batch: 240 | Loss: 3.490 | Acc: 45.296,72.890,91.134,% | Adaptive Acc: 86.249% | clf_exit: 0.207 0.400 0.393
Batch: 260 | Loss: 3.487 | Acc: 45.387,72.797,91.026,% | Adaptive Acc: 86.087% | clf_exit: 0.208 0.401 0.391
Batch: 280 | Loss: 3.490 | Acc: 45.360,72.684,90.950,% | Adaptive Acc: 85.996% | clf_exit: 0.208 0.401 0.391
Batch: 300 | Loss: 3.493 | Acc: 45.338,72.591,90.872,% | Adaptive Acc: 85.873% | clf_exit: 0.209 0.401 0.390
Batch: 320 | Loss: 3.495 | Acc: 45.468,72.583,90.769,% | Adaptive Acc: 85.816% | clf_exit: 0.210 0.400 0.390
Batch: 340 | Loss: 3.497 | Acc: 45.496,72.588,90.627,% | Adaptive Acc: 85.706% | clf_exit: 0.211 0.399 0.390
Batch: 360 | Loss: 3.503 | Acc: 45.429,72.574,90.569,% | Adaptive Acc: 85.676% | clf_exit: 0.211 0.399 0.390
Batch: 380 | Loss: 3.510 | Acc: 45.485,72.523,90.391,% | Adaptive Acc: 85.503% | clf_exit: 0.211 0.398 0.391
Batch: 0 | Loss: 4.506 | Acc: 48.438,66.406,69.531,% | Adaptive Acc: 67.969% | clf_exit: 0.297 0.391 0.312
Batch: 20 | Loss: 4.996 | Acc: 42.299,62.574,67.113,% | Adaptive Acc: 64.062% | clf_exit: 0.277 0.388 0.334
Batch: 40 | Loss: 4.992 | Acc: 42.111,61.871,66.197,% | Adaptive Acc: 63.243% | clf_exit: 0.270 0.388 0.342
Batch: 60 | Loss: 5.032 | Acc: 42.034,61.219,65.523,% | Adaptive Acc: 62.615% | clf_exit: 0.271 0.387 0.343
Train all parameters

Epoch: 134
Batch: 0 | Loss: 3.273 | Acc: 52.344,72.656,92.969,% | Adaptive Acc: 88.281% | clf_exit: 0.227 0.422 0.352
Batch: 20 | Loss: 3.393 | Acc: 46.243,75.856,93.341,% | Adaptive Acc: 88.430% | clf_exit: 0.205 0.406 0.388
Batch: 40 | Loss: 3.410 | Acc: 45.465,74.790,92.626,% | Adaptive Acc: 87.481% | clf_exit: 0.204 0.410 0.386
Batch: 60 | Loss: 3.385 | Acc: 46.017,74.436,92.585,% | Adaptive Acc: 87.308% | clf_exit: 0.212 0.409 0.380
Batch: 80 | Loss: 3.386 | Acc: 46.074,74.257,92.612,% | Adaptive Acc: 87.452% | clf_exit: 0.211 0.410 0.379
Batch: 100 | Loss: 3.380 | Acc: 46.140,74.064,92.582,% | Adaptive Acc: 87.485% | clf_exit: 0.211 0.409 0.380
Batch: 120 | Loss: 3.406 | Acc: 45.713,73.986,92.342,% | Adaptive Acc: 87.280% | clf_exit: 0.209 0.409 0.382
Batch: 140 | Loss: 3.420 | Acc: 45.573,73.809,92.298,% | Adaptive Acc: 87.212% | clf_exit: 0.207 0.407 0.386
Batch: 160 | Loss: 3.422 | Acc: 45.618,73.636,92.037,% | Adaptive Acc: 86.893% | clf_exit: 0.210 0.406 0.384
Batch: 180 | Loss: 3.428 | Acc: 45.653,73.791,91.916,% | Adaptive Acc: 86.762% | clf_exit: 0.211 0.404 0.385
Batch: 200 | Loss: 3.441 | Acc: 45.553,73.554,91.717,% | Adaptive Acc: 86.583% | clf_exit: 0.211 0.403 0.386
Batch: 220 | Loss: 3.447 | Acc: 45.567,73.512,91.629,% | Adaptive Acc: 86.553% | clf_exit: 0.211 0.403 0.386
Batch: 240 | Loss: 3.443 | Acc: 45.747,73.583,91.555,% | Adaptive Acc: 86.476% | clf_exit: 0.212 0.404 0.384
Batch: 260 | Loss: 3.455 | Acc: 45.582,73.470,91.364,% | Adaptive Acc: 86.270% | clf_exit: 0.212 0.403 0.385
Batch: 280 | Loss: 3.457 | Acc: 45.577,73.404,91.181,% | Adaptive Acc: 86.152% | clf_exit: 0.212 0.403 0.385
Batch: 300 | Loss: 3.459 | Acc: 45.642,73.334,91.045,% | Adaptive Acc: 86.039% | clf_exit: 0.212 0.403 0.385
Batch: 320 | Loss: 3.466 | Acc: 45.527,73.204,90.980,% | Adaptive Acc: 85.989% | clf_exit: 0.212 0.403 0.385
Batch: 340 | Loss: 3.465 | Acc: 45.569,73.158,90.950,% | Adaptive Acc: 85.995% | clf_exit: 0.213 0.402 0.385
Batch: 360 | Loss: 3.471 | Acc: 45.529,73.074,90.911,% | Adaptive Acc: 85.920% | clf_exit: 0.213 0.401 0.386
Batch: 380 | Loss: 3.479 | Acc: 45.444,72.999,90.810,% | Adaptive Acc: 85.862% | clf_exit: 0.213 0.401 0.386
Batch: 0 | Loss: 4.889 | Acc: 42.969,64.062,66.406,% | Adaptive Acc: 64.062% | clf_exit: 0.336 0.352 0.312
Batch: 20 | Loss: 5.095 | Acc: 40.885,61.607,65.811,% | Adaptive Acc: 62.016% | clf_exit: 0.288 0.379 0.333
Batch: 40 | Loss: 5.112 | Acc: 40.911,61.090,65.206,% | Adaptive Acc: 61.966% | clf_exit: 0.284 0.378 0.338
Batch: 60 | Loss: 5.145 | Acc: 40.459,60.989,65.484,% | Adaptive Acc: 61.860% | clf_exit: 0.284 0.374 0.342
Train classifier parameters

Epoch: 135
Batch: 0 | Loss: 3.917 | Acc: 39.844,67.969,90.625,% | Adaptive Acc: 82.031% | clf_exit: 0.234 0.406 0.359
Batch: 20 | Loss: 4.028 | Acc: 42.039,69.680,83.482,% | Adaptive Acc: 80.469% | clf_exit: 0.170 0.394 0.436
Batch: 40 | Loss: 4.280 | Acc: 41.235,67.931,80.050,% | Adaptive Acc: 77.839% | clf_exit: 0.162 0.376 0.462
Batch: 60 | Loss: 4.402 | Acc: 40.766,66.586,78.676,% | Adaptive Acc: 76.895% | clf_exit: 0.156 0.366 0.478
Batch: 80 | Loss: 4.467 | Acc: 39.988,66.233,78.337,% | Adaptive Acc: 76.534% | clf_exit: 0.151 0.362 0.487
Batch: 100 | Loss: 4.499 | Acc: 39.929,66.004,77.731,% | Adaptive Acc: 76.037% | clf_exit: 0.149 0.362 0.489
Batch: 120 | Loss: 4.492 | Acc: 40.083,65.941,77.763,% | Adaptive Acc: 76.033% | clf_exit: 0.149 0.360 0.491
Batch: 140 | Loss: 4.481 | Acc: 40.259,65.836,77.793,% | Adaptive Acc: 75.947% | clf_exit: 0.149 0.361 0.490
Batch: 160 | Loss: 4.490 | Acc: 40.339,65.722,77.776,% | Adaptive Acc: 75.796% | clf_exit: 0.148 0.360 0.492
Batch: 180 | Loss: 4.477 | Acc: 40.474,65.716,77.987,% | Adaptive Acc: 76.023% | clf_exit: 0.149 0.359 0.493
Batch: 200 | Loss: 4.481 | Acc: 40.388,65.656,78.008,% | Adaptive Acc: 76.007% | clf_exit: 0.148 0.357 0.494
Batch: 220 | Loss: 4.471 | Acc: 40.508,65.657,78.118,% | Adaptive Acc: 76.149% | clf_exit: 0.149 0.356 0.495
Batch: 240 | Loss: 4.468 | Acc: 40.654,65.654,78.096,% | Adaptive Acc: 76.099% | clf_exit: 0.150 0.355 0.494
Batch: 260 | Loss: 4.466 | Acc: 40.646,65.622,78.065,% | Adaptive Acc: 76.018% | clf_exit: 0.150 0.356 0.494
Batch: 280 | Loss: 4.472 | Acc: 40.586,65.530,78.022,% | Adaptive Acc: 75.998% | clf_exit: 0.150 0.355 0.495
Batch: 300 | Loss: 4.464 | Acc: 40.589,65.739,78.081,% | Adaptive Acc: 76.051% | clf_exit: 0.150 0.356 0.494
Batch: 320 | Loss: 4.460 | Acc: 40.608,65.781,78.135,% | Adaptive Acc: 76.090% | clf_exit: 0.150 0.356 0.494
Batch: 340 | Loss: 4.460 | Acc: 40.643,65.751,78.178,% | Adaptive Acc: 76.081% | clf_exit: 0.150 0.356 0.494
Batch: 360 | Loss: 4.460 | Acc: 40.569,65.651,78.190,% | Adaptive Acc: 76.067% | clf_exit: 0.150 0.355 0.495
Batch: 380 | Loss: 4.450 | Acc: 40.732,65.728,78.285,% | Adaptive Acc: 76.138% | clf_exit: 0.151 0.355 0.494
Batch: 0 | Loss: 5.280 | Acc: 43.750,60.938,60.938,% | Adaptive Acc: 60.156% | clf_exit: 0.250 0.438 0.312
Batch: 20 | Loss: 5.601 | Acc: 39.174,58.854,60.565,% | Adaptive Acc: 60.119% | clf_exit: 0.212 0.368 0.420
Batch: 40 | Loss: 5.617 | Acc: 39.558,58.346,59.737,% | Adaptive Acc: 59.223% | clf_exit: 0.208 0.366 0.425
Batch: 60 | Loss: 5.651 | Acc: 39.575,57.838,59.349,% | Adaptive Acc: 58.952% | clf_exit: 0.204 0.370 0.425
Train classifier parameters

Epoch: 136
Batch: 0 | Loss: 4.347 | Acc: 42.188,65.625,75.000,% | Adaptive Acc: 73.438% | clf_exit: 0.188 0.344 0.469
Batch: 20 | Loss: 4.178 | Acc: 43.713,68.452,80.990,% | Adaptive Acc: 78.943% | clf_exit: 0.162 0.370 0.468
Batch: 40 | Loss: 4.257 | Acc: 42.188,67.264,80.278,% | Adaptive Acc: 78.030% | clf_exit: 0.156 0.362 0.482
Batch: 60 | Loss: 4.311 | Acc: 41.829,67.252,79.918,% | Adaptive Acc: 77.792% | clf_exit: 0.153 0.360 0.486
Batch: 80 | Loss: 4.294 | Acc: 41.570,67.670,79.938,% | Adaptive Acc: 77.749% | clf_exit: 0.152 0.360 0.487
Batch: 100 | Loss: 4.282 | Acc: 41.917,67.574,79.920,% | Adaptive Acc: 77.669% | clf_exit: 0.152 0.361 0.487
Batch: 120 | Loss: 4.294 | Acc: 41.865,67.194,79.991,% | Adaptive Acc: 77.673% | clf_exit: 0.153 0.359 0.488
Batch: 140 | Loss: 4.303 | Acc: 41.838,67.210,79.937,% | Adaptive Acc: 77.565% | clf_exit: 0.154 0.358 0.488
Batch: 160 | Loss: 4.292 | Acc: 42.061,67.319,79.930,% | Adaptive Acc: 77.548% | clf_exit: 0.156 0.360 0.485
Batch: 180 | Loss: 4.288 | Acc: 42.162,67.222,79.998,% | Adaptive Acc: 77.603% | clf_exit: 0.158 0.357 0.484
Batch: 200 | Loss: 4.287 | Acc: 42.133,67.250,80.088,% | Adaptive Acc: 77.670% | clf_exit: 0.158 0.358 0.484
Batch: 220 | Loss: 4.294 | Acc: 42.060,67.145,80.090,% | Adaptive Acc: 77.662% | clf_exit: 0.158 0.357 0.485
Batch: 240 | Loss: 4.293 | Acc: 42.074,67.158,80.096,% | Adaptive Acc: 77.629% | clf_exit: 0.160 0.357 0.483
Batch: 260 | Loss: 4.293 | Acc: 41.960,67.223,80.157,% | Adaptive Acc: 77.751% | clf_exit: 0.159 0.358 0.483
Batch: 280 | Loss: 4.283 | Acc: 42.035,67.282,80.277,% | Adaptive Acc: 77.900% | clf_exit: 0.159 0.358 0.483
Batch: 300 | Loss: 4.284 | Acc: 42.099,67.286,80.274,% | Adaptive Acc: 77.943% | clf_exit: 0.160 0.357 0.483
Batch: 320 | Loss: 4.276 | Acc: 42.209,67.331,80.328,% | Adaptive Acc: 77.986% | clf_exit: 0.160 0.357 0.483
Batch: 340 | Loss: 4.277 | Acc: 42.229,67.236,80.322,% | Adaptive Acc: 77.949% | clf_exit: 0.160 0.357 0.483
Batch: 360 | Loss: 4.272 | Acc: 42.246,67.272,80.393,% | Adaptive Acc: 77.982% | clf_exit: 0.160 0.357 0.483
Batch: 380 | Loss: 4.270 | Acc: 42.302,67.288,80.358,% | Adaptive Acc: 77.988% | clf_exit: 0.161 0.357 0.482
Batch: 0 | Loss: 5.137 | Acc: 44.531,61.719,63.281,% | Adaptive Acc: 59.375% | clf_exit: 0.250 0.414 0.336
Batch: 20 | Loss: 5.455 | Acc: 40.439,59.338,61.868,% | Adaptive Acc: 60.900% | clf_exit: 0.233 0.365 0.402
Batch: 40 | Loss: 5.481 | Acc: 40.377,58.765,61.090,% | Adaptive Acc: 60.271% | clf_exit: 0.226 0.362 0.412
Batch: 60 | Loss: 5.515 | Acc: 40.087,58.261,60.758,% | Adaptive Acc: 59.900% | clf_exit: 0.222 0.365 0.412
Train classifier parameters

Epoch: 137
Batch: 0 | Loss: 3.832 | Acc: 48.438,71.875,81.250,% | Adaptive Acc: 78.125% | clf_exit: 0.180 0.422 0.398
Batch: 20 | Loss: 4.142 | Acc: 43.676,68.080,81.362,% | Adaptive Acc: 78.423% | clf_exit: 0.175 0.366 0.459
Batch: 40 | Loss: 4.162 | Acc: 42.664,67.302,81.726,% | Adaptive Acc: 78.525% | clf_exit: 0.167 0.361 0.472
Batch: 60 | Loss: 4.177 | Acc: 42.431,67.264,81.596,% | Adaptive Acc: 78.650% | clf_exit: 0.165 0.360 0.475
Batch: 80 | Loss: 4.184 | Acc: 42.670,67.371,81.424,% | Adaptive Acc: 78.569% | clf_exit: 0.166 0.359 0.475
Batch: 100 | Loss: 4.180 | Acc: 42.760,67.567,81.459,% | Adaptive Acc: 78.543% | clf_exit: 0.167 0.355 0.478
Batch: 120 | Loss: 4.184 | Acc: 42.911,67.743,81.450,% | Adaptive Acc: 78.461% | clf_exit: 0.168 0.355 0.478
Batch: 140 | Loss: 4.180 | Acc: 42.808,67.681,81.560,% | Adaptive Acc: 78.552% | clf_exit: 0.167 0.357 0.476
Batch: 160 | Loss: 4.197 | Acc: 42.663,67.508,81.449,% | Adaptive Acc: 78.499% | clf_exit: 0.165 0.357 0.478
Batch: 180 | Loss: 4.202 | Acc: 42.511,67.520,81.436,% | Adaptive Acc: 78.488% | clf_exit: 0.165 0.359 0.476
Batch: 200 | Loss: 4.196 | Acc: 42.584,67.600,81.569,% | Adaptive Acc: 78.642% | clf_exit: 0.165 0.359 0.476
Batch: 220 | Loss: 4.198 | Acc: 42.562,67.580,81.586,% | Adaptive Acc: 78.698% | clf_exit: 0.164 0.360 0.476
Batch: 240 | Loss: 4.191 | Acc: 42.609,67.709,81.665,% | Adaptive Acc: 78.715% | clf_exit: 0.166 0.359 0.475
Batch: 260 | Loss: 4.191 | Acc: 42.732,67.732,81.561,% | Adaptive Acc: 78.673% | clf_exit: 0.165 0.359 0.476
Batch: 280 | Loss: 4.190 | Acc: 42.796,67.707,81.545,% | Adaptive Acc: 78.628% | clf_exit: 0.167 0.357 0.476
Batch: 300 | Loss: 4.193 | Acc: 42.743,67.725,81.512,% | Adaptive Acc: 78.649% | clf_exit: 0.167 0.357 0.476
Batch: 320 | Loss: 4.194 | Acc: 42.686,67.735,81.486,% | Adaptive Acc: 78.602% | clf_exit: 0.167 0.357 0.476
Batch: 340 | Loss: 4.187 | Acc: 42.728,67.827,81.486,% | Adaptive Acc: 78.634% | clf_exit: 0.168 0.357 0.475
Batch: 360 | Loss: 4.186 | Acc: 42.709,67.852,81.531,% | Adaptive Acc: 78.686% | clf_exit: 0.168 0.357 0.475
Batch: 380 | Loss: 4.183 | Acc: 42.741,67.979,81.521,% | Adaptive Acc: 78.693% | clf_exit: 0.168 0.358 0.474
Batch: 0 | Loss: 5.075 | Acc: 42.188,63.281,64.062,% | Adaptive Acc: 63.281% | clf_exit: 0.250 0.383 0.367
Batch: 20 | Loss: 5.405 | Acc: 40.030,59.561,62.351,% | Adaptive Acc: 61.533% | clf_exit: 0.227 0.365 0.408
Batch: 40 | Loss: 5.418 | Acc: 40.263,59.184,61.547,% | Adaptive Acc: 60.785% | clf_exit: 0.224 0.365 0.411
Batch: 60 | Loss: 5.455 | Acc: 40.074,58.645,61.104,% | Adaptive Acc: 60.131% | clf_exit: 0.222 0.371 0.408
Train classifier parameters

Epoch: 138
Batch: 0 | Loss: 4.499 | Acc: 38.281,65.625,79.688,% | Adaptive Acc: 77.344% | clf_exit: 0.164 0.336 0.500
Batch: 20 | Loss: 4.143 | Acc: 41.406,67.932,82.999,% | Adaptive Acc: 79.390% | clf_exit: 0.158 0.373 0.469
Batch: 40 | Loss: 4.099 | Acc: 42.797,68.750,83.232,% | Adaptive Acc: 79.973% | clf_exit: 0.161 0.369 0.470
Batch: 60 | Loss: 4.096 | Acc: 43.212,68.481,83.274,% | Adaptive Acc: 80.213% | clf_exit: 0.163 0.363 0.474
Batch: 80 | Loss: 4.095 | Acc: 43.056,68.557,82.976,% | Adaptive Acc: 80.006% | clf_exit: 0.163 0.364 0.473
Batch: 100 | Loss: 4.113 | Acc: 42.799,68.441,82.511,% | Adaptive Acc: 79.564% | clf_exit: 0.164 0.364 0.472
Batch: 120 | Loss: 4.130 | Acc: 42.872,68.311,82.503,% | Adaptive Acc: 79.500% | clf_exit: 0.165 0.363 0.472
Batch: 140 | Loss: 4.129 | Acc: 42.913,68.362,82.364,% | Adaptive Acc: 79.344% | clf_exit: 0.166 0.361 0.473
Batch: 160 | Loss: 4.124 | Acc: 43.095,68.401,82.453,% | Adaptive Acc: 79.445% | clf_exit: 0.165 0.364 0.471
Batch: 180 | Loss: 4.121 | Acc: 43.176,68.461,82.390,% | Adaptive Acc: 79.394% | clf_exit: 0.165 0.364 0.471
Batch: 200 | Loss: 4.120 | Acc: 43.058,68.470,82.354,% | Adaptive Acc: 79.454% | clf_exit: 0.166 0.364 0.470
Batch: 220 | Loss: 4.129 | Acc: 42.951,68.503,82.296,% | Adaptive Acc: 79.437% | clf_exit: 0.166 0.363 0.471
Batch: 240 | Loss: 4.128 | Acc: 42.875,68.461,82.329,% | Adaptive Acc: 79.477% | clf_exit: 0.166 0.363 0.470
Batch: 260 | Loss: 4.129 | Acc: 42.963,68.523,82.310,% | Adaptive Acc: 79.532% | clf_exit: 0.166 0.363 0.471
Batch: 280 | Loss: 4.123 | Acc: 43.022,68.594,82.362,% | Adaptive Acc: 79.582% | clf_exit: 0.167 0.363 0.470
Batch: 300 | Loss: 4.121 | Acc: 43.021,68.542,82.400,% | Adaptive Acc: 79.566% | clf_exit: 0.168 0.362 0.470
Batch: 320 | Loss: 4.125 | Acc: 43.059,68.485,82.357,% | Adaptive Acc: 79.488% | clf_exit: 0.169 0.362 0.470
Batch: 340 | Loss: 4.122 | Acc: 43.106,68.470,82.423,% | Adaptive Acc: 79.493% | clf_exit: 0.169 0.363 0.468
Batch: 360 | Loss: 4.122 | Acc: 43.075,68.475,82.477,% | Adaptive Acc: 79.521% | clf_exit: 0.169 0.362 0.468
Batch: 380 | Loss: 4.125 | Acc: 43.036,68.395,82.439,% | Adaptive Acc: 79.474% | clf_exit: 0.170 0.362 0.469
Batch: 0 | Loss: 5.037 | Acc: 42.969,61.719,61.719,% | Adaptive Acc: 60.156% | clf_exit: 0.266 0.414 0.320
Batch: 20 | Loss: 5.338 | Acc: 40.365,60.231,62.872,% | Adaptive Acc: 61.868% | clf_exit: 0.239 0.366 0.395
Batch: 40 | Loss: 5.360 | Acc: 40.816,60.042,62.043,% | Adaptive Acc: 61.071% | clf_exit: 0.234 0.366 0.400
Batch: 60 | Loss: 5.395 | Acc: 40.574,59.541,61.872,% | Adaptive Acc: 60.720% | clf_exit: 0.234 0.367 0.399
Train classifier parameters

Epoch: 139
Batch: 0 | Loss: 4.040 | Acc: 43.750,70.312,85.156,% | Adaptive Acc: 82.812% | clf_exit: 0.219 0.336 0.445
Batch: 20 | Loss: 4.104 | Acc: 44.382,68.713,83.594,% | Adaptive Acc: 80.208% | clf_exit: 0.177 0.353 0.470
Batch: 40 | Loss: 4.079 | Acc: 43.807,68.293,83.155,% | Adaptive Acc: 80.354% | clf_exit: 0.176 0.352 0.473
Batch: 60 | Loss: 4.127 | Acc: 42.841,67.853,82.569,% | Adaptive Acc: 79.611% | clf_exit: 0.173 0.351 0.476
Batch: 80 | Loss: 4.090 | Acc: 43.316,68.355,82.784,% | Adaptive Acc: 79.880% | clf_exit: 0.172 0.358 0.470
Batch: 100 | Loss: 4.092 | Acc: 43.417,68.371,82.712,% | Adaptive Acc: 79.896% | clf_exit: 0.172 0.358 0.470
Batch: 120 | Loss: 4.095 | Acc: 43.279,68.427,82.612,% | Adaptive Acc: 79.759% | clf_exit: 0.174 0.356 0.469
Batch: 140 | Loss: 4.091 | Acc: 43.118,68.528,82.663,% | Adaptive Acc: 79.837% | clf_exit: 0.174 0.357 0.469
Batch: 160 | Loss: 4.087 | Acc: 43.119,68.532,82.711,% | Adaptive Acc: 79.848% | clf_exit: 0.174 0.359 0.467
Batch: 180 | Loss: 4.067 | Acc: 43.344,68.690,82.959,% | Adaptive Acc: 80.054% | clf_exit: 0.174 0.361 0.466
Batch: 200 | Loss: 4.070 | Acc: 43.307,68.676,82.991,% | Adaptive Acc: 80.107% | clf_exit: 0.174 0.361 0.466
Batch: 220 | Loss: 4.068 | Acc: 43.375,68.732,82.979,% | Adaptive Acc: 80.055% | clf_exit: 0.175 0.360 0.465
Batch: 240 | Loss: 4.066 | Acc: 43.261,68.828,83.026,% | Adaptive Acc: 80.106% | clf_exit: 0.174 0.361 0.464
Batch: 260 | Loss: 4.069 | Acc: 43.310,68.738,82.998,% | Adaptive Acc: 80.059% | clf_exit: 0.174 0.361 0.465
Batch: 280 | Loss: 4.064 | Acc: 43.386,68.767,83.027,% | Adaptive Acc: 80.052% | clf_exit: 0.175 0.361 0.465
Batch: 300 | Loss: 4.069 | Acc: 43.322,68.766,83.020,% | Adaptive Acc: 80.053% | clf_exit: 0.175 0.360 0.465
Batch: 320 | Loss: 4.064 | Acc: 43.314,68.850,83.083,% | Adaptive Acc: 80.113% | clf_exit: 0.174 0.362 0.464
Batch: 340 | Loss: 4.069 | Acc: 43.262,68.777,83.023,% | Adaptive Acc: 80.027% | clf_exit: 0.174 0.361 0.465
Batch: 360 | Loss: 4.072 | Acc: 43.291,68.713,83.016,% | Adaptive Acc: 79.975% | clf_exit: 0.173 0.361 0.465
Batch: 380 | Loss: 4.070 | Acc: 43.264,68.721,83.020,% | Adaptive Acc: 79.956% | clf_exit: 0.174 0.362 0.464
Batch: 0 | Loss: 5.001 | Acc: 43.750,61.719,63.281,% | Adaptive Acc: 62.500% | clf_exit: 0.250 0.406 0.344
Batch: 20 | Loss: 5.308 | Acc: 41.071,60.565,63.318,% | Adaptive Acc: 62.202% | clf_exit: 0.231 0.373 0.396
Batch: 40 | Loss: 5.335 | Acc: 41.349,59.832,62.462,% | Adaptive Acc: 61.471% | clf_exit: 0.228 0.373 0.399
Batch: 60 | Loss: 5.372 | Acc: 41.086,59.670,62.218,% | Adaptive Acc: 61.168% | clf_exit: 0.228 0.373 0.399
Train classifier parameters

Epoch: 140
Batch: 0 | Loss: 4.544 | Acc: 35.156,62.500,75.781,% | Adaptive Acc: 71.875% | clf_exit: 0.141 0.320 0.539
Batch: 20 | Loss: 4.007 | Acc: 43.713,70.461,83.854,% | Adaptive Acc: 81.138% | clf_exit: 0.165 0.371 0.464
Batch: 40 | Loss: 3.984 | Acc: 44.303,70.465,84.146,% | Adaptive Acc: 81.155% | clf_exit: 0.170 0.374 0.456
Batch: 60 | Loss: 3.981 | Acc: 44.557,70.236,83.850,% | Adaptive Acc: 80.789% | clf_exit: 0.177 0.363 0.459
Batch: 80 | Loss: 3.978 | Acc: 44.329,70.100,83.864,% | Adaptive Acc: 80.739% | clf_exit: 0.179 0.362 0.459
Batch: 100 | Loss: 3.957 | Acc: 44.438,70.127,83.950,% | Adaptive Acc: 80.902% | clf_exit: 0.180 0.363 0.457
Batch: 120 | Loss: 3.982 | Acc: 44.202,69.809,83.865,% | Adaptive Acc: 80.766% | clf_exit: 0.178 0.364 0.458
Batch: 140 | Loss: 3.986 | Acc: 44.193,69.747,83.738,% | Adaptive Acc: 80.663% | clf_exit: 0.179 0.364 0.457
Batch: 160 | Loss: 4.000 | Acc: 43.930,69.604,83.540,% | Adaptive Acc: 80.512% | clf_exit: 0.177 0.365 0.458
Batch: 180 | Loss: 4.002 | Acc: 43.974,69.596,83.503,% | Adaptive Acc: 80.628% | clf_exit: 0.178 0.364 0.458
Batch: 200 | Loss: 4.006 | Acc: 43.913,69.543,83.516,% | Adaptive Acc: 80.609% | clf_exit: 0.176 0.366 0.458
Batch: 220 | Loss: 4.011 | Acc: 43.895,69.528,83.534,% | Adaptive Acc: 80.504% | clf_exit: 0.177 0.366 0.458
Batch: 240 | Loss: 4.005 | Acc: 43.996,69.408,83.584,% | Adaptive Acc: 80.517% | clf_exit: 0.179 0.364 0.457
Batch: 260 | Loss: 4.009 | Acc: 43.948,69.358,83.567,% | Adaptive Acc: 80.502% | clf_exit: 0.178 0.365 0.457
Batch: 280 | Loss: 4.022 | Acc: 43.831,69.231,83.382,% | Adaptive Acc: 80.307% | clf_exit: 0.178 0.365 0.458
Batch: 300 | Loss: 4.030 | Acc: 43.742,69.098,83.371,% | Adaptive Acc: 80.186% | clf_exit: 0.177 0.365 0.458
Batch: 320 | Loss: 4.035 | Acc: 43.772,69.088,83.353,% | Adaptive Acc: 80.172% | clf_exit: 0.177 0.365 0.458
Batch: 340 | Loss: 4.038 | Acc: 43.709,69.048,83.326,% | Adaptive Acc: 80.143% | clf_exit: 0.177 0.364 0.459
Batch: 360 | Loss: 4.038 | Acc: 43.744,69.053,83.362,% | Adaptive Acc: 80.157% | clf_exit: 0.177 0.364 0.459
Batch: 380 | Loss: 4.040 | Acc: 43.742,69.047,83.348,% | Adaptive Acc: 80.159% | clf_exit: 0.177 0.364 0.459
Batch: 0 | Loss: 4.956 | Acc: 42.969,62.500,64.844,% | Adaptive Acc: 62.500% | clf_exit: 0.258 0.391 0.352
Batch: 20 | Loss: 5.283 | Acc: 40.848,60.826,63.616,% | Adaptive Acc: 62.574% | clf_exit: 0.238 0.373 0.390
Batch: 40 | Loss: 5.302 | Acc: 41.025,60.118,62.805,% | Adaptive Acc: 61.795% | clf_exit: 0.232 0.373 0.395
Batch: 60 | Loss: 5.337 | Acc: 40.830,59.593,62.513,% | Adaptive Acc: 61.270% | clf_exit: 0.231 0.373 0.396
Train classifier parameters

Epoch: 141
Batch: 0 | Loss: 3.560 | Acc: 48.438,71.094,82.812,% | Adaptive Acc: 81.250% | clf_exit: 0.188 0.453 0.359
Batch: 20 | Loss: 4.113 | Acc: 41.592,67.560,83.482,% | Adaptive Acc: 79.985% | clf_exit: 0.180 0.348 0.472
Batch: 40 | Loss: 4.061 | Acc: 42.893,67.950,83.899,% | Adaptive Acc: 80.335% | clf_exit: 0.172 0.364 0.464
Batch: 60 | Loss: 4.048 | Acc: 42.277,68.558,83.888,% | Adaptive Acc: 80.507% | clf_exit: 0.172 0.363 0.465
Batch: 80 | Loss: 4.033 | Acc: 42.660,68.856,83.767,% | Adaptive Acc: 80.363% | clf_exit: 0.171 0.367 0.462
Batch: 100 | Loss: 4.012 | Acc: 43.139,69.059,83.756,% | Adaptive Acc: 80.523% | clf_exit: 0.176 0.365 0.459
Batch: 120 | Loss: 3.994 | Acc: 43.304,69.305,83.865,% | Adaptive Acc: 80.695% | clf_exit: 0.177 0.364 0.459
Batch: 140 | Loss: 3.991 | Acc: 43.401,69.459,83.976,% | Adaptive Acc: 80.724% | clf_exit: 0.177 0.365 0.458
Batch: 160 | Loss: 3.992 | Acc: 43.546,69.439,83.914,% | Adaptive Acc: 80.711% | clf_exit: 0.178 0.367 0.454
Batch: 180 | Loss: 4.002 | Acc: 43.482,69.320,83.909,% | Adaptive Acc: 80.646% | clf_exit: 0.178 0.367 0.455
Batch: 200 | Loss: 4.005 | Acc: 43.556,69.290,83.800,% | Adaptive Acc: 80.546% | clf_exit: 0.177 0.368 0.455
Batch: 220 | Loss: 4.002 | Acc: 43.580,69.383,83.802,% | Adaptive Acc: 80.554% | clf_exit: 0.178 0.368 0.454
Batch: 240 | Loss: 4.006 | Acc: 43.504,69.359,83.808,% | Adaptive Acc: 80.582% | clf_exit: 0.178 0.367 0.455
Batch: 260 | Loss: 4.003 | Acc: 43.612,69.406,83.821,% | Adaptive Acc: 80.615% | clf_exit: 0.178 0.368 0.455
Batch: 280 | Loss: 4.002 | Acc: 43.658,69.434,83.819,% | Adaptive Acc: 80.588% | clf_exit: 0.178 0.367 0.455
Batch: 300 | Loss: 4.007 | Acc: 43.683,69.443,83.749,% | Adaptive Acc: 80.591% | clf_exit: 0.177 0.368 0.455
Batch: 320 | Loss: 4.014 | Acc: 43.650,69.332,83.676,% | Adaptive Acc: 80.530% | clf_exit: 0.178 0.367 0.455
Batch: 340 | Loss: 4.015 | Acc: 43.647,69.284,83.665,% | Adaptive Acc: 80.508% | clf_exit: 0.178 0.368 0.455
Batch: 360 | Loss: 4.021 | Acc: 43.637,69.170,83.531,% | Adaptive Acc: 80.395% | clf_exit: 0.178 0.367 0.455
Batch: 380 | Loss: 4.023 | Acc: 43.596,69.133,83.473,% | Adaptive Acc: 80.368% | clf_exit: 0.178 0.367 0.455
Batch: 0 | Loss: 4.940 | Acc: 46.094,63.281,66.406,% | Adaptive Acc: 63.281% | clf_exit: 0.250 0.406 0.344
Batch: 20 | Loss: 5.250 | Acc: 41.555,60.379,63.690,% | Adaptive Acc: 62.351% | clf_exit: 0.239 0.373 0.388
Batch: 40 | Loss: 5.278 | Acc: 41.463,60.061,62.881,% | Adaptive Acc: 61.643% | clf_exit: 0.234 0.373 0.392
Batch: 60 | Loss: 5.311 | Acc: 41.265,59.657,62.654,% | Adaptive Acc: 61.335% | clf_exit: 0.235 0.374 0.390
Train classifier parameters

Epoch: 142
Batch: 0 | Loss: 4.200 | Acc: 45.312,66.406,79.688,% | Adaptive Acc: 75.000% | clf_exit: 0.242 0.328 0.430
Batch: 20 | Loss: 3.990 | Acc: 44.159,69.680,83.445,% | Adaptive Acc: 79.762% | clf_exit: 0.185 0.371 0.444
Batch: 40 | Loss: 4.064 | Acc: 43.121,68.826,83.270,% | Adaptive Acc: 79.459% | clf_exit: 0.177 0.365 0.458
Batch: 60 | Loss: 4.055 | Acc: 43.225,68.840,83.094,% | Adaptive Acc: 79.598% | clf_exit: 0.180 0.363 0.457
Batch: 80 | Loss: 4.036 | Acc: 43.046,69.300,83.565,% | Adaptive Acc: 80.044% | clf_exit: 0.178 0.365 0.457
Batch: 100 | Loss: 4.020 | Acc: 43.433,69.330,83.632,% | Adaptive Acc: 80.074% | clf_exit: 0.179 0.366 0.455
Batch: 120 | Loss: 4.017 | Acc: 43.311,69.383,83.768,% | Adaptive Acc: 80.236% | clf_exit: 0.179 0.366 0.455
Batch: 140 | Loss: 4.003 | Acc: 43.611,69.537,83.860,% | Adaptive Acc: 80.436% | clf_exit: 0.178 0.365 0.457
Batch: 160 | Loss: 4.010 | Acc: 43.381,69.318,83.802,% | Adaptive Acc: 80.478% | clf_exit: 0.177 0.364 0.459
Batch: 180 | Loss: 4.002 | Acc: 43.456,69.415,83.779,% | Adaptive Acc: 80.533% | clf_exit: 0.177 0.363 0.460
Batch: 200 | Loss: 3.999 | Acc: 43.536,69.372,83.776,% | Adaptive Acc: 80.566% | clf_exit: 0.178 0.364 0.458
Batch: 220 | Loss: 3.994 | Acc: 43.647,69.429,83.763,% | Adaptive Acc: 80.522% | clf_exit: 0.179 0.365 0.457
Batch: 240 | Loss: 3.991 | Acc: 43.662,69.444,83.766,% | Adaptive Acc: 80.501% | clf_exit: 0.179 0.365 0.455
Batch: 260 | Loss: 3.992 | Acc: 43.732,69.409,83.767,% | Adaptive Acc: 80.502% | clf_exit: 0.180 0.365 0.455
Batch: 280 | Loss: 3.988 | Acc: 43.750,69.417,83.877,% | Adaptive Acc: 80.616% | clf_exit: 0.180 0.366 0.455
Batch: 300 | Loss: 3.983 | Acc: 43.802,69.518,83.996,% | Adaptive Acc: 80.697% | clf_exit: 0.180 0.366 0.454
Batch: 320 | Loss: 3.987 | Acc: 43.733,69.512,84.003,% | Adaptive Acc: 80.724% | clf_exit: 0.180 0.366 0.454
Batch: 340 | Loss: 3.987 | Acc: 43.732,69.586,83.958,% | Adaptive Acc: 80.723% | clf_exit: 0.181 0.366 0.454
Batch: 360 | Loss: 3.986 | Acc: 43.707,69.546,83.914,% | Adaptive Acc: 80.668% | clf_exit: 0.181 0.366 0.453
Batch: 380 | Loss: 3.990 | Acc: 43.699,69.484,83.879,% | Adaptive Acc: 80.633% | clf_exit: 0.181 0.366 0.453
Batch: 0 | Loss: 4.931 | Acc: 44.531,64.062,66.406,% | Adaptive Acc: 62.500% | clf_exit: 0.250 0.398 0.352
Batch: 20 | Loss: 5.258 | Acc: 41.220,60.565,63.876,% | Adaptive Acc: 62.760% | clf_exit: 0.242 0.371 0.387
Batch: 40 | Loss: 5.282 | Acc: 41.139,60.099,63.053,% | Adaptive Acc: 61.947% | clf_exit: 0.239 0.369 0.392
Batch: 60 | Loss: 5.312 | Acc: 41.073,59.887,62.756,% | Adaptive Acc: 61.578% | clf_exit: 0.239 0.370 0.391
Train classifier parameters

Epoch: 143
Batch: 0 | Loss: 3.983 | Acc: 40.625,72.656,82.031,% | Adaptive Acc: 81.250% | clf_exit: 0.172 0.297 0.531
Batch: 20 | Loss: 4.052 | Acc: 42.783,69.940,83.929,% | Adaptive Acc: 81.287% | clf_exit: 0.189 0.351 0.459
Batch: 40 | Loss: 3.990 | Acc: 43.178,69.436,83.994,% | Adaptive Acc: 80.888% | clf_exit: 0.189 0.353 0.458
Batch: 60 | Loss: 3.979 | Acc: 43.635,69.582,84.132,% | Adaptive Acc: 80.738% | clf_exit: 0.189 0.355 0.455
Batch: 80 | Loss: 3.987 | Acc: 43.692,69.319,84.153,% | Adaptive Acc: 80.681% | clf_exit: 0.187 0.358 0.455
Batch: 100 | Loss: 3.978 | Acc: 43.533,69.346,84.073,% | Adaptive Acc: 80.639% | clf_exit: 0.186 0.359 0.455
Batch: 120 | Loss: 3.985 | Acc: 43.666,69.267,84.078,% | Adaptive Acc: 80.669% | clf_exit: 0.185 0.361 0.454
Batch: 140 | Loss: 3.998 | Acc: 43.307,68.977,84.015,% | Adaptive Acc: 80.629% | clf_exit: 0.183 0.359 0.458
Batch: 160 | Loss: 4.000 | Acc: 43.231,69.046,83.851,% | Adaptive Acc: 80.435% | clf_exit: 0.181 0.362 0.457
Batch: 180 | Loss: 3.986 | Acc: 43.336,69.164,83.922,% | Adaptive Acc: 80.508% | clf_exit: 0.181 0.363 0.456
Batch: 200 | Loss: 3.990 | Acc: 43.361,69.310,83.850,% | Adaptive Acc: 80.550% | clf_exit: 0.181 0.364 0.455
Batch: 220 | Loss: 3.983 | Acc: 43.418,69.369,83.827,% | Adaptive Acc: 80.515% | clf_exit: 0.181 0.365 0.454
Batch: 240 | Loss: 3.987 | Acc: 43.410,69.369,83.791,% | Adaptive Acc: 80.508% | clf_exit: 0.181 0.366 0.454
Batch: 260 | Loss: 3.991 | Acc: 43.322,69.241,83.860,% | Adaptive Acc: 80.594% | clf_exit: 0.181 0.365 0.454
Batch: 280 | Loss: 3.989 | Acc: 43.486,69.281,83.894,% | Adaptive Acc: 80.630% | clf_exit: 0.181 0.366 0.453
Batch: 300 | Loss: 3.987 | Acc: 43.464,69.329,83.903,% | Adaptive Acc: 80.663% | clf_exit: 0.181 0.367 0.452
Batch: 320 | Loss: 3.981 | Acc: 43.524,69.402,83.934,% | Adaptive Acc: 80.695% | clf_exit: 0.181 0.367 0.452
Batch: 340 | Loss: 3.980 | Acc: 43.587,69.437,84.006,% | Adaptive Acc: 80.735% | clf_exit: 0.181 0.368 0.451
Batch: 360 | Loss: 3.979 | Acc: 43.555,69.458,84.033,% | Adaptive Acc: 80.778% | clf_exit: 0.180 0.368 0.451
Batch: 380 | Loss: 3.976 | Acc: 43.617,69.472,84.051,% | Adaptive Acc: 80.815% | clf_exit: 0.181 0.368 0.451
Batch: 0 | Loss: 4.897 | Acc: 42.969,62.500,64.844,% | Adaptive Acc: 63.281% | clf_exit: 0.266 0.367 0.367
Batch: 20 | Loss: 5.236 | Acc: 41.369,60.751,63.876,% | Adaptive Acc: 62.388% | clf_exit: 0.240 0.369 0.392
Batch: 40 | Loss: 5.265 | Acc: 41.673,60.404,63.014,% | Adaptive Acc: 61.890% | clf_exit: 0.239 0.368 0.393
Batch: 60 | Loss: 5.298 | Acc: 41.265,60.143,62.756,% | Adaptive Acc: 61.514% | clf_exit: 0.239 0.371 0.390
Train classifier parameters

Epoch: 144
Batch: 0 | Loss: 4.359 | Acc: 40.625,64.844,81.250,% | Adaptive Acc: 77.344% | clf_exit: 0.141 0.320 0.539
Batch: 20 | Loss: 4.037 | Acc: 43.229,67.857,84.077,% | Adaptive Acc: 80.878% | clf_exit: 0.174 0.376 0.450
Batch: 40 | Loss: 3.980 | Acc: 43.426,69.036,83.841,% | Adaptive Acc: 80.545% | clf_exit: 0.177 0.375 0.447
Batch: 60 | Loss: 3.970 | Acc: 43.097,69.429,83.914,% | Adaptive Acc: 80.469% | clf_exit: 0.179 0.371 0.450
Batch: 80 | Loss: 3.976 | Acc: 43.345,69.473,83.825,% | Adaptive Acc: 80.633% | clf_exit: 0.181 0.369 0.451
Batch: 100 | Loss: 3.964 | Acc: 43.479,69.740,83.911,% | Adaptive Acc: 80.739% | clf_exit: 0.184 0.369 0.447
Batch: 120 | Loss: 3.984 | Acc: 43.272,69.635,83.955,% | Adaptive Acc: 80.798% | clf_exit: 0.182 0.368 0.450
Batch: 140 | Loss: 3.981 | Acc: 43.506,69.548,83.948,% | Adaptive Acc: 80.746% | clf_exit: 0.182 0.371 0.447
Batch: 160 | Loss: 3.987 | Acc: 43.493,69.556,83.865,% | Adaptive Acc: 80.745% | clf_exit: 0.182 0.371 0.447
Batch: 180 | Loss: 3.983 | Acc: 43.452,69.600,83.870,% | Adaptive Acc: 80.754% | clf_exit: 0.181 0.372 0.447
Batch: 200 | Loss: 3.973 | Acc: 43.633,69.729,83.990,% | Adaptive Acc: 80.749% | clf_exit: 0.182 0.373 0.446
Batch: 220 | Loss: 3.968 | Acc: 43.665,69.750,84.014,% | Adaptive Acc: 80.790% | clf_exit: 0.181 0.373 0.446
Batch: 240 | Loss: 3.964 | Acc: 43.662,69.755,84.018,% | Adaptive Acc: 80.780% | clf_exit: 0.181 0.373 0.445
Batch: 260 | Loss: 3.964 | Acc: 43.735,69.819,84.049,% | Adaptive Acc: 80.819% | clf_exit: 0.181 0.373 0.445
Batch: 280 | Loss: 3.958 | Acc: 43.772,69.907,84.130,% | Adaptive Acc: 80.894% | clf_exit: 0.182 0.373 0.445
Batch: 300 | Loss: 3.956 | Acc: 43.805,69.887,84.196,% | Adaptive Acc: 81.001% | clf_exit: 0.182 0.372 0.446
Batch: 320 | Loss: 3.961 | Acc: 43.806,69.745,84.098,% | Adaptive Acc: 80.895% | clf_exit: 0.182 0.371 0.447
Batch: 340 | Loss: 3.956 | Acc: 43.844,69.811,84.128,% | Adaptive Acc: 80.950% | clf_exit: 0.182 0.372 0.446
Batch: 360 | Loss: 3.955 | Acc: 43.854,69.860,84.191,% | Adaptive Acc: 80.977% | clf_exit: 0.182 0.371 0.447
Batch: 380 | Loss: 3.955 | Acc: 43.887,69.831,84.182,% | Adaptive Acc: 80.940% | clf_exit: 0.182 0.371 0.447
Batch: 0 | Loss: 4.902 | Acc: 46.094,63.281,69.531,% | Adaptive Acc: 66.406% | clf_exit: 0.250 0.383 0.367
Batch: 20 | Loss: 5.230 | Acc: 41.592,60.826,64.137,% | Adaptive Acc: 63.058% | clf_exit: 0.237 0.376 0.387
Batch: 40 | Loss: 5.254 | Acc: 41.444,60.252,63.300,% | Adaptive Acc: 62.233% | clf_exit: 0.236 0.375 0.389
Batch: 60 | Loss: 5.289 | Acc: 41.176,59.887,62.884,% | Adaptive Acc: 61.527% | clf_exit: 0.238 0.375 0.387
Train classifier parameters

Epoch: 145
Batch: 0 | Loss: 3.739 | Acc: 46.875,73.438,89.844,% | Adaptive Acc: 84.375% | clf_exit: 0.203 0.336 0.461
Batch: 20 | Loss: 3.881 | Acc: 44.308,70.610,85.640,% | Adaptive Acc: 81.808% | clf_exit: 0.185 0.383 0.432
Batch: 40 | Loss: 3.896 | Acc: 44.760,69.912,84.928,% | Adaptive Acc: 80.850% | clf_exit: 0.182 0.379 0.438
Batch: 60 | Loss: 3.908 | Acc: 44.480,70.108,84.721,% | Adaptive Acc: 81.032% | clf_exit: 0.183 0.375 0.442
Batch: 80 | Loss: 3.919 | Acc: 44.406,69.792,84.713,% | Adaptive Acc: 81.028% | clf_exit: 0.182 0.370 0.447
Batch: 100 | Loss: 3.924 | Acc: 44.276,70.104,84.607,% | Adaptive Acc: 81.149% | clf_exit: 0.182 0.369 0.450
Batch: 120 | Loss: 3.934 | Acc: 43.957,69.770,84.491,% | Adaptive Acc: 81.095% | clf_exit: 0.183 0.366 0.451
Batch: 140 | Loss: 3.931 | Acc: 43.927,69.825,84.419,% | Adaptive Acc: 81.006% | clf_exit: 0.185 0.365 0.450
Batch: 160 | Loss: 3.921 | Acc: 44.046,69.832,84.457,% | Adaptive Acc: 81.080% | clf_exit: 0.187 0.366 0.447
Batch: 180 | Loss: 3.908 | Acc: 44.138,69.911,84.630,% | Adaptive Acc: 81.108% | clf_exit: 0.188 0.367 0.445
Batch: 200 | Loss: 3.917 | Acc: 44.108,69.967,84.527,% | Adaptive Acc: 80.978% | clf_exit: 0.189 0.365 0.446
Batch: 220 | Loss: 3.921 | Acc: 44.132,69.970,84.467,% | Adaptive Acc: 80.921% | clf_exit: 0.189 0.365 0.446
Batch: 240 | Loss: 3.929 | Acc: 44.120,69.901,84.401,% | Adaptive Acc: 80.880% | clf_exit: 0.188 0.366 0.446
Batch: 260 | Loss: 3.934 | Acc: 44.082,69.905,84.426,% | Adaptive Acc: 80.942% | clf_exit: 0.188 0.366 0.446
Batch: 280 | Loss: 3.935 | Acc: 44.142,69.845,84.450,% | Adaptive Acc: 80.964% | clf_exit: 0.188 0.366 0.446
Batch: 300 | Loss: 3.942 | Acc: 44.077,69.760,84.398,% | Adaptive Acc: 80.939% | clf_exit: 0.187 0.366 0.447
Batch: 320 | Loss: 3.941 | Acc: 43.996,69.733,84.397,% | Adaptive Acc: 80.934% | clf_exit: 0.187 0.366 0.446
Batch: 340 | Loss: 3.941 | Acc: 44.007,69.696,84.419,% | Adaptive Acc: 80.879% | clf_exit: 0.188 0.367 0.446
Batch: 360 | Loss: 3.947 | Acc: 43.945,69.694,84.390,% | Adaptive Acc: 80.841% | clf_exit: 0.187 0.368 0.446
Batch: 380 | Loss: 3.944 | Acc: 43.951,69.783,84.426,% | Adaptive Acc: 80.865% | clf_exit: 0.186 0.368 0.445
Batch: 0 | Loss: 4.967 | Acc: 45.312,61.719,64.062,% | Adaptive Acc: 60.938% | clf_exit: 0.273 0.398 0.328
Batch: 20 | Loss: 5.230 | Acc: 41.146,60.454,64.397,% | Adaptive Acc: 62.574% | clf_exit: 0.249 0.371 0.380
Batch: 40 | Loss: 5.256 | Acc: 41.482,60.118,63.491,% | Adaptive Acc: 61.947% | clf_exit: 0.246 0.371 0.383
Batch: 60 | Loss: 5.291 | Acc: 41.150,59.900,63.076,% | Adaptive Acc: 61.514% | clf_exit: 0.244 0.374 0.381
Train classifier parameters

Epoch: 146
Batch: 0 | Loss: 3.638 | Acc: 42.188,71.875,89.062,% | Adaptive Acc: 85.938% | clf_exit: 0.141 0.375 0.484
Batch: 20 | Loss: 3.832 | Acc: 44.978,70.275,85.863,% | Adaptive Acc: 82.552% | clf_exit: 0.188 0.367 0.445
Batch: 40 | Loss: 3.922 | Acc: 44.436,70.579,84.585,% | Adaptive Acc: 81.955% | clf_exit: 0.173 0.371 0.456
Batch: 60 | Loss: 3.935 | Acc: 44.518,70.095,84.273,% | Adaptive Acc: 81.519% | clf_exit: 0.175 0.366 0.459
Batch: 80 | Loss: 3.946 | Acc: 43.953,70.062,84.240,% | Adaptive Acc: 81.346% | clf_exit: 0.173 0.369 0.457
Batch: 100 | Loss: 3.961 | Acc: 43.680,69.585,84.050,% | Adaptive Acc: 80.910% | clf_exit: 0.178 0.364 0.458
Batch: 120 | Loss: 3.951 | Acc: 43.737,69.951,84.220,% | Adaptive Acc: 81.121% | clf_exit: 0.179 0.366 0.455
Batch: 140 | Loss: 3.949 | Acc: 43.811,69.908,84.236,% | Adaptive Acc: 81.239% | clf_exit: 0.180 0.364 0.455
Batch: 160 | Loss: 3.946 | Acc: 43.896,69.929,84.249,% | Adaptive Acc: 81.294% | clf_exit: 0.181 0.366 0.453
Batch: 180 | Loss: 3.928 | Acc: 44.044,70.148,84.401,% | Adaptive Acc: 81.362% | clf_exit: 0.182 0.367 0.451
Batch: 200 | Loss: 3.940 | Acc: 43.964,69.986,84.258,% | Adaptive Acc: 81.145% | clf_exit: 0.182 0.367 0.451
Batch: 220 | Loss: 3.939 | Acc: 44.047,70.016,84.294,% | Adaptive Acc: 81.190% | clf_exit: 0.183 0.368 0.450
Batch: 240 | Loss: 3.938 | Acc: 44.048,70.118,84.346,% | Adaptive Acc: 81.237% | clf_exit: 0.182 0.368 0.450
Batch: 260 | Loss: 3.939 | Acc: 44.019,70.022,84.375,% | Adaptive Acc: 81.268% | clf_exit: 0.183 0.368 0.450
Batch: 280 | Loss: 3.931 | Acc: 44.117,70.115,84.445,% | Adaptive Acc: 81.414% | clf_exit: 0.183 0.369 0.449
Batch: 300 | Loss: 3.927 | Acc: 44.056,70.157,84.453,% | Adaptive Acc: 81.385% | clf_exit: 0.183 0.369 0.448
Batch: 320 | Loss: 3.933 | Acc: 43.986,70.091,84.446,% | Adaptive Acc: 81.381% | clf_exit: 0.183 0.369 0.448
Batch: 340 | Loss: 3.930 | Acc: 44.027,70.074,84.455,% | Adaptive Acc: 81.390% | clf_exit: 0.183 0.369 0.447
Batch: 360 | Loss: 3.938 | Acc: 43.973,69.958,84.423,% | Adaptive Acc: 81.324% | clf_exit: 0.183 0.369 0.448
Batch: 380 | Loss: 3.937 | Acc: 43.996,69.980,84.445,% | Adaptive Acc: 81.314% | clf_exit: 0.184 0.369 0.448
Batch: 0 | Loss: 4.877 | Acc: 46.875,64.062,65.625,% | Adaptive Acc: 61.719% | clf_exit: 0.281 0.367 0.352
Batch: 20 | Loss: 5.215 | Acc: 41.592,60.826,64.769,% | Adaptive Acc: 63.170% | clf_exit: 0.251 0.371 0.379
Batch: 40 | Loss: 5.238 | Acc: 41.406,60.347,63.567,% | Adaptive Acc: 62.024% | clf_exit: 0.248 0.370 0.382
Batch: 60 | Loss: 5.270 | Acc: 41.406,60.259,63.345,% | Adaptive Acc: 61.578% | clf_exit: 0.246 0.374 0.380
Train classifier parameters

Epoch: 147
Batch: 0 | Loss: 3.489 | Acc: 52.344,78.125,86.719,% | Adaptive Acc: 88.281% | clf_exit: 0.227 0.375 0.398
Batch: 20 | Loss: 3.863 | Acc: 44.234,70.573,84.970,% | Adaptive Acc: 82.292% | clf_exit: 0.178 0.385 0.438
Batch: 40 | Loss: 3.855 | Acc: 44.112,70.293,85.404,% | Adaptive Acc: 82.127% | clf_exit: 0.183 0.379 0.438
Batch: 60 | Loss: 3.870 | Acc: 43.750,70.261,85.566,% | Adaptive Acc: 82.057% | clf_exit: 0.181 0.382 0.437
Batch: 80 | Loss: 3.922 | Acc: 43.451,69.522,85.041,% | Adaptive Acc: 81.588% | clf_exit: 0.181 0.376 0.443
Batch: 100 | Loss: 3.909 | Acc: 43.719,69.717,85.179,% | Adaptive Acc: 81.699% | clf_exit: 0.183 0.377 0.440
Batch: 120 | Loss: 3.908 | Acc: 43.950,69.951,85.227,% | Adaptive Acc: 81.889% | clf_exit: 0.182 0.377 0.441
Batch: 140 | Loss: 3.917 | Acc: 43.916,69.808,84.951,% | Adaptive Acc: 81.660% | clf_exit: 0.182 0.377 0.441
Batch: 160 | Loss: 3.919 | Acc: 43.959,69.774,84.836,% | Adaptive Acc: 81.624% | clf_exit: 0.183 0.375 0.442
Batch: 180 | Loss: 3.932 | Acc: 43.961,69.618,84.625,% | Adaptive Acc: 81.379% | clf_exit: 0.184 0.374 0.442
Batch: 200 | Loss: 3.927 | Acc: 43.933,69.710,84.771,% | Adaptive Acc: 81.510% | clf_exit: 0.183 0.375 0.442
Batch: 220 | Loss: 3.933 | Acc: 44.026,69.772,84.746,% | Adaptive Acc: 81.490% | clf_exit: 0.183 0.374 0.443
Batch: 240 | Loss: 3.929 | Acc: 44.032,69.868,84.728,% | Adaptive Acc: 81.503% | clf_exit: 0.182 0.375 0.443
Batch: 260 | Loss: 3.935 | Acc: 43.864,69.777,84.695,% | Adaptive Acc: 81.448% | clf_exit: 0.183 0.374 0.443
Batch: 280 | Loss: 3.937 | Acc: 43.867,69.754,84.639,% | Adaptive Acc: 81.353% | clf_exit: 0.183 0.373 0.443
Batch: 300 | Loss: 3.934 | Acc: 43.867,69.739,84.673,% | Adaptive Acc: 81.375% | clf_exit: 0.183 0.374 0.443
Batch: 320 | Loss: 3.936 | Acc: 43.850,69.738,84.631,% | Adaptive Acc: 81.369% | clf_exit: 0.183 0.373 0.444
Batch: 340 | Loss: 3.933 | Acc: 43.929,69.751,84.604,% | Adaptive Acc: 81.337% | clf_exit: 0.184 0.372 0.444
Batch: 360 | Loss: 3.928 | Acc: 44.014,69.787,84.624,% | Adaptive Acc: 81.367% | clf_exit: 0.185 0.372 0.443
Batch: 380 | Loss: 3.929 | Acc: 43.969,69.824,84.596,% | Adaptive Acc: 81.357% | clf_exit: 0.185 0.372 0.443
Batch: 0 | Loss: 4.895 | Acc: 45.312,62.500,66.406,% | Adaptive Acc: 64.062% | clf_exit: 0.242 0.398 0.359
Batch: 20 | Loss: 5.229 | Acc: 40.923,60.826,64.025,% | Adaptive Acc: 62.277% | clf_exit: 0.250 0.366 0.383
Batch: 40 | Loss: 5.257 | Acc: 41.025,60.328,63.167,% | Adaptive Acc: 61.623% | clf_exit: 0.246 0.366 0.388
Batch: 60 | Loss: 5.289 | Acc: 40.996,60.028,62.961,% | Adaptive Acc: 61.245% | clf_exit: 0.247 0.366 0.387
Train classifier parameters

Epoch: 148
Batch: 0 | Loss: 3.594 | Acc: 49.219,74.219,88.281,% | Adaptive Acc: 85.156% | clf_exit: 0.211 0.383 0.406
Batch: 20 | Loss: 3.958 | Acc: 44.122,69.531,84.598,% | Adaptive Acc: 80.878% | clf_exit: 0.183 0.373 0.443
Batch: 40 | Loss: 3.937 | Acc: 43.712,69.512,85.347,% | Adaptive Acc: 81.517% | clf_exit: 0.182 0.372 0.446
Batch: 60 | Loss: 3.928 | Acc: 43.916,69.685,85.182,% | Adaptive Acc: 81.519% | clf_exit: 0.188 0.370 0.442
Batch: 80 | Loss: 3.923 | Acc: 44.271,70.052,85.098,% | Adaptive Acc: 81.588% | clf_exit: 0.189 0.368 0.443
Batch: 100 | Loss: 3.931 | Acc: 44.299,70.073,84.924,% | Adaptive Acc: 81.598% | clf_exit: 0.188 0.366 0.446
Batch: 120 | Loss: 3.934 | Acc: 44.338,69.667,84.885,% | Adaptive Acc: 81.463% | clf_exit: 0.188 0.365 0.447
Batch: 140 | Loss: 3.932 | Acc: 44.437,69.758,84.807,% | Adaptive Acc: 81.278% | clf_exit: 0.189 0.364 0.446
Batch: 160 | Loss: 3.941 | Acc: 44.434,69.643,84.700,% | Adaptive Acc: 81.221% | clf_exit: 0.189 0.364 0.447
Batch: 180 | Loss: 3.952 | Acc: 44.242,69.423,84.530,% | Adaptive Acc: 80.995% | clf_exit: 0.189 0.363 0.448
Batch: 200 | Loss: 3.943 | Acc: 44.298,69.590,84.527,% | Adaptive Acc: 81.091% | clf_exit: 0.190 0.363 0.447
Batch: 220 | Loss: 3.940 | Acc: 44.202,69.584,84.598,% | Adaptive Acc: 81.123% | clf_exit: 0.190 0.363 0.447
Batch: 240 | Loss: 3.945 | Acc: 44.009,69.457,84.582,% | Adaptive Acc: 81.098% | clf_exit: 0.188 0.365 0.447
Batch: 260 | Loss: 3.939 | Acc: 44.016,69.585,84.626,% | Adaptive Acc: 81.157% | clf_exit: 0.188 0.366 0.446
Batch: 280 | Loss: 3.936 | Acc: 44.142,69.623,84.625,% | Adaptive Acc: 81.206% | clf_exit: 0.187 0.367 0.446
Batch: 300 | Loss: 3.926 | Acc: 44.196,69.708,84.614,% | Adaptive Acc: 81.333% | clf_exit: 0.187 0.367 0.446
Batch: 320 | Loss: 3.931 | Acc: 44.035,69.643,84.553,% | Adaptive Acc: 81.265% | clf_exit: 0.187 0.366 0.447
Batch: 340 | Loss: 3.933 | Acc: 43.904,69.657,84.531,% | Adaptive Acc: 81.250% | clf_exit: 0.186 0.366 0.447
Batch: 360 | Loss: 3.927 | Acc: 44.012,69.709,84.583,% | Adaptive Acc: 81.313% | clf_exit: 0.187 0.366 0.447
Batch: 380 | Loss: 3.928 | Acc: 43.992,69.695,84.557,% | Adaptive Acc: 81.307% | clf_exit: 0.187 0.367 0.447
Batch: 0 | Loss: 4.933 | Acc: 43.750,59.375,67.969,% | Adaptive Acc: 62.500% | clf_exit: 0.273 0.414 0.312
Batch: 20 | Loss: 5.212 | Acc: 41.555,60.863,64.435,% | Adaptive Acc: 62.946% | clf_exit: 0.241 0.372 0.386
Batch: 40 | Loss: 5.243 | Acc: 41.521,60.252,63.186,% | Adaptive Acc: 62.176% | clf_exit: 0.237 0.373 0.391
Batch: 60 | Loss: 5.275 | Acc: 41.560,59.990,62.846,% | Adaptive Acc: 61.834% | clf_exit: 0.236 0.374 0.390
Train classifier parameters

Epoch: 149
Batch: 0 | Loss: 3.904 | Acc: 46.094,65.625,78.125,% | Adaptive Acc: 74.219% | clf_exit: 0.195 0.367 0.438
Batch: 20 | Loss: 3.824 | Acc: 44.457,70.536,85.454,% | Adaptive Acc: 82.031% | clf_exit: 0.191 0.370 0.439
Batch: 40 | Loss: 3.921 | Acc: 43.140,70.141,85.461,% | Adaptive Acc: 81.841% | clf_exit: 0.182 0.371 0.446
Batch: 60 | Loss: 3.895 | Acc: 43.724,70.428,85.284,% | Adaptive Acc: 81.737% | clf_exit: 0.184 0.366 0.449
Batch: 80 | Loss: 3.910 | Acc: 43.326,69.975,85.233,% | Adaptive Acc: 81.539% | clf_exit: 0.184 0.367 0.449
Batch: 100 | Loss: 3.903 | Acc: 43.789,69.995,85.125,% | Adaptive Acc: 81.505% | clf_exit: 0.185 0.367 0.448
Batch: 120 | Loss: 3.910 | Acc: 43.808,69.835,84.866,% | Adaptive Acc: 81.244% | clf_exit: 0.186 0.367 0.447
Batch: 140 | Loss: 3.893 | Acc: 44.005,70.008,84.863,% | Adaptive Acc: 81.350% | clf_exit: 0.187 0.369 0.444
Batch: 160 | Loss: 3.897 | Acc: 43.983,70.041,84.841,% | Adaptive Acc: 81.235% | clf_exit: 0.188 0.368 0.444
Batch: 180 | Loss: 3.906 | Acc: 43.845,69.864,84.772,% | Adaptive Acc: 81.207% | clf_exit: 0.188 0.368 0.444
Batch: 200 | Loss: 3.916 | Acc: 43.762,69.885,84.764,% | Adaptive Acc: 81.172% | clf_exit: 0.187 0.369 0.444
Batch: 220 | Loss: 3.919 | Acc: 43.807,69.871,84.771,% | Adaptive Acc: 81.123% | clf_exit: 0.186 0.370 0.444
Batch: 240 | Loss: 3.915 | Acc: 43.870,69.914,84.751,% | Adaptive Acc: 81.140% | clf_exit: 0.186 0.370 0.444
Batch: 260 | Loss: 3.912 | Acc: 43.936,69.995,84.737,% | Adaptive Acc: 81.124% | clf_exit: 0.186 0.371 0.443
Batch: 280 | Loss: 3.915 | Acc: 43.986,70.007,84.728,% | Adaptive Acc: 81.125% | clf_exit: 0.186 0.370 0.445
Batch: 300 | Loss: 3.912 | Acc: 44.077,70.079,84.759,% | Adaptive Acc: 81.219% | clf_exit: 0.185 0.370 0.445
Batch: 320 | Loss: 3.915 | Acc: 43.976,70.047,84.733,% | Adaptive Acc: 81.167% | clf_exit: 0.186 0.370 0.445
Batch: 340 | Loss: 3.916 | Acc: 43.878,70.031,84.728,% | Adaptive Acc: 81.232% | clf_exit: 0.185 0.370 0.445
Batch: 360 | Loss: 3.916 | Acc: 43.923,70.005,84.710,% | Adaptive Acc: 81.196% | clf_exit: 0.185 0.370 0.445
Batch: 380 | Loss: 3.913 | Acc: 44.008,70.052,84.752,% | Adaptive Acc: 81.213% | clf_exit: 0.185 0.371 0.444
Batch: 0 | Loss: 4.843 | Acc: 46.875,64.844,68.750,% | Adaptive Acc: 66.406% | clf_exit: 0.266 0.352 0.383
Batch: 20 | Loss: 5.202 | Acc: 41.146,61.644,64.621,% | Adaptive Acc: 62.686% | clf_exit: 0.239 0.377 0.384
Batch: 40 | Loss: 5.233 | Acc: 41.254,60.957,63.529,% | Adaptive Acc: 61.966% | clf_exit: 0.240 0.373 0.387
Batch: 60 | Loss: 5.267 | Acc: 41.176,60.553,63.051,% | Adaptive Acc: 61.578% | clf_exit: 0.241 0.372 0.387
Train all parameters

Epoch: 150
Batch: 0 | Loss: 4.092 | Acc: 43.750,67.969,78.906,% | Adaptive Acc: 78.906% | clf_exit: 0.164 0.320 0.516
Batch: 20 | Loss: 3.547 | Acc: 44.978,72.656,89.769,% | Adaptive Acc: 84.375% | clf_exit: 0.222 0.403 0.375
Batch: 40 | Loss: 3.358 | Acc: 46.227,75.019,92.416,% | Adaptive Acc: 86.966% | clf_exit: 0.228 0.416 0.356
Batch: 60 | Loss: 3.294 | Acc: 46.235,75.487,93.404,% | Adaptive Acc: 87.385% | clf_exit: 0.234 0.417 0.349
Batch: 80 | Loss: 3.258 | Acc: 46.615,75.550,93.692,% | Adaptive Acc: 87.394% | clf_exit: 0.240 0.417 0.343
Batch: 100 | Loss: 3.230 | Acc: 46.573,75.596,94.261,% | Adaptive Acc: 87.809% | clf_exit: 0.241 0.417 0.343
Batch: 120 | Loss: 3.213 | Acc: 46.565,75.678,94.518,% | Adaptive Acc: 87.971% | clf_exit: 0.241 0.418 0.341
Batch: 140 | Loss: 3.206 | Acc: 46.398,75.798,94.703,% | Adaptive Acc: 88.065% | clf_exit: 0.240 0.420 0.340
Batch: 160 | Loss: 3.205 | Acc: 46.186,75.752,94.856,% | Adaptive Acc: 88.257% | clf_exit: 0.239 0.420 0.341
Batch: 180 | Loss: 3.201 | Acc: 46.249,75.691,95.019,% | Adaptive Acc: 88.458% | clf_exit: 0.238 0.421 0.341
Batch: 200 | Loss: 3.192 | Acc: 46.354,75.738,95.083,% | Adaptive Acc: 88.534% | clf_exit: 0.240 0.420 0.340
Batch: 220 | Loss: 3.177 | Acc: 46.504,75.870,95.199,% | Adaptive Acc: 88.631% | clf_exit: 0.241 0.420 0.339
Batch: 240 | Loss: 3.172 | Acc: 46.583,75.879,95.319,% | Adaptive Acc: 88.725% | clf_exit: 0.241 0.421 0.338
Batch: 260 | Loss: 3.162 | Acc: 46.674,75.910,95.462,% | Adaptive Acc: 88.838% | clf_exit: 0.241 0.421 0.337
Batch: 280 | Loss: 3.160 | Acc: 46.578,75.934,95.582,% | Adaptive Acc: 88.926% | clf_exit: 0.241 0.421 0.338
Batch: 300 | Loss: 3.152 | Acc: 46.584,76.015,95.710,% | Adaptive Acc: 89.065% | clf_exit: 0.240 0.422 0.338
Batch: 320 | Loss: 3.150 | Acc: 46.568,76.010,95.807,% | Adaptive Acc: 89.155% | clf_exit: 0.240 0.422 0.338
Batch: 340 | Loss: 3.146 | Acc: 46.609,75.974,95.883,% | Adaptive Acc: 89.189% | clf_exit: 0.240 0.423 0.337
Batch: 360 | Loss: 3.139 | Acc: 46.626,76.099,95.962,% | Adaptive Acc: 89.277% | clf_exit: 0.239 0.424 0.337
Batch: 380 | Loss: 3.137 | Acc: 46.629,76.091,95.989,% | Adaptive Acc: 89.270% | clf_exit: 0.239 0.423 0.337
Batch: 0 | Loss: 4.082 | Acc: 43.750,70.312,71.875,% | Adaptive Acc: 71.094% | clf_exit: 0.305 0.461 0.234
Batch: 20 | Loss: 4.478 | Acc: 44.568,66.592,72.433,% | Adaptive Acc: 67.746% | clf_exit: 0.312 0.407 0.281
Batch: 40 | Loss: 4.483 | Acc: 44.474,66.482,72.180,% | Adaptive Acc: 67.854% | clf_exit: 0.315 0.396 0.289
Batch: 60 | Loss: 4.510 | Acc: 44.390,65.996,72.029,% | Adaptive Acc: 67.585% | clf_exit: 0.316 0.390 0.293
Train all parameters

Epoch: 151
Batch: 0 | Loss: 2.791 | Acc: 46.875,78.906,99.219,% | Adaptive Acc: 89.844% | clf_exit: 0.289 0.430 0.281
Batch: 20 | Loss: 2.931 | Acc: 48.475,79.353,98.549,% | Adaptive Acc: 91.667% | clf_exit: 0.254 0.427 0.319
Batch: 40 | Loss: 2.953 | Acc: 48.190,78.506,98.571,% | Adaptive Acc: 91.597% | clf_exit: 0.250 0.430 0.321
Batch: 60 | Loss: 2.971 | Acc: 48.297,78.227,98.591,% | Adaptive Acc: 91.893% | clf_exit: 0.246 0.426 0.328
Batch: 80 | Loss: 2.969 | Acc: 48.061,78.115,98.505,% | Adaptive Acc: 91.734% | clf_exit: 0.248 0.429 0.323
Batch: 100 | Loss: 2.971 | Acc: 48.058,78.110,98.445,% | Adaptive Acc: 91.739% | clf_exit: 0.247 0.430 0.323
Batch: 120 | Loss: 2.965 | Acc: 48.102,78.125,98.438,% | Adaptive Acc: 91.652% | clf_exit: 0.247 0.433 0.320
Batch: 140 | Loss: 2.972 | Acc: 47.845,78.092,98.426,% | Adaptive Acc: 91.639% | clf_exit: 0.245 0.433 0.322
Batch: 160 | Loss: 2.969 | Acc: 47.909,78.217,98.404,% | Adaptive Acc: 91.625% | clf_exit: 0.244 0.434 0.323
Batch: 180 | Loss: 2.968 | Acc: 47.803,78.306,98.407,% | Adaptive Acc: 91.575% | clf_exit: 0.243 0.436 0.321
Batch: 200 | Loss: 2.971 | Acc: 47.785,78.183,98.375,% | Adaptive Acc: 91.445% | clf_exit: 0.243 0.437 0.321
Batch: 220 | Loss: 2.970 | Acc: 47.709,78.220,98.399,% | Adaptive Acc: 91.403% | clf_exit: 0.243 0.438 0.320
Batch: 240 | Loss: 2.974 | Acc: 47.715,78.109,98.369,% | Adaptive Acc: 91.364% | clf_exit: 0.241 0.439 0.320
Batch: 260 | Loss: 2.974 | Acc: 47.668,78.158,98.309,% | Adaptive Acc: 91.316% | clf_exit: 0.241 0.439 0.320
Batch: 280 | Loss: 2.974 | Acc: 47.717,78.081,98.296,% | Adaptive Acc: 91.264% | clf_exit: 0.241 0.439 0.320
Batch: 300 | Loss: 2.973 | Acc: 47.648,78.076,98.282,% | Adaptive Acc: 91.245% | clf_exit: 0.241 0.438 0.320
Batch: 320 | Loss: 2.972 | Acc: 47.693,78.037,98.277,% | Adaptive Acc: 91.195% | clf_exit: 0.242 0.439 0.319
Batch: 340 | Loss: 2.976 | Acc: 47.629,78.033,98.286,% | Adaptive Acc: 91.186% | clf_exit: 0.241 0.439 0.320
Batch: 360 | Loss: 2.975 | Acc: 47.708,77.963,98.251,% | Adaptive Acc: 91.147% | clf_exit: 0.242 0.437 0.321
Batch: 380 | Loss: 2.981 | Acc: 47.630,77.871,98.263,% | Adaptive Acc: 91.125% | clf_exit: 0.241 0.436 0.322
Batch: 0 | Loss: 4.131 | Acc: 42.188,68.750,72.656,% | Adaptive Acc: 67.969% | clf_exit: 0.289 0.414 0.297
Batch: 20 | Loss: 4.422 | Acc: 45.089,67.225,72.173,% | Adaptive Acc: 68.490% | clf_exit: 0.306 0.407 0.287
Batch: 40 | Loss: 4.452 | Acc: 45.370,66.521,71.627,% | Adaptive Acc: 68.274% | clf_exit: 0.299 0.405 0.296
Batch: 60 | Loss: 4.457 | Acc: 45.197,66.304,71.696,% | Adaptive Acc: 68.212% | clf_exit: 0.297 0.403 0.300
Train all parameters

Epoch: 152
Batch: 0 | Loss: 2.701 | Acc: 52.344,83.594,96.094,% | Adaptive Acc: 89.844% | clf_exit: 0.203 0.547 0.250
Batch: 20 | Loss: 2.948 | Acc: 47.731,78.274,98.289,% | Adaptive Acc: 91.778% | clf_exit: 0.247 0.436 0.317
Batch: 40 | Loss: 2.911 | Acc: 48.247,78.963,98.418,% | Adaptive Acc: 91.521% | clf_exit: 0.253 0.435 0.311
Batch: 60 | Loss: 2.925 | Acc: 47.848,79.162,98.438,% | Adaptive Acc: 91.650% | clf_exit: 0.251 0.436 0.313
Batch: 80 | Loss: 2.913 | Acc: 48.216,79.099,98.418,% | Adaptive Acc: 91.628% | clf_exit: 0.252 0.435 0.313
Batch: 100 | Loss: 2.915 | Acc: 48.391,78.976,98.430,% | Adaptive Acc: 91.685% | clf_exit: 0.251 0.436 0.313
Batch: 120 | Loss: 2.914 | Acc: 48.599,78.951,98.438,% | Adaptive Acc: 91.561% | clf_exit: 0.251 0.436 0.313
Batch: 140 | Loss: 2.918 | Acc: 48.504,78.773,98.465,% | Adaptive Acc: 91.600% | clf_exit: 0.248 0.438 0.313
Batch: 160 | Loss: 2.922 | Acc: 48.447,78.664,98.471,% | Adaptive Acc: 91.479% | clf_exit: 0.247 0.438 0.314
Batch: 180 | Loss: 2.929 | Acc: 48.153,78.652,98.463,% | Adaptive Acc: 91.518% | clf_exit: 0.245 0.440 0.315
Batch: 200 | Loss: 2.929 | Acc: 48.018,78.685,98.457,% | Adaptive Acc: 91.457% | clf_exit: 0.245 0.441 0.314
Batch: 220 | Loss: 2.931 | Acc: 48.010,78.719,98.483,% | Adaptive Acc: 91.544% | clf_exit: 0.244 0.440 0.315
Batch: 240 | Loss: 2.933 | Acc: 47.980,78.699,98.512,% | Adaptive Acc: 91.568% | clf_exit: 0.245 0.438 0.317
Batch: 260 | Loss: 2.935 | Acc: 47.997,78.664,98.491,% | Adaptive Acc: 91.541% | clf_exit: 0.245 0.438 0.317
Batch: 280 | Loss: 2.935 | Acc: 47.979,78.631,98.482,% | Adaptive Acc: 91.542% | clf_exit: 0.244 0.439 0.317
Batch: 300 | Loss: 2.939 | Acc: 47.955,78.582,98.495,% | Adaptive Acc: 91.585% | clf_exit: 0.243 0.439 0.318
Batch: 320 | Loss: 2.938 | Acc: 47.958,78.595,98.474,% | Adaptive Acc: 91.586% | clf_exit: 0.243 0.440 0.318
Batch: 340 | Loss: 2.938 | Acc: 47.982,78.574,98.451,% | Adaptive Acc: 91.622% | clf_exit: 0.243 0.439 0.318
Batch: 360 | Loss: 2.939 | Acc: 47.899,78.508,98.472,% | Adaptive Acc: 91.599% | clf_exit: 0.243 0.439 0.318
Batch: 380 | Loss: 2.941 | Acc: 47.826,78.480,98.460,% | Adaptive Acc: 91.626% | clf_exit: 0.242 0.440 0.318
Batch: 0 | Loss: 4.225 | Acc: 46.875,71.875,74.219,% | Adaptive Acc: 67.969% | clf_exit: 0.328 0.477 0.195
Batch: 20 | Loss: 4.528 | Acc: 44.382,66.704,71.801,% | Adaptive Acc: 66.815% | clf_exit: 0.325 0.404 0.271
Batch: 40 | Loss: 4.519 | Acc: 44.493,66.578,71.341,% | Adaptive Acc: 67.111% | clf_exit: 0.321 0.398 0.282
Batch: 60 | Loss: 4.533 | Acc: 44.326,65.894,71.568,% | Adaptive Acc: 67.252% | clf_exit: 0.321 0.394 0.284
Train all parameters

Epoch: 153
Batch: 0 | Loss: 3.188 | Acc: 39.062,73.438,98.438,% | Adaptive Acc: 84.375% | clf_exit: 0.234 0.508 0.258
Batch: 20 | Loss: 2.937 | Acc: 47.321,79.613,98.996,% | Adaptive Acc: 91.704% | clf_exit: 0.230 0.470 0.300
Batch: 40 | Loss: 2.901 | Acc: 48.438,79.478,99.181,% | Adaptive Acc: 91.616% | clf_exit: 0.246 0.455 0.299
Batch: 60 | Loss: 2.880 | Acc: 49.027,79.764,99.027,% | Adaptive Acc: 91.816% | clf_exit: 0.247 0.455 0.298
Batch: 80 | Loss: 2.896 | Acc: 48.457,79.495,98.968,% | Adaptive Acc: 91.696% | clf_exit: 0.245 0.451 0.304
Batch: 100 | Loss: 2.909 | Acc: 48.352,79.293,98.871,% | Adaptive Acc: 91.731% | clf_exit: 0.243 0.449 0.307
Batch: 120 | Loss: 2.907 | Acc: 48.321,79.403,98.877,% | Adaptive Acc: 91.742% | clf_exit: 0.242 0.451 0.307
Batch: 140 | Loss: 2.907 | Acc: 48.183,79.244,98.908,% | Adaptive Acc: 91.739% | clf_exit: 0.242 0.449 0.309
Batch: 160 | Loss: 2.917 | Acc: 48.015,79.110,98.884,% | Adaptive Acc: 91.678% | clf_exit: 0.242 0.447 0.311
Batch: 180 | Loss: 2.917 | Acc: 47.950,79.182,98.843,% | Adaptive Acc: 91.652% | clf_exit: 0.242 0.448 0.310
Batch: 200 | Loss: 2.921 | Acc: 48.002,79.085,98.815,% | Adaptive Acc: 91.624% | clf_exit: 0.243 0.447 0.310
Batch: 220 | Loss: 2.919 | Acc: 47.996,79.079,98.787,% | Adaptive Acc: 91.555% | clf_exit: 0.243 0.448 0.309
Batch: 240 | Loss: 2.917 | Acc: 48.084,79.140,98.771,% | Adaptive Acc: 91.630% | clf_exit: 0.243 0.448 0.309
Batch: 260 | Loss: 2.919 | Acc: 48.174,79.089,98.728,% | Adaptive Acc: 91.556% | clf_exit: 0.244 0.447 0.310
Batch: 280 | Loss: 2.922 | Acc: 48.109,79.012,98.741,% | Adaptive Acc: 91.609% | clf_exit: 0.244 0.446 0.310
Batch: 300 | Loss: 2.922 | Acc: 47.996,78.922,98.765,% | Adaptive Acc: 91.549% | clf_exit: 0.243 0.447 0.310
Batch: 320 | Loss: 2.922 | Acc: 48.060,78.824,98.730,% | Adaptive Acc: 91.586% | clf_exit: 0.243 0.445 0.312
Batch: 340 | Loss: 2.918 | Acc: 48.085,78.808,98.726,% | Adaptive Acc: 91.573% | clf_exit: 0.243 0.445 0.311
Batch: 360 | Loss: 2.917 | Acc: 48.057,78.783,98.732,% | Adaptive Acc: 91.540% | clf_exit: 0.244 0.444 0.312
Batch: 380 | Loss: 2.916 | Acc: 48.073,78.824,98.735,% | Adaptive Acc: 91.544% | clf_exit: 0.245 0.444 0.312
Batch: 0 | Loss: 4.175 | Acc: 47.656,71.094,75.000,% | Adaptive Acc: 70.312% | clf_exit: 0.312 0.469 0.219
Batch: 20 | Loss: 4.503 | Acc: 45.461,66.815,71.801,% | Adaptive Acc: 67.671% | clf_exit: 0.318 0.407 0.276
Batch: 40 | Loss: 4.485 | Acc: 45.694,66.482,71.704,% | Adaptive Acc: 67.721% | clf_exit: 0.310 0.402 0.288
Batch: 60 | Loss: 4.498 | Acc: 45.453,66.073,71.811,% | Adaptive Acc: 67.495% | clf_exit: 0.309 0.399 0.292
Train all parameters

Epoch: 154
Batch: 0 | Loss: 2.720 | Acc: 47.656,77.344,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.266 0.422 0.312
Batch: 20 | Loss: 2.771 | Acc: 49.702,80.655,99.256,% | Adaptive Acc: 92.522% | clf_exit: 0.250 0.458 0.292
Batch: 40 | Loss: 2.820 | Acc: 49.104,80.354,99.238,% | Adaptive Acc: 92.416% | clf_exit: 0.244 0.451 0.304
Batch: 60 | Loss: 2.839 | Acc: 48.975,80.200,99.244,% | Adaptive Acc: 92.431% | clf_exit: 0.246 0.448 0.306
Batch: 80 | Loss: 2.823 | Acc: 49.064,80.179,99.199,% | Adaptive Acc: 92.351% | clf_exit: 0.251 0.445 0.304
Batch: 100 | Loss: 2.846 | Acc: 48.639,79.827,99.080,% | Adaptive Acc: 92.149% | clf_exit: 0.250 0.445 0.305
Batch: 120 | Loss: 2.857 | Acc: 48.638,79.765,99.064,% | Adaptive Acc: 92.136% | clf_exit: 0.248 0.445 0.308
Batch: 140 | Loss: 2.866 | Acc: 48.498,79.521,99.041,% | Adaptive Acc: 92.160% | clf_exit: 0.247 0.442 0.310
Batch: 160 | Loss: 2.876 | Acc: 48.258,79.421,99.034,% | Adaptive Acc: 91.993% | clf_exit: 0.246 0.443 0.311
Batch: 180 | Loss: 2.884 | Acc: 48.161,79.243,99.020,% | Adaptive Acc: 92.062% | clf_exit: 0.245 0.442 0.313
Batch: 200 | Loss: 2.886 | Acc: 48.146,79.159,99.028,% | Adaptive Acc: 92.067% | clf_exit: 0.246 0.441 0.313
Batch: 220 | Loss: 2.891 | Acc: 48.179,79.143,99.017,% | Adaptive Acc: 92.074% | clf_exit: 0.245 0.440 0.315
Batch: 240 | Loss: 2.890 | Acc: 48.256,79.098,99.002,% | Adaptive Acc: 92.084% | clf_exit: 0.244 0.441 0.315
Batch: 260 | Loss: 2.896 | Acc: 48.123,78.999,98.991,% | Adaptive Acc: 92.038% | clf_exit: 0.244 0.441 0.315
Batch: 280 | Loss: 2.898 | Acc: 48.171,78.976,99.005,% | Adaptive Acc: 92.012% | clf_exit: 0.243 0.442 0.315
Batch: 300 | Loss: 2.898 | Acc: 48.142,78.831,98.962,% | Adaptive Acc: 91.931% | clf_exit: 0.243 0.442 0.314
Batch: 320 | Loss: 2.897 | Acc: 48.121,78.931,98.956,% | Adaptive Acc: 91.944% | clf_exit: 0.243 0.444 0.313
Batch: 340 | Loss: 2.898 | Acc: 48.101,78.938,98.930,% | Adaptive Acc: 91.899% | clf_exit: 0.242 0.445 0.313
Batch: 360 | Loss: 2.898 | Acc: 48.130,78.947,98.909,% | Adaptive Acc: 91.902% | clf_exit: 0.243 0.445 0.313
Batch: 380 | Loss: 2.898 | Acc: 48.175,78.937,98.895,% | Adaptive Acc: 91.870% | clf_exit: 0.243 0.445 0.312
Batch: 0 | Loss: 4.192 | Acc: 43.750,69.531,74.219,% | Adaptive Acc: 70.312% | clf_exit: 0.281 0.508 0.211
Batch: 20 | Loss: 4.477 | Acc: 44.196,67.001,72.470,% | Adaptive Acc: 68.080% | clf_exit: 0.312 0.424 0.264
Batch: 40 | Loss: 4.479 | Acc: 44.398,66.597,71.780,% | Adaptive Acc: 67.550% | clf_exit: 0.311 0.409 0.280
Batch: 60 | Loss: 4.487 | Acc: 44.659,66.214,71.760,% | Adaptive Acc: 67.559% | clf_exit: 0.311 0.405 0.284
Train all parameters

Epoch: 155
Batch: 0 | Loss: 2.947 | Acc: 49.219,82.031,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.250 0.484 0.266
Batch: 20 | Loss: 2.853 | Acc: 48.214,79.278,99.256,% | Adaptive Acc: 91.555% | clf_exit: 0.251 0.455 0.294
Batch: 40 | Loss: 2.855 | Acc: 48.438,79.992,99.314,% | Adaptive Acc: 91.959% | clf_exit: 0.250 0.451 0.299
Batch: 60 | Loss: 2.867 | Acc: 48.105,79.816,99.321,% | Adaptive Acc: 92.136% | clf_exit: 0.244 0.454 0.302
Batch: 80 | Loss: 2.884 | Acc: 48.206,79.215,99.363,% | Adaptive Acc: 92.110% | clf_exit: 0.244 0.446 0.310
Batch: 100 | Loss: 2.879 | Acc: 48.151,79.548,99.327,% | Adaptive Acc: 92.188% | clf_exit: 0.244 0.447 0.309
Batch: 120 | Loss: 2.866 | Acc: 48.224,79.591,99.219,% | Adaptive Acc: 92.020% | clf_exit: 0.245 0.446 0.309
Batch: 140 | Loss: 2.862 | Acc: 48.255,79.732,99.208,% | Adaptive Acc: 92.060% | clf_exit: 0.247 0.444 0.309
Batch: 160 | Loss: 2.861 | Acc: 48.404,79.702,99.175,% | Adaptive Acc: 92.081% | clf_exit: 0.246 0.445 0.309
Batch: 180 | Loss: 2.859 | Acc: 48.459,79.636,99.150,% | Adaptive Acc: 92.023% | clf_exit: 0.247 0.445 0.308
Batch: 200 | Loss: 2.858 | Acc: 48.531,79.676,99.153,% | Adaptive Acc: 92.086% | clf_exit: 0.247 0.445 0.308
Batch: 220 | Loss: 2.865 | Acc: 48.466,79.564,99.123,% | Adaptive Acc: 92.060% | clf_exit: 0.247 0.445 0.309
Batch: 240 | Loss: 2.867 | Acc: 48.366,79.584,99.089,% | Adaptive Acc: 92.061% | clf_exit: 0.246 0.446 0.308
Batch: 260 | Loss: 2.869 | Acc: 48.321,79.559,99.084,% | Adaptive Acc: 92.083% | clf_exit: 0.246 0.445 0.309
Batch: 280 | Loss: 2.877 | Acc: 48.235,79.421,99.069,% | Adaptive Acc: 92.062% | clf_exit: 0.245 0.445 0.309
Batch: 300 | Loss: 2.878 | Acc: 48.155,79.407,99.055,% | Adaptive Acc: 92.053% | clf_exit: 0.245 0.445 0.310
Batch: 320 | Loss: 2.879 | Acc: 48.121,79.357,99.029,% | Adaptive Acc: 92.000% | clf_exit: 0.246 0.444 0.310
Batch: 340 | Loss: 2.886 | Acc: 48.009,79.200,99.001,% | Adaptive Acc: 91.913% | clf_exit: 0.245 0.444 0.310
Batch: 360 | Loss: 2.888 | Acc: 47.998,79.138,98.979,% | Adaptive Acc: 91.910% | clf_exit: 0.246 0.443 0.311
Batch: 380 | Loss: 2.887 | Acc: 48.107,79.122,98.983,% | Adaptive Acc: 91.941% | clf_exit: 0.246 0.443 0.311
Batch: 0 | Loss: 4.089 | Acc: 43.750,67.188,73.438,% | Adaptive Acc: 68.750% | clf_exit: 0.320 0.445 0.234
Batch: 20 | Loss: 4.435 | Acc: 44.978,67.039,72.173,% | Adaptive Acc: 68.415% | clf_exit: 0.310 0.414 0.276
Batch: 40 | Loss: 4.433 | Acc: 45.770,66.825,71.475,% | Adaptive Acc: 68.521% | clf_exit: 0.307 0.410 0.283
Batch: 60 | Loss: 4.456 | Acc: 45.492,66.534,71.478,% | Adaptive Acc: 68.199% | clf_exit: 0.309 0.408 0.283
Train all parameters

Epoch: 156
Batch: 0 | Loss: 2.887 | Acc: 47.656,81.250,100.000,% | Adaptive Acc: 89.844% | clf_exit: 0.266 0.414 0.320
Batch: 20 | Loss: 2.762 | Acc: 50.000,80.952,99.107,% | Adaptive Acc: 92.113% | clf_exit: 0.247 0.461 0.292
Batch: 40 | Loss: 2.811 | Acc: 48.857,79.916,99.085,% | Adaptive Acc: 92.092% | clf_exit: 0.248 0.454 0.298
Batch: 60 | Loss: 2.846 | Acc: 48.566,79.854,99.027,% | Adaptive Acc: 92.136% | clf_exit: 0.246 0.451 0.304
Batch: 80 | Loss: 2.856 | Acc: 48.727,79.524,99.045,% | Adaptive Acc: 92.207% | clf_exit: 0.248 0.445 0.307
Batch: 100 | Loss: 2.852 | Acc: 48.933,79.448,99.064,% | Adaptive Acc: 92.280% | clf_exit: 0.248 0.444 0.307
Batch: 120 | Loss: 2.865 | Acc: 48.676,79.268,99.025,% | Adaptive Acc: 92.252% | clf_exit: 0.247 0.443 0.310
Batch: 140 | Loss: 2.854 | Acc: 48.770,79.366,99.075,% | Adaptive Acc: 92.309% | clf_exit: 0.247 0.445 0.308
Batch: 160 | Loss: 2.858 | Acc: 48.525,79.406,99.059,% | Adaptive Acc: 92.299% | clf_exit: 0.246 0.447 0.307
Batch: 180 | Loss: 2.857 | Acc: 48.507,79.485,99.094,% | Adaptive Acc: 92.265% | clf_exit: 0.246 0.447 0.307
Batch: 200 | Loss: 2.867 | Acc: 48.426,79.427,99.087,% | Adaptive Acc: 92.300% | clf_exit: 0.245 0.446 0.309
Batch: 220 | Loss: 2.870 | Acc: 48.402,79.398,99.088,% | Adaptive Acc: 92.286% | clf_exit: 0.245 0.446 0.309
Batch: 240 | Loss: 2.869 | Acc: 48.522,79.464,99.096,% | Adaptive Acc: 92.333% | clf_exit: 0.244 0.447 0.309
Batch: 260 | Loss: 2.876 | Acc: 48.554,79.382,99.063,% | Adaptive Acc: 92.247% | clf_exit: 0.243 0.447 0.310
Batch: 280 | Loss: 2.877 | Acc: 48.465,79.284,99.046,% | Adaptive Acc: 92.182% | clf_exit: 0.242 0.448 0.310
Batch: 300 | Loss: 2.876 | Acc: 48.479,79.270,99.055,% | Adaptive Acc: 92.195% | clf_exit: 0.242 0.447 0.311
Batch: 320 | Loss: 2.873 | Acc: 48.486,79.332,99.029,% | Adaptive Acc: 92.163% | clf_exit: 0.243 0.448 0.309
Batch: 340 | Loss: 2.869 | Acc: 48.612,79.344,98.985,% | Adaptive Acc: 92.155% | clf_exit: 0.244 0.448 0.309
Batch: 360 | Loss: 2.869 | Acc: 48.669,79.346,98.983,% | Adaptive Acc: 92.185% | clf_exit: 0.243 0.447 0.309
Batch: 380 | Loss: 2.871 | Acc: 48.577,79.310,98.989,% | Adaptive Acc: 92.188% | clf_exit: 0.243 0.447 0.310
Batch: 0 | Loss: 3.997 | Acc: 48.438,68.750,75.000,% | Adaptive Acc: 71.875% | clf_exit: 0.320 0.438 0.242
Batch: 20 | Loss: 4.468 | Acc: 45.647,66.927,72.582,% | Adaptive Acc: 68.750% | clf_exit: 0.320 0.400 0.280
Batch: 40 | Loss: 4.452 | Acc: 45.636,66.654,71.818,% | Adaptive Acc: 68.483% | clf_exit: 0.316 0.394 0.289
Batch: 60 | Loss: 4.456 | Acc: 45.274,66.317,71.875,% | Adaptive Acc: 68.122% | clf_exit: 0.316 0.393 0.291
Train all parameters

Epoch: 157
Batch: 0 | Loss: 2.748 | Acc: 48.438,83.594,99.219,% | Adaptive Acc: 92.188% | clf_exit: 0.297 0.430 0.273
Batch: 20 | Loss: 2.872 | Acc: 48.698,79.874,99.256,% | Adaptive Acc: 92.225% | clf_exit: 0.248 0.447 0.305
Batch: 40 | Loss: 2.859 | Acc: 48.685,80.278,99.009,% | Adaptive Acc: 92.321% | clf_exit: 0.240 0.450 0.310
Batch: 60 | Loss: 2.847 | Acc: 48.540,80.123,99.052,% | Adaptive Acc: 92.316% | clf_exit: 0.246 0.445 0.309
Batch: 80 | Loss: 2.838 | Acc: 48.688,80.141,99.132,% | Adaptive Acc: 92.342% | clf_exit: 0.251 0.443 0.306
Batch: 100 | Loss: 2.844 | Acc: 48.793,80.082,99.165,% | Adaptive Acc: 92.149% | clf_exit: 0.250 0.445 0.305
Batch: 120 | Loss: 2.846 | Acc: 48.754,79.978,99.206,% | Adaptive Acc: 92.116% | clf_exit: 0.251 0.444 0.305
Batch: 140 | Loss: 2.858 | Acc: 48.643,79.787,99.158,% | Adaptive Acc: 92.093% | clf_exit: 0.250 0.442 0.308
Batch: 160 | Loss: 2.858 | Acc: 48.554,79.770,99.161,% | Adaptive Acc: 92.018% | clf_exit: 0.250 0.442 0.308
Batch: 180 | Loss: 2.868 | Acc: 48.364,79.580,99.163,% | Adaptive Acc: 92.088% | clf_exit: 0.248 0.442 0.310
Batch: 200 | Loss: 2.865 | Acc: 48.379,79.555,99.160,% | Adaptive Acc: 92.005% | clf_exit: 0.249 0.441 0.310
Batch: 220 | Loss: 2.871 | Acc: 48.317,79.440,99.155,% | Adaptive Acc: 92.000% | clf_exit: 0.248 0.441 0.312
Batch: 240 | Loss: 2.872 | Acc: 48.376,79.386,99.157,% | Adaptive Acc: 91.918% | clf_exit: 0.247 0.442 0.311
Batch: 260 | Loss: 2.871 | Acc: 48.381,79.361,99.150,% | Adaptive Acc: 91.981% | clf_exit: 0.245 0.443 0.312
Batch: 280 | Loss: 2.869 | Acc: 48.468,79.332,99.124,% | Adaptive Acc: 91.987% | clf_exit: 0.246 0.443 0.311
Batch: 300 | Loss: 2.867 | Acc: 48.559,79.319,99.105,% | Adaptive Acc: 92.021% | clf_exit: 0.246 0.443 0.311
Batch: 320 | Loss: 2.865 | Acc: 48.588,79.371,99.080,% | Adaptive Acc: 91.985% | clf_exit: 0.246 0.444 0.310
Batch: 340 | Loss: 2.862 | Acc: 48.550,79.431,99.061,% | Adaptive Acc: 91.995% | clf_exit: 0.245 0.445 0.309
Batch: 360 | Loss: 2.862 | Acc: 48.572,79.441,99.056,% | Adaptive Acc: 91.956% | clf_exit: 0.246 0.445 0.309
Batch: 380 | Loss: 2.862 | Acc: 48.517,79.419,99.059,% | Adaptive Acc: 91.921% | clf_exit: 0.247 0.445 0.308
Batch: 0 | Loss: 4.041 | Acc: 46.875,68.750,75.000,% | Adaptive Acc: 67.969% | clf_exit: 0.320 0.453 0.227
Batch: 20 | Loss: 4.475 | Acc: 45.759,66.257,71.168,% | Adaptive Acc: 67.299% | clf_exit: 0.320 0.404 0.276
Batch: 40 | Loss: 4.477 | Acc: 45.922,66.654,71.227,% | Adaptive Acc: 67.378% | clf_exit: 0.319 0.395 0.286
Batch: 60 | Loss: 4.477 | Acc: 45.620,66.342,71.273,% | Adaptive Acc: 67.431% | clf_exit: 0.316 0.396 0.288
Train all parameters

Epoch: 158
Batch: 0 | Loss: 3.291 | Acc: 42.969,78.125,98.438,% | Adaptive Acc: 92.188% | clf_exit: 0.203 0.469 0.328
Batch: 20 | Loss: 2.944 | Acc: 46.243,79.241,99.368,% | Adaptive Acc: 92.820% | clf_exit: 0.227 0.450 0.323
Batch: 40 | Loss: 2.920 | Acc: 46.913,79.878,99.143,% | Adaptive Acc: 92.912% | clf_exit: 0.226 0.455 0.319
Batch: 60 | Loss: 2.893 | Acc: 46.977,80.149,99.219,% | Adaptive Acc: 92.943% | clf_exit: 0.231 0.454 0.315
Batch: 80 | Loss: 2.878 | Acc: 47.338,80.295,99.199,% | Adaptive Acc: 92.843% | clf_exit: 0.236 0.453 0.311
Batch: 100 | Loss: 2.866 | Acc: 47.633,80.283,99.180,% | Adaptive Acc: 92.567% | clf_exit: 0.239 0.454 0.307
Batch: 120 | Loss: 2.871 | Acc: 47.747,80.159,99.154,% | Adaptive Acc: 92.472% | clf_exit: 0.239 0.454 0.307
Batch: 140 | Loss: 2.867 | Acc: 47.839,80.153,99.147,% | Adaptive Acc: 92.431% | clf_exit: 0.241 0.455 0.305
Batch: 160 | Loss: 2.863 | Acc: 47.947,80.071,99.151,% | Adaptive Acc: 92.328% | clf_exit: 0.241 0.455 0.304
Batch: 180 | Loss: 2.861 | Acc: 48.019,79.942,99.154,% | Adaptive Acc: 92.377% | clf_exit: 0.242 0.454 0.304
Batch: 200 | Loss: 2.856 | Acc: 48.251,79.901,99.168,% | Adaptive Acc: 92.405% | clf_exit: 0.242 0.454 0.304
Batch: 220 | Loss: 2.859 | Acc: 48.225,79.765,99.145,% | Adaptive Acc: 92.286% | clf_exit: 0.244 0.452 0.305
Batch: 240 | Loss: 2.857 | Acc: 48.249,79.827,99.134,% | Adaptive Acc: 92.252% | clf_exit: 0.243 0.452 0.305
Batch: 260 | Loss: 2.853 | Acc: 48.339,79.777,99.123,% | Adaptive Acc: 92.185% | clf_exit: 0.244 0.451 0.305
Batch: 280 | Loss: 2.851 | Acc: 48.435,79.782,99.110,% | Adaptive Acc: 92.263% | clf_exit: 0.244 0.451 0.306
Batch: 300 | Loss: 2.850 | Acc: 48.482,79.786,99.099,% | Adaptive Acc: 92.273% | clf_exit: 0.244 0.451 0.306
Batch: 320 | Loss: 2.851 | Acc: 48.420,79.778,99.099,% | Adaptive Acc: 92.263% | clf_exit: 0.244 0.450 0.306
Batch: 340 | Loss: 2.854 | Acc: 48.451,79.749,99.068,% | Adaptive Acc: 92.194% | clf_exit: 0.245 0.449 0.306
Batch: 360 | Loss: 2.855 | Acc: 48.444,79.685,99.048,% | Adaptive Acc: 92.188% | clf_exit: 0.245 0.449 0.306
Batch: 380 | Loss: 2.859 | Acc: 48.384,79.630,99.047,% | Adaptive Acc: 92.161% | clf_exit: 0.244 0.449 0.307
Batch: 0 | Loss: 4.075 | Acc: 46.875,65.625,77.344,% | Adaptive Acc: 69.531% | clf_exit: 0.305 0.469 0.227
Batch: 20 | Loss: 4.495 | Acc: 45.312,65.885,71.875,% | Adaptive Acc: 68.006% | clf_exit: 0.286 0.435 0.279
Batch: 40 | Loss: 4.491 | Acc: 45.465,66.063,71.037,% | Adaptive Acc: 67.740% | clf_exit: 0.282 0.422 0.295
Batch: 60 | Loss: 4.497 | Acc: 45.197,66.189,71.491,% | Adaptive Acc: 68.174% | clf_exit: 0.281 0.421 0.298
Train all parameters

Epoch: 159
Batch: 0 | Loss: 3.047 | Acc: 40.625,85.938,99.219,% | Adaptive Acc: 95.312% | clf_exit: 0.180 0.508 0.312
Batch: 20 | Loss: 2.775 | Acc: 49.554,81.473,99.256,% | Adaptive Acc: 93.527% | clf_exit: 0.242 0.454 0.304
Batch: 40 | Loss: 2.797 | Acc: 49.543,80.545,99.238,% | Adaptive Acc: 92.969% | clf_exit: 0.250 0.447 0.303
Batch: 60 | Loss: 2.824 | Acc: 48.963,80.328,99.180,% | Adaptive Acc: 92.636% | clf_exit: 0.248 0.452 0.301
Batch: 80 | Loss: 2.809 | Acc: 48.978,80.729,99.219,% | Adaptive Acc: 92.679% | clf_exit: 0.247 0.455 0.298
Batch: 100 | Loss: 2.818 | Acc: 48.871,80.593,99.149,% | Adaptive Acc: 92.597% | clf_exit: 0.246 0.454 0.301
Batch: 120 | Loss: 2.821 | Acc: 48.857,80.682,99.141,% | Adaptive Acc: 92.594% | clf_exit: 0.246 0.453 0.302
Batch: 140 | Loss: 2.830 | Acc: 48.814,80.568,99.147,% | Adaptive Acc: 92.642% | clf_exit: 0.246 0.451 0.303
Batch: 160 | Loss: 2.835 | Acc: 48.719,80.435,99.093,% | Adaptive Acc: 92.522% | clf_exit: 0.245 0.452 0.303
Batch: 180 | Loss: 2.835 | Acc: 48.826,80.305,99.042,% | Adaptive Acc: 92.386% | clf_exit: 0.247 0.451 0.302
Batch: 200 | Loss: 2.831 | Acc: 48.881,80.375,99.056,% | Adaptive Acc: 92.413% | clf_exit: 0.246 0.452 0.302
Batch: 220 | Loss: 2.837 | Acc: 48.780,80.154,99.053,% | Adaptive Acc: 92.382% | clf_exit: 0.247 0.451 0.302
Batch: 240 | Loss: 2.841 | Acc: 48.720,80.112,99.066,% | Adaptive Acc: 92.427% | clf_exit: 0.246 0.450 0.304
Batch: 260 | Loss: 2.844 | Acc: 48.644,80.041,99.066,% | Adaptive Acc: 92.379% | clf_exit: 0.247 0.448 0.305
Batch: 280 | Loss: 2.845 | Acc: 48.593,79.941,99.060,% | Adaptive Acc: 92.257% | clf_exit: 0.248 0.447 0.305
Batch: 300 | Loss: 2.847 | Acc: 48.513,79.869,99.058,% | Adaptive Acc: 92.239% | clf_exit: 0.247 0.448 0.305
Batch: 320 | Loss: 2.846 | Acc: 48.496,79.882,99.053,% | Adaptive Acc: 92.246% | clf_exit: 0.246 0.449 0.305
Batch: 340 | Loss: 2.846 | Acc: 48.582,79.820,99.042,% | Adaptive Acc: 92.208% | clf_exit: 0.247 0.448 0.305
Batch: 360 | Loss: 2.848 | Acc: 48.533,79.731,99.065,% | Adaptive Acc: 92.170% | clf_exit: 0.246 0.448 0.305
Batch: 380 | Loss: 2.847 | Acc: 48.612,79.714,99.063,% | Adaptive Acc: 92.165% | clf_exit: 0.247 0.448 0.305
Batch: 0 | Loss: 3.898 | Acc: 50.000,70.312,78.125,% | Adaptive Acc: 72.656% | clf_exit: 0.305 0.461 0.234
Batch: 20 | Loss: 4.475 | Acc: 45.685,66.741,71.949,% | Adaptive Acc: 67.783% | clf_exit: 0.306 0.405 0.289
Batch: 40 | Loss: 4.475 | Acc: 45.941,66.444,71.551,% | Adaptive Acc: 67.759% | clf_exit: 0.307 0.401 0.292
Batch: 60 | Loss: 4.482 | Acc: 45.697,66.406,71.465,% | Adaptive Acc: 67.610% | clf_exit: 0.308 0.398 0.295
Train all parameters

Epoch: 160
Batch: 0 | Loss: 2.889 | Acc: 47.656,79.688,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.180 0.516 0.305
Batch: 20 | Loss: 2.774 | Acc: 49.926,80.729,99.368,% | Adaptive Acc: 92.783% | clf_exit: 0.248 0.456 0.296
Batch: 40 | Loss: 2.840 | Acc: 48.723,80.240,99.257,% | Adaptive Acc: 92.302% | clf_exit: 0.245 0.450 0.305
Batch: 60 | Loss: 2.835 | Acc: 48.566,80.405,99.296,% | Adaptive Acc: 92.380% | clf_exit: 0.249 0.449 0.303
Batch: 80 | Loss: 2.819 | Acc: 48.428,80.768,99.267,% | Adaptive Acc: 92.602% | clf_exit: 0.246 0.455 0.299
Batch: 100 | Loss: 2.817 | Acc: 48.786,80.678,99.265,% | Adaptive Acc: 92.497% | clf_exit: 0.248 0.455 0.297
Batch: 120 | Loss: 2.824 | Acc: 48.463,80.746,99.296,% | Adaptive Acc: 92.433% | clf_exit: 0.247 0.454 0.299
Batch: 140 | Loss: 2.824 | Acc: 48.521,80.618,99.296,% | Adaptive Acc: 92.337% | clf_exit: 0.247 0.454 0.298
Batch: 160 | Loss: 2.823 | Acc: 48.602,80.576,99.243,% | Adaptive Acc: 92.304% | clf_exit: 0.248 0.453 0.299
Batch: 180 | Loss: 2.822 | Acc: 48.658,80.585,99.262,% | Adaptive Acc: 92.360% | clf_exit: 0.247 0.453 0.299
Batch: 200 | Loss: 2.817 | Acc: 48.698,80.515,99.277,% | Adaptive Acc: 92.296% | clf_exit: 0.249 0.453 0.298
Batch: 220 | Loss: 2.815 | Acc: 48.727,80.497,99.261,% | Adaptive Acc: 92.262% | clf_exit: 0.250 0.451 0.299
Batch: 240 | Loss: 2.818 | Acc: 48.661,80.488,99.251,% | Adaptive Acc: 92.272% | clf_exit: 0.249 0.452 0.299
Batch: 260 | Loss: 2.815 | Acc: 48.791,80.538,99.252,% | Adaptive Acc: 92.322% | clf_exit: 0.248 0.454 0.298
Batch: 280 | Loss: 2.824 | Acc: 48.677,80.344,99.249,% | Adaptive Acc: 92.293% | clf_exit: 0.248 0.452 0.300
Batch: 300 | Loss: 2.824 | Acc: 48.614,80.352,99.250,% | Adaptive Acc: 92.260% | clf_exit: 0.248 0.452 0.300
Batch: 320 | Loss: 2.831 | Acc: 48.574,80.208,99.236,% | Adaptive Acc: 92.222% | clf_exit: 0.248 0.451 0.301
Batch: 340 | Loss: 2.835 | Acc: 48.566,80.059,99.242,% | Adaptive Acc: 92.215% | clf_exit: 0.247 0.452 0.301
Batch: 360 | Loss: 2.837 | Acc: 48.606,79.965,99.232,% | Adaptive Acc: 92.166% | clf_exit: 0.248 0.451 0.301
Batch: 380 | Loss: 2.835 | Acc: 48.632,79.958,99.237,% | Adaptive Acc: 92.149% | clf_exit: 0.248 0.451 0.301
Batch: 0 | Loss: 4.110 | Acc: 46.094,68.750,75.781,% | Adaptive Acc: 69.531% | clf_exit: 0.320 0.438 0.242
Batch: 20 | Loss: 4.504 | Acc: 45.871,67.150,71.243,% | Adaptive Acc: 67.299% | clf_exit: 0.315 0.411 0.274
Batch: 40 | Loss: 4.458 | Acc: 45.865,66.864,71.246,% | Adaptive Acc: 67.435% | clf_exit: 0.313 0.401 0.286
Batch: 60 | Loss: 4.478 | Acc: 45.620,66.201,71.260,% | Adaptive Acc: 67.264% | clf_exit: 0.310 0.403 0.287
Train all parameters

Epoch: 161
Batch: 0 | Loss: 2.703 | Acc: 50.000,81.250,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.312 0.398 0.289
Batch: 20 | Loss: 2.753 | Acc: 50.037,80.729,99.107,% | Adaptive Acc: 91.964% | clf_exit: 0.247 0.462 0.291
Batch: 40 | Loss: 2.768 | Acc: 49.428,81.079,99.200,% | Adaptive Acc: 92.492% | clf_exit: 0.240 0.463 0.297
Batch: 60 | Loss: 2.765 | Acc: 49.769,81.416,99.168,% | Adaptive Acc: 92.520% | clf_exit: 0.244 0.460 0.296
Batch: 80 | Loss: 2.783 | Acc: 49.392,81.115,99.171,% | Adaptive Acc: 92.535% | clf_exit: 0.243 0.458 0.298
Batch: 100 | Loss: 2.783 | Acc: 49.397,80.902,99.149,% | Adaptive Acc: 92.327% | clf_exit: 0.244 0.458 0.298
Batch: 120 | Loss: 2.795 | Acc: 49.077,80.759,99.167,% | Adaptive Acc: 92.233% | clf_exit: 0.244 0.457 0.299
Batch: 140 | Loss: 2.796 | Acc: 48.997,80.701,99.141,% | Adaptive Acc: 92.077% | clf_exit: 0.245 0.458 0.298
Batch: 160 | Loss: 2.798 | Acc: 48.923,80.643,99.161,% | Adaptive Acc: 92.086% | clf_exit: 0.246 0.455 0.299
Batch: 180 | Loss: 2.812 | Acc: 48.809,80.404,99.132,% | Adaptive Acc: 92.015% | clf_exit: 0.246 0.454 0.300
Batch: 200 | Loss: 2.809 | Acc: 48.923,80.383,99.157,% | Adaptive Acc: 92.102% | clf_exit: 0.246 0.454 0.300
Batch: 220 | Loss: 2.819 | Acc: 48.752,80.313,99.173,% | Adaptive Acc: 92.156% | clf_exit: 0.246 0.452 0.302
Batch: 240 | Loss: 2.821 | Acc: 48.778,80.248,99.186,% | Adaptive Acc: 92.181% | clf_exit: 0.245 0.453 0.302
Batch: 260 | Loss: 2.821 | Acc: 48.764,80.211,99.204,% | Adaptive Acc: 92.176% | clf_exit: 0.245 0.453 0.302
Batch: 280 | Loss: 2.824 | Acc: 48.754,80.205,99.202,% | Adaptive Acc: 92.235% | clf_exit: 0.245 0.452 0.303
Batch: 300 | Loss: 2.822 | Acc: 48.785,80.212,99.208,% | Adaptive Acc: 92.237% | clf_exit: 0.245 0.453 0.302
Batch: 320 | Loss: 2.820 | Acc: 48.795,80.269,99.209,% | Adaptive Acc: 92.307% | clf_exit: 0.245 0.452 0.302
Batch: 340 | Loss: 2.822 | Acc: 48.857,80.240,99.210,% | Adaptive Acc: 92.318% | clf_exit: 0.246 0.452 0.302
Batch: 360 | Loss: 2.822 | Acc: 48.885,80.244,99.191,% | Adaptive Acc: 92.332% | clf_exit: 0.246 0.451 0.303
Batch: 380 | Loss: 2.825 | Acc: 48.887,80.141,99.180,% | Adaptive Acc: 92.331% | clf_exit: 0.247 0.450 0.303
Batch: 0 | Loss: 4.088 | Acc: 50.000,71.875,77.344,% | Adaptive Acc: 69.531% | clf_exit: 0.328 0.500 0.172
Batch: 20 | Loss: 4.484 | Acc: 45.312,66.109,71.987,% | Adaptive Acc: 68.192% | clf_exit: 0.311 0.412 0.277
Batch: 40 | Loss: 4.466 | Acc: 45.655,66.463,71.284,% | Adaptive Acc: 68.064% | clf_exit: 0.310 0.409 0.281
Batch: 60 | Loss: 4.480 | Acc: 45.479,66.201,71.158,% | Adaptive Acc: 67.777% | clf_exit: 0.311 0.405 0.283
Train all parameters

Epoch: 162
Batch: 0 | Loss: 2.837 | Acc: 50.781,83.594,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.273 0.414 0.312
Batch: 20 | Loss: 2.796 | Acc: 49.182,80.283,99.442,% | Adaptive Acc: 92.671% | clf_exit: 0.239 0.477 0.283
Batch: 40 | Loss: 2.766 | Acc: 49.352,80.793,99.295,% | Adaptive Acc: 92.816% | clf_exit: 0.239 0.472 0.289
Batch: 60 | Loss: 2.813 | Acc: 48.770,80.405,99.103,% | Adaptive Acc: 92.354% | clf_exit: 0.242 0.466 0.292
Batch: 80 | Loss: 2.809 | Acc: 48.775,80.285,99.199,% | Adaptive Acc: 92.361% | clf_exit: 0.245 0.462 0.293
Batch: 100 | Loss: 2.810 | Acc: 48.847,80.043,99.226,% | Adaptive Acc: 92.358% | clf_exit: 0.246 0.458 0.295
Batch: 120 | Loss: 2.821 | Acc: 48.670,79.939,99.219,% | Adaptive Acc: 92.413% | clf_exit: 0.245 0.458 0.298
Batch: 140 | Loss: 2.816 | Acc: 48.980,80.098,99.219,% | Adaptive Acc: 92.442% | clf_exit: 0.246 0.456 0.298
Batch: 160 | Loss: 2.821 | Acc: 48.971,80.061,99.238,% | Adaptive Acc: 92.420% | clf_exit: 0.245 0.455 0.300
Batch: 180 | Loss: 2.823 | Acc: 48.822,79.959,99.271,% | Adaptive Acc: 92.464% | clf_exit: 0.243 0.457 0.300
Batch: 200 | Loss: 2.814 | Acc: 49.001,80.053,99.262,% | Adaptive Acc: 92.475% | clf_exit: 0.244 0.457 0.299
Batch: 220 | Loss: 2.809 | Acc: 49.113,80.069,99.272,% | Adaptive Acc: 92.474% | clf_exit: 0.245 0.456 0.299
Batch: 240 | Loss: 2.810 | Acc: 49.024,80.145,99.267,% | Adaptive Acc: 92.460% | clf_exit: 0.246 0.456 0.298
Batch: 260 | Loss: 2.811 | Acc: 49.030,80.125,99.279,% | Adaptive Acc: 92.412% | clf_exit: 0.247 0.456 0.297
Batch: 280 | Loss: 2.813 | Acc: 48.868,79.999,99.274,% | Adaptive Acc: 92.327% | clf_exit: 0.247 0.455 0.298
Batch: 300 | Loss: 2.819 | Acc: 48.863,79.939,99.278,% | Adaptive Acc: 92.359% | clf_exit: 0.246 0.455 0.299
Batch: 320 | Loss: 2.815 | Acc: 48.912,80.033,99.260,% | Adaptive Acc: 92.404% | clf_exit: 0.247 0.454 0.298
Batch: 340 | Loss: 2.815 | Acc: 48.930,80.008,99.253,% | Adaptive Acc: 92.405% | clf_exit: 0.248 0.454 0.298
Batch: 360 | Loss: 2.816 | Acc: 48.892,80.040,99.266,% | Adaptive Acc: 92.387% | clf_exit: 0.248 0.453 0.299
Batch: 380 | Loss: 2.816 | Acc: 48.913,79.958,99.262,% | Adaptive Acc: 92.372% | clf_exit: 0.248 0.453 0.299
Batch: 0 | Loss: 4.187 | Acc: 50.000,69.531,75.781,% | Adaptive Acc: 71.094% | clf_exit: 0.320 0.438 0.242
Batch: 20 | Loss: 4.451 | Acc: 45.499,66.704,72.545,% | Adaptive Acc: 68.341% | clf_exit: 0.321 0.408 0.270
Batch: 40 | Loss: 4.454 | Acc: 45.941,66.502,71.627,% | Adaptive Acc: 67.797% | clf_exit: 0.320 0.401 0.279
Batch: 60 | Loss: 4.463 | Acc: 45.902,66.291,71.644,% | Adaptive Acc: 67.815% | clf_exit: 0.319 0.402 0.279
Train all parameters

Epoch: 163
Batch: 0 | Loss: 2.894 | Acc: 43.750,80.469,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.273 0.359 0.367
Batch: 20 | Loss: 2.705 | Acc: 49.070,81.287,99.554,% | Adaptive Acc: 93.266% | clf_exit: 0.269 0.442 0.288
Batch: 40 | Loss: 2.737 | Acc: 49.257,81.421,99.505,% | Adaptive Acc: 93.026% | clf_exit: 0.258 0.453 0.290
Batch: 60 | Loss: 2.771 | Acc: 48.809,81.084,99.424,% | Adaptive Acc: 92.956% | clf_exit: 0.251 0.460 0.290
Batch: 80 | Loss: 2.775 | Acc: 48.872,80.951,99.306,% | Adaptive Acc: 92.660% | clf_exit: 0.251 0.458 0.291
Batch: 100 | Loss: 2.798 | Acc: 48.291,80.623,99.304,% | Adaptive Acc: 92.489% | clf_exit: 0.250 0.456 0.294
Batch: 120 | Loss: 2.794 | Acc: 48.476,80.624,99.316,% | Adaptive Acc: 92.439% | clf_exit: 0.249 0.455 0.295
Batch: 140 | Loss: 2.796 | Acc: 48.504,80.591,99.324,% | Adaptive Acc: 92.320% | clf_exit: 0.249 0.455 0.295
Batch: 160 | Loss: 2.792 | Acc: 48.729,80.658,99.330,% | Adaptive Acc: 92.450% | clf_exit: 0.248 0.455 0.296
Batch: 180 | Loss: 2.794 | Acc: 48.718,80.598,99.296,% | Adaptive Acc: 92.438% | clf_exit: 0.250 0.453 0.297
Batch: 200 | Loss: 2.791 | Acc: 48.884,80.496,99.277,% | Adaptive Acc: 92.425% | clf_exit: 0.250 0.453 0.297
Batch: 220 | Loss: 2.797 | Acc: 48.742,80.479,99.261,% | Adaptive Acc: 92.336% | clf_exit: 0.250 0.453 0.297
Batch: 240 | Loss: 2.803 | Acc: 48.674,80.342,99.264,% | Adaptive Acc: 92.294% | clf_exit: 0.248 0.454 0.298
Batch: 260 | Loss: 2.804 | Acc: 48.719,80.349,99.255,% | Adaptive Acc: 92.322% | clf_exit: 0.248 0.454 0.298
Batch: 280 | Loss: 2.805 | Acc: 48.691,80.327,99.263,% | Adaptive Acc: 92.315% | clf_exit: 0.248 0.454 0.298
Batch: 300 | Loss: 2.805 | Acc: 48.687,80.352,99.271,% | Adaptive Acc: 92.338% | clf_exit: 0.248 0.453 0.299
Batch: 320 | Loss: 2.809 | Acc: 48.698,80.262,99.250,% | Adaptive Acc: 92.314% | clf_exit: 0.247 0.455 0.298
Batch: 340 | Loss: 2.810 | Acc: 48.719,80.196,99.253,% | Adaptive Acc: 92.348% | clf_exit: 0.247 0.455 0.298
Batch: 360 | Loss: 2.815 | Acc: 48.656,80.157,99.253,% | Adaptive Acc: 92.326% | clf_exit: 0.247 0.455 0.299
Batch: 380 | Loss: 2.819 | Acc: 48.602,80.087,99.254,% | Adaptive Acc: 92.319% | clf_exit: 0.246 0.454 0.300
Batch: 0 | Loss: 4.289 | Acc: 46.875,67.969,73.438,% | Adaptive Acc: 67.969% | clf_exit: 0.352 0.422 0.227
Batch: 20 | Loss: 4.518 | Acc: 44.866,66.853,71.094,% | Adaptive Acc: 67.485% | clf_exit: 0.325 0.410 0.266
Batch: 40 | Loss: 4.502 | Acc: 45.217,66.463,70.865,% | Adaptive Acc: 67.302% | clf_exit: 0.325 0.401 0.275
Batch: 60 | Loss: 4.502 | Acc: 45.172,66.253,71.081,% | Adaptive Acc: 67.123% | clf_exit: 0.325 0.400 0.274
Train all parameters

Epoch: 164
Batch: 0 | Loss: 2.809 | Acc: 45.312,80.469,97.656,% | Adaptive Acc: 91.406% | clf_exit: 0.234 0.477 0.289
Batch: 20 | Loss: 2.770 | Acc: 48.958,80.841,99.516,% | Adaptive Acc: 92.560% | clf_exit: 0.257 0.457 0.285
Batch: 40 | Loss: 2.771 | Acc: 49.333,80.755,99.543,% | Adaptive Acc: 92.569% | clf_exit: 0.248 0.456 0.296
Batch: 60 | Loss: 2.778 | Acc: 49.334,80.725,99.526,% | Adaptive Acc: 92.610% | clf_exit: 0.251 0.453 0.296
Batch: 80 | Loss: 2.783 | Acc: 48.920,80.835,99.421,% | Adaptive Acc: 92.506% | clf_exit: 0.251 0.452 0.297
Batch: 100 | Loss: 2.800 | Acc: 48.670,80.778,99.404,% | Adaptive Acc: 92.319% | clf_exit: 0.250 0.453 0.297
Batch: 120 | Loss: 2.813 | Acc: 48.534,80.682,99.400,% | Adaptive Acc: 92.381% | clf_exit: 0.249 0.450 0.300
Batch: 140 | Loss: 2.808 | Acc: 48.748,80.646,99.402,% | Adaptive Acc: 92.442% | clf_exit: 0.249 0.452 0.299
Batch: 160 | Loss: 2.801 | Acc: 48.937,80.721,99.355,% | Adaptive Acc: 92.401% | clf_exit: 0.250 0.452 0.298
Batch: 180 | Loss: 2.801 | Acc: 49.016,80.723,99.331,% | Adaptive Acc: 92.347% | clf_exit: 0.249 0.453 0.298
Batch: 200 | Loss: 2.799 | Acc: 49.145,80.605,99.293,% | Adaptive Acc: 92.292% | clf_exit: 0.250 0.452 0.298
Batch: 220 | Loss: 2.800 | Acc: 49.123,80.642,99.304,% | Adaptive Acc: 92.343% | clf_exit: 0.249 0.452 0.299
Batch: 240 | Loss: 2.804 | Acc: 49.096,80.589,99.313,% | Adaptive Acc: 92.356% | clf_exit: 0.248 0.454 0.298
Batch: 260 | Loss: 2.815 | Acc: 48.922,80.514,99.318,% | Adaptive Acc: 92.367% | clf_exit: 0.246 0.454 0.299
Batch: 280 | Loss: 2.811 | Acc: 48.974,80.611,99.310,% | Adaptive Acc: 92.379% | clf_exit: 0.247 0.454 0.299
Batch: 300 | Loss: 2.816 | Acc: 48.879,80.518,99.312,% | Adaptive Acc: 92.374% | clf_exit: 0.247 0.453 0.300
Batch: 320 | Loss: 2.813 | Acc: 48.905,80.461,99.304,% | Adaptive Acc: 92.382% | clf_exit: 0.248 0.453 0.300
Batch: 340 | Loss: 2.814 | Acc: 48.861,80.416,99.306,% | Adaptive Acc: 92.373% | clf_exit: 0.248 0.453 0.299
Batch: 360 | Loss: 2.814 | Acc: 48.879,80.387,99.305,% | Adaptive Acc: 92.397% | clf_exit: 0.248 0.452 0.300
Batch: 380 | Loss: 2.816 | Acc: 48.837,80.350,99.295,% | Adaptive Acc: 92.380% | clf_exit: 0.248 0.452 0.300
Batch: 0 | Loss: 4.164 | Acc: 50.781,67.188,72.656,% | Adaptive Acc: 70.312% | clf_exit: 0.320 0.445 0.234
Batch: 20 | Loss: 4.520 | Acc: 45.573,66.220,71.094,% | Adaptive Acc: 66.778% | clf_exit: 0.325 0.398 0.277
Batch: 40 | Loss: 4.497 | Acc: 46.113,66.159,70.922,% | Adaptive Acc: 67.321% | clf_exit: 0.322 0.396 0.282
Batch: 60 | Loss: 4.503 | Acc: 45.812,65.894,71.017,% | Adaptive Acc: 67.392% | clf_exit: 0.321 0.395 0.284
Train all parameters

Epoch: 165
Batch: 0 | Loss: 2.658 | Acc: 47.656,78.125,98.438,% | Adaptive Acc: 88.281% | clf_exit: 0.297 0.453 0.250
Batch: 20 | Loss: 2.785 | Acc: 49.033,81.436,99.330,% | Adaptive Acc: 93.378% | clf_exit: 0.255 0.454 0.291
Batch: 40 | Loss: 2.796 | Acc: 48.438,81.136,99.333,% | Adaptive Acc: 93.007% | clf_exit: 0.253 0.446 0.301
Batch: 60 | Loss: 2.783 | Acc: 48.694,81.224,99.398,% | Adaptive Acc: 92.777% | clf_exit: 0.251 0.451 0.298
Batch: 80 | Loss: 2.772 | Acc: 49.171,81.115,99.334,% | Adaptive Acc: 92.670% | clf_exit: 0.253 0.450 0.297
Batch: 100 | Loss: 2.787 | Acc: 48.878,81.126,99.373,% | Adaptive Acc: 92.713% | clf_exit: 0.247 0.455 0.297
Batch: 120 | Loss: 2.778 | Acc: 48.948,81.231,99.374,% | Adaptive Acc: 92.588% | clf_exit: 0.250 0.456 0.294
Batch: 140 | Loss: 2.778 | Acc: 49.019,80.990,99.363,% | Adaptive Acc: 92.598% | clf_exit: 0.250 0.456 0.294
Batch: 160 | Loss: 2.793 | Acc: 48.734,81.007,99.369,% | Adaptive Acc: 92.542% | clf_exit: 0.246 0.457 0.296
Batch: 180 | Loss: 2.786 | Acc: 48.766,80.995,99.387,% | Adaptive Acc: 92.580% | clf_exit: 0.247 0.457 0.295
Batch: 200 | Loss: 2.780 | Acc: 48.916,81.036,99.366,% | Adaptive Acc: 92.475% | clf_exit: 0.251 0.456 0.293
Batch: 220 | Loss: 2.784 | Acc: 48.971,81.031,99.342,% | Adaptive Acc: 92.421% | clf_exit: 0.251 0.455 0.294
Batch: 240 | Loss: 2.783 | Acc: 49.092,81.007,99.345,% | Adaptive Acc: 92.470% | clf_exit: 0.253 0.454 0.293
Batch: 260 | Loss: 2.786 | Acc: 48.997,80.849,99.330,% | Adaptive Acc: 92.340% | clf_exit: 0.253 0.454 0.293
Batch: 280 | Loss: 2.787 | Acc: 49.088,80.797,99.324,% | Adaptive Acc: 92.254% | clf_exit: 0.253 0.455 0.293
Batch: 300 | Loss: 2.789 | Acc: 49.162,80.736,99.317,% | Adaptive Acc: 92.284% | clf_exit: 0.252 0.455 0.293
Batch: 320 | Loss: 2.786 | Acc: 49.199,80.761,99.309,% | Adaptive Acc: 92.319% | clf_exit: 0.252 0.455 0.293
Batch: 340 | Loss: 2.793 | Acc: 49.157,80.620,99.290,% | Adaptive Acc: 92.300% | clf_exit: 0.252 0.454 0.294
Batch: 360 | Loss: 2.796 | Acc: 49.175,80.519,99.282,% | Adaptive Acc: 92.281% | clf_exit: 0.251 0.455 0.294
Batch: 380 | Loss: 2.801 | Acc: 49.049,80.385,99.270,% | Adaptive Acc: 92.274% | clf_exit: 0.250 0.454 0.295
Batch: 0 | Loss: 4.153 | Acc: 48.438,68.750,73.438,% | Adaptive Acc: 69.531% | clf_exit: 0.297 0.438 0.266
Batch: 20 | Loss: 4.529 | Acc: 45.908,66.815,71.838,% | Adaptive Acc: 67.671% | clf_exit: 0.305 0.419 0.276
Batch: 40 | Loss: 4.504 | Acc: 46.132,66.749,71.284,% | Adaptive Acc: 67.740% | clf_exit: 0.301 0.417 0.282
Batch: 60 | Loss: 4.512 | Acc: 45.838,66.675,71.376,% | Adaptive Acc: 67.853% | clf_exit: 0.301 0.413 0.286
Train all parameters

Epoch: 166
Batch: 0 | Loss: 2.937 | Acc: 51.562,78.125,99.219,% | Adaptive Acc: 93.750% | clf_exit: 0.242 0.430 0.328
Batch: 20 | Loss: 2.778 | Acc: 50.037,81.213,99.256,% | Adaptive Acc: 92.746% | clf_exit: 0.252 0.454 0.294
Batch: 40 | Loss: 2.780 | Acc: 49.676,81.098,99.257,% | Adaptive Acc: 92.683% | clf_exit: 0.252 0.457 0.290
Batch: 60 | Loss: 2.790 | Acc: 49.411,80.930,99.270,% | Adaptive Acc: 92.815% | clf_exit: 0.248 0.459 0.293
Batch: 80 | Loss: 2.772 | Acc: 49.450,81.076,99.315,% | Adaptive Acc: 92.737% | clf_exit: 0.250 0.460 0.290
Batch: 100 | Loss: 2.772 | Acc: 49.343,81.149,99.350,% | Adaptive Acc: 92.737% | clf_exit: 0.249 0.459 0.291
Batch: 120 | Loss: 2.766 | Acc: 49.412,81.192,99.341,% | Adaptive Acc: 92.717% | clf_exit: 0.249 0.461 0.290
Batch: 140 | Loss: 2.772 | Acc: 49.346,81.189,99.368,% | Adaptive Acc: 92.747% | clf_exit: 0.250 0.458 0.292
Batch: 160 | Loss: 2.782 | Acc: 49.141,80.969,99.393,% | Adaptive Acc: 92.663% | clf_exit: 0.250 0.457 0.294
Batch: 180 | Loss: 2.789 | Acc: 48.921,80.913,99.370,% | Adaptive Acc: 92.585% | clf_exit: 0.249 0.456 0.294
Batch: 200 | Loss: 2.798 | Acc: 48.830,80.706,99.378,% | Adaptive Acc: 92.592% | clf_exit: 0.248 0.456 0.297
Batch: 220 | Loss: 2.799 | Acc: 48.756,80.674,99.364,% | Adaptive Acc: 92.562% | clf_exit: 0.247 0.457 0.296
Batch: 240 | Loss: 2.798 | Acc: 48.768,80.679,99.361,% | Adaptive Acc: 92.521% | clf_exit: 0.247 0.457 0.296
Batch: 260 | Loss: 2.799 | Acc: 48.683,80.669,99.359,% | Adaptive Acc: 92.454% | clf_exit: 0.247 0.457 0.296
Batch: 280 | Loss: 2.798 | Acc: 48.763,80.638,99.369,% | Adaptive Acc: 92.482% | clf_exit: 0.247 0.458 0.296
Batch: 300 | Loss: 2.797 | Acc: 48.770,80.560,99.374,% | Adaptive Acc: 92.457% | clf_exit: 0.247 0.457 0.296
Batch: 320 | Loss: 2.797 | Acc: 48.866,80.486,99.379,% | Adaptive Acc: 92.484% | clf_exit: 0.248 0.456 0.296
Batch: 340 | Loss: 2.795 | Acc: 48.999,80.494,99.379,% | Adaptive Acc: 92.478% | clf_exit: 0.248 0.456 0.296
Batch: 360 | Loss: 2.793 | Acc: 49.002,80.469,99.368,% | Adaptive Acc: 92.465% | clf_exit: 0.249 0.455 0.296
Batch: 380 | Loss: 2.798 | Acc: 48.942,80.428,99.360,% | Adaptive Acc: 92.468% | clf_exit: 0.248 0.454 0.297
Batch: 0 | Loss: 4.232 | Acc: 45.312,68.750,75.000,% | Adaptive Acc: 71.094% | clf_exit: 0.312 0.461 0.227
Batch: 20 | Loss: 4.468 | Acc: 45.387,66.741,71.838,% | Adaptive Acc: 69.085% | clf_exit: 0.310 0.412 0.278
Batch: 40 | Loss: 4.477 | Acc: 45.579,66.082,71.265,% | Adaptive Acc: 68.236% | clf_exit: 0.304 0.409 0.287
Batch: 60 | Loss: 4.503 | Acc: 45.453,66.035,71.427,% | Adaptive Acc: 67.918% | clf_exit: 0.305 0.406 0.289
Train all parameters

Epoch: 167
Batch: 0 | Loss: 3.018 | Acc: 41.406,75.781,99.219,% | Adaptive Acc: 86.719% | clf_exit: 0.219 0.523 0.258
Batch: 20 | Loss: 2.789 | Acc: 48.624,81.213,99.368,% | Adaptive Acc: 91.741% | clf_exit: 0.256 0.467 0.278
Batch: 40 | Loss: 2.750 | Acc: 49.428,81.574,99.428,% | Adaptive Acc: 92.245% | clf_exit: 0.256 0.464 0.280
Batch: 60 | Loss: 2.735 | Acc: 50.013,81.532,99.436,% | Adaptive Acc: 92.700% | clf_exit: 0.257 0.462 0.281
Batch: 80 | Loss: 2.762 | Acc: 49.373,81.356,99.354,% | Adaptive Acc: 92.641% | clf_exit: 0.254 0.459 0.287
Batch: 100 | Loss: 2.766 | Acc: 49.288,81.443,99.343,% | Adaptive Acc: 92.644% | clf_exit: 0.254 0.458 0.288
Batch: 120 | Loss: 2.777 | Acc: 49.212,81.185,99.380,% | Adaptive Acc: 92.646% | clf_exit: 0.252 0.457 0.291
Batch: 140 | Loss: 2.780 | Acc: 49.064,81.028,99.379,% | Adaptive Acc: 92.398% | clf_exit: 0.254 0.455 0.291
Batch: 160 | Loss: 2.778 | Acc: 49.088,80.983,99.369,% | Adaptive Acc: 92.508% | clf_exit: 0.253 0.456 0.291
Batch: 180 | Loss: 2.777 | Acc: 49.111,81.060,99.370,% | Adaptive Acc: 92.520% | clf_exit: 0.253 0.456 0.291
Batch: 200 | Loss: 2.779 | Acc: 49.094,81.017,99.378,% | Adaptive Acc: 92.533% | clf_exit: 0.252 0.455 0.293
Batch: 220 | Loss: 2.784 | Acc: 49.017,80.981,99.381,% | Adaptive Acc: 92.513% | clf_exit: 0.253 0.454 0.293
Batch: 240 | Loss: 2.786 | Acc: 48.979,80.923,99.381,% | Adaptive Acc: 92.434% | clf_exit: 0.253 0.454 0.293
Batch: 260 | Loss: 2.786 | Acc: 49.039,80.813,99.365,% | Adaptive Acc: 92.406% | clf_exit: 0.253 0.453 0.294
Batch: 280 | Loss: 2.789 | Acc: 48.893,80.833,99.358,% | Adaptive Acc: 92.402% | clf_exit: 0.253 0.453 0.294
Batch: 300 | Loss: 2.792 | Acc: 48.848,80.770,99.338,% | Adaptive Acc: 92.333% | clf_exit: 0.253 0.453 0.294
Batch: 320 | Loss: 2.791 | Acc: 48.958,80.668,99.328,% | Adaptive Acc: 92.319% | clf_exit: 0.252 0.454 0.294
Batch: 340 | Loss: 2.791 | Acc: 49.006,80.634,99.331,% | Adaptive Acc: 92.339% | clf_exit: 0.252 0.453 0.295
Batch: 360 | Loss: 2.792 | Acc: 48.966,80.579,99.320,% | Adaptive Acc: 92.322% | clf_exit: 0.252 0.454 0.294
Batch: 380 | Loss: 2.796 | Acc: 48.897,80.565,99.305,% | Adaptive Acc: 92.368% | clf_exit: 0.251 0.453 0.296
Batch: 0 | Loss: 4.323 | Acc: 45.312,67.969,71.094,% | Adaptive Acc: 66.406% | clf_exit: 0.320 0.453 0.227
Batch: 20 | Loss: 4.501 | Acc: 45.350,66.927,71.429,% | Adaptive Acc: 68.043% | clf_exit: 0.331 0.408 0.260
Batch: 40 | Loss: 4.497 | Acc: 45.446,66.883,70.941,% | Adaptive Acc: 67.550% | clf_exit: 0.332 0.400 0.268
Batch: 60 | Loss: 4.519 | Acc: 45.223,66.265,70.786,% | Adaptive Acc: 67.162% | clf_exit: 0.329 0.403 0.268
Train all parameters

Epoch: 168
Batch: 0 | Loss: 2.877 | Acc: 44.531,85.938,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.258 0.391 0.352
Batch: 20 | Loss: 2.759 | Acc: 48.921,81.622,99.144,% | Adaptive Acc: 93.118% | clf_exit: 0.250 0.458 0.292
Batch: 40 | Loss: 2.742 | Acc: 49.047,81.383,99.276,% | Adaptive Acc: 93.293% | clf_exit: 0.251 0.455 0.293
Batch: 60 | Loss: 2.761 | Acc: 48.950,81.148,99.334,% | Adaptive Acc: 93.161% | clf_exit: 0.250 0.456 0.294
Batch: 80 | Loss: 2.774 | Acc: 48.958,80.990,99.402,% | Adaptive Acc: 93.113% | clf_exit: 0.252 0.452 0.296
Batch: 100 | Loss: 2.772 | Acc: 48.987,81.165,99.451,% | Adaptive Acc: 93.015% | clf_exit: 0.253 0.454 0.294
Batch: 120 | Loss: 2.767 | Acc: 49.019,81.082,99.419,% | Adaptive Acc: 92.962% | clf_exit: 0.251 0.455 0.293
Batch: 140 | Loss: 2.773 | Acc: 48.964,81.045,99.402,% | Adaptive Acc: 93.035% | clf_exit: 0.249 0.456 0.295
Batch: 160 | Loss: 2.780 | Acc: 48.855,80.905,99.389,% | Adaptive Acc: 92.974% | clf_exit: 0.249 0.456 0.295
Batch: 180 | Loss: 2.777 | Acc: 48.955,81.000,99.404,% | Adaptive Acc: 92.964% | clf_exit: 0.248 0.458 0.294
Batch: 200 | Loss: 2.777 | Acc: 49.017,80.993,99.382,% | Adaptive Acc: 93.046% | clf_exit: 0.247 0.459 0.295
Batch: 220 | Loss: 2.786 | Acc: 48.908,80.889,99.357,% | Adaptive Acc: 92.940% | clf_exit: 0.247 0.458 0.295
Batch: 240 | Loss: 2.786 | Acc: 48.911,80.897,99.339,% | Adaptive Acc: 92.878% | clf_exit: 0.247 0.458 0.295
Batch: 260 | Loss: 2.786 | Acc: 48.872,80.792,99.338,% | Adaptive Acc: 92.822% | clf_exit: 0.248 0.457 0.295
Batch: 280 | Loss: 2.786 | Acc: 48.821,80.825,99.333,% | Adaptive Acc: 92.771% | clf_exit: 0.248 0.458 0.295
Batch: 300 | Loss: 2.785 | Acc: 48.850,80.793,99.330,% | Adaptive Acc: 92.730% | clf_exit: 0.248 0.458 0.294
Batch: 320 | Loss: 2.782 | Acc: 48.919,80.851,99.343,% | Adaptive Acc: 92.713% | clf_exit: 0.250 0.456 0.294
Batch: 340 | Loss: 2.784 | Acc: 48.948,80.778,99.349,% | Adaptive Acc: 92.662% | clf_exit: 0.250 0.456 0.294
Batch: 360 | Loss: 2.788 | Acc: 48.903,80.726,99.346,% | Adaptive Acc: 92.612% | clf_exit: 0.250 0.456 0.294
Batch: 380 | Loss: 2.789 | Acc: 48.913,80.637,99.352,% | Adaptive Acc: 92.561% | clf_exit: 0.250 0.455 0.295
Batch: 0 | Loss: 4.027 | Acc: 46.094,66.406,78.125,% | Adaptive Acc: 70.312% | clf_exit: 0.359 0.398 0.242
Batch: 20 | Loss: 4.520 | Acc: 45.499,66.443,71.838,% | Adaptive Acc: 67.746% | clf_exit: 0.331 0.401 0.267
Batch: 40 | Loss: 4.486 | Acc: 45.293,66.463,71.494,% | Adaptive Acc: 67.683% | clf_exit: 0.325 0.393 0.282
Batch: 60 | Loss: 4.507 | Acc: 45.377,66.253,71.247,% | Adaptive Acc: 67.456% | clf_exit: 0.323 0.395 0.282
Train all parameters

Epoch: 169
Batch: 0 | Loss: 2.773 | Acc: 47.656,82.031,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.281 0.414 0.305
Batch: 20 | Loss: 2.807 | Acc: 48.549,81.287,99.479,% | Adaptive Acc: 93.155% | clf_exit: 0.255 0.439 0.306
Batch: 40 | Loss: 2.775 | Acc: 48.418,81.479,99.466,% | Adaptive Acc: 93.293% | clf_exit: 0.249 0.444 0.307
Batch: 60 | Loss: 2.759 | Acc: 48.642,81.429,99.436,% | Adaptive Acc: 92.661% | clf_exit: 0.253 0.446 0.301
Batch: 80 | Loss: 2.771 | Acc: 48.881,81.395,99.402,% | Adaptive Acc: 92.650% | clf_exit: 0.252 0.449 0.299
Batch: 100 | Loss: 2.771 | Acc: 48.909,81.211,99.366,% | Adaptive Acc: 92.559% | clf_exit: 0.253 0.452 0.295
Batch: 120 | Loss: 2.764 | Acc: 49.148,81.457,99.380,% | Adaptive Acc: 92.646% | clf_exit: 0.253 0.454 0.293
Batch: 140 | Loss: 2.762 | Acc: 49.174,81.394,99.407,% | Adaptive Acc: 92.614% | clf_exit: 0.253 0.455 0.292
Batch: 160 | Loss: 2.755 | Acc: 49.398,81.376,99.437,% | Adaptive Acc: 92.644% | clf_exit: 0.253 0.455 0.291
Batch: 180 | Loss: 2.769 | Acc: 49.119,81.211,99.422,% | Adaptive Acc: 92.528% | clf_exit: 0.253 0.453 0.294
Batch: 200 | Loss: 2.774 | Acc: 48.912,81.067,99.413,% | Adaptive Acc: 92.526% | clf_exit: 0.252 0.455 0.293
Batch: 220 | Loss: 2.778 | Acc: 48.901,80.942,99.392,% | Adaptive Acc: 92.403% | clf_exit: 0.252 0.456 0.293
Batch: 240 | Loss: 2.783 | Acc: 48.875,80.842,99.378,% | Adaptive Acc: 92.356% | clf_exit: 0.252 0.454 0.294
Batch: 260 | Loss: 2.783 | Acc: 49.042,80.807,99.338,% | Adaptive Acc: 92.364% | clf_exit: 0.251 0.454 0.295
Batch: 280 | Loss: 2.778 | Acc: 49.096,80.891,99.324,% | Adaptive Acc: 92.360% | clf_exit: 0.252 0.455 0.294
Batch: 300 | Loss: 2.779 | Acc: 49.118,80.887,99.317,% | Adaptive Acc: 92.338% | clf_exit: 0.251 0.455 0.294
Batch: 320 | Loss: 2.780 | Acc: 49.126,80.863,99.314,% | Adaptive Acc: 92.346% | clf_exit: 0.251 0.456 0.294
Batch: 340 | Loss: 2.782 | Acc: 49.189,80.872,99.297,% | Adaptive Acc: 92.380% | clf_exit: 0.251 0.456 0.294
Batch: 360 | Loss: 2.785 | Acc: 49.195,80.813,99.301,% | Adaptive Acc: 92.391% | clf_exit: 0.250 0.456 0.294
Batch: 380 | Loss: 2.790 | Acc: 49.059,80.709,99.282,% | Adaptive Acc: 92.372% | clf_exit: 0.250 0.456 0.295
Batch: 0 | Loss: 4.029 | Acc: 46.094,66.406,77.344,% | Adaptive Acc: 69.531% | clf_exit: 0.297 0.438 0.266
Batch: 20 | Loss: 4.496 | Acc: 46.429,65.960,71.354,% | Adaptive Acc: 67.597% | clf_exit: 0.315 0.417 0.268
Batch: 40 | Loss: 4.484 | Acc: 46.113,66.292,71.399,% | Adaptive Acc: 67.740% | clf_exit: 0.311 0.411 0.277
Batch: 60 | Loss: 4.488 | Acc: 45.940,65.868,71.363,% | Adaptive Acc: 67.636% | clf_exit: 0.310 0.411 0.278
Train all parameters

Epoch: 170
Batch: 0 | Loss: 2.649 | Acc: 53.125,82.812,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.297 0.477 0.227
Batch: 20 | Loss: 2.713 | Acc: 50.484,82.180,99.442,% | Adaptive Acc: 92.188% | clf_exit: 0.255 0.465 0.279
Batch: 40 | Loss: 2.730 | Acc: 49.867,81.745,99.314,% | Adaptive Acc: 92.188% | clf_exit: 0.258 0.458 0.284
Batch: 60 | Loss: 2.748 | Acc: 49.936,81.519,99.308,% | Adaptive Acc: 92.226% | clf_exit: 0.255 0.461 0.284
Batch: 80 | Loss: 2.755 | Acc: 49.701,81.279,99.315,% | Adaptive Acc: 92.284% | clf_exit: 0.253 0.459 0.288
Batch: 100 | Loss: 2.754 | Acc: 49.930,81.173,99.312,% | Adaptive Acc: 92.365% | clf_exit: 0.251 0.461 0.287
Batch: 120 | Loss: 2.762 | Acc: 49.671,81.140,99.296,% | Adaptive Acc: 92.304% | clf_exit: 0.250 0.461 0.288
Batch: 140 | Loss: 2.772 | Acc: 49.512,81.084,99.285,% | Adaptive Acc: 92.304% | clf_exit: 0.250 0.459 0.291
Batch: 160 | Loss: 2.769 | Acc: 49.558,81.066,99.296,% | Adaptive Acc: 92.362% | clf_exit: 0.250 0.459 0.291
Batch: 180 | Loss: 2.771 | Acc: 49.629,81.000,99.288,% | Adaptive Acc: 92.369% | clf_exit: 0.250 0.459 0.292
Batch: 200 | Loss: 2.778 | Acc: 49.479,80.916,99.281,% | Adaptive Acc: 92.343% | clf_exit: 0.250 0.457 0.293
Batch: 220 | Loss: 2.776 | Acc: 49.586,80.939,99.297,% | Adaptive Acc: 92.325% | clf_exit: 0.252 0.455 0.292
Batch: 240 | Loss: 2.778 | Acc: 49.507,80.923,99.264,% | Adaptive Acc: 92.317% | clf_exit: 0.252 0.455 0.294
Batch: 260 | Loss: 2.781 | Acc: 49.383,80.852,99.252,% | Adaptive Acc: 92.418% | clf_exit: 0.251 0.454 0.295
Batch: 280 | Loss: 2.778 | Acc: 49.408,80.802,99.269,% | Adaptive Acc: 92.388% | clf_exit: 0.251 0.454 0.295
Batch: 300 | Loss: 2.781 | Acc: 49.354,80.762,99.232,% | Adaptive Acc: 92.364% | clf_exit: 0.249 0.456 0.295
Batch: 320 | Loss: 2.781 | Acc: 49.404,80.685,99.219,% | Adaptive Acc: 92.363% | clf_exit: 0.249 0.456 0.295
Batch: 340 | Loss: 2.779 | Acc: 49.420,80.741,99.216,% | Adaptive Acc: 92.385% | clf_exit: 0.250 0.455 0.295
Batch: 360 | Loss: 2.785 | Acc: 49.372,80.648,99.199,% | Adaptive Acc: 92.309% | clf_exit: 0.250 0.454 0.296
Batch: 380 | Loss: 2.789 | Acc: 49.317,80.625,99.198,% | Adaptive Acc: 92.296% | clf_exit: 0.250 0.454 0.296
Batch: 0 | Loss: 3.941 | Acc: 50.000,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.312 0.477 0.211
Batch: 20 | Loss: 4.459 | Acc: 46.094,67.225,72.359,% | Adaptive Acc: 68.341% | clf_exit: 0.309 0.420 0.271
Batch: 40 | Loss: 4.436 | Acc: 46.094,66.921,71.932,% | Adaptive Acc: 68.216% | clf_exit: 0.312 0.409 0.280
Batch: 60 | Loss: 4.472 | Acc: 45.927,66.445,71.465,% | Adaptive Acc: 67.636% | clf_exit: 0.313 0.404 0.283
Train all parameters

Epoch: 171
Batch: 0 | Loss: 3.321 | Acc: 46.875,78.125,99.219,% | Adaptive Acc: 89.844% | clf_exit: 0.203 0.398 0.398
Batch: 20 | Loss: 2.761 | Acc: 47.619,81.064,99.182,% | Adaptive Acc: 92.225% | clf_exit: 0.244 0.462 0.294
Batch: 40 | Loss: 2.794 | Acc: 47.828,81.040,99.257,% | Adaptive Acc: 92.416% | clf_exit: 0.240 0.463 0.297
Batch: 60 | Loss: 2.780 | Acc: 48.297,80.917,99.296,% | Adaptive Acc: 92.392% | clf_exit: 0.241 0.465 0.294
Batch: 80 | Loss: 2.767 | Acc: 48.592,80.980,99.334,% | Adaptive Acc: 92.573% | clf_exit: 0.242 0.466 0.292
Batch: 100 | Loss: 2.767 | Acc: 48.832,81.041,99.304,% | Adaptive Acc: 92.652% | clf_exit: 0.244 0.463 0.293
Batch: 120 | Loss: 2.767 | Acc: 48.902,81.179,99.361,% | Adaptive Acc: 92.627% | clf_exit: 0.246 0.462 0.293
Batch: 140 | Loss: 2.774 | Acc: 48.798,81.001,99.363,% | Adaptive Acc: 92.509% | clf_exit: 0.248 0.460 0.292
Batch: 160 | Loss: 2.770 | Acc: 48.908,80.998,99.321,% | Adaptive Acc: 92.396% | clf_exit: 0.249 0.461 0.290
Batch: 180 | Loss: 2.781 | Acc: 48.791,80.961,99.335,% | Adaptive Acc: 92.446% | clf_exit: 0.248 0.461 0.291
Batch: 200 | Loss: 2.775 | Acc: 49.040,81.025,99.324,% | Adaptive Acc: 92.460% | clf_exit: 0.249 0.460 0.291
Batch: 220 | Loss: 2.777 | Acc: 49.014,80.953,99.325,% | Adaptive Acc: 92.410% | clf_exit: 0.249 0.461 0.291
Batch: 240 | Loss: 2.773 | Acc: 49.060,81.075,99.352,% | Adaptive Acc: 92.466% | clf_exit: 0.249 0.461 0.290
Batch: 260 | Loss: 2.771 | Acc: 49.072,81.070,99.347,% | Adaptive Acc: 92.445% | clf_exit: 0.249 0.461 0.289
Batch: 280 | Loss: 2.771 | Acc: 49.074,81.033,99.347,% | Adaptive Acc: 92.418% | clf_exit: 0.249 0.461 0.289
Batch: 300 | Loss: 2.773 | Acc: 49.081,80.970,99.320,% | Adaptive Acc: 92.359% | clf_exit: 0.250 0.461 0.289
Batch: 320 | Loss: 2.776 | Acc: 49.000,80.924,99.331,% | Adaptive Acc: 92.368% | clf_exit: 0.249 0.461 0.290
Batch: 340 | Loss: 2.780 | Acc: 48.964,80.812,99.308,% | Adaptive Acc: 92.336% | clf_exit: 0.249 0.460 0.291
Batch: 360 | Loss: 2.784 | Acc: 48.944,80.754,99.307,% | Adaptive Acc: 92.283% | clf_exit: 0.250 0.459 0.291
Batch: 380 | Loss: 2.782 | Acc: 48.983,80.778,99.311,% | Adaptive Acc: 92.306% | clf_exit: 0.250 0.459 0.291
Batch: 0 | Loss: 4.052 | Acc: 47.656,68.750,75.781,% | Adaptive Acc: 69.531% | clf_exit: 0.320 0.430 0.250
Batch: 20 | Loss: 4.446 | Acc: 45.982,67.150,71.987,% | Adaptive Acc: 68.192% | clf_exit: 0.324 0.396 0.280
Batch: 40 | Loss: 4.442 | Acc: 45.751,67.054,71.475,% | Adaptive Acc: 67.854% | clf_exit: 0.326 0.388 0.287
Batch: 60 | Loss: 4.450 | Acc: 45.902,66.701,71.580,% | Adaptive Acc: 67.713% | clf_exit: 0.324 0.391 0.285
Train all parameters

Epoch: 172
Batch: 0 | Loss: 2.876 | Acc: 48.438,78.125,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.242 0.484 0.273
Batch: 20 | Loss: 2.708 | Acc: 49.851,81.845,99.628,% | Adaptive Acc: 92.969% | clf_exit: 0.250 0.471 0.279
Batch: 40 | Loss: 2.702 | Acc: 49.809,81.917,99.371,% | Adaptive Acc: 92.931% | clf_exit: 0.259 0.461 0.281
Batch: 60 | Loss: 2.708 | Acc: 49.718,81.865,99.385,% | Adaptive Acc: 93.007% | clf_exit: 0.255 0.459 0.286
Batch: 80 | Loss: 2.724 | Acc: 49.797,81.626,99.363,% | Adaptive Acc: 92.814% | clf_exit: 0.257 0.456 0.287
Batch: 100 | Loss: 2.731 | Acc: 49.621,81.219,99.373,% | Adaptive Acc: 92.582% | clf_exit: 0.258 0.455 0.287
Batch: 120 | Loss: 2.740 | Acc: 49.348,81.224,99.374,% | Adaptive Acc: 92.407% | clf_exit: 0.257 0.456 0.286
Batch: 140 | Loss: 2.750 | Acc: 49.208,81.172,99.391,% | Adaptive Acc: 92.376% | clf_exit: 0.258 0.454 0.288
Batch: 160 | Loss: 2.752 | Acc: 49.083,81.104,99.369,% | Adaptive Acc: 92.285% | clf_exit: 0.258 0.454 0.288
Batch: 180 | Loss: 2.757 | Acc: 49.219,81.103,99.370,% | Adaptive Acc: 92.352% | clf_exit: 0.257 0.455 0.288
Batch: 200 | Loss: 2.756 | Acc: 49.316,81.130,99.366,% | Adaptive Acc: 92.483% | clf_exit: 0.256 0.455 0.289
Batch: 220 | Loss: 2.757 | Acc: 49.300,81.126,99.346,% | Adaptive Acc: 92.499% | clf_exit: 0.257 0.454 0.290
Batch: 240 | Loss: 2.759 | Acc: 49.280,81.075,99.368,% | Adaptive Acc: 92.482% | clf_exit: 0.255 0.455 0.290
Batch: 260 | Loss: 2.762 | Acc: 49.428,80.969,99.359,% | Adaptive Acc: 92.478% | clf_exit: 0.254 0.456 0.290
Batch: 280 | Loss: 2.763 | Acc: 49.361,80.927,99.366,% | Adaptive Acc: 92.454% | clf_exit: 0.253 0.457 0.290
Batch: 300 | Loss: 2.766 | Acc: 49.304,80.845,99.351,% | Adaptive Acc: 92.437% | clf_exit: 0.253 0.456 0.291
Batch: 320 | Loss: 2.767 | Acc: 49.243,80.802,99.350,% | Adaptive Acc: 92.441% | clf_exit: 0.252 0.456 0.292
Batch: 340 | Loss: 2.771 | Acc: 49.212,80.780,99.356,% | Adaptive Acc: 92.451% | clf_exit: 0.252 0.455 0.292
Batch: 360 | Loss: 2.773 | Acc: 49.128,80.759,99.351,% | Adaptive Acc: 92.430% | clf_exit: 0.252 0.455 0.293
Batch: 380 | Loss: 2.777 | Acc: 49.131,80.746,99.340,% | Adaptive Acc: 92.462% | clf_exit: 0.251 0.455 0.293
Batch: 0 | Loss: 4.207 | Acc: 49.219,67.188,73.438,% | Adaptive Acc: 70.312% | clf_exit: 0.328 0.422 0.250
Batch: 20 | Loss: 4.505 | Acc: 45.350,66.443,71.726,% | Adaptive Acc: 68.043% | clf_exit: 0.319 0.418 0.263
Batch: 40 | Loss: 4.489 | Acc: 45.427,66.330,71.399,% | Adaptive Acc: 67.969% | clf_exit: 0.315 0.411 0.274
Batch: 60 | Loss: 4.493 | Acc: 45.402,66.124,71.657,% | Adaptive Acc: 67.994% | clf_exit: 0.313 0.408 0.279
Train all parameters

Epoch: 173
Batch: 0 | Loss: 2.461 | Acc: 58.594,84.375,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.297 0.484 0.219
Batch: 20 | Loss: 2.724 | Acc: 49.591,82.812,99.516,% | Adaptive Acc: 93.266% | clf_exit: 0.254 0.463 0.283
Batch: 40 | Loss: 2.749 | Acc: 49.409,82.298,99.447,% | Adaptive Acc: 92.797% | clf_exit: 0.255 0.456 0.289
Batch: 60 | Loss: 2.757 | Acc: 49.411,81.711,99.436,% | Adaptive Acc: 92.482% | clf_exit: 0.254 0.456 0.290
Batch: 80 | Loss: 2.751 | Acc: 49.286,81.713,99.402,% | Adaptive Acc: 92.515% | clf_exit: 0.252 0.459 0.289
Batch: 100 | Loss: 2.744 | Acc: 49.397,81.745,99.389,% | Adaptive Acc: 92.443% | clf_exit: 0.251 0.460 0.290
Batch: 120 | Loss: 2.743 | Acc: 49.580,81.663,99.393,% | Adaptive Acc: 92.452% | clf_exit: 0.251 0.461 0.289
Batch: 140 | Loss: 2.743 | Acc: 49.579,81.660,99.407,% | Adaptive Acc: 92.415% | clf_exit: 0.252 0.462 0.286
Batch: 160 | Loss: 2.756 | Acc: 49.301,81.468,99.384,% | Adaptive Acc: 92.416% | clf_exit: 0.250 0.462 0.289
Batch: 180 | Loss: 2.767 | Acc: 49.331,81.397,99.353,% | Adaptive Acc: 92.446% | clf_exit: 0.249 0.461 0.290
Batch: 200 | Loss: 2.771 | Acc: 49.386,81.336,99.331,% | Adaptive Acc: 92.467% | clf_exit: 0.248 0.461 0.291
Batch: 220 | Loss: 2.775 | Acc: 49.314,81.190,99.314,% | Adaptive Acc: 92.424% | clf_exit: 0.249 0.461 0.291
Batch: 240 | Loss: 2.776 | Acc: 49.306,81.286,99.303,% | Adaptive Acc: 92.486% | clf_exit: 0.248 0.460 0.291
Batch: 260 | Loss: 2.774 | Acc: 49.315,81.286,99.306,% | Adaptive Acc: 92.514% | clf_exit: 0.249 0.460 0.291
Batch: 280 | Loss: 2.778 | Acc: 49.333,81.211,99.302,% | Adaptive Acc: 92.421% | clf_exit: 0.250 0.460 0.291
Batch: 300 | Loss: 2.781 | Acc: 49.364,81.146,99.294,% | Adaptive Acc: 92.413% | clf_exit: 0.249 0.459 0.291
Batch: 320 | Loss: 2.779 | Acc: 49.411,81.140,99.289,% | Adaptive Acc: 92.419% | clf_exit: 0.250 0.459 0.292
Batch: 340 | Loss: 2.777 | Acc: 49.407,81.138,99.292,% | Adaptive Acc: 92.410% | clf_exit: 0.249 0.460 0.291
Batch: 360 | Loss: 2.784 | Acc: 49.238,81.016,99.288,% | Adaptive Acc: 92.417% | clf_exit: 0.249 0.458 0.293
Batch: 380 | Loss: 2.780 | Acc: 49.282,81.080,99.286,% | Adaptive Acc: 92.444% | clf_exit: 0.249 0.459 0.292
Batch: 0 | Loss: 4.200 | Acc: 46.875,67.188,71.875,% | Adaptive Acc: 68.750% | clf_exit: 0.328 0.430 0.242
Batch: 20 | Loss: 4.437 | Acc: 46.577,66.890,72.210,% | Adaptive Acc: 68.527% | clf_exit: 0.313 0.417 0.270
Batch: 40 | Loss: 4.430 | Acc: 46.532,67.092,71.551,% | Adaptive Acc: 68.388% | clf_exit: 0.313 0.412 0.276
Batch: 60 | Loss: 4.453 | Acc: 46.171,66.560,71.516,% | Adaptive Acc: 67.918% | clf_exit: 0.316 0.405 0.279
Train all parameters

Epoch: 174
Batch: 0 | Loss: 2.983 | Acc: 46.094,75.781,98.438,% | Adaptive Acc: 90.625% | clf_exit: 0.250 0.398 0.352
Batch: 20 | Loss: 2.778 | Acc: 47.656,82.589,99.219,% | Adaptive Acc: 92.820% | clf_exit: 0.250 0.462 0.288
Batch: 40 | Loss: 2.744 | Acc: 48.819,82.774,99.314,% | Adaptive Acc: 93.045% | clf_exit: 0.254 0.456 0.290
Batch: 60 | Loss: 2.733 | Acc: 48.950,82.505,99.321,% | Adaptive Acc: 92.841% | clf_exit: 0.256 0.459 0.285
Batch: 80 | Loss: 2.729 | Acc: 48.949,82.108,99.334,% | Adaptive Acc: 92.872% | clf_exit: 0.257 0.455 0.288
Batch: 100 | Loss: 2.720 | Acc: 49.226,82.147,99.319,% | Adaptive Acc: 92.806% | clf_exit: 0.257 0.456 0.286
Batch: 120 | Loss: 2.729 | Acc: 49.329,81.993,99.322,% | Adaptive Acc: 92.859% | clf_exit: 0.257 0.454 0.289
Batch: 140 | Loss: 2.733 | Acc: 49.324,81.876,99.313,% | Adaptive Acc: 92.747% | clf_exit: 0.255 0.457 0.287
Batch: 160 | Loss: 2.743 | Acc: 49.267,81.711,99.296,% | Adaptive Acc: 92.760% | clf_exit: 0.253 0.458 0.288
Batch: 180 | Loss: 2.746 | Acc: 49.206,81.556,99.305,% | Adaptive Acc: 92.718% | clf_exit: 0.253 0.459 0.288
Batch: 200 | Loss: 2.748 | Acc: 49.324,81.374,99.296,% | Adaptive Acc: 92.708% | clf_exit: 0.252 0.460 0.288
Batch: 220 | Loss: 2.753 | Acc: 49.190,81.275,99.293,% | Adaptive Acc: 92.587% | clf_exit: 0.252 0.459 0.288
Batch: 240 | Loss: 2.760 | Acc: 49.167,81.182,99.277,% | Adaptive Acc: 92.538% | clf_exit: 0.252 0.459 0.289
Batch: 260 | Loss: 2.760 | Acc: 49.216,81.223,99.261,% | Adaptive Acc: 92.556% | clf_exit: 0.252 0.459 0.289
Batch: 280 | Loss: 2.760 | Acc: 49.263,81.130,99.260,% | Adaptive Acc: 92.529% | clf_exit: 0.252 0.459 0.289
Batch: 300 | Loss: 2.761 | Acc: 49.227,81.068,99.250,% | Adaptive Acc: 92.522% | clf_exit: 0.252 0.459 0.289
Batch: 320 | Loss: 2.762 | Acc: 49.255,81.072,99.258,% | Adaptive Acc: 92.550% | clf_exit: 0.253 0.457 0.290
Batch: 340 | Loss: 2.767 | Acc: 49.150,80.948,99.244,% | Adaptive Acc: 92.524% | clf_exit: 0.253 0.456 0.291
Batch: 360 | Loss: 2.769 | Acc: 49.063,80.930,99.253,% | Adaptive Acc: 92.562% | clf_exit: 0.252 0.457 0.291
Batch: 380 | Loss: 2.770 | Acc: 49.030,80.889,99.254,% | Adaptive Acc: 92.542% | clf_exit: 0.252 0.457 0.292
Batch: 0 | Loss: 4.053 | Acc: 48.438,72.656,77.344,% | Adaptive Acc: 73.438% | clf_exit: 0.281 0.461 0.258
Batch: 20 | Loss: 4.414 | Acc: 46.540,67.820,72.247,% | Adaptive Acc: 68.527% | clf_exit: 0.308 0.421 0.272
Batch: 40 | Loss: 4.394 | Acc: 46.532,67.721,71.932,% | Adaptive Acc: 68.407% | clf_exit: 0.305 0.417 0.278
Batch: 60 | Loss: 4.428 | Acc: 46.401,66.931,71.696,% | Adaptive Acc: 68.058% | clf_exit: 0.307 0.415 0.278
Train all parameters

Epoch: 175
Batch: 0 | Loss: 2.741 | Acc: 48.438,78.125,96.875,% | Adaptive Acc: 92.188% | clf_exit: 0.227 0.477 0.297
Batch: 20 | Loss: 2.733 | Acc: 50.372,83.631,99.144,% | Adaptive Acc: 93.452% | clf_exit: 0.253 0.466 0.282
Batch: 40 | Loss: 2.734 | Acc: 49.619,83.575,99.352,% | Adaptive Acc: 93.140% | clf_exit: 0.250 0.469 0.282
Batch: 60 | Loss: 2.727 | Acc: 49.360,83.517,99.385,% | Adaptive Acc: 93.046% | clf_exit: 0.246 0.474 0.280
Batch: 80 | Loss: 2.739 | Acc: 49.691,82.716,99.460,% | Adaptive Acc: 92.737% | clf_exit: 0.248 0.470 0.282
Batch: 100 | Loss: 2.744 | Acc: 49.536,82.395,99.482,% | Adaptive Acc: 92.683% | clf_exit: 0.249 0.467 0.284
Batch: 120 | Loss: 2.747 | Acc: 49.716,82.277,99.471,% | Adaptive Acc: 92.717% | clf_exit: 0.249 0.466 0.285
Batch: 140 | Loss: 2.751 | Acc: 49.668,82.070,99.468,% | Adaptive Acc: 92.670% | clf_exit: 0.250 0.465 0.285
Batch: 160 | Loss: 2.757 | Acc: 49.583,81.958,99.461,% | Adaptive Acc: 92.707% | clf_exit: 0.250 0.464 0.287
Batch: 180 | Loss: 2.755 | Acc: 49.603,81.893,99.430,% | Adaptive Acc: 92.727% | clf_exit: 0.252 0.462 0.287
Batch: 200 | Loss: 2.757 | Acc: 49.518,81.736,99.413,% | Adaptive Acc: 92.607% | clf_exit: 0.254 0.459 0.287
Batch: 220 | Loss: 2.758 | Acc: 49.427,81.692,99.378,% | Adaptive Acc: 92.626% | clf_exit: 0.253 0.460 0.287
Batch: 240 | Loss: 2.766 | Acc: 49.297,81.474,99.387,% | Adaptive Acc: 92.531% | clf_exit: 0.252 0.460 0.288
Batch: 260 | Loss: 2.764 | Acc: 49.344,81.472,99.341,% | Adaptive Acc: 92.547% | clf_exit: 0.253 0.459 0.288
Batch: 280 | Loss: 2.765 | Acc: 49.447,81.333,99.341,% | Adaptive Acc: 92.552% | clf_exit: 0.254 0.460 0.287
Batch: 300 | Loss: 2.767 | Acc: 49.437,81.268,99.328,% | Adaptive Acc: 92.548% | clf_exit: 0.253 0.459 0.288
Batch: 320 | Loss: 2.768 | Acc: 49.421,81.196,99.321,% | Adaptive Acc: 92.504% | clf_exit: 0.253 0.459 0.288
Batch: 340 | Loss: 2.768 | Acc: 49.446,81.142,99.329,% | Adaptive Acc: 92.472% | clf_exit: 0.253 0.459 0.288
Batch: 360 | Loss: 2.771 | Acc: 49.342,81.049,99.333,% | Adaptive Acc: 92.417% | clf_exit: 0.253 0.459 0.289
Batch: 380 | Loss: 2.771 | Acc: 49.299,81.053,99.319,% | Adaptive Acc: 92.466% | clf_exit: 0.252 0.459 0.289
Batch: 0 | Loss: 4.199 | Acc: 45.312,67.188,75.781,% | Adaptive Acc: 70.312% | clf_exit: 0.328 0.430 0.242
Batch: 20 | Loss: 4.520 | Acc: 45.610,67.262,71.391,% | Adaptive Acc: 67.894% | clf_exit: 0.327 0.405 0.268
Batch: 40 | Loss: 4.498 | Acc: 45.903,66.845,70.979,% | Adaptive Acc: 67.435% | clf_exit: 0.326 0.402 0.272
Batch: 60 | Loss: 4.515 | Acc: 45.735,66.496,71.183,% | Adaptive Acc: 67.341% | clf_exit: 0.324 0.406 0.270
Train all parameters

Epoch: 176
Batch: 0 | Loss: 2.645 | Acc: 50.000,85.156,98.438,% | Adaptive Acc: 92.969% | clf_exit: 0.258 0.500 0.242
Batch: 20 | Loss: 2.767 | Acc: 48.103,81.399,99.479,% | Adaptive Acc: 92.485% | clf_exit: 0.254 0.457 0.289
Batch: 40 | Loss: 2.761 | Acc: 48.780,81.345,99.486,% | Adaptive Acc: 92.759% | clf_exit: 0.253 0.453 0.293
Batch: 60 | Loss: 2.733 | Acc: 49.232,81.570,99.462,% | Adaptive Acc: 92.892% | clf_exit: 0.257 0.453 0.290
Batch: 80 | Loss: 2.726 | Acc: 49.479,81.636,99.460,% | Adaptive Acc: 92.978% | clf_exit: 0.258 0.455 0.287
Batch: 100 | Loss: 2.730 | Acc: 49.513,81.428,99.443,% | Adaptive Acc: 92.783% | clf_exit: 0.262 0.451 0.288
Batch: 120 | Loss: 2.738 | Acc: 49.529,81.418,99.412,% | Adaptive Acc: 92.840% | clf_exit: 0.260 0.450 0.290
Batch: 140 | Loss: 2.742 | Acc: 49.512,81.322,99.407,% | Adaptive Acc: 92.814% | clf_exit: 0.258 0.452 0.290
Batch: 160 | Loss: 2.756 | Acc: 49.398,81.129,99.369,% | Adaptive Acc: 92.697% | clf_exit: 0.256 0.454 0.290
Batch: 180 | Loss: 2.763 | Acc: 49.271,81.069,99.348,% | Adaptive Acc: 92.654% | clf_exit: 0.255 0.455 0.290
Batch: 200 | Loss: 2.768 | Acc: 49.145,81.032,99.335,% | Adaptive Acc: 92.669% | clf_exit: 0.255 0.453 0.292
Batch: 220 | Loss: 2.767 | Acc: 49.162,81.052,99.353,% | Adaptive Acc: 92.739% | clf_exit: 0.254 0.454 0.292
Batch: 240 | Loss: 2.766 | Acc: 49.209,81.039,99.319,% | Adaptive Acc: 92.716% | clf_exit: 0.253 0.455 0.291
Batch: 260 | Loss: 2.765 | Acc: 49.201,81.061,99.312,% | Adaptive Acc: 92.762% | clf_exit: 0.253 0.456 0.291
Batch: 280 | Loss: 2.764 | Acc: 49.213,81.041,99.313,% | Adaptive Acc: 92.752% | clf_exit: 0.252 0.457 0.291
Batch: 300 | Loss: 2.768 | Acc: 49.188,80.977,99.302,% | Adaptive Acc: 92.751% | clf_exit: 0.252 0.457 0.291
Batch: 320 | Loss: 2.769 | Acc: 49.216,80.948,99.292,% | Adaptive Acc: 92.696% | clf_exit: 0.252 0.457 0.291
Batch: 340 | Loss: 2.769 | Acc: 49.260,80.916,99.274,% | Adaptive Acc: 92.669% | clf_exit: 0.253 0.455 0.292
Batch: 360 | Loss: 2.771 | Acc: 49.264,80.834,99.277,% | Adaptive Acc: 92.610% | clf_exit: 0.253 0.456 0.291
Batch: 380 | Loss: 2.773 | Acc: 49.184,80.780,99.291,% | Adaptive Acc: 92.612% | clf_exit: 0.252 0.456 0.292
Batch: 0 | Loss: 4.075 | Acc: 51.562,67.969,75.781,% | Adaptive Acc: 69.531% | clf_exit: 0.320 0.438 0.242
Batch: 20 | Loss: 4.501 | Acc: 46.652,66.369,71.094,% | Adaptive Acc: 67.560% | clf_exit: 0.299 0.420 0.281
Batch: 40 | Loss: 4.475 | Acc: 46.418,66.216,71.075,% | Adaptive Acc: 68.197% | clf_exit: 0.298 0.410 0.292
Batch: 60 | Loss: 4.486 | Acc: 46.171,66.150,71.017,% | Adaptive Acc: 67.956% | clf_exit: 0.297 0.411 0.292
Train all parameters

Epoch: 177
Batch: 0 | Loss: 2.609 | Acc: 50.781,84.375,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.242 0.531 0.227
Batch: 20 | Loss: 2.652 | Acc: 50.632,82.961,99.442,% | Adaptive Acc: 93.266% | clf_exit: 0.263 0.457 0.279
Batch: 40 | Loss: 2.695 | Acc: 50.171,82.984,99.390,% | Adaptive Acc: 93.293% | clf_exit: 0.253 0.466 0.281
Batch: 60 | Loss: 2.712 | Acc: 50.026,82.390,99.321,% | Adaptive Acc: 93.135% | clf_exit: 0.254 0.458 0.287
Batch: 80 | Loss: 2.721 | Acc: 49.508,82.243,99.325,% | Adaptive Acc: 92.921% | clf_exit: 0.255 0.457 0.288
Batch: 100 | Loss: 2.732 | Acc: 49.482,82.163,99.358,% | Adaptive Acc: 92.969% | clf_exit: 0.251 0.459 0.290
Batch: 120 | Loss: 2.735 | Acc: 49.477,82.102,99.309,% | Adaptive Acc: 92.962% | clf_exit: 0.249 0.462 0.288
Batch: 140 | Loss: 2.729 | Acc: 49.457,81.987,99.313,% | Adaptive Acc: 92.919% | clf_exit: 0.252 0.460 0.288
Batch: 160 | Loss: 2.727 | Acc: 49.490,82.060,99.345,% | Adaptive Acc: 92.872% | clf_exit: 0.252 0.462 0.286
Batch: 180 | Loss: 2.726 | Acc: 49.612,82.044,99.374,% | Adaptive Acc: 92.800% | clf_exit: 0.253 0.463 0.285
Batch: 200 | Loss: 2.736 | Acc: 49.588,81.880,99.382,% | Adaptive Acc: 92.771% | clf_exit: 0.252 0.462 0.286
Batch: 220 | Loss: 2.737 | Acc: 49.480,81.915,99.378,% | Adaptive Acc: 92.721% | clf_exit: 0.251 0.463 0.286
Batch: 240 | Loss: 2.742 | Acc: 49.352,81.885,99.381,% | Adaptive Acc: 92.752% | clf_exit: 0.250 0.464 0.287
Batch: 260 | Loss: 2.746 | Acc: 49.341,81.804,99.371,% | Adaptive Acc: 92.663% | clf_exit: 0.250 0.464 0.286
Batch: 280 | Loss: 2.747 | Acc: 49.458,81.739,99.369,% | Adaptive Acc: 92.657% | clf_exit: 0.250 0.464 0.286
Batch: 300 | Loss: 2.746 | Acc: 49.473,81.663,99.364,% | Adaptive Acc: 92.603% | clf_exit: 0.250 0.465 0.285
Batch: 320 | Loss: 2.748 | Acc: 49.389,81.627,99.353,% | Adaptive Acc: 92.592% | clf_exit: 0.251 0.464 0.285
Batch: 340 | Loss: 2.749 | Acc: 49.441,81.573,99.349,% | Adaptive Acc: 92.586% | clf_exit: 0.251 0.464 0.285
Batch: 360 | Loss: 2.753 | Acc: 49.359,81.432,99.349,% | Adaptive Acc: 92.553% | clf_exit: 0.251 0.462 0.287
Batch: 380 | Loss: 2.758 | Acc: 49.311,81.338,99.338,% | Adaptive Acc: 92.544% | clf_exit: 0.251 0.461 0.287
Batch: 0 | Loss: 4.158 | Acc: 48.438,66.406,74.219,% | Adaptive Acc: 71.875% | clf_exit: 0.328 0.406 0.266
Batch: 20 | Loss: 4.519 | Acc: 45.610,66.034,71.205,% | Adaptive Acc: 67.634% | clf_exit: 0.318 0.414 0.268
Batch: 40 | Loss: 4.487 | Acc: 46.341,66.159,71.075,% | Adaptive Acc: 67.854% | clf_exit: 0.313 0.412 0.275
Batch: 60 | Loss: 4.505 | Acc: 46.183,66.022,71.196,% | Adaptive Acc: 67.636% | clf_exit: 0.314 0.411 0.275
Train all parameters

Epoch: 178
Batch: 0 | Loss: 2.672 | Acc: 53.906,85.938,100.000,% | Adaptive Acc: 97.656% | clf_exit: 0.219 0.516 0.266
Batch: 20 | Loss: 2.782 | Acc: 48.958,81.622,99.256,% | Adaptive Acc: 93.378% | clf_exit: 0.247 0.462 0.291
Batch: 40 | Loss: 2.764 | Acc: 48.876,81.479,99.352,% | Adaptive Acc: 92.873% | clf_exit: 0.256 0.455 0.289
Batch: 60 | Loss: 2.749 | Acc: 49.385,81.634,99.398,% | Adaptive Acc: 92.828% | clf_exit: 0.252 0.462 0.286
Batch: 80 | Loss: 2.740 | Acc: 49.392,81.742,99.450,% | Adaptive Acc: 92.882% | clf_exit: 0.253 0.463 0.284
Batch: 100 | Loss: 2.735 | Acc: 49.265,81.644,99.451,% | Adaptive Acc: 92.837% | clf_exit: 0.251 0.463 0.286
Batch: 120 | Loss: 2.747 | Acc: 49.070,81.418,99.387,% | Adaptive Acc: 92.743% | clf_exit: 0.249 0.464 0.287
Batch: 140 | Loss: 2.745 | Acc: 49.330,81.455,99.391,% | Adaptive Acc: 92.775% | clf_exit: 0.249 0.464 0.287
Batch: 160 | Loss: 2.739 | Acc: 49.476,81.483,99.345,% | Adaptive Acc: 92.644% | clf_exit: 0.251 0.463 0.286
Batch: 180 | Loss: 2.744 | Acc: 49.361,81.410,99.322,% | Adaptive Acc: 92.503% | clf_exit: 0.250 0.465 0.285
Batch: 200 | Loss: 2.745 | Acc: 49.394,81.421,99.328,% | Adaptive Acc: 92.514% | clf_exit: 0.251 0.463 0.286
Batch: 220 | Loss: 2.742 | Acc: 49.526,81.384,99.335,% | Adaptive Acc: 92.470% | clf_exit: 0.251 0.463 0.286
Batch: 240 | Loss: 2.742 | Acc: 49.533,81.315,99.339,% | Adaptive Acc: 92.440% | clf_exit: 0.251 0.464 0.285
Batch: 260 | Loss: 2.737 | Acc: 49.587,81.406,99.327,% | Adaptive Acc: 92.457% | clf_exit: 0.252 0.463 0.285
Batch: 280 | Loss: 2.738 | Acc: 49.505,81.392,99.338,% | Adaptive Acc: 92.493% | clf_exit: 0.251 0.464 0.285
Batch: 300 | Loss: 2.737 | Acc: 49.567,81.450,99.351,% | Adaptive Acc: 92.556% | clf_exit: 0.251 0.464 0.285
Batch: 320 | Loss: 2.734 | Acc: 49.652,81.491,99.353,% | Adaptive Acc: 92.592% | clf_exit: 0.251 0.464 0.285
Batch: 340 | Loss: 2.738 | Acc: 49.649,81.438,99.356,% | Adaptive Acc: 92.561% | clf_exit: 0.252 0.462 0.286
Batch: 360 | Loss: 2.745 | Acc: 49.608,81.350,99.346,% | Adaptive Acc: 92.571% | clf_exit: 0.251 0.461 0.287
Batch: 380 | Loss: 2.747 | Acc: 49.592,81.303,99.344,% | Adaptive Acc: 92.518% | clf_exit: 0.252 0.461 0.287
Batch: 0 | Loss: 4.155 | Acc: 45.312,68.750,74.219,% | Adaptive Acc: 67.969% | clf_exit: 0.320 0.422 0.258
Batch: 20 | Loss: 4.466 | Acc: 45.908,67.039,71.466,% | Adaptive Acc: 67.336% | clf_exit: 0.317 0.413 0.270
Batch: 40 | Loss: 4.472 | Acc: 46.437,66.635,71.494,% | Adaptive Acc: 67.664% | clf_exit: 0.316 0.406 0.278
Batch: 60 | Loss: 4.480 | Acc: 46.196,66.534,71.619,% | Adaptive Acc: 68.007% | clf_exit: 0.315 0.403 0.282
Train all parameters

Epoch: 179
Batch: 0 | Loss: 2.997 | Acc: 47.656,80.469,99.219,% | Adaptive Acc: 89.844% | clf_exit: 0.211 0.461 0.328
Batch: 20 | Loss: 2.705 | Acc: 50.632,82.626,99.293,% | Adaptive Acc: 92.820% | clf_exit: 0.253 0.465 0.282
Batch: 40 | Loss: 2.701 | Acc: 50.705,83.098,99.371,% | Adaptive Acc: 93.102% | clf_exit: 0.250 0.471 0.279
Batch: 60 | Loss: 2.715 | Acc: 50.243,82.812,99.219,% | Adaptive Acc: 92.764% | clf_exit: 0.250 0.468 0.283
Batch: 80 | Loss: 2.704 | Acc: 50.260,82.890,99.209,% | Adaptive Acc: 92.679% | clf_exit: 0.252 0.467 0.282
Batch: 100 | Loss: 2.718 | Acc: 49.938,82.503,99.265,% | Adaptive Acc: 92.574% | clf_exit: 0.251 0.464 0.285
Batch: 120 | Loss: 2.734 | Acc: 49.690,82.225,99.290,% | Adaptive Acc: 92.497% | clf_exit: 0.251 0.464 0.286
Batch: 140 | Loss: 2.728 | Acc: 49.701,82.186,99.302,% | Adaptive Acc: 92.553% | clf_exit: 0.252 0.462 0.285
Batch: 160 | Loss: 2.731 | Acc: 49.709,81.983,99.330,% | Adaptive Acc: 92.522% | clf_exit: 0.253 0.461 0.286
Batch: 180 | Loss: 2.732 | Acc: 49.728,82.023,99.314,% | Adaptive Acc: 92.524% | clf_exit: 0.253 0.462 0.286
Batch: 200 | Loss: 2.739 | Acc: 49.584,81.884,99.308,% | Adaptive Acc: 92.471% | clf_exit: 0.252 0.462 0.287
Batch: 220 | Loss: 2.745 | Acc: 49.569,81.759,99.275,% | Adaptive Acc: 92.446% | clf_exit: 0.253 0.459 0.288
Batch: 240 | Loss: 2.749 | Acc: 49.520,81.639,99.274,% | Adaptive Acc: 92.401% | clf_exit: 0.252 0.461 0.288
Batch: 260 | Loss: 2.748 | Acc: 49.512,81.702,99.282,% | Adaptive Acc: 92.463% | clf_exit: 0.252 0.461 0.287
Batch: 280 | Loss: 2.752 | Acc: 49.461,81.661,99.274,% | Adaptive Acc: 92.415% | clf_exit: 0.252 0.462 0.286
Batch: 300 | Loss: 2.755 | Acc: 49.426,81.561,99.278,% | Adaptive Acc: 92.408% | clf_exit: 0.252 0.460 0.288
Batch: 320 | Loss: 2.758 | Acc: 49.336,81.437,99.275,% | Adaptive Acc: 92.392% | clf_exit: 0.253 0.460 0.288
Batch: 340 | Loss: 2.756 | Acc: 49.407,81.417,99.267,% | Adaptive Acc: 92.410% | clf_exit: 0.253 0.460 0.287
Batch: 360 | Loss: 2.757 | Acc: 49.385,81.399,99.258,% | Adaptive Acc: 92.354% | clf_exit: 0.252 0.460 0.287
Batch: 380 | Loss: 2.757 | Acc: 49.379,81.353,99.254,% | Adaptive Acc: 92.347% | clf_exit: 0.253 0.459 0.288
Batch: 0 | Loss: 4.164 | Acc: 48.438,67.188,75.000,% | Adaptive Acc: 68.750% | clf_exit: 0.328 0.445 0.227
Batch: 20 | Loss: 4.504 | Acc: 45.982,66.406,72.359,% | Adaptive Acc: 68.006% | clf_exit: 0.336 0.408 0.256
Batch: 40 | Loss: 4.478 | Acc: 46.303,66.864,71.799,% | Adaptive Acc: 67.778% | clf_exit: 0.334 0.404 0.262
Batch: 60 | Loss: 4.489 | Acc: 45.902,66.522,71.644,% | Adaptive Acc: 67.751% | clf_exit: 0.331 0.404 0.265
Train all parameters

Epoch: 180
Batch: 0 | Loss: 2.746 | Acc: 39.844,82.031,99.219,% | Adaptive Acc: 92.188% | clf_exit: 0.266 0.453 0.281
Batch: 20 | Loss: 2.762 | Acc: 48.438,81.808,99.219,% | Adaptive Acc: 92.522% | clf_exit: 0.253 0.469 0.278
Batch: 40 | Loss: 2.745 | Acc: 49.123,81.669,99.352,% | Adaptive Acc: 92.550% | clf_exit: 0.257 0.459 0.285
Batch: 60 | Loss: 2.745 | Acc: 49.372,81.826,99.308,% | Adaptive Acc: 92.713% | clf_exit: 0.254 0.460 0.286
Batch: 80 | Loss: 2.729 | Acc: 49.633,81.703,99.392,% | Adaptive Acc: 92.872% | clf_exit: 0.255 0.458 0.287
Batch: 100 | Loss: 2.726 | Acc: 49.822,81.683,99.428,% | Adaptive Acc: 92.768% | clf_exit: 0.257 0.458 0.285
Batch: 120 | Loss: 2.738 | Acc: 49.793,81.553,99.387,% | Adaptive Acc: 92.723% | clf_exit: 0.254 0.461 0.285
Batch: 140 | Loss: 2.740 | Acc: 49.596,81.516,99.440,% | Adaptive Acc: 92.764% | clf_exit: 0.253 0.463 0.284
Batch: 160 | Loss: 2.749 | Acc: 49.330,81.366,99.423,% | Adaptive Acc: 92.697% | clf_exit: 0.253 0.462 0.285
Batch: 180 | Loss: 2.754 | Acc: 49.275,81.228,99.383,% | Adaptive Acc: 92.615% | clf_exit: 0.252 0.462 0.285
Batch: 200 | Loss: 2.754 | Acc: 49.386,81.199,99.374,% | Adaptive Acc: 92.588% | clf_exit: 0.252 0.463 0.285
Batch: 220 | Loss: 2.748 | Acc: 49.399,81.296,99.374,% | Adaptive Acc: 92.601% | clf_exit: 0.252 0.465 0.283
Batch: 240 | Loss: 2.751 | Acc: 49.433,81.273,99.368,% | Adaptive Acc: 92.521% | clf_exit: 0.253 0.464 0.284
Batch: 260 | Loss: 2.750 | Acc: 49.485,81.289,99.371,% | Adaptive Acc: 92.493% | clf_exit: 0.253 0.463 0.284
Batch: 280 | Loss: 2.755 | Acc: 49.475,81.211,99.366,% | Adaptive Acc: 92.474% | clf_exit: 0.254 0.462 0.285
Batch: 300 | Loss: 2.756 | Acc: 49.494,81.167,99.362,% | Adaptive Acc: 92.502% | clf_exit: 0.253 0.462 0.285
Batch: 320 | Loss: 2.755 | Acc: 49.603,81.143,99.365,% | Adaptive Acc: 92.460% | clf_exit: 0.253 0.462 0.284
Batch: 340 | Loss: 2.756 | Acc: 49.592,81.177,99.372,% | Adaptive Acc: 92.513% | clf_exit: 0.252 0.464 0.284
Batch: 360 | Loss: 2.751 | Acc: 49.626,81.211,99.375,% | Adaptive Acc: 92.536% | clf_exit: 0.252 0.464 0.284
Batch: 380 | Loss: 2.753 | Acc: 49.545,81.178,99.368,% | Adaptive Acc: 92.532% | clf_exit: 0.252 0.463 0.285
Batch: 0 | Loss: 4.011 | Acc: 47.656,70.312,76.562,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.398 0.266
Batch: 20 | Loss: 4.500 | Acc: 46.280,66.704,71.280,% | Adaptive Acc: 67.485% | clf_exit: 0.325 0.401 0.274
Batch: 40 | Loss: 4.482 | Acc: 46.361,66.673,71.056,% | Adaptive Acc: 67.264% | clf_exit: 0.320 0.403 0.278
Batch: 60 | Loss: 4.497 | Acc: 46.094,66.393,70.825,% | Adaptive Acc: 67.059% | clf_exit: 0.318 0.401 0.281
Train all parameters

Epoch: 181
Batch: 0 | Loss: 2.501 | Acc: 58.594,83.594,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.227 0.508 0.266
Batch: 20 | Loss: 2.743 | Acc: 48.363,81.064,99.516,% | Adaptive Acc: 93.043% | clf_exit: 0.256 0.459 0.285
Batch: 40 | Loss: 2.708 | Acc: 49.352,82.146,99.428,% | Adaptive Acc: 93.007% | clf_exit: 0.264 0.457 0.279
Batch: 60 | Loss: 2.718 | Acc: 49.283,82.262,99.513,% | Adaptive Acc: 93.007% | clf_exit: 0.263 0.462 0.275
Batch: 80 | Loss: 2.735 | Acc: 49.064,82.099,99.470,% | Adaptive Acc: 92.843% | clf_exit: 0.257 0.465 0.278
Batch: 100 | Loss: 2.739 | Acc: 48.940,82.124,99.420,% | Adaptive Acc: 92.783% | clf_exit: 0.254 0.468 0.278
Batch: 120 | Loss: 2.724 | Acc: 49.270,82.180,99.419,% | Adaptive Acc: 92.665% | clf_exit: 0.255 0.468 0.277
Batch: 140 | Loss: 2.709 | Acc: 49.490,82.358,99.429,% | Adaptive Acc: 92.592% | clf_exit: 0.254 0.471 0.275
Batch: 160 | Loss: 2.714 | Acc: 49.437,82.322,99.447,% | Adaptive Acc: 92.585% | clf_exit: 0.253 0.470 0.277
Batch: 180 | Loss: 2.720 | Acc: 49.417,82.191,99.443,% | Adaptive Acc: 92.567% | clf_exit: 0.252 0.469 0.279
Batch: 200 | Loss: 2.719 | Acc: 49.366,82.128,99.444,% | Adaptive Acc: 92.600% | clf_exit: 0.252 0.468 0.280
Batch: 220 | Loss: 2.725 | Acc: 49.328,82.024,99.434,% | Adaptive Acc: 92.665% | clf_exit: 0.252 0.466 0.282
Batch: 240 | Loss: 2.731 | Acc: 49.313,81.872,99.410,% | Adaptive Acc: 92.602% | clf_exit: 0.251 0.466 0.283
Batch: 260 | Loss: 2.735 | Acc: 49.291,81.747,99.386,% | Adaptive Acc: 92.469% | clf_exit: 0.250 0.467 0.283
Batch: 280 | Loss: 2.739 | Acc: 49.341,81.614,99.358,% | Adaptive Acc: 92.429% | clf_exit: 0.251 0.465 0.284
Batch: 300 | Loss: 2.742 | Acc: 49.380,81.619,99.364,% | Adaptive Acc: 92.457% | clf_exit: 0.251 0.465 0.284
Batch: 320 | Loss: 2.743 | Acc: 49.399,81.579,99.365,% | Adaptive Acc: 92.470% | clf_exit: 0.251 0.465 0.284
Batch: 340 | Loss: 2.742 | Acc: 49.471,81.550,99.359,% | Adaptive Acc: 92.462% | clf_exit: 0.252 0.464 0.285
Batch: 360 | Loss: 2.741 | Acc: 49.498,81.549,99.359,% | Adaptive Acc: 92.421% | clf_exit: 0.252 0.464 0.284
Batch: 380 | Loss: 2.742 | Acc: 49.584,81.498,99.366,% | Adaptive Acc: 92.401% | clf_exit: 0.253 0.463 0.284
Batch: 0 | Loss: 4.103 | Acc: 47.656,67.969,74.219,% | Adaptive Acc: 70.312% | clf_exit: 0.328 0.414 0.258
Batch: 20 | Loss: 4.480 | Acc: 46.205,65.923,71.317,% | Adaptive Acc: 67.299% | clf_exit: 0.326 0.421 0.254
Batch: 40 | Loss: 4.460 | Acc: 46.189,66.120,71.151,% | Adaptive Acc: 67.511% | clf_exit: 0.323 0.409 0.268
Batch: 60 | Loss: 4.508 | Acc: 45.825,65.651,71.094,% | Adaptive Acc: 67.200% | clf_exit: 0.320 0.410 0.270
Train all parameters

Epoch: 182
Batch: 0 | Loss: 2.570 | Acc: 45.312,85.156,98.438,% | Adaptive Acc: 96.094% | clf_exit: 0.250 0.477 0.273
Batch: 20 | Loss: 2.740 | Acc: 49.182,82.292,99.144,% | Adaptive Acc: 92.188% | clf_exit: 0.257 0.455 0.287
Batch: 40 | Loss: 2.704 | Acc: 50.019,82.908,99.238,% | Adaptive Acc: 92.397% | clf_exit: 0.262 0.460 0.278
Batch: 60 | Loss: 2.697 | Acc: 49.898,82.684,99.360,% | Adaptive Acc: 92.636% | clf_exit: 0.256 0.468 0.276
Batch: 80 | Loss: 2.720 | Acc: 49.576,82.388,99.306,% | Adaptive Acc: 92.544% | clf_exit: 0.256 0.468 0.277
Batch: 100 | Loss: 2.719 | Acc: 49.884,82.302,99.288,% | Adaptive Acc: 92.667% | clf_exit: 0.256 0.464 0.280
Batch: 120 | Loss: 2.720 | Acc: 49.916,82.238,99.296,% | Adaptive Acc: 92.627% | clf_exit: 0.255 0.465 0.280
Batch: 140 | Loss: 2.715 | Acc: 49.873,82.281,99.330,% | Adaptive Acc: 92.631% | clf_exit: 0.256 0.465 0.279
Batch: 160 | Loss: 2.718 | Acc: 49.743,82.172,99.355,% | Adaptive Acc: 92.648% | clf_exit: 0.254 0.466 0.280
Batch: 180 | Loss: 2.717 | Acc: 49.728,82.191,99.370,% | Adaptive Acc: 92.546% | clf_exit: 0.254 0.468 0.278
Batch: 200 | Loss: 2.719 | Acc: 49.790,82.051,99.363,% | Adaptive Acc: 92.533% | clf_exit: 0.255 0.465 0.280
Batch: 220 | Loss: 2.718 | Acc: 49.841,81.964,99.367,% | Adaptive Acc: 92.470% | clf_exit: 0.256 0.464 0.280
Batch: 240 | Loss: 2.718 | Acc: 49.932,81.944,99.371,% | Adaptive Acc: 92.444% | clf_exit: 0.256 0.465 0.279
Batch: 260 | Loss: 2.723 | Acc: 49.868,81.923,99.356,% | Adaptive Acc: 92.409% | clf_exit: 0.256 0.464 0.280
Batch: 280 | Loss: 2.726 | Acc: 49.780,81.906,99.344,% | Adaptive Acc: 92.329% | clf_exit: 0.255 0.466 0.279
Batch: 300 | Loss: 2.728 | Acc: 49.805,81.850,99.354,% | Adaptive Acc: 92.380% | clf_exit: 0.254 0.466 0.280
Batch: 320 | Loss: 2.732 | Acc: 49.761,81.756,99.345,% | Adaptive Acc: 92.394% | clf_exit: 0.254 0.465 0.281
Batch: 340 | Loss: 2.739 | Acc: 49.668,81.612,99.338,% | Adaptive Acc: 92.355% | clf_exit: 0.254 0.464 0.283
Batch: 360 | Loss: 2.739 | Acc: 49.662,81.564,99.338,% | Adaptive Acc: 92.335% | clf_exit: 0.254 0.463 0.283
Batch: 380 | Loss: 2.741 | Acc: 49.604,81.500,99.315,% | Adaptive Acc: 92.311% | clf_exit: 0.254 0.463 0.283
Batch: 0 | Loss: 4.173 | Acc: 48.438,67.188,75.781,% | Adaptive Acc: 70.312% | clf_exit: 0.328 0.469 0.203
Batch: 20 | Loss: 4.523 | Acc: 45.387,66.295,71.205,% | Adaptive Acc: 67.708% | clf_exit: 0.318 0.408 0.273
Batch: 40 | Loss: 4.548 | Acc: 45.141,65.835,70.636,% | Adaptive Acc: 67.149% | clf_exit: 0.316 0.410 0.274
Batch: 60 | Loss: 4.565 | Acc: 44.890,65.241,70.274,% | Adaptive Acc: 66.650% | clf_exit: 0.315 0.408 0.277
Train all parameters

Epoch: 183
Batch: 0 | Loss: 2.916 | Acc: 42.188,75.781,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.258 0.422 0.320
Batch: 20 | Loss: 2.763 | Acc: 49.070,81.659,99.628,% | Adaptive Acc: 93.378% | clf_exit: 0.249 0.456 0.295
Batch: 40 | Loss: 2.703 | Acc: 50.038,82.165,99.676,% | Adaptive Acc: 93.655% | clf_exit: 0.251 0.466 0.284
Batch: 60 | Loss: 2.711 | Acc: 49.731,82.198,99.539,% | Adaptive Acc: 93.186% | clf_exit: 0.250 0.469 0.280
Batch: 80 | Loss: 2.711 | Acc: 49.730,82.494,99.450,% | Adaptive Acc: 93.248% | clf_exit: 0.249 0.471 0.280
Batch: 100 | Loss: 2.721 | Acc: 49.729,82.163,99.428,% | Adaptive Acc: 93.069% | clf_exit: 0.249 0.470 0.280
Batch: 120 | Loss: 2.723 | Acc: 49.780,81.941,99.432,% | Adaptive Acc: 93.007% | clf_exit: 0.250 0.467 0.282
Batch: 140 | Loss: 2.717 | Acc: 49.751,82.009,99.418,% | Adaptive Acc: 92.958% | clf_exit: 0.250 0.468 0.282
Batch: 160 | Loss: 2.725 | Acc: 49.539,81.808,99.403,% | Adaptive Acc: 92.843% | clf_exit: 0.249 0.470 0.281
Batch: 180 | Loss: 2.726 | Acc: 49.633,81.729,99.430,% | Adaptive Acc: 92.770% | clf_exit: 0.249 0.469 0.282
Batch: 200 | Loss: 2.722 | Acc: 49.771,81.763,99.425,% | Adaptive Acc: 92.809% | clf_exit: 0.251 0.466 0.283
Batch: 220 | Loss: 2.724 | Acc: 49.767,81.724,99.424,% | Adaptive Acc: 92.785% | clf_exit: 0.251 0.466 0.283
Batch: 240 | Loss: 2.731 | Acc: 49.653,81.629,99.410,% | Adaptive Acc: 92.706% | clf_exit: 0.252 0.464 0.284
Batch: 260 | Loss: 2.738 | Acc: 49.503,81.540,99.410,% | Adaptive Acc: 92.654% | clf_exit: 0.251 0.464 0.285
Batch: 280 | Loss: 2.743 | Acc: 49.550,81.386,99.386,% | Adaptive Acc: 92.632% | clf_exit: 0.252 0.462 0.286
Batch: 300 | Loss: 2.743 | Acc: 49.494,81.343,99.369,% | Adaptive Acc: 92.590% | clf_exit: 0.252 0.463 0.286
Batch: 320 | Loss: 2.743 | Acc: 49.581,81.304,99.362,% | Adaptive Acc: 92.611% | clf_exit: 0.252 0.462 0.286
Batch: 340 | Loss: 2.745 | Acc: 49.539,81.280,99.354,% | Adaptive Acc: 92.623% | clf_exit: 0.251 0.463 0.286
Batch: 360 | Loss: 2.747 | Acc: 49.578,81.261,99.349,% | Adaptive Acc: 92.599% | clf_exit: 0.251 0.462 0.287
Batch: 380 | Loss: 2.750 | Acc: 49.383,81.232,99.344,% | Adaptive Acc: 92.567% | clf_exit: 0.250 0.462 0.288
Batch: 0 | Loss: 4.249 | Acc: 46.875,67.188,73.438,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.406 0.258
Batch: 20 | Loss: 4.523 | Acc: 45.796,65.699,71.019,% | Adaptive Acc: 66.667% | clf_exit: 0.344 0.390 0.266
Batch: 40 | Loss: 4.514 | Acc: 46.208,65.758,70.617,% | Adaptive Acc: 66.673% | clf_exit: 0.343 0.390 0.267
Batch: 60 | Loss: 4.518 | Acc: 46.145,65.791,70.722,% | Adaptive Acc: 66.406% | clf_exit: 0.344 0.393 0.264
Train all parameters

Epoch: 184
Batch: 0 | Loss: 2.711 | Acc: 54.688,82.031,98.438,% | Adaptive Acc: 91.406% | clf_exit: 0.195 0.523 0.281
Batch: 20 | Loss: 2.720 | Acc: 50.074,82.515,99.405,% | Adaptive Acc: 92.597% | clf_exit: 0.258 0.460 0.282
Batch: 40 | Loss: 2.715 | Acc: 49.676,82.774,99.390,% | Adaptive Acc: 92.607% | clf_exit: 0.256 0.464 0.280
Batch: 60 | Loss: 2.711 | Acc: 49.898,82.198,99.372,% | Adaptive Acc: 92.252% | clf_exit: 0.257 0.464 0.278
Batch: 80 | Loss: 2.717 | Acc: 49.981,82.128,99.373,% | Adaptive Acc: 92.419% | clf_exit: 0.255 0.466 0.279
Batch: 100 | Loss: 2.720 | Acc: 49.961,82.062,99.327,% | Adaptive Acc: 92.420% | clf_exit: 0.254 0.464 0.281
Batch: 120 | Loss: 2.723 | Acc: 49.897,82.038,99.296,% | Adaptive Acc: 92.575% | clf_exit: 0.254 0.465 0.281
Batch: 140 | Loss: 2.724 | Acc: 49.873,82.048,99.296,% | Adaptive Acc: 92.503% | clf_exit: 0.255 0.463 0.281
Batch: 160 | Loss: 2.726 | Acc: 49.709,81.983,99.316,% | Adaptive Acc: 92.741% | clf_exit: 0.253 0.464 0.283
Batch: 180 | Loss: 2.727 | Acc: 49.728,81.988,99.249,% | Adaptive Acc: 92.654% | clf_exit: 0.254 0.463 0.282
Batch: 200 | Loss: 2.729 | Acc: 49.685,81.992,99.258,% | Adaptive Acc: 92.584% | clf_exit: 0.255 0.464 0.281
Batch: 220 | Loss: 2.727 | Acc: 49.823,81.946,99.293,% | Adaptive Acc: 92.608% | clf_exit: 0.256 0.463 0.281
Batch: 240 | Loss: 2.729 | Acc: 49.770,81.853,99.284,% | Adaptive Acc: 92.615% | clf_exit: 0.255 0.463 0.282
Batch: 260 | Loss: 2.736 | Acc: 49.593,81.786,99.294,% | Adaptive Acc: 92.580% | clf_exit: 0.255 0.463 0.283
Batch: 280 | Loss: 2.736 | Acc: 49.555,81.709,99.283,% | Adaptive Acc: 92.541% | clf_exit: 0.254 0.464 0.282
Batch: 300 | Loss: 2.739 | Acc: 49.580,81.683,99.271,% | Adaptive Acc: 92.543% | clf_exit: 0.254 0.462 0.283
Batch: 320 | Loss: 2.744 | Acc: 49.491,81.605,99.292,% | Adaptive Acc: 92.538% | clf_exit: 0.253 0.463 0.284
Batch: 340 | Loss: 2.743 | Acc: 49.565,81.564,99.297,% | Adaptive Acc: 92.552% | clf_exit: 0.253 0.463 0.284
Batch: 360 | Loss: 2.742 | Acc: 49.546,81.549,99.292,% | Adaptive Acc: 92.592% | clf_exit: 0.253 0.463 0.284
Batch: 380 | Loss: 2.741 | Acc: 49.602,81.525,99.282,% | Adaptive Acc: 92.563% | clf_exit: 0.253 0.462 0.285
Batch: 0 | Loss: 3.982 | Acc: 46.875,71.094,72.656,% | Adaptive Acc: 70.312% | clf_exit: 0.312 0.461 0.227
Batch: 20 | Loss: 4.526 | Acc: 46.094,65.960,70.833,% | Adaptive Acc: 67.225% | clf_exit: 0.333 0.408 0.259
Batch: 40 | Loss: 4.524 | Acc: 46.208,66.254,70.465,% | Adaptive Acc: 67.054% | clf_exit: 0.329 0.406 0.265
Batch: 60 | Loss: 4.546 | Acc: 45.863,65.920,70.402,% | Adaptive Acc: 66.778% | clf_exit: 0.331 0.401 0.268
Train classifier parameters

Epoch: 185
Batch: 0 | Loss: 2.642 | Acc: 51.562,81.250,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.211 0.516 0.273
Batch: 20 | Loss: 2.784 | Acc: 47.470,82.031,99.293,% | Adaptive Acc: 92.820% | clf_exit: 0.231 0.475 0.294
Batch: 40 | Loss: 2.869 | Acc: 47.447,80.297,98.628,% | Adaptive Acc: 91.730% | clf_exit: 0.233 0.466 0.300
Batch: 60 | Loss: 2.908 | Acc: 47.016,79.521,98.450,% | Adaptive Acc: 91.137% | clf_exit: 0.233 0.465 0.302
Batch: 80 | Loss: 2.909 | Acc: 47.560,79.225,98.341,% | Adaptive Acc: 91.146% | clf_exit: 0.239 0.457 0.304
Batch: 100 | Loss: 2.932 | Acc: 47.331,79.115,98.321,% | Adaptive Acc: 91.105% | clf_exit: 0.237 0.457 0.306
Batch: 120 | Loss: 2.935 | Acc: 47.230,79.190,98.289,% | Adaptive Acc: 91.251% | clf_exit: 0.236 0.456 0.308
Batch: 140 | Loss: 2.933 | Acc: 47.418,79.161,98.227,% | Adaptive Acc: 91.212% | clf_exit: 0.236 0.456 0.308
Batch: 160 | Loss: 2.935 | Acc: 47.346,79.047,98.224,% | Adaptive Acc: 91.135% | clf_exit: 0.238 0.454 0.308
Batch: 180 | Loss: 2.937 | Acc: 47.333,79.049,98.200,% | Adaptive Acc: 91.290% | clf_exit: 0.236 0.454 0.310
Batch: 200 | Loss: 2.939 | Acc: 47.365,79.081,98.134,% | Adaptive Acc: 91.235% | clf_exit: 0.236 0.453 0.311
Batch: 220 | Loss: 2.942 | Acc: 47.317,79.048,98.091,% | Adaptive Acc: 91.187% | clf_exit: 0.235 0.454 0.311
Batch: 240 | Loss: 2.935 | Acc: 47.452,79.172,98.130,% | Adaptive Acc: 91.196% | clf_exit: 0.236 0.454 0.310
Batch: 260 | Loss: 2.929 | Acc: 47.638,79.170,98.153,% | Adaptive Acc: 91.215% | clf_exit: 0.237 0.454 0.309
Batch: 280 | Loss: 2.929 | Acc: 47.740,79.159,98.162,% | Adaptive Acc: 91.245% | clf_exit: 0.238 0.453 0.309
Batch: 300 | Loss: 2.929 | Acc: 47.781,79.137,98.157,% | Adaptive Acc: 91.240% | clf_exit: 0.239 0.452 0.309
Batch: 320 | Loss: 2.931 | Acc: 47.690,79.123,98.165,% | Adaptive Acc: 91.250% | clf_exit: 0.238 0.453 0.309
Batch: 340 | Loss: 2.936 | Acc: 47.585,79.090,98.165,% | Adaptive Acc: 91.292% | clf_exit: 0.236 0.453 0.310
Batch: 360 | Loss: 2.940 | Acc: 47.503,79.075,98.148,% | Adaptive Acc: 91.294% | clf_exit: 0.236 0.453 0.311
Batch: 380 | Loss: 2.939 | Acc: 47.511,79.101,98.148,% | Adaptive Acc: 91.337% | clf_exit: 0.236 0.452 0.311
Batch: 0 | Loss: 4.245 | Acc: 48.438,65.625,71.094,% | Adaptive Acc: 67.188% | clf_exit: 0.328 0.453 0.219
Batch: 20 | Loss: 4.729 | Acc: 45.610,63.765,69.234,% | Adaptive Acc: 65.551% | clf_exit: 0.298 0.419 0.283
Batch: 40 | Loss: 4.734 | Acc: 45.084,64.120,69.112,% | Adaptive Acc: 66.101% | clf_exit: 0.296 0.412 0.292
Batch: 60 | Loss: 4.752 | Acc: 44.980,64.127,68.788,% | Adaptive Acc: 65.856% | clf_exit: 0.294 0.413 0.294
Train classifier parameters

Epoch: 186
Batch: 0 | Loss: 3.040 | Acc: 42.969,80.469,97.656,% | Adaptive Acc: 94.531% | clf_exit: 0.227 0.445 0.328
Batch: 20 | Loss: 2.922 | Acc: 48.140,78.795,97.917,% | Adaptive Acc: 91.220% | clf_exit: 0.243 0.463 0.295
Batch: 40 | Loss: 2.921 | Acc: 47.675,79.040,97.923,% | Adaptive Acc: 91.349% | clf_exit: 0.236 0.463 0.301
Batch: 60 | Loss: 2.911 | Acc: 47.797,79.060,98.117,% | Adaptive Acc: 91.445% | clf_exit: 0.239 0.460 0.301
Batch: 80 | Loss: 2.929 | Acc: 47.772,78.935,98.216,% | Adaptive Acc: 91.638% | clf_exit: 0.236 0.458 0.307
Batch: 100 | Loss: 2.922 | Acc: 47.896,79.045,98.182,% | Adaptive Acc: 91.638% | clf_exit: 0.238 0.454 0.308
Batch: 120 | Loss: 2.930 | Acc: 47.753,79.003,98.224,% | Adaptive Acc: 91.606% | clf_exit: 0.235 0.456 0.309
Batch: 140 | Loss: 2.926 | Acc: 47.839,79.172,98.205,% | Adaptive Acc: 91.689% | clf_exit: 0.237 0.453 0.310
Batch: 160 | Loss: 2.934 | Acc: 47.778,79.047,98.243,% | Adaptive Acc: 91.678% | clf_exit: 0.236 0.452 0.312
Batch: 180 | Loss: 2.938 | Acc: 47.734,78.949,98.252,% | Adaptive Acc: 91.618% | clf_exit: 0.236 0.452 0.312
Batch: 200 | Loss: 2.942 | Acc: 47.610,78.797,98.259,% | Adaptive Acc: 91.663% | clf_exit: 0.235 0.450 0.315
Batch: 220 | Loss: 2.941 | Acc: 47.628,78.857,98.240,% | Adaptive Acc: 91.682% | clf_exit: 0.236 0.451 0.314
Batch: 240 | Loss: 2.941 | Acc: 47.553,78.874,98.256,% | Adaptive Acc: 91.633% | clf_exit: 0.235 0.452 0.313
Batch: 260 | Loss: 2.941 | Acc: 47.540,78.915,98.234,% | Adaptive Acc: 91.583% | clf_exit: 0.235 0.452 0.313
Batch: 280 | Loss: 2.940 | Acc: 47.573,79.017,98.240,% | Adaptive Acc: 91.606% | clf_exit: 0.235 0.451 0.314
Batch: 300 | Loss: 2.940 | Acc: 47.687,79.059,98.240,% | Adaptive Acc: 91.624% | clf_exit: 0.236 0.451 0.313
Batch: 320 | Loss: 2.941 | Acc: 47.664,79.091,98.235,% | Adaptive Acc: 91.569% | clf_exit: 0.235 0.453 0.312
Batch: 340 | Loss: 2.938 | Acc: 47.661,79.149,98.270,% | Adaptive Acc: 91.569% | clf_exit: 0.235 0.453 0.312
Batch: 360 | Loss: 2.932 | Acc: 47.810,79.237,98.254,% | Adaptive Acc: 91.603% | clf_exit: 0.235 0.453 0.312
Batch: 380 | Loss: 2.933 | Acc: 47.777,79.259,98.267,% | Adaptive Acc: 91.638% | clf_exit: 0.235 0.453 0.312
Batch: 0 | Loss: 4.248 | Acc: 46.875,66.406,72.656,% | Adaptive Acc: 67.969% | clf_exit: 0.336 0.445 0.219
Batch: 20 | Loss: 4.710 | Acc: 45.573,64.360,69.754,% | Adaptive Acc: 65.699% | clf_exit: 0.301 0.418 0.282
Batch: 40 | Loss: 4.718 | Acc: 45.389,64.787,69.226,% | Adaptive Acc: 66.063% | clf_exit: 0.300 0.412 0.288
Batch: 60 | Loss: 4.739 | Acc: 45.108,64.652,69.083,% | Adaptive Acc: 66.124% | clf_exit: 0.298 0.411 0.291
Train classifier parameters

Epoch: 187
Batch: 0 | Loss: 3.030 | Acc: 50.781,80.469,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.273 0.430 0.297
Batch: 20 | Loss: 2.903 | Acc: 49.516,79.762,97.991,% | Adaptive Acc: 91.109% | clf_exit: 0.243 0.465 0.292
Batch: 40 | Loss: 2.945 | Acc: 48.304,79.078,97.999,% | Adaptive Acc: 91.292% | clf_exit: 0.234 0.464 0.302
Batch: 60 | Loss: 2.925 | Acc: 47.964,79.239,97.976,% | Adaptive Acc: 91.650% | clf_exit: 0.234 0.464 0.302
Batch: 80 | Loss: 2.934 | Acc: 47.984,79.147,97.975,% | Adaptive Acc: 91.503% | clf_exit: 0.232 0.467 0.301
Batch: 100 | Loss: 2.925 | Acc: 48.422,79.394,98.089,% | Adaptive Acc: 91.654% | clf_exit: 0.234 0.463 0.304
Batch: 120 | Loss: 2.920 | Acc: 48.463,79.462,98.095,% | Adaptive Acc: 91.600% | clf_exit: 0.234 0.464 0.303
Batch: 140 | Loss: 2.918 | Acc: 48.327,79.333,98.127,% | Adaptive Acc: 91.622% | clf_exit: 0.234 0.461 0.305
Batch: 160 | Loss: 2.925 | Acc: 48.083,79.333,98.156,% | Adaptive Acc: 91.770% | clf_exit: 0.232 0.461 0.307
Batch: 180 | Loss: 2.914 | Acc: 48.299,79.368,98.204,% | Adaptive Acc: 91.734% | clf_exit: 0.235 0.459 0.306
Batch: 200 | Loss: 2.918 | Acc: 48.231,79.415,98.193,% | Adaptive Acc: 91.690% | clf_exit: 0.234 0.460 0.306
Batch: 220 | Loss: 2.914 | Acc: 48.183,79.440,98.254,% | Adaptive Acc: 91.763% | clf_exit: 0.235 0.460 0.305
Batch: 240 | Loss: 2.915 | Acc: 48.143,79.396,98.249,% | Adaptive Acc: 91.701% | clf_exit: 0.236 0.458 0.306
Batch: 260 | Loss: 2.920 | Acc: 48.084,79.250,98.234,% | Adaptive Acc: 91.658% | clf_exit: 0.236 0.458 0.307
Batch: 280 | Loss: 2.917 | Acc: 48.090,79.368,98.229,% | Adaptive Acc: 91.643% | clf_exit: 0.235 0.458 0.307
Batch: 300 | Loss: 2.921 | Acc: 48.043,79.262,98.232,% | Adaptive Acc: 91.609% | clf_exit: 0.236 0.457 0.307
Batch: 320 | Loss: 2.924 | Acc: 47.982,79.203,98.226,% | Adaptive Acc: 91.577% | clf_exit: 0.236 0.457 0.307
Batch: 340 | Loss: 2.925 | Acc: 47.984,79.151,98.220,% | Adaptive Acc: 91.548% | clf_exit: 0.236 0.457 0.308
Batch: 360 | Loss: 2.929 | Acc: 47.955,79.105,98.223,% | Adaptive Acc: 91.556% | clf_exit: 0.235 0.456 0.309
Batch: 380 | Loss: 2.929 | Acc: 47.978,79.138,98.214,% | Adaptive Acc: 91.529% | clf_exit: 0.235 0.456 0.308
Batch: 0 | Loss: 4.180 | Acc: 48.438,66.406,72.656,% | Adaptive Acc: 69.531% | clf_exit: 0.328 0.445 0.227
Batch: 20 | Loss: 4.690 | Acc: 45.871,64.211,69.754,% | Adaptive Acc: 66.146% | clf_exit: 0.302 0.410 0.288
Batch: 40 | Loss: 4.691 | Acc: 45.541,64.710,69.226,% | Adaptive Acc: 66.349% | clf_exit: 0.299 0.408 0.293
Batch: 60 | Loss: 4.710 | Acc: 45.389,64.511,69.070,% | Adaptive Acc: 66.176% | clf_exit: 0.296 0.407 0.296
Train classifier parameters

Epoch: 188
Batch: 0 | Loss: 3.277 | Acc: 41.406,75.781,97.656,% | Adaptive Acc: 86.719% | clf_exit: 0.242 0.422 0.336
Batch: 20 | Loss: 2.912 | Acc: 48.177,79.539,98.661,% | Adaptive Acc: 92.113% | clf_exit: 0.235 0.452 0.313
Batch: 40 | Loss: 2.918 | Acc: 48.095,79.802,98.361,% | Adaptive Acc: 91.749% | clf_exit: 0.236 0.451 0.313
Batch: 60 | Loss: 2.924 | Acc: 47.759,79.623,98.271,% | Adaptive Acc: 91.714% | clf_exit: 0.235 0.455 0.310
Batch: 80 | Loss: 2.939 | Acc: 47.560,79.437,98.264,% | Adaptive Acc: 91.676% | clf_exit: 0.235 0.455 0.310
Batch: 100 | Loss: 2.933 | Acc: 47.803,79.363,98.144,% | Adaptive Acc: 91.631% | clf_exit: 0.237 0.452 0.310
Batch: 120 | Loss: 2.928 | Acc: 47.960,79.416,98.115,% | Adaptive Acc: 91.503% | clf_exit: 0.239 0.452 0.309
Batch: 140 | Loss: 2.928 | Acc: 47.872,79.510,98.100,% | Adaptive Acc: 91.528% | clf_exit: 0.238 0.453 0.309
Batch: 160 | Loss: 2.920 | Acc: 47.913,79.532,98.127,% | Adaptive Acc: 91.600% | clf_exit: 0.237 0.453 0.309
Batch: 180 | Loss: 2.923 | Acc: 47.928,79.498,98.191,% | Adaptive Acc: 91.700% | clf_exit: 0.237 0.453 0.310
Batch: 200 | Loss: 2.926 | Acc: 47.952,79.450,98.165,% | Adaptive Acc: 91.729% | clf_exit: 0.235 0.453 0.312
Batch: 220 | Loss: 2.928 | Acc: 48.042,79.373,98.133,% | Adaptive Acc: 91.724% | clf_exit: 0.235 0.452 0.314
Batch: 240 | Loss: 2.923 | Acc: 48.065,79.496,98.133,% | Adaptive Acc: 91.711% | clf_exit: 0.236 0.451 0.312
Batch: 260 | Loss: 2.922 | Acc: 48.018,79.454,98.144,% | Adaptive Acc: 91.694% | clf_exit: 0.237 0.450 0.313
Batch: 280 | Loss: 2.925 | Acc: 47.920,79.443,98.165,% | Adaptive Acc: 91.723% | clf_exit: 0.237 0.449 0.314
Batch: 300 | Loss: 2.924 | Acc: 47.898,79.431,98.178,% | Adaptive Acc: 91.684% | clf_exit: 0.237 0.449 0.314
Batch: 320 | Loss: 2.922 | Acc: 47.934,79.490,98.180,% | Adaptive Acc: 91.657% | clf_exit: 0.237 0.450 0.314
Batch: 340 | Loss: 2.921 | Acc: 47.986,79.495,98.163,% | Adaptive Acc: 91.683% | clf_exit: 0.237 0.449 0.314
Batch: 360 | Loss: 2.922 | Acc: 48.011,79.467,98.156,% | Adaptive Acc: 91.709% | clf_exit: 0.235 0.451 0.314
Batch: 380 | Loss: 2.923 | Acc: 48.009,79.437,98.175,% | Adaptive Acc: 91.691% | clf_exit: 0.236 0.450 0.314
Batch: 0 | Loss: 4.220 | Acc: 47.656,64.062,71.094,% | Adaptive Acc: 65.625% | clf_exit: 0.328 0.453 0.219
Batch: 20 | Loss: 4.691 | Acc: 45.908,64.249,70.201,% | Adaptive Acc: 66.109% | clf_exit: 0.303 0.410 0.287
Batch: 40 | Loss: 4.698 | Acc: 45.694,64.558,69.607,% | Adaptive Acc: 66.178% | clf_exit: 0.301 0.406 0.293
Batch: 60 | Loss: 4.717 | Acc: 45.274,64.536,69.326,% | Adaptive Acc: 66.201% | clf_exit: 0.299 0.407 0.294
Train classifier parameters

Epoch: 189
Batch: 0 | Loss: 2.795 | Acc: 46.875,85.938,97.656,% | Adaptive Acc: 90.625% | clf_exit: 0.250 0.461 0.289
Batch: 20 | Loss: 2.821 | Acc: 50.372,81.064,98.177,% | Adaptive Acc: 92.039% | clf_exit: 0.241 0.459 0.300
Batch: 40 | Loss: 2.870 | Acc: 48.876,80.354,98.438,% | Adaptive Acc: 92.016% | clf_exit: 0.234 0.455 0.312
Batch: 60 | Loss: 2.893 | Acc: 48.502,80.046,98.425,% | Adaptive Acc: 92.021% | clf_exit: 0.232 0.454 0.313
Batch: 80 | Loss: 2.895 | Acc: 48.476,79.938,98.428,% | Adaptive Acc: 92.072% | clf_exit: 0.233 0.453 0.315
Batch: 100 | Loss: 2.898 | Acc: 48.275,79.896,98.368,% | Adaptive Acc: 91.940% | clf_exit: 0.234 0.454 0.312
Batch: 120 | Loss: 2.900 | Acc: 48.502,79.978,98.399,% | Adaptive Acc: 91.865% | clf_exit: 0.237 0.452 0.312
Batch: 140 | Loss: 2.898 | Acc: 48.377,79.981,98.354,% | Adaptive Acc: 91.661% | clf_exit: 0.237 0.453 0.309
Batch: 160 | Loss: 2.902 | Acc: 48.433,79.920,98.365,% | Adaptive Acc: 91.634% | clf_exit: 0.238 0.451 0.310
Batch: 180 | Loss: 2.909 | Acc: 48.308,79.688,98.373,% | Adaptive Acc: 91.639% | clf_exit: 0.238 0.452 0.311
Batch: 200 | Loss: 2.902 | Acc: 48.395,79.707,98.387,% | Adaptive Acc: 91.717% | clf_exit: 0.238 0.452 0.310
Batch: 220 | Loss: 2.906 | Acc: 48.307,79.684,98.331,% | Adaptive Acc: 91.717% | clf_exit: 0.238 0.451 0.311
Batch: 240 | Loss: 2.908 | Acc: 48.253,79.629,98.373,% | Adaptive Acc: 91.743% | clf_exit: 0.237 0.450 0.312
Batch: 260 | Loss: 2.906 | Acc: 48.231,79.643,98.384,% | Adaptive Acc: 91.786% | clf_exit: 0.238 0.450 0.312
Batch: 280 | Loss: 2.903 | Acc: 48.287,79.657,98.390,% | Adaptive Acc: 91.826% | clf_exit: 0.238 0.451 0.311
Batch: 300 | Loss: 2.905 | Acc: 48.196,79.612,98.401,% | Adaptive Acc: 91.803% | clf_exit: 0.238 0.451 0.311
Batch: 320 | Loss: 2.906 | Acc: 48.233,79.624,98.401,% | Adaptive Acc: 91.793% | clf_exit: 0.239 0.451 0.310
Batch: 340 | Loss: 2.905 | Acc: 48.206,79.616,98.396,% | Adaptive Acc: 91.752% | clf_exit: 0.239 0.451 0.310
Batch: 360 | Loss: 2.907 | Acc: 48.163,79.644,98.360,% | Adaptive Acc: 91.750% | clf_exit: 0.239 0.450 0.311
Batch: 380 | Loss: 2.912 | Acc: 48.130,79.612,98.360,% | Adaptive Acc: 91.779% | clf_exit: 0.239 0.450 0.311
Batch: 0 | Loss: 4.203 | Acc: 47.656,64.844,71.094,% | Adaptive Acc: 68.750% | clf_exit: 0.320 0.445 0.234
Batch: 20 | Loss: 4.687 | Acc: 45.982,64.211,69.866,% | Adaptive Acc: 66.443% | clf_exit: 0.295 0.416 0.288
Batch: 40 | Loss: 4.693 | Acc: 45.636,64.386,69.322,% | Adaptive Acc: 66.540% | clf_exit: 0.296 0.410 0.294
Batch: 60 | Loss: 4.714 | Acc: 45.325,64.293,69.045,% | Adaptive Acc: 66.265% | clf_exit: 0.296 0.410 0.294
Train classifier parameters

Epoch: 190
Batch: 0 | Loss: 2.805 | Acc: 49.219,83.594,97.656,% | Adaptive Acc: 94.531% | clf_exit: 0.234 0.406 0.359
Batch: 20 | Loss: 2.875 | Acc: 47.917,80.171,98.400,% | Adaptive Acc: 91.629% | clf_exit: 0.249 0.432 0.319
Batch: 40 | Loss: 2.865 | Acc: 48.342,80.202,98.399,% | Adaptive Acc: 91.845% | clf_exit: 0.244 0.444 0.311
Batch: 60 | Loss: 2.857 | Acc: 48.092,80.174,98.399,% | Adaptive Acc: 91.637% | clf_exit: 0.245 0.445 0.310
Batch: 80 | Loss: 2.892 | Acc: 47.791,79.900,98.409,% | Adaptive Acc: 91.686% | clf_exit: 0.238 0.449 0.313
Batch: 100 | Loss: 2.902 | Acc: 47.625,79.811,98.314,% | Adaptive Acc: 91.592% | clf_exit: 0.238 0.449 0.314
Batch: 120 | Loss: 2.906 | Acc: 47.695,79.700,98.321,% | Adaptive Acc: 91.658% | clf_exit: 0.237 0.449 0.313
Batch: 140 | Loss: 2.908 | Acc: 48.144,79.710,98.360,% | Adaptive Acc: 91.689% | clf_exit: 0.238 0.450 0.312
Batch: 160 | Loss: 2.902 | Acc: 48.083,79.852,98.336,% | Adaptive Acc: 91.751% | clf_exit: 0.238 0.450 0.311
Batch: 180 | Loss: 2.900 | Acc: 48.101,79.804,98.338,% | Adaptive Acc: 91.743% | clf_exit: 0.240 0.449 0.311
Batch: 200 | Loss: 2.903 | Acc: 47.971,79.831,98.348,% | Adaptive Acc: 91.791% | clf_exit: 0.239 0.449 0.313
Batch: 220 | Loss: 2.905 | Acc: 47.971,79.861,98.324,% | Adaptive Acc: 91.809% | clf_exit: 0.239 0.448 0.313
Batch: 240 | Loss: 2.903 | Acc: 48.065,79.934,98.347,% | Adaptive Acc: 91.792% | clf_exit: 0.238 0.449 0.312
Batch: 260 | Loss: 2.905 | Acc: 48.093,79.870,98.345,% | Adaptive Acc: 91.807% | clf_exit: 0.238 0.449 0.313
Batch: 280 | Loss: 2.910 | Acc: 47.984,79.782,98.321,% | Adaptive Acc: 91.812% | clf_exit: 0.238 0.449 0.314
Batch: 300 | Loss: 2.907 | Acc: 48.085,79.760,98.347,% | Adaptive Acc: 91.853% | clf_exit: 0.237 0.449 0.313
Batch: 320 | Loss: 2.912 | Acc: 48.055,79.717,98.362,% | Adaptive Acc: 91.822% | clf_exit: 0.237 0.449 0.314
Batch: 340 | Loss: 2.909 | Acc: 48.096,79.720,98.344,% | Adaptive Acc: 91.876% | clf_exit: 0.237 0.449 0.314
Batch: 360 | Loss: 2.908 | Acc: 48.158,79.688,98.327,% | Adaptive Acc: 91.859% | clf_exit: 0.237 0.449 0.313
Batch: 380 | Loss: 2.911 | Acc: 48.111,79.638,98.310,% | Adaptive Acc: 91.825% | clf_exit: 0.237 0.449 0.314
Batch: 0 | Loss: 4.200 | Acc: 47.656,65.625,71.094,% | Adaptive Acc: 68.750% | clf_exit: 0.320 0.445 0.234
Batch: 20 | Loss: 4.679 | Acc: 45.982,64.286,69.717,% | Adaptive Acc: 65.923% | clf_exit: 0.300 0.414 0.285
Batch: 40 | Loss: 4.681 | Acc: 45.751,64.787,69.188,% | Adaptive Acc: 66.101% | clf_exit: 0.300 0.411 0.289
Batch: 60 | Loss: 4.697 | Acc: 45.492,64.600,69.134,% | Adaptive Acc: 66.176% | clf_exit: 0.299 0.410 0.292
Train classifier parameters

Epoch: 191
Batch: 0 | Loss: 2.795 | Acc: 46.875,82.812,96.094,% | Adaptive Acc: 88.281% | clf_exit: 0.281 0.461 0.258
Batch: 20 | Loss: 2.838 | Acc: 49.405,80.841,98.661,% | Adaptive Acc: 91.555% | clf_exit: 0.242 0.462 0.296
Batch: 40 | Loss: 2.875 | Acc: 48.838,80.221,98.533,% | Adaptive Acc: 91.692% | clf_exit: 0.245 0.456 0.299
Batch: 60 | Loss: 2.899 | Acc: 48.194,79.918,98.450,% | Adaptive Acc: 91.662% | clf_exit: 0.239 0.459 0.302
Batch: 80 | Loss: 2.915 | Acc: 48.341,79.591,98.399,% | Adaptive Acc: 91.406% | clf_exit: 0.242 0.455 0.302
Batch: 100 | Loss: 2.908 | Acc: 48.175,79.571,98.407,% | Adaptive Acc: 91.414% | clf_exit: 0.243 0.456 0.302
Batch: 120 | Loss: 2.914 | Acc: 48.024,79.403,98.366,% | Adaptive Acc: 91.322% | clf_exit: 0.241 0.454 0.305
Batch: 140 | Loss: 2.917 | Acc: 47.806,79.333,98.360,% | Adaptive Acc: 91.462% | clf_exit: 0.240 0.455 0.306
Batch: 160 | Loss: 2.908 | Acc: 48.020,79.416,98.355,% | Adaptive Acc: 91.528% | clf_exit: 0.239 0.455 0.306
Batch: 180 | Loss: 2.906 | Acc: 48.101,79.463,98.386,% | Adaptive Acc: 91.557% | clf_exit: 0.239 0.454 0.307
Batch: 200 | Loss: 2.903 | Acc: 48.142,79.618,98.383,% | Adaptive Acc: 91.639% | clf_exit: 0.239 0.453 0.307
Batch: 220 | Loss: 2.907 | Acc: 48.141,79.606,98.392,% | Adaptive Acc: 91.565% | clf_exit: 0.239 0.453 0.307
Batch: 240 | Loss: 2.912 | Acc: 48.055,79.538,98.399,% | Adaptive Acc: 91.594% | clf_exit: 0.239 0.452 0.309
Batch: 260 | Loss: 2.917 | Acc: 47.947,79.562,98.396,% | Adaptive Acc: 91.592% | clf_exit: 0.239 0.452 0.309
Batch: 280 | Loss: 2.910 | Acc: 48.023,79.654,98.426,% | Adaptive Acc: 91.634% | clf_exit: 0.238 0.452 0.309
Batch: 300 | Loss: 2.904 | Acc: 48.149,79.724,98.432,% | Adaptive Acc: 91.661% | clf_exit: 0.239 0.453 0.309
Batch: 320 | Loss: 2.904 | Acc: 48.221,79.768,98.440,% | Adaptive Acc: 91.689% | clf_exit: 0.240 0.452 0.308
Batch: 340 | Loss: 2.906 | Acc: 48.224,79.747,98.435,% | Adaptive Acc: 91.667% | clf_exit: 0.239 0.452 0.309
Batch: 360 | Loss: 2.909 | Acc: 48.163,79.705,98.422,% | Adaptive Acc: 91.666% | clf_exit: 0.239 0.451 0.309
Batch: 380 | Loss: 2.904 | Acc: 48.216,79.712,98.425,% | Adaptive Acc: 91.693% | clf_exit: 0.239 0.452 0.309
Batch: 0 | Loss: 4.201 | Acc: 47.656,65.625,71.875,% | Adaptive Acc: 67.969% | clf_exit: 0.328 0.461 0.211
Batch: 20 | Loss: 4.668 | Acc: 46.131,64.583,70.052,% | Adaptive Acc: 65.997% | clf_exit: 0.302 0.413 0.286
Batch: 40 | Loss: 4.671 | Acc: 46.018,64.901,69.531,% | Adaptive Acc: 66.178% | clf_exit: 0.302 0.407 0.291
Batch: 60 | Loss: 4.689 | Acc: 45.735,64.677,69.326,% | Adaptive Acc: 66.137% | clf_exit: 0.302 0.405 0.293
Train classifier parameters

Epoch: 192
Batch: 0 | Loss: 2.843 | Acc: 52.344,76.562,97.656,% | Adaptive Acc: 92.969% | clf_exit: 0.242 0.438 0.320
Batch: 20 | Loss: 2.901 | Acc: 48.363,79.650,98.549,% | Adaptive Acc: 91.667% | clf_exit: 0.241 0.446 0.313
Batch: 40 | Loss: 2.895 | Acc: 48.075,79.668,98.571,% | Adaptive Acc: 91.921% | clf_exit: 0.242 0.453 0.305
Batch: 60 | Loss: 2.907 | Acc: 48.117,79.444,98.630,% | Adaptive Acc: 91.714% | clf_exit: 0.237 0.456 0.307
Batch: 80 | Loss: 2.913 | Acc: 47.801,79.427,98.524,% | Adaptive Acc: 91.676% | clf_exit: 0.236 0.454 0.310
Batch: 100 | Loss: 2.917 | Acc: 47.780,79.370,98.507,% | Adaptive Acc: 91.607% | clf_exit: 0.236 0.456 0.308
Batch: 120 | Loss: 2.912 | Acc: 47.695,79.455,98.586,% | Adaptive Acc: 91.645% | clf_exit: 0.236 0.455 0.308
Batch: 140 | Loss: 2.902 | Acc: 48.027,79.671,98.543,% | Adaptive Acc: 91.861% | clf_exit: 0.237 0.456 0.307
Batch: 160 | Loss: 2.899 | Acc: 48.054,79.683,98.569,% | Adaptive Acc: 91.935% | clf_exit: 0.237 0.456 0.308
Batch: 180 | Loss: 2.899 | Acc: 48.027,79.705,98.537,% | Adaptive Acc: 91.885% | clf_exit: 0.238 0.454 0.308
Batch: 200 | Loss: 2.904 | Acc: 48.053,79.586,98.488,% | Adaptive Acc: 91.830% | clf_exit: 0.237 0.454 0.309
Batch: 220 | Loss: 2.906 | Acc: 48.102,79.691,98.480,% | Adaptive Acc: 91.883% | clf_exit: 0.237 0.455 0.308
Batch: 240 | Loss: 2.899 | Acc: 48.217,79.794,98.460,% | Adaptive Acc: 91.854% | clf_exit: 0.236 0.457 0.307
Batch: 260 | Loss: 2.902 | Acc: 48.204,79.708,98.479,% | Adaptive Acc: 91.783% | clf_exit: 0.236 0.456 0.307
Batch: 280 | Loss: 2.901 | Acc: 48.221,79.796,98.479,% | Adaptive Acc: 91.779% | clf_exit: 0.236 0.456 0.307
Batch: 300 | Loss: 2.899 | Acc: 48.305,79.820,98.492,% | Adaptive Acc: 91.767% | clf_exit: 0.237 0.456 0.307
Batch: 320 | Loss: 2.898 | Acc: 48.350,79.778,98.511,% | Adaptive Acc: 91.813% | clf_exit: 0.236 0.457 0.307
Batch: 340 | Loss: 2.895 | Acc: 48.383,79.830,98.504,% | Adaptive Acc: 91.798% | clf_exit: 0.237 0.456 0.308
Batch: 360 | Loss: 2.896 | Acc: 48.364,79.919,98.513,% | Adaptive Acc: 91.869% | clf_exit: 0.237 0.455 0.308
Batch: 380 | Loss: 2.898 | Acc: 48.306,79.866,98.515,% | Adaptive Acc: 91.880% | clf_exit: 0.237 0.454 0.309
Batch: 0 | Loss: 4.222 | Acc: 46.875,64.844,75.000,% | Adaptive Acc: 69.531% | clf_exit: 0.305 0.469 0.227
Batch: 20 | Loss: 4.674 | Acc: 45.424,64.472,70.052,% | Adaptive Acc: 66.146% | clf_exit: 0.302 0.405 0.292
Batch: 40 | Loss: 4.682 | Acc: 45.636,64.425,69.512,% | Adaptive Acc: 66.101% | clf_exit: 0.300 0.403 0.297
Batch: 60 | Loss: 4.702 | Acc: 45.248,64.472,69.224,% | Adaptive Acc: 66.048% | clf_exit: 0.298 0.402 0.300
Train classifier parameters

Epoch: 193
Batch: 0 | Loss: 3.012 | Acc: 46.094,78.906,99.219,% | Adaptive Acc: 88.281% | clf_exit: 0.258 0.445 0.297
Batch: 20 | Loss: 2.839 | Acc: 48.921,80.990,98.586,% | Adaptive Acc: 92.299% | clf_exit: 0.249 0.438 0.313
Batch: 40 | Loss: 2.876 | Acc: 48.056,80.755,98.476,% | Adaptive Acc: 92.092% | clf_exit: 0.241 0.446 0.313
Batch: 60 | Loss: 2.879 | Acc: 48.117,80.379,98.463,% | Adaptive Acc: 92.098% | clf_exit: 0.242 0.448 0.310
Batch: 80 | Loss: 2.876 | Acc: 48.573,80.324,98.457,% | Adaptive Acc: 92.052% | clf_exit: 0.241 0.450 0.309
Batch: 100 | Loss: 2.891 | Acc: 48.407,80.090,98.461,% | Adaptive Acc: 92.164% | clf_exit: 0.239 0.450 0.312
Batch: 120 | Loss: 2.889 | Acc: 48.399,80.249,98.509,% | Adaptive Acc: 92.149% | clf_exit: 0.239 0.450 0.312
Batch: 140 | Loss: 2.899 | Acc: 48.476,80.020,98.493,% | Adaptive Acc: 92.138% | clf_exit: 0.240 0.448 0.312
Batch: 160 | Loss: 2.897 | Acc: 48.471,79.988,98.515,% | Adaptive Acc: 92.037% | clf_exit: 0.239 0.450 0.312
Batch: 180 | Loss: 2.894 | Acc: 48.442,80.154,98.489,% | Adaptive Acc: 92.015% | clf_exit: 0.238 0.451 0.310
Batch: 200 | Loss: 2.889 | Acc: 48.554,80.193,98.492,% | Adaptive Acc: 92.020% | clf_exit: 0.238 0.453 0.309
Batch: 220 | Loss: 2.889 | Acc: 48.593,80.214,98.522,% | Adaptive Acc: 92.131% | clf_exit: 0.237 0.453 0.310
Batch: 240 | Loss: 2.888 | Acc: 48.564,80.209,98.496,% | Adaptive Acc: 92.139% | clf_exit: 0.237 0.453 0.310
Batch: 260 | Loss: 2.886 | Acc: 48.581,80.229,98.494,% | Adaptive Acc: 92.188% | clf_exit: 0.237 0.453 0.310
Batch: 280 | Loss: 2.892 | Acc: 48.451,80.049,98.499,% | Adaptive Acc: 92.129% | clf_exit: 0.236 0.454 0.311
Batch: 300 | Loss: 2.892 | Acc: 48.448,80.090,98.505,% | Adaptive Acc: 92.136% | clf_exit: 0.236 0.452 0.311
Batch: 320 | Loss: 2.892 | Acc: 48.438,80.060,98.515,% | Adaptive Acc: 92.132% | clf_exit: 0.237 0.452 0.311
Batch: 340 | Loss: 2.894 | Acc: 48.499,80.004,98.520,% | Adaptive Acc: 92.096% | clf_exit: 0.237 0.452 0.311
Batch: 360 | Loss: 2.892 | Acc: 48.502,80.025,98.507,% | Adaptive Acc: 92.125% | clf_exit: 0.237 0.451 0.312
Batch: 380 | Loss: 2.895 | Acc: 48.425,80.028,98.497,% | Adaptive Acc: 92.091% | clf_exit: 0.238 0.451 0.311
Batch: 0 | Loss: 4.229 | Acc: 47.656,64.062,71.094,% | Adaptive Acc: 69.531% | clf_exit: 0.297 0.469 0.234
Batch: 20 | Loss: 4.664 | Acc: 45.796,64.472,70.052,% | Adaptive Acc: 66.518% | clf_exit: 0.300 0.410 0.290
Batch: 40 | Loss: 4.668 | Acc: 45.732,64.691,69.360,% | Adaptive Acc: 66.273% | clf_exit: 0.303 0.407 0.291
Batch: 60 | Loss: 4.682 | Acc: 45.466,64.626,69.083,% | Adaptive Acc: 66.099% | clf_exit: 0.301 0.406 0.293
Train classifier parameters

Epoch: 194
Batch: 0 | Loss: 2.849 | Acc: 49.219,78.906,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.281 0.406 0.312
Batch: 20 | Loss: 2.876 | Acc: 48.624,79.315,98.586,% | Adaptive Acc: 92.485% | clf_exit: 0.244 0.435 0.321
Batch: 40 | Loss: 2.899 | Acc: 48.075,79.421,98.514,% | Adaptive Acc: 91.997% | clf_exit: 0.242 0.441 0.317
Batch: 60 | Loss: 2.897 | Acc: 47.643,79.841,98.476,% | Adaptive Acc: 92.034% | clf_exit: 0.238 0.446 0.316
Batch: 80 | Loss: 2.908 | Acc: 47.849,79.813,98.409,% | Adaptive Acc: 92.014% | clf_exit: 0.237 0.446 0.318
Batch: 100 | Loss: 2.895 | Acc: 48.229,79.989,98.476,% | Adaptive Acc: 92.188% | clf_exit: 0.236 0.450 0.314
Batch: 120 | Loss: 2.904 | Acc: 47.973,79.959,98.502,% | Adaptive Acc: 92.104% | clf_exit: 0.236 0.450 0.314
Batch: 140 | Loss: 2.908 | Acc: 47.867,79.765,98.515,% | Adaptive Acc: 91.971% | clf_exit: 0.236 0.451 0.313
Batch: 160 | Loss: 2.910 | Acc: 47.841,79.751,98.471,% | Adaptive Acc: 91.950% | clf_exit: 0.237 0.450 0.313
Batch: 180 | Loss: 2.903 | Acc: 48.027,79.873,98.520,% | Adaptive Acc: 91.976% | clf_exit: 0.239 0.450 0.311
Batch: 200 | Loss: 2.902 | Acc: 48.068,79.901,98.500,% | Adaptive Acc: 91.943% | clf_exit: 0.239 0.450 0.311
Batch: 220 | Loss: 2.904 | Acc: 48.006,79.864,98.476,% | Adaptive Acc: 91.933% | clf_exit: 0.239 0.451 0.311
Batch: 240 | Loss: 2.903 | Acc: 48.036,79.879,98.525,% | Adaptive Acc: 91.996% | clf_exit: 0.238 0.451 0.311
Batch: 260 | Loss: 2.904 | Acc: 48.099,79.810,98.488,% | Adaptive Acc: 92.026% | clf_exit: 0.238 0.450 0.312
Batch: 280 | Loss: 2.902 | Acc: 48.148,79.807,98.496,% | Adaptive Acc: 92.001% | clf_exit: 0.238 0.451 0.311
Batch: 300 | Loss: 2.898 | Acc: 48.238,79.833,98.523,% | Adaptive Acc: 92.034% | clf_exit: 0.238 0.451 0.311
Batch: 320 | Loss: 2.903 | Acc: 48.204,79.804,98.528,% | Adaptive Acc: 92.010% | clf_exit: 0.238 0.452 0.310
Batch: 340 | Loss: 2.900 | Acc: 48.291,79.809,98.545,% | Adaptive Acc: 92.073% | clf_exit: 0.238 0.452 0.310
Batch: 360 | Loss: 2.899 | Acc: 48.305,79.843,98.541,% | Adaptive Acc: 92.071% | clf_exit: 0.238 0.452 0.310
Batch: 380 | Loss: 2.901 | Acc: 48.298,79.776,98.524,% | Adaptive Acc: 92.058% | clf_exit: 0.238 0.452 0.310
Batch: 0 | Loss: 4.219 | Acc: 46.875,65.625,71.875,% | Adaptive Acc: 68.750% | clf_exit: 0.320 0.445 0.234
Batch: 20 | Loss: 4.647 | Acc: 45.982,64.546,70.201,% | Adaptive Acc: 66.406% | clf_exit: 0.304 0.413 0.283
Batch: 40 | Loss: 4.658 | Acc: 45.903,64.844,69.398,% | Adaptive Acc: 66.292% | clf_exit: 0.304 0.404 0.292
Batch: 60 | Loss: 4.677 | Acc: 45.671,64.626,69.198,% | Adaptive Acc: 66.214% | clf_exit: 0.302 0.405 0.293
Train classifier parameters

Epoch: 195
Batch: 0 | Loss: 2.858 | Acc: 47.656,77.344,97.656,% | Adaptive Acc: 92.188% | clf_exit: 0.227 0.477 0.297
Batch: 20 | Loss: 2.851 | Acc: 49.442,80.320,98.586,% | Adaptive Acc: 93.006% | clf_exit: 0.231 0.468 0.301
Batch: 40 | Loss: 2.880 | Acc: 48.761,80.164,98.552,% | Adaptive Acc: 92.893% | clf_exit: 0.238 0.454 0.308
Batch: 60 | Loss: 2.881 | Acc: 48.681,80.136,98.527,% | Adaptive Acc: 92.533% | clf_exit: 0.240 0.455 0.305
Batch: 80 | Loss: 2.881 | Acc: 48.630,80.276,98.486,% | Adaptive Acc: 92.467% | clf_exit: 0.239 0.454 0.307
Batch: 100 | Loss: 2.875 | Acc: 48.778,80.206,98.468,% | Adaptive Acc: 92.311% | clf_exit: 0.240 0.452 0.308
Batch: 120 | Loss: 2.873 | Acc: 48.883,80.139,98.463,% | Adaptive Acc: 92.297% | clf_exit: 0.240 0.452 0.307
Batch: 140 | Loss: 2.875 | Acc: 48.848,79.992,98.454,% | Adaptive Acc: 92.165% | clf_exit: 0.240 0.453 0.307
Batch: 160 | Loss: 2.874 | Acc: 48.826,80.017,98.433,% | Adaptive Acc: 92.163% | clf_exit: 0.239 0.455 0.306
Batch: 180 | Loss: 2.878 | Acc: 48.822,79.929,98.368,% | Adaptive Acc: 92.144% | clf_exit: 0.238 0.456 0.306
Batch: 200 | Loss: 2.877 | Acc: 48.962,79.913,98.360,% | Adaptive Acc: 92.055% | clf_exit: 0.239 0.454 0.306
Batch: 220 | Loss: 2.880 | Acc: 48.844,79.893,98.399,% | Adaptive Acc: 92.113% | clf_exit: 0.238 0.454 0.307
Batch: 240 | Loss: 2.886 | Acc: 48.684,79.791,98.412,% | Adaptive Acc: 92.038% | clf_exit: 0.238 0.454 0.308
Batch: 260 | Loss: 2.884 | Acc: 48.740,79.810,98.414,% | Adaptive Acc: 92.008% | clf_exit: 0.238 0.455 0.306
Batch: 280 | Loss: 2.885 | Acc: 48.718,79.799,98.401,% | Adaptive Acc: 92.004% | clf_exit: 0.239 0.453 0.307
Batch: 300 | Loss: 2.891 | Acc: 48.609,79.802,98.396,% | Adaptive Acc: 92.019% | clf_exit: 0.237 0.454 0.309
Batch: 320 | Loss: 2.888 | Acc: 48.581,79.877,98.379,% | Adaptive Acc: 91.998% | clf_exit: 0.237 0.454 0.309
Batch: 340 | Loss: 2.888 | Acc: 48.545,79.885,98.403,% | Adaptive Acc: 91.954% | clf_exit: 0.237 0.455 0.308
Batch: 360 | Loss: 2.888 | Acc: 48.535,79.878,98.422,% | Adaptive Acc: 91.980% | clf_exit: 0.238 0.453 0.309
Batch: 380 | Loss: 2.889 | Acc: 48.565,79.858,98.427,% | Adaptive Acc: 92.015% | clf_exit: 0.237 0.454 0.309
Batch: 0 | Loss: 4.189 | Acc: 46.094,65.625,73.438,% | Adaptive Acc: 71.875% | clf_exit: 0.320 0.445 0.234
Batch: 20 | Loss: 4.650 | Acc: 45.908,64.695,69.978,% | Adaptive Acc: 66.518% | clf_exit: 0.300 0.411 0.289
Batch: 40 | Loss: 4.653 | Acc: 45.922,64.748,69.493,% | Adaptive Acc: 66.578% | clf_exit: 0.299 0.411 0.291
Batch: 60 | Loss: 4.669 | Acc: 45.697,64.626,69.326,% | Adaptive Acc: 66.368% | clf_exit: 0.299 0.408 0.293
Train classifier parameters

Epoch: 196
Batch: 0 | Loss: 2.661 | Acc: 52.344,83.594,97.656,% | Adaptive Acc: 92.969% | clf_exit: 0.227 0.516 0.258
Batch: 20 | Loss: 2.953 | Acc: 47.024,78.274,98.400,% | Adaptive Acc: 91.592% | clf_exit: 0.237 0.451 0.312
Batch: 40 | Loss: 2.930 | Acc: 47.694,79.249,98.285,% | Adaptive Acc: 91.654% | clf_exit: 0.240 0.448 0.313
Batch: 60 | Loss: 2.903 | Acc: 48.591,79.944,98.207,% | Adaptive Acc: 91.983% | clf_exit: 0.245 0.446 0.309
Batch: 80 | Loss: 2.910 | Acc: 48.582,79.552,98.235,% | Adaptive Acc: 91.898% | clf_exit: 0.242 0.448 0.310
Batch: 100 | Loss: 2.900 | Acc: 48.755,79.765,98.275,% | Adaptive Acc: 91.901% | clf_exit: 0.243 0.447 0.309
Batch: 120 | Loss: 2.899 | Acc: 48.722,79.952,98.347,% | Adaptive Acc: 91.955% | clf_exit: 0.242 0.449 0.308
Batch: 140 | Loss: 2.900 | Acc: 48.515,79.959,98.371,% | Adaptive Acc: 92.010% | clf_exit: 0.241 0.451 0.308
Batch: 160 | Loss: 2.897 | Acc: 48.476,79.935,98.350,% | Adaptive Acc: 92.003% | clf_exit: 0.240 0.451 0.309
Batch: 180 | Loss: 2.904 | Acc: 48.386,79.864,98.373,% | Adaptive Acc: 91.985% | clf_exit: 0.238 0.452 0.310
Batch: 200 | Loss: 2.899 | Acc: 48.527,79.897,98.348,% | Adaptive Acc: 91.989% | clf_exit: 0.239 0.451 0.310
Batch: 220 | Loss: 2.892 | Acc: 48.685,80.027,98.331,% | Adaptive Acc: 92.007% | clf_exit: 0.239 0.452 0.308
Batch: 240 | Loss: 2.894 | Acc: 48.742,79.950,98.340,% | Adaptive Acc: 92.009% | clf_exit: 0.240 0.452 0.308
Batch: 260 | Loss: 2.894 | Acc: 48.764,79.909,98.345,% | Adaptive Acc: 91.984% | clf_exit: 0.240 0.452 0.308
Batch: 280 | Loss: 2.898 | Acc: 48.752,79.896,98.357,% | Adaptive Acc: 92.029% | clf_exit: 0.239 0.452 0.310
Batch: 300 | Loss: 2.894 | Acc: 48.757,79.947,98.383,% | Adaptive Acc: 92.055% | clf_exit: 0.238 0.452 0.309
Batch: 320 | Loss: 2.893 | Acc: 48.705,79.936,98.401,% | Adaptive Acc: 92.066% | clf_exit: 0.239 0.452 0.309
Batch: 340 | Loss: 2.891 | Acc: 48.678,80.036,98.394,% | Adaptive Acc: 92.061% | clf_exit: 0.239 0.453 0.309
Batch: 360 | Loss: 2.891 | Acc: 48.676,80.101,98.377,% | Adaptive Acc: 92.017% | clf_exit: 0.239 0.453 0.308
Batch: 380 | Loss: 2.891 | Acc: 48.638,80.108,98.380,% | Adaptive Acc: 92.017% | clf_exit: 0.238 0.453 0.308
Batch: 0 | Loss: 4.230 | Acc: 47.656,65.625,71.875,% | Adaptive Acc: 69.531% | clf_exit: 0.320 0.438 0.242
Batch: 20 | Loss: 4.652 | Acc: 46.057,64.583,70.126,% | Adaptive Acc: 66.295% | clf_exit: 0.297 0.411 0.292
Batch: 40 | Loss: 4.654 | Acc: 45.941,64.825,69.684,% | Adaptive Acc: 66.559% | clf_exit: 0.300 0.405 0.295
Batch: 60 | Loss: 4.672 | Acc: 45.505,64.716,69.403,% | Adaptive Acc: 66.355% | clf_exit: 0.300 0.402 0.298
Train classifier parameters

Epoch: 197
Batch: 0 | Loss: 2.522 | Acc: 54.688,82.031,98.438,% | Adaptive Acc: 94.531% | clf_exit: 0.258 0.461 0.281
Batch: 20 | Loss: 2.902 | Acc: 48.586,79.762,98.735,% | Adaptive Acc: 91.853% | clf_exit: 0.230 0.469 0.302
Batch: 40 | Loss: 2.897 | Acc: 48.876,79.478,98.780,% | Adaptive Acc: 92.245% | clf_exit: 0.226 0.461 0.313
Batch: 60 | Loss: 2.881 | Acc: 49.180,79.662,98.668,% | Adaptive Acc: 91.970% | clf_exit: 0.231 0.459 0.310
Batch: 80 | Loss: 2.861 | Acc: 49.421,79.977,98.630,% | Adaptive Acc: 91.995% | clf_exit: 0.235 0.459 0.306
Batch: 100 | Loss: 2.853 | Acc: 49.335,80.221,98.615,% | Adaptive Acc: 91.925% | clf_exit: 0.237 0.458 0.305
Batch: 120 | Loss: 2.865 | Acc: 49.329,79.939,98.631,% | Adaptive Acc: 91.955% | clf_exit: 0.236 0.459 0.305
Batch: 140 | Loss: 2.875 | Acc: 48.947,79.998,98.565,% | Adaptive Acc: 91.977% | clf_exit: 0.236 0.458 0.306
Batch: 160 | Loss: 2.886 | Acc: 48.767,79.901,98.481,% | Adaptive Acc: 91.896% | clf_exit: 0.236 0.457 0.307
Batch: 180 | Loss: 2.886 | Acc: 48.740,79.934,98.502,% | Adaptive Acc: 91.998% | clf_exit: 0.235 0.455 0.309
Batch: 200 | Loss: 2.888 | Acc: 48.756,79.890,98.476,% | Adaptive Acc: 91.939% | clf_exit: 0.237 0.453 0.310
Batch: 220 | Loss: 2.890 | Acc: 48.685,79.924,98.483,% | Adaptive Acc: 91.954% | clf_exit: 0.237 0.452 0.311
Batch: 240 | Loss: 2.898 | Acc: 48.567,79.908,98.483,% | Adaptive Acc: 91.974% | clf_exit: 0.235 0.453 0.311
Batch: 260 | Loss: 2.893 | Acc: 48.713,79.981,98.452,% | Adaptive Acc: 92.050% | clf_exit: 0.236 0.453 0.311
Batch: 280 | Loss: 2.892 | Acc: 48.727,79.993,98.474,% | Adaptive Acc: 92.074% | clf_exit: 0.237 0.452 0.311
Batch: 300 | Loss: 2.891 | Acc: 48.752,80.048,98.443,% | Adaptive Acc: 92.073% | clf_exit: 0.237 0.452 0.311
Batch: 320 | Loss: 2.889 | Acc: 48.744,80.096,98.433,% | Adaptive Acc: 92.044% | clf_exit: 0.237 0.453 0.310
Batch: 340 | Loss: 2.890 | Acc: 48.708,80.070,98.428,% | Adaptive Acc: 92.048% | clf_exit: 0.236 0.454 0.310
Batch: 360 | Loss: 2.888 | Acc: 48.715,80.112,98.412,% | Adaptive Acc: 92.062% | clf_exit: 0.236 0.454 0.310
Batch: 380 | Loss: 2.888 | Acc: 48.727,80.114,98.413,% | Adaptive Acc: 92.087% | clf_exit: 0.236 0.453 0.310
Batch: 0 | Loss: 4.219 | Acc: 47.656,66.406,73.438,% | Adaptive Acc: 71.094% | clf_exit: 0.320 0.438 0.242
Batch: 20 | Loss: 4.636 | Acc: 45.982,64.621,70.052,% | Adaptive Acc: 66.183% | clf_exit: 0.302 0.410 0.288
Batch: 40 | Loss: 4.642 | Acc: 45.808,64.634,69.569,% | Adaptive Acc: 66.387% | clf_exit: 0.303 0.403 0.294
Batch: 60 | Loss: 4.658 | Acc: 45.556,64.562,69.378,% | Adaptive Acc: 66.368% | clf_exit: 0.304 0.401 0.295
Train classifier parameters

Epoch: 198
Batch: 0 | Loss: 3.173 | Acc: 47.656,78.906,99.219,% | Adaptive Acc: 85.938% | clf_exit: 0.305 0.430 0.266
Batch: 20 | Loss: 2.884 | Acc: 47.805,79.278,98.363,% | Adaptive Acc: 90.923% | clf_exit: 0.245 0.450 0.305
Batch: 40 | Loss: 2.879 | Acc: 48.304,79.497,98.514,% | Adaptive Acc: 91.692% | clf_exit: 0.242 0.445 0.313
Batch: 60 | Loss: 2.883 | Acc: 48.335,79.803,98.425,% | Adaptive Acc: 91.919% | clf_exit: 0.239 0.449 0.312
Batch: 80 | Loss: 2.875 | Acc: 48.544,80.093,98.466,% | Adaptive Acc: 92.207% | clf_exit: 0.239 0.450 0.312
Batch: 100 | Loss: 2.882 | Acc: 48.515,79.958,98.523,% | Adaptive Acc: 92.327% | clf_exit: 0.239 0.449 0.312
Batch: 120 | Loss: 2.895 | Acc: 48.315,79.946,98.496,% | Adaptive Acc: 92.368% | clf_exit: 0.238 0.448 0.314
Batch: 140 | Loss: 2.896 | Acc: 48.266,79.876,98.476,% | Adaptive Acc: 92.271% | clf_exit: 0.238 0.448 0.314
Batch: 160 | Loss: 2.901 | Acc: 48.379,79.945,98.491,% | Adaptive Acc: 92.255% | clf_exit: 0.238 0.449 0.313
Batch: 180 | Loss: 2.893 | Acc: 48.343,80.067,98.507,% | Adaptive Acc: 92.179% | clf_exit: 0.239 0.450 0.312
Batch: 200 | Loss: 2.895 | Acc: 48.352,79.991,98.507,% | Adaptive Acc: 92.149% | clf_exit: 0.239 0.450 0.311
Batch: 220 | Loss: 2.901 | Acc: 48.261,79.878,98.491,% | Adaptive Acc: 92.081% | clf_exit: 0.239 0.449 0.312
Batch: 240 | Loss: 2.903 | Acc: 48.230,79.846,98.473,% | Adaptive Acc: 91.987% | clf_exit: 0.238 0.450 0.312
Batch: 260 | Loss: 2.900 | Acc: 48.327,79.912,98.476,% | Adaptive Acc: 91.972% | clf_exit: 0.238 0.451 0.311
Batch: 280 | Loss: 2.899 | Acc: 48.379,79.843,98.476,% | Adaptive Acc: 91.971% | clf_exit: 0.239 0.450 0.311
Batch: 300 | Loss: 2.895 | Acc: 48.393,79.885,98.479,% | Adaptive Acc: 92.027% | clf_exit: 0.239 0.450 0.311
Batch: 320 | Loss: 2.893 | Acc: 48.403,79.916,98.464,% | Adaptive Acc: 92.003% | clf_exit: 0.238 0.451 0.310
Batch: 340 | Loss: 2.890 | Acc: 48.463,79.958,98.447,% | Adaptive Acc: 92.004% | clf_exit: 0.239 0.451 0.310
Batch: 360 | Loss: 2.889 | Acc: 48.455,79.988,98.470,% | Adaptive Acc: 92.017% | clf_exit: 0.239 0.451 0.310
Batch: 380 | Loss: 2.890 | Acc: 48.485,79.958,98.487,% | Adaptive Acc: 91.999% | clf_exit: 0.239 0.451 0.310
Batch: 0 | Loss: 4.210 | Acc: 47.656,66.406,71.875,% | Adaptive Acc: 67.969% | clf_exit: 0.336 0.445 0.219
Batch: 20 | Loss: 4.637 | Acc: 46.131,65.030,69.978,% | Adaptive Acc: 66.518% | clf_exit: 0.298 0.412 0.290
Batch: 40 | Loss: 4.644 | Acc: 45.960,65.034,69.379,% | Adaptive Acc: 66.482% | clf_exit: 0.299 0.405 0.296
Batch: 60 | Loss: 4.664 | Acc: 45.645,64.869,69.211,% | Adaptive Acc: 66.342% | clf_exit: 0.299 0.404 0.297
Train classifier parameters

Epoch: 199
Batch: 0 | Loss: 2.852 | Acc: 47.656,80.469,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.172 0.516 0.312
Batch: 20 | Loss: 2.878 | Acc: 48.400,80.022,98.921,% | Adaptive Acc: 91.927% | clf_exit: 0.241 0.443 0.316
Batch: 40 | Loss: 2.885 | Acc: 49.162,80.354,98.723,% | Adaptive Acc: 92.340% | clf_exit: 0.242 0.443 0.315
Batch: 60 | Loss: 2.880 | Acc: 49.078,80.251,98.706,% | Adaptive Acc: 92.444% | clf_exit: 0.243 0.443 0.313
Batch: 80 | Loss: 2.863 | Acc: 49.209,80.430,98.621,% | Adaptive Acc: 92.361% | clf_exit: 0.244 0.445 0.311
Batch: 100 | Loss: 2.866 | Acc: 49.296,80.623,98.608,% | Adaptive Acc: 92.342% | clf_exit: 0.241 0.448 0.311
Batch: 120 | Loss: 2.873 | Acc: 49.115,80.307,98.573,% | Adaptive Acc: 92.246% | clf_exit: 0.240 0.448 0.312
Batch: 140 | Loss: 2.879 | Acc: 49.058,80.120,98.554,% | Adaptive Acc: 92.226% | clf_exit: 0.241 0.447 0.312
Batch: 160 | Loss: 2.880 | Acc: 49.097,80.076,98.525,% | Adaptive Acc: 92.188% | clf_exit: 0.239 0.450 0.311
Batch: 180 | Loss: 2.872 | Acc: 49.227,80.154,98.520,% | Adaptive Acc: 92.183% | clf_exit: 0.239 0.452 0.309
Batch: 200 | Loss: 2.877 | Acc: 49.106,80.100,98.515,% | Adaptive Acc: 92.188% | clf_exit: 0.239 0.452 0.309
Batch: 220 | Loss: 2.882 | Acc: 48.908,79.988,98.494,% | Adaptive Acc: 92.085% | clf_exit: 0.239 0.452 0.309
Batch: 240 | Loss: 2.888 | Acc: 48.788,80.051,98.496,% | Adaptive Acc: 92.116% | clf_exit: 0.238 0.452 0.310
Batch: 260 | Loss: 2.889 | Acc: 48.749,80.086,98.500,% | Adaptive Acc: 92.152% | clf_exit: 0.238 0.451 0.310
Batch: 280 | Loss: 2.888 | Acc: 48.813,80.096,98.501,% | Adaptive Acc: 92.087% | clf_exit: 0.239 0.451 0.310
Batch: 300 | Loss: 2.893 | Acc: 48.653,80.028,98.510,% | Adaptive Acc: 92.089% | clf_exit: 0.238 0.451 0.311
Batch: 320 | Loss: 2.890 | Acc: 48.754,80.048,98.511,% | Adaptive Acc: 92.083% | clf_exit: 0.238 0.451 0.310
Batch: 340 | Loss: 2.887 | Acc: 48.754,80.038,98.518,% | Adaptive Acc: 92.089% | clf_exit: 0.239 0.452 0.309
Batch: 360 | Loss: 2.886 | Acc: 48.727,80.075,98.522,% | Adaptive Acc: 92.105% | clf_exit: 0.238 0.452 0.309
Batch: 380 | Loss: 2.886 | Acc: 48.671,80.110,98.538,% | Adaptive Acc: 92.146% | clf_exit: 0.238 0.453 0.310
Batch: 0 | Loss: 4.204 | Acc: 46.875,67.188,71.875,% | Adaptive Acc: 69.531% | clf_exit: 0.312 0.461 0.227
Batch: 20 | Loss: 4.642 | Acc: 46.243,64.881,69.903,% | Adaptive Acc: 66.555% | clf_exit: 0.298 0.416 0.286
Batch: 40 | Loss: 4.642 | Acc: 46.265,65.091,69.493,% | Adaptive Acc: 66.482% | clf_exit: 0.300 0.409 0.292
Batch: 60 | Loss: 4.658 | Acc: 45.825,64.882,69.314,% | Adaptive Acc: 66.419% | clf_exit: 0.299 0.406 0.294
Train all parameters

Epoch: 200
Batch: 0 | Loss: 2.506 | Acc: 47.656,84.375,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.250 0.523 0.227
Batch: 20 | Loss: 2.660 | Acc: 51.637,83.445,99.219,% | Adaptive Acc: 92.932% | clf_exit: 0.266 0.467 0.266
Batch: 40 | Loss: 2.731 | Acc: 50.019,81.955,99.181,% | Adaptive Acc: 92.759% | clf_exit: 0.253 0.465 0.283
Batch: 60 | Loss: 2.744 | Acc: 49.795,81.737,99.219,% | Adaptive Acc: 92.559% | clf_exit: 0.253 0.464 0.283
Batch: 80 | Loss: 2.756 | Acc: 49.691,81.414,99.238,% | Adaptive Acc: 92.506% | clf_exit: 0.251 0.463 0.285
Batch: 100 | Loss: 2.755 | Acc: 49.366,81.358,99.257,% | Adaptive Acc: 92.311% | clf_exit: 0.252 0.464 0.284
Batch: 120 | Loss: 2.754 | Acc: 49.283,81.534,99.270,% | Adaptive Acc: 92.342% | clf_exit: 0.252 0.464 0.284
Batch: 140 | Loss: 2.752 | Acc: 49.590,81.466,99.291,% | Adaptive Acc: 92.431% | clf_exit: 0.251 0.466 0.284
Batch: 160 | Loss: 2.753 | Acc: 49.520,81.328,99.277,% | Adaptive Acc: 92.260% | clf_exit: 0.252 0.465 0.283
Batch: 180 | Loss: 2.750 | Acc: 49.672,81.328,99.266,% | Adaptive Acc: 92.295% | clf_exit: 0.252 0.465 0.283
Batch: 200 | Loss: 2.754 | Acc: 49.518,81.336,99.277,% | Adaptive Acc: 92.312% | clf_exit: 0.252 0.463 0.285
Batch: 220 | Loss: 2.754 | Acc: 49.519,81.381,99.268,% | Adaptive Acc: 92.354% | clf_exit: 0.253 0.462 0.285
Batch: 240 | Loss: 2.763 | Acc: 49.322,81.263,99.271,% | Adaptive Acc: 92.343% | clf_exit: 0.252 0.462 0.287
Batch: 260 | Loss: 2.761 | Acc: 49.470,81.292,99.258,% | Adaptive Acc: 92.451% | clf_exit: 0.253 0.460 0.287
Batch: 280 | Loss: 2.756 | Acc: 49.583,81.347,99.260,% | Adaptive Acc: 92.516% | clf_exit: 0.253 0.461 0.287
Batch: 300 | Loss: 2.758 | Acc: 49.486,81.346,99.245,% | Adaptive Acc: 92.455% | clf_exit: 0.252 0.460 0.287
Batch: 320 | Loss: 2.759 | Acc: 49.452,81.330,99.236,% | Adaptive Acc: 92.460% | clf_exit: 0.252 0.461 0.287
Batch: 340 | Loss: 2.764 | Acc: 49.418,81.291,99.244,% | Adaptive Acc: 92.423% | clf_exit: 0.252 0.460 0.288
Batch: 360 | Loss: 2.770 | Acc: 49.362,81.183,99.234,% | Adaptive Acc: 92.352% | clf_exit: 0.252 0.460 0.289
Batch: 380 | Loss: 2.773 | Acc: 49.284,81.150,99.233,% | Adaptive Acc: 92.374% | clf_exit: 0.251 0.460 0.289
Batch: 0 | Loss: 4.308 | Acc: 46.094,65.625,74.219,% | Adaptive Acc: 67.969% | clf_exit: 0.344 0.453 0.203
Batch: 20 | Loss: 4.520 | Acc: 45.759,66.443,71.243,% | Adaptive Acc: 67.671% | clf_exit: 0.322 0.424 0.253
Batch: 40 | Loss: 4.505 | Acc: 45.922,66.940,71.418,% | Adaptive Acc: 67.778% | clf_exit: 0.321 0.412 0.267
Batch: 60 | Loss: 4.535 | Acc: 45.799,66.393,71.017,% | Adaptive Acc: 67.354% | clf_exit: 0.320 0.412 0.268
Train all parameters

Epoch: 201
Batch: 0 | Loss: 2.591 | Acc: 48.438,87.500,98.438,% | Adaptive Acc: 92.188% | clf_exit: 0.250 0.484 0.266
Batch: 20 | Loss: 2.744 | Acc: 49.851,82.292,98.958,% | Adaptive Acc: 92.113% | clf_exit: 0.251 0.475 0.273
Batch: 40 | Loss: 2.730 | Acc: 49.600,82.546,99.143,% | Adaptive Acc: 92.759% | clf_exit: 0.251 0.464 0.285
Batch: 60 | Loss: 2.739 | Acc: 49.718,82.467,99.193,% | Adaptive Acc: 92.815% | clf_exit: 0.249 0.464 0.287
Batch: 80 | Loss: 2.744 | Acc: 49.778,82.128,99.238,% | Adaptive Acc: 92.679% | clf_exit: 0.249 0.463 0.288
Batch: 100 | Loss: 2.737 | Acc: 49.675,82.186,99.265,% | Adaptive Acc: 92.737% | clf_exit: 0.251 0.462 0.288
Batch: 120 | Loss: 2.746 | Acc: 49.658,81.947,99.277,% | Adaptive Acc: 92.588% | clf_exit: 0.251 0.462 0.287
Batch: 140 | Loss: 2.749 | Acc: 49.629,81.848,99.285,% | Adaptive Acc: 92.548% | clf_exit: 0.251 0.462 0.287
Batch: 160 | Loss: 2.752 | Acc: 49.554,81.755,99.296,% | Adaptive Acc: 92.508% | clf_exit: 0.252 0.462 0.286
Batch: 180 | Loss: 2.759 | Acc: 49.482,81.634,99.292,% | Adaptive Acc: 92.446% | clf_exit: 0.251 0.462 0.287
Batch: 200 | Loss: 2.751 | Acc: 49.631,81.740,99.262,% | Adaptive Acc: 92.510% | clf_exit: 0.251 0.463 0.285
Batch: 220 | Loss: 2.762 | Acc: 49.576,81.618,99.229,% | Adaptive Acc: 92.548% | clf_exit: 0.250 0.461 0.288
Batch: 240 | Loss: 2.766 | Acc: 49.391,81.555,99.251,% | Adaptive Acc: 92.521% | clf_exit: 0.251 0.461 0.289
Batch: 260 | Loss: 2.767 | Acc: 49.395,81.483,99.234,% | Adaptive Acc: 92.457% | clf_exit: 0.251 0.460 0.289
Batch: 280 | Loss: 2.773 | Acc: 49.394,81.339,99.241,% | Adaptive Acc: 92.424% | clf_exit: 0.251 0.460 0.290
Batch: 300 | Loss: 2.769 | Acc: 49.408,81.343,99.234,% | Adaptive Acc: 92.442% | clf_exit: 0.251 0.460 0.289
Batch: 320 | Loss: 2.771 | Acc: 49.379,81.287,99.243,% | Adaptive Acc: 92.482% | clf_exit: 0.251 0.460 0.289
Batch: 340 | Loss: 2.772 | Acc: 49.393,81.277,99.216,% | Adaptive Acc: 92.476% | clf_exit: 0.251 0.460 0.289
Batch: 360 | Loss: 2.775 | Acc: 49.390,81.265,99.186,% | Adaptive Acc: 92.449% | clf_exit: 0.250 0.460 0.289
Batch: 380 | Loss: 2.778 | Acc: 49.368,81.139,99.165,% | Adaptive Acc: 92.427% | clf_exit: 0.250 0.459 0.290
Batch: 0 | Loss: 4.298 | Acc: 50.000,68.750,75.000,% | Adaptive Acc: 70.312% | clf_exit: 0.336 0.453 0.211
Batch: 20 | Loss: 4.475 | Acc: 45.499,66.964,71.094,% | Adaptive Acc: 67.560% | clf_exit: 0.332 0.404 0.263
Batch: 40 | Loss: 4.500 | Acc: 45.903,66.540,71.075,% | Adaptive Acc: 67.359% | clf_exit: 0.325 0.400 0.275
Batch: 60 | Loss: 4.513 | Acc: 45.838,66.355,70.812,% | Adaptive Acc: 67.213% | clf_exit: 0.328 0.395 0.278
Train all parameters

Epoch: 202
Batch: 0 | Loss: 2.810 | Acc: 47.656,82.812,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.227 0.461 0.312
Batch: 20 | Loss: 2.719 | Acc: 50.781,82.478,98.847,% | Adaptive Acc: 93.080% | clf_exit: 0.246 0.476 0.278
Batch: 40 | Loss: 2.715 | Acc: 50.229,82.012,99.047,% | Adaptive Acc: 92.550% | clf_exit: 0.251 0.472 0.277
Batch: 60 | Loss: 2.738 | Acc: 49.744,82.018,99.001,% | Adaptive Acc: 92.636% | clf_exit: 0.246 0.477 0.277
Batch: 80 | Loss: 2.744 | Acc: 49.392,81.916,99.035,% | Adaptive Acc: 92.583% | clf_exit: 0.245 0.475 0.280
Batch: 100 | Loss: 2.755 | Acc: 49.513,81.606,99.064,% | Adaptive Acc: 92.644% | clf_exit: 0.245 0.471 0.284
Batch: 120 | Loss: 2.758 | Acc: 49.412,81.612,99.064,% | Adaptive Acc: 92.594% | clf_exit: 0.247 0.469 0.284
Batch: 140 | Loss: 2.760 | Acc: 49.512,81.494,98.964,% | Adaptive Acc: 92.470% | clf_exit: 0.249 0.468 0.283
Batch: 160 | Loss: 2.766 | Acc: 49.466,81.464,98.991,% | Adaptive Acc: 92.508% | clf_exit: 0.249 0.465 0.286
Batch: 180 | Loss: 2.767 | Acc: 49.288,81.526,98.977,% | Adaptive Acc: 92.494% | clf_exit: 0.248 0.467 0.286
Batch: 200 | Loss: 2.762 | Acc: 49.331,81.611,98.974,% | Adaptive Acc: 92.514% | clf_exit: 0.250 0.464 0.286
Batch: 220 | Loss: 2.758 | Acc: 49.314,81.628,99.003,% | Adaptive Acc: 92.513% | clf_exit: 0.250 0.464 0.286
Batch: 240 | Loss: 2.767 | Acc: 49.183,81.480,98.956,% | Adaptive Acc: 92.424% | clf_exit: 0.251 0.463 0.287
Batch: 260 | Loss: 2.772 | Acc: 49.105,81.406,98.970,% | Adaptive Acc: 92.421% | clf_exit: 0.251 0.462 0.288
Batch: 280 | Loss: 2.775 | Acc: 49.135,81.289,98.960,% | Adaptive Acc: 92.390% | clf_exit: 0.251 0.461 0.288
Batch: 300 | Loss: 2.780 | Acc: 49.086,81.203,98.954,% | Adaptive Acc: 92.387% | clf_exit: 0.251 0.460 0.289
Batch: 320 | Loss: 2.780 | Acc: 49.146,81.123,98.973,% | Adaptive Acc: 92.377% | clf_exit: 0.251 0.461 0.289
Batch: 340 | Loss: 2.776 | Acc: 49.278,81.110,98.985,% | Adaptive Acc: 92.378% | clf_exit: 0.251 0.461 0.288
Batch: 360 | Loss: 2.782 | Acc: 49.199,81.066,98.968,% | Adaptive Acc: 92.352% | clf_exit: 0.251 0.461 0.288
Batch: 380 | Loss: 2.783 | Acc: 49.233,81.022,98.950,% | Adaptive Acc: 92.362% | clf_exit: 0.251 0.461 0.288
Batch: 0 | Loss: 4.014 | Acc: 43.750,64.062,77.344,% | Adaptive Acc: 67.188% | clf_exit: 0.312 0.469 0.219
Batch: 20 | Loss: 4.562 | Acc: 46.168,65.290,70.536,% | Adaptive Acc: 67.299% | clf_exit: 0.301 0.439 0.260
Batch: 40 | Loss: 4.512 | Acc: 46.208,65.777,70.617,% | Adaptive Acc: 67.473% | clf_exit: 0.302 0.426 0.272
Batch: 60 | Loss: 4.524 | Acc: 45.889,65.523,70.325,% | Adaptive Acc: 67.136% | clf_exit: 0.302 0.422 0.276
Train all parameters

Epoch: 203
Batch: 0 | Loss: 2.555 | Acc: 52.344,81.250,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.289 0.445 0.266
Batch: 20 | Loss: 2.694 | Acc: 50.409,81.622,99.256,% | Adaptive Acc: 92.560% | clf_exit: 0.263 0.458 0.280
Batch: 40 | Loss: 2.697 | Acc: 50.324,82.088,99.219,% | Adaptive Acc: 92.530% | clf_exit: 0.262 0.459 0.279
Batch: 60 | Loss: 2.722 | Acc: 50.077,81.993,99.180,% | Adaptive Acc: 92.636% | clf_exit: 0.261 0.457 0.282
Batch: 80 | Loss: 2.736 | Acc: 49.894,81.800,99.142,% | Adaptive Acc: 92.699% | clf_exit: 0.259 0.456 0.285
Batch: 100 | Loss: 2.748 | Acc: 49.606,81.675,99.196,% | Adaptive Acc: 92.675% | clf_exit: 0.259 0.455 0.287
Batch: 120 | Loss: 2.759 | Acc: 49.516,81.502,99.232,% | Adaptive Acc: 92.601% | clf_exit: 0.256 0.456 0.288
Batch: 140 | Loss: 2.755 | Acc: 49.446,81.571,99.213,% | Adaptive Acc: 92.548% | clf_exit: 0.256 0.457 0.287
Batch: 160 | Loss: 2.756 | Acc: 49.311,81.565,99.224,% | Adaptive Acc: 92.503% | clf_exit: 0.254 0.458 0.288
Batch: 180 | Loss: 2.755 | Acc: 49.448,81.608,99.227,% | Adaptive Acc: 92.585% | clf_exit: 0.254 0.457 0.289
Batch: 200 | Loss: 2.759 | Acc: 49.347,81.530,99.184,% | Adaptive Acc: 92.592% | clf_exit: 0.253 0.458 0.290
Batch: 220 | Loss: 2.763 | Acc: 49.314,81.307,99.169,% | Adaptive Acc: 92.502% | clf_exit: 0.252 0.459 0.290
Batch: 240 | Loss: 2.763 | Acc: 49.384,81.334,99.180,% | Adaptive Acc: 92.577% | clf_exit: 0.251 0.459 0.290
Batch: 260 | Loss: 2.762 | Acc: 49.470,81.301,99.171,% | Adaptive Acc: 92.562% | clf_exit: 0.252 0.459 0.290
Batch: 280 | Loss: 2.762 | Acc: 49.494,81.294,99.158,% | Adaptive Acc: 92.538% | clf_exit: 0.252 0.459 0.289
Batch: 300 | Loss: 2.765 | Acc: 49.445,81.279,99.164,% | Adaptive Acc: 92.491% | clf_exit: 0.252 0.459 0.289
Batch: 320 | Loss: 2.766 | Acc: 49.387,81.260,99.160,% | Adaptive Acc: 92.482% | clf_exit: 0.252 0.459 0.289
Batch: 340 | Loss: 2.765 | Acc: 49.443,81.275,99.132,% | Adaptive Acc: 92.481% | clf_exit: 0.252 0.459 0.289
Batch: 360 | Loss: 2.766 | Acc: 49.439,81.222,99.137,% | Adaptive Acc: 92.460% | clf_exit: 0.253 0.457 0.290
Batch: 380 | Loss: 2.771 | Acc: 49.418,81.113,99.100,% | Adaptive Acc: 92.380% | clf_exit: 0.253 0.457 0.289
Batch: 0 | Loss: 4.150 | Acc: 48.438,65.625,75.000,% | Adaptive Acc: 70.312% | clf_exit: 0.328 0.422 0.250
Batch: 20 | Loss: 4.500 | Acc: 45.499,66.295,71.205,% | Adaptive Acc: 67.894% | clf_exit: 0.304 0.430 0.267
Batch: 40 | Loss: 4.487 | Acc: 46.037,66.025,70.694,% | Adaptive Acc: 67.264% | clf_exit: 0.303 0.426 0.270
Batch: 60 | Loss: 4.503 | Acc: 45.825,65.497,70.479,% | Adaptive Acc: 67.175% | clf_exit: 0.303 0.423 0.275
Train all parameters

Epoch: 204
Batch: 0 | Loss: 2.758 | Acc: 46.094,82.812,98.438,% | Adaptive Acc: 92.969% | clf_exit: 0.211 0.531 0.258
Batch: 20 | Loss: 2.758 | Acc: 48.810,81.250,99.070,% | Adaptive Acc: 91.778% | clf_exit: 0.256 0.459 0.285
Batch: 40 | Loss: 2.756 | Acc: 48.476,81.421,99.181,% | Adaptive Acc: 92.283% | clf_exit: 0.252 0.463 0.285
Batch: 60 | Loss: 2.741 | Acc: 49.168,81.737,99.257,% | Adaptive Acc: 92.405% | clf_exit: 0.255 0.458 0.288
Batch: 80 | Loss: 2.745 | Acc: 49.190,81.520,99.238,% | Adaptive Acc: 92.438% | clf_exit: 0.254 0.460 0.287
Batch: 100 | Loss: 2.748 | Acc: 49.265,81.459,99.180,% | Adaptive Acc: 92.497% | clf_exit: 0.255 0.456 0.289
Batch: 120 | Loss: 2.763 | Acc: 49.225,81.289,99.174,% | Adaptive Acc: 92.465% | clf_exit: 0.254 0.455 0.290
Batch: 140 | Loss: 2.761 | Acc: 49.418,81.256,99.191,% | Adaptive Acc: 92.531% | clf_exit: 0.254 0.455 0.291
Batch: 160 | Loss: 2.745 | Acc: 49.549,81.478,99.141,% | Adaptive Acc: 92.556% | clf_exit: 0.257 0.456 0.288
Batch: 180 | Loss: 2.747 | Acc: 49.530,81.354,99.150,% | Adaptive Acc: 92.585% | clf_exit: 0.257 0.454 0.289
Batch: 200 | Loss: 2.749 | Acc: 49.499,81.382,99.168,% | Adaptive Acc: 92.596% | clf_exit: 0.257 0.455 0.288
Batch: 220 | Loss: 2.751 | Acc: 49.463,81.328,99.198,% | Adaptive Acc: 92.661% | clf_exit: 0.256 0.456 0.289
Batch: 240 | Loss: 2.755 | Acc: 49.423,81.276,99.167,% | Adaptive Acc: 92.544% | clf_exit: 0.255 0.458 0.287
Batch: 260 | Loss: 2.759 | Acc: 49.294,81.286,99.162,% | Adaptive Acc: 92.523% | clf_exit: 0.254 0.459 0.287
Batch: 280 | Loss: 2.761 | Acc: 49.308,81.258,99.171,% | Adaptive Acc: 92.518% | clf_exit: 0.254 0.459 0.287
Batch: 300 | Loss: 2.761 | Acc: 49.247,81.221,99.169,% | Adaptive Acc: 92.494% | clf_exit: 0.253 0.459 0.287
Batch: 320 | Loss: 2.763 | Acc: 49.204,81.187,99.163,% | Adaptive Acc: 92.460% | clf_exit: 0.253 0.459 0.288
Batch: 340 | Loss: 2.768 | Acc: 49.232,81.085,99.139,% | Adaptive Acc: 92.364% | clf_exit: 0.254 0.457 0.289
Batch: 360 | Loss: 2.768 | Acc: 49.286,81.038,99.119,% | Adaptive Acc: 92.307% | clf_exit: 0.254 0.458 0.288
Batch: 380 | Loss: 2.774 | Acc: 49.196,80.967,99.094,% | Adaptive Acc: 92.247% | clf_exit: 0.253 0.458 0.289
Batch: 0 | Loss: 4.303 | Acc: 51.562,67.188,73.438,% | Adaptive Acc: 68.750% | clf_exit: 0.297 0.477 0.227
Batch: 20 | Loss: 4.636 | Acc: 44.866,65.067,70.350,% | Adaptive Acc: 66.518% | clf_exit: 0.337 0.411 0.251
Batch: 40 | Loss: 4.620 | Acc: 44.893,65.130,70.103,% | Adaptive Acc: 66.406% | clf_exit: 0.339 0.398 0.263
Batch: 60 | Loss: 4.622 | Acc: 44.775,64.869,69.967,% | Adaptive Acc: 65.920% | clf_exit: 0.336 0.401 0.263
Train all parameters

Epoch: 205
Batch: 0 | Loss: 2.839 | Acc: 46.094,82.031,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.250 0.453 0.297
Batch: 20 | Loss: 2.707 | Acc: 51.228,81.882,99.033,% | Adaptive Acc: 91.853% | clf_exit: 0.268 0.465 0.267
Batch: 40 | Loss: 2.719 | Acc: 50.934,81.784,99.047,% | Adaptive Acc: 92.149% | clf_exit: 0.263 0.459 0.278
Batch: 60 | Loss: 2.727 | Acc: 50.307,81.852,99.129,% | Adaptive Acc: 92.546% | clf_exit: 0.254 0.463 0.282
Batch: 80 | Loss: 2.726 | Acc: 49.961,81.848,99.180,% | Adaptive Acc: 92.660% | clf_exit: 0.253 0.464 0.283
Batch: 100 | Loss: 2.737 | Acc: 49.582,81.923,99.211,% | Adaptive Acc: 92.744% | clf_exit: 0.251 0.465 0.284
Batch: 120 | Loss: 2.741 | Acc: 49.542,81.909,99.206,% | Adaptive Acc: 92.627% | clf_exit: 0.252 0.462 0.286
Batch: 140 | Loss: 2.734 | Acc: 49.762,81.909,99.147,% | Adaptive Acc: 92.575% | clf_exit: 0.252 0.465 0.283
Batch: 160 | Loss: 2.737 | Acc: 49.723,81.905,99.131,% | Adaptive Acc: 92.576% | clf_exit: 0.253 0.463 0.284
Batch: 180 | Loss: 2.735 | Acc: 49.767,81.984,99.137,% | Adaptive Acc: 92.546% | clf_exit: 0.252 0.465 0.283
Batch: 200 | Loss: 2.738 | Acc: 49.693,81.899,99.125,% | Adaptive Acc: 92.607% | clf_exit: 0.252 0.464 0.284
Batch: 220 | Loss: 2.736 | Acc: 49.632,81.904,99.102,% | Adaptive Acc: 92.484% | clf_exit: 0.253 0.463 0.284
Batch: 240 | Loss: 2.740 | Acc: 49.517,81.746,99.115,% | Adaptive Acc: 92.515% | clf_exit: 0.253 0.462 0.285
Batch: 260 | Loss: 2.745 | Acc: 49.491,81.627,99.099,% | Adaptive Acc: 92.472% | clf_exit: 0.252 0.462 0.286
Batch: 280 | Loss: 2.747 | Acc: 49.383,81.573,99.094,% | Adaptive Acc: 92.479% | clf_exit: 0.251 0.464 0.285
Batch: 300 | Loss: 2.753 | Acc: 49.284,81.463,99.107,% | Adaptive Acc: 92.465% | clf_exit: 0.251 0.463 0.286
Batch: 320 | Loss: 2.760 | Acc: 49.211,81.362,99.107,% | Adaptive Acc: 92.404% | clf_exit: 0.251 0.462 0.287
Batch: 340 | Loss: 2.762 | Acc: 49.184,81.319,99.116,% | Adaptive Acc: 92.389% | clf_exit: 0.252 0.461 0.287
Batch: 360 | Loss: 2.764 | Acc: 49.227,81.239,99.128,% | Adaptive Acc: 92.395% | clf_exit: 0.253 0.460 0.288
Batch: 380 | Loss: 2.764 | Acc: 49.235,81.205,99.114,% | Adaptive Acc: 92.370% | clf_exit: 0.253 0.459 0.288
Batch: 0 | Loss: 4.015 | Acc: 45.312,71.875,76.562,% | Adaptive Acc: 69.531% | clf_exit: 0.312 0.438 0.250
Batch: 20 | Loss: 4.499 | Acc: 45.908,66.741,71.168,% | Adaptive Acc: 67.671% | clf_exit: 0.320 0.420 0.259
Batch: 40 | Loss: 4.494 | Acc: 46.265,65.777,70.636,% | Adaptive Acc: 67.054% | clf_exit: 0.323 0.406 0.271
Batch: 60 | Loss: 4.514 | Acc: 46.222,65.318,70.441,% | Adaptive Acc: 66.983% | clf_exit: 0.324 0.401 0.275
Train all parameters

Epoch: 206
Batch: 0 | Loss: 3.008 | Acc: 49.219,77.344,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.266 0.406 0.328
Batch: 20 | Loss: 2.687 | Acc: 50.112,82.254,99.554,% | Adaptive Acc: 93.118% | clf_exit: 0.247 0.471 0.283
Batch: 40 | Loss: 2.688 | Acc: 50.457,82.946,99.466,% | Adaptive Acc: 92.702% | clf_exit: 0.255 0.469 0.276
Batch: 60 | Loss: 2.699 | Acc: 50.538,82.454,99.449,% | Adaptive Acc: 92.815% | clf_exit: 0.255 0.468 0.277
Batch: 80 | Loss: 2.704 | Acc: 50.299,82.128,99.373,% | Adaptive Acc: 92.978% | clf_exit: 0.255 0.466 0.280
Batch: 100 | Loss: 2.704 | Acc: 50.340,82.031,99.420,% | Adaptive Acc: 93.077% | clf_exit: 0.256 0.463 0.281
Batch: 120 | Loss: 2.704 | Acc: 50.413,81.993,99.393,% | Adaptive Acc: 93.072% | clf_exit: 0.255 0.464 0.280
Batch: 140 | Loss: 2.717 | Acc: 50.166,81.765,99.363,% | Adaptive Acc: 92.886% | clf_exit: 0.254 0.465 0.281
Batch: 160 | Loss: 2.728 | Acc: 49.966,81.730,99.350,% | Adaptive Acc: 92.799% | clf_exit: 0.254 0.463 0.282
Batch: 180 | Loss: 2.728 | Acc: 49.905,81.578,99.344,% | Adaptive Acc: 92.757% | clf_exit: 0.254 0.463 0.283
Batch: 200 | Loss: 2.735 | Acc: 49.782,81.448,99.343,% | Adaptive Acc: 92.728% | clf_exit: 0.253 0.464 0.283
Batch: 220 | Loss: 2.735 | Acc: 49.767,81.497,99.339,% | Adaptive Acc: 92.640% | clf_exit: 0.253 0.463 0.284
Batch: 240 | Loss: 2.737 | Acc: 49.812,81.441,99.313,% | Adaptive Acc: 92.557% | clf_exit: 0.254 0.462 0.283
Batch: 260 | Loss: 2.740 | Acc: 49.841,81.474,99.309,% | Adaptive Acc: 92.541% | clf_exit: 0.254 0.462 0.284
Batch: 280 | Loss: 2.743 | Acc: 49.730,81.456,99.319,% | Adaptive Acc: 92.524% | clf_exit: 0.253 0.463 0.284
Batch: 300 | Loss: 2.753 | Acc: 49.595,81.276,99.294,% | Adaptive Acc: 92.473% | clf_exit: 0.252 0.462 0.286
Batch: 320 | Loss: 2.758 | Acc: 49.504,81.267,99.270,% | Adaptive Acc: 92.484% | clf_exit: 0.252 0.462 0.286
Batch: 340 | Loss: 2.757 | Acc: 49.478,81.241,99.262,% | Adaptive Acc: 92.446% | clf_exit: 0.253 0.461 0.286
Batch: 360 | Loss: 2.756 | Acc: 49.565,81.248,99.243,% | Adaptive Acc: 92.473% | clf_exit: 0.254 0.460 0.287
Batch: 380 | Loss: 2.760 | Acc: 49.561,81.186,99.243,% | Adaptive Acc: 92.460% | clf_exit: 0.253 0.460 0.287
Batch: 0 | Loss: 4.328 | Acc: 50.000,67.188,75.000,% | Adaptive Acc: 69.531% | clf_exit: 0.344 0.445 0.211
Batch: 20 | Loss: 4.527 | Acc: 46.019,66.257,70.573,% | Adaptive Acc: 66.741% | clf_exit: 0.343 0.411 0.245
Batch: 40 | Loss: 4.521 | Acc: 46.037,66.197,70.922,% | Adaptive Acc: 66.825% | clf_exit: 0.341 0.401 0.259
Batch: 60 | Loss: 4.528 | Acc: 45.697,65.894,70.902,% | Adaptive Acc: 66.573% | clf_exit: 0.338 0.399 0.263
Train all parameters

Epoch: 207
Batch: 0 | Loss: 2.890 | Acc: 45.312,78.125,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.219 0.430 0.352
Batch: 20 | Loss: 2.764 | Acc: 48.028,81.696,99.256,% | Adaptive Acc: 92.783% | clf_exit: 0.250 0.463 0.287
Batch: 40 | Loss: 2.770 | Acc: 48.685,81.421,99.352,% | Adaptive Acc: 92.378% | clf_exit: 0.251 0.456 0.294
Batch: 60 | Loss: 2.753 | Acc: 49.103,81.852,99.411,% | Adaptive Acc: 92.520% | clf_exit: 0.256 0.452 0.292
Batch: 80 | Loss: 2.740 | Acc: 49.238,81.867,99.402,% | Adaptive Acc: 92.554% | clf_exit: 0.255 0.457 0.288
Batch: 100 | Loss: 2.742 | Acc: 49.273,81.822,99.366,% | Adaptive Acc: 92.582% | clf_exit: 0.257 0.455 0.288
Batch: 120 | Loss: 2.742 | Acc: 49.380,81.792,99.316,% | Adaptive Acc: 92.497% | clf_exit: 0.259 0.455 0.286
Batch: 140 | Loss: 2.750 | Acc: 49.468,81.627,99.291,% | Adaptive Acc: 92.431% | clf_exit: 0.258 0.456 0.286
Batch: 160 | Loss: 2.745 | Acc: 49.427,81.803,99.243,% | Adaptive Acc: 92.469% | clf_exit: 0.257 0.458 0.284
Batch: 180 | Loss: 2.751 | Acc: 49.387,81.828,99.219,% | Adaptive Acc: 92.472% | clf_exit: 0.257 0.458 0.285
Batch: 200 | Loss: 2.748 | Acc: 49.639,81.713,99.195,% | Adaptive Acc: 92.425% | clf_exit: 0.257 0.459 0.284
Batch: 220 | Loss: 2.749 | Acc: 49.590,81.724,99.187,% | Adaptive Acc: 92.495% | clf_exit: 0.256 0.459 0.285
Batch: 240 | Loss: 2.755 | Acc: 49.465,81.639,99.193,% | Adaptive Acc: 92.499% | clf_exit: 0.254 0.459 0.287
Batch: 260 | Loss: 2.752 | Acc: 49.572,81.657,99.183,% | Adaptive Acc: 92.505% | clf_exit: 0.255 0.459 0.286
Batch: 280 | Loss: 2.749 | Acc: 49.655,81.573,99.177,% | Adaptive Acc: 92.435% | clf_exit: 0.255 0.459 0.285
Batch: 300 | Loss: 2.750 | Acc: 49.670,81.465,99.190,% | Adaptive Acc: 92.447% | clf_exit: 0.256 0.459 0.285
Batch: 320 | Loss: 2.754 | Acc: 49.645,81.462,99.170,% | Adaptive Acc: 92.438% | clf_exit: 0.255 0.459 0.286
Batch: 340 | Loss: 2.749 | Acc: 49.663,81.500,99.148,% | Adaptive Acc: 92.435% | clf_exit: 0.256 0.459 0.285
Batch: 360 | Loss: 2.747 | Acc: 49.678,81.475,99.139,% | Adaptive Acc: 92.426% | clf_exit: 0.256 0.459 0.285
Batch: 380 | Loss: 2.749 | Acc: 49.653,81.426,99.126,% | Adaptive Acc: 92.384% | clf_exit: 0.257 0.458 0.286
Batch: 0 | Loss: 4.076 | Acc: 46.094,69.531,76.562,% | Adaptive Acc: 71.094% | clf_exit: 0.305 0.469 0.227
Batch: 20 | Loss: 4.484 | Acc: 46.354,67.150,71.763,% | Adaptive Acc: 68.490% | clf_exit: 0.316 0.418 0.266
Batch: 40 | Loss: 4.448 | Acc: 46.399,67.264,71.608,% | Adaptive Acc: 68.579% | clf_exit: 0.311 0.410 0.279
Batch: 60 | Loss: 4.463 | Acc: 45.991,66.893,71.311,% | Adaptive Acc: 68.199% | clf_exit: 0.310 0.409 0.281
Train all parameters

Epoch: 208
Batch: 0 | Loss: 2.597 | Acc: 53.125,84.375,99.219,% | Adaptive Acc: 95.312% | clf_exit: 0.273 0.461 0.266
Batch: 20 | Loss: 2.702 | Acc: 49.963,82.887,99.293,% | Adaptive Acc: 93.155% | clf_exit: 0.258 0.448 0.294
Batch: 40 | Loss: 2.736 | Acc: 49.581,82.127,99.428,% | Adaptive Acc: 93.159% | clf_exit: 0.255 0.450 0.296
Batch: 60 | Loss: 2.745 | Acc: 49.795,81.788,99.360,% | Adaptive Acc: 92.892% | clf_exit: 0.253 0.454 0.293
Batch: 80 | Loss: 2.728 | Acc: 50.212,81.761,99.392,% | Adaptive Acc: 92.766% | clf_exit: 0.259 0.452 0.289
Batch: 100 | Loss: 2.742 | Acc: 49.884,81.513,99.319,% | Adaptive Acc: 92.644% | clf_exit: 0.256 0.453 0.291
Batch: 120 | Loss: 2.748 | Acc: 49.845,81.457,99.296,% | Adaptive Acc: 92.530% | clf_exit: 0.256 0.454 0.290
Batch: 140 | Loss: 2.752 | Acc: 49.762,81.339,99.280,% | Adaptive Acc: 92.476% | clf_exit: 0.255 0.456 0.290
Batch: 160 | Loss: 2.753 | Acc: 49.782,81.430,99.248,% | Adaptive Acc: 92.508% | clf_exit: 0.254 0.457 0.290
Batch: 180 | Loss: 2.747 | Acc: 49.888,81.539,99.258,% | Adaptive Acc: 92.615% | clf_exit: 0.254 0.457 0.289
Batch: 200 | Loss: 2.749 | Acc: 49.810,81.526,99.234,% | Adaptive Acc: 92.541% | clf_exit: 0.253 0.458 0.289
Batch: 220 | Loss: 2.749 | Acc: 49.862,81.501,99.226,% | Adaptive Acc: 92.513% | clf_exit: 0.254 0.459 0.287
Batch: 240 | Loss: 2.746 | Acc: 49.867,81.581,99.199,% | Adaptive Acc: 92.508% | clf_exit: 0.254 0.460 0.286
Batch: 260 | Loss: 2.741 | Acc: 49.919,81.630,99.216,% | Adaptive Acc: 92.556% | clf_exit: 0.254 0.460 0.286
Batch: 280 | Loss: 2.739 | Acc: 49.983,81.661,99.219,% | Adaptive Acc: 92.532% | clf_exit: 0.256 0.459 0.285
Batch: 300 | Loss: 2.750 | Acc: 49.792,81.660,99.214,% | Adaptive Acc: 92.530% | clf_exit: 0.254 0.460 0.286
Batch: 320 | Loss: 2.752 | Acc: 49.781,81.625,99.187,% | Adaptive Acc: 92.523% | clf_exit: 0.254 0.460 0.286
Batch: 340 | Loss: 2.753 | Acc: 49.746,81.603,99.180,% | Adaptive Acc: 92.467% | clf_exit: 0.254 0.460 0.286
Batch: 360 | Loss: 2.756 | Acc: 49.706,81.614,99.169,% | Adaptive Acc: 92.465% | clf_exit: 0.254 0.461 0.286
Batch: 380 | Loss: 2.758 | Acc: 49.692,81.609,99.153,% | Adaptive Acc: 92.458% | clf_exit: 0.253 0.461 0.286
Batch: 0 | Loss: 4.074 | Acc: 47.656,68.750,75.000,% | Adaptive Acc: 71.094% | clf_exit: 0.289 0.539 0.172
Batch: 20 | Loss: 4.556 | Acc: 45.796,66.034,70.424,% | Adaptive Acc: 67.225% | clf_exit: 0.321 0.427 0.253
Batch: 40 | Loss: 4.556 | Acc: 46.341,65.777,70.274,% | Adaptive Acc: 67.188% | clf_exit: 0.317 0.423 0.261
Batch: 60 | Loss: 4.571 | Acc: 45.850,65.228,70.159,% | Adaptive Acc: 66.726% | clf_exit: 0.314 0.418 0.268
Train all parameters

Epoch: 209
Batch: 0 | Loss: 2.658 | Acc: 47.656,84.375,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.273 0.461 0.266
Batch: 20 | Loss: 2.642 | Acc: 50.223,83.705,99.144,% | Adaptive Acc: 92.746% | clf_exit: 0.261 0.469 0.270
Batch: 40 | Loss: 2.700 | Acc: 49.295,82.622,99.238,% | Adaptive Acc: 92.340% | clf_exit: 0.255 0.472 0.273
Batch: 60 | Loss: 2.692 | Acc: 49.590,82.787,99.360,% | Adaptive Acc: 92.661% | clf_exit: 0.258 0.469 0.273
Batch: 80 | Loss: 2.696 | Acc: 49.653,82.514,99.354,% | Adaptive Acc: 92.506% | clf_exit: 0.257 0.467 0.276
Batch: 100 | Loss: 2.705 | Acc: 49.528,82.511,99.381,% | Adaptive Acc: 92.690% | clf_exit: 0.255 0.466 0.278
Batch: 120 | Loss: 2.705 | Acc: 49.613,82.483,99.412,% | Adaptive Acc: 92.698% | clf_exit: 0.258 0.466 0.277
Batch: 140 | Loss: 2.709 | Acc: 49.601,82.342,99.379,% | Adaptive Acc: 92.487% | clf_exit: 0.259 0.466 0.275
Batch: 160 | Loss: 2.711 | Acc: 49.786,82.405,99.335,% | Adaptive Acc: 92.469% | clf_exit: 0.258 0.467 0.275
Batch: 180 | Loss: 2.714 | Acc: 49.732,82.472,99.322,% | Adaptive Acc: 92.490% | clf_exit: 0.257 0.467 0.276
Batch: 200 | Loss: 2.718 | Acc: 49.708,82.319,99.320,% | Adaptive Acc: 92.467% | clf_exit: 0.257 0.466 0.277
Batch: 220 | Loss: 2.715 | Acc: 49.760,82.325,99.335,% | Adaptive Acc: 92.513% | clf_exit: 0.258 0.465 0.277
Batch: 240 | Loss: 2.714 | Acc: 49.809,82.274,99.310,% | Adaptive Acc: 92.499% | clf_exit: 0.258 0.465 0.277
Batch: 260 | Loss: 2.718 | Acc: 49.829,82.172,99.306,% | Adaptive Acc: 92.451% | clf_exit: 0.258 0.465 0.277
Batch: 280 | Loss: 2.725 | Acc: 49.797,82.006,99.316,% | Adaptive Acc: 92.404% | clf_exit: 0.258 0.464 0.278
Batch: 300 | Loss: 2.730 | Acc: 49.722,81.969,99.294,% | Adaptive Acc: 92.361% | clf_exit: 0.259 0.462 0.279
Batch: 320 | Loss: 2.732 | Acc: 49.671,81.907,99.289,% | Adaptive Acc: 92.353% | clf_exit: 0.258 0.462 0.280
Batch: 340 | Loss: 2.738 | Acc: 49.613,81.736,99.262,% | Adaptive Acc: 92.309% | clf_exit: 0.257 0.462 0.281
Batch: 360 | Loss: 2.740 | Acc: 49.636,81.674,99.258,% | Adaptive Acc: 92.302% | clf_exit: 0.257 0.462 0.281
Batch: 380 | Loss: 2.742 | Acc: 49.647,81.654,99.264,% | Adaptive Acc: 92.343% | clf_exit: 0.256 0.462 0.282
Batch: 0 | Loss: 4.133 | Acc: 47.656,67.969,75.781,% | Adaptive Acc: 68.750% | clf_exit: 0.305 0.445 0.250
Batch: 20 | Loss: 4.530 | Acc: 45.461,65.774,70.424,% | Adaptive Acc: 66.964% | clf_exit: 0.310 0.422 0.268
Batch: 40 | Loss: 4.508 | Acc: 45.884,65.282,70.732,% | Adaptive Acc: 67.092% | clf_exit: 0.307 0.417 0.276
Batch: 60 | Loss: 4.516 | Acc: 45.671,65.343,70.722,% | Adaptive Acc: 66.983% | clf_exit: 0.309 0.419 0.273
Train all parameters

Epoch: 210
Batch: 0 | Loss: 2.204 | Acc: 49.219,89.844,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.250 0.531 0.219
Batch: 20 | Loss: 2.650 | Acc: 51.190,84.115,99.368,% | Adaptive Acc: 93.750% | clf_exit: 0.251 0.480 0.268
Batch: 40 | Loss: 2.668 | Acc: 51.296,83.327,99.257,% | Adaptive Acc: 93.426% | clf_exit: 0.254 0.478 0.268
Batch: 60 | Loss: 2.680 | Acc: 50.935,82.992,99.321,% | Adaptive Acc: 93.353% | clf_exit: 0.253 0.476 0.272
Batch: 80 | Loss: 2.675 | Acc: 50.463,83.102,99.344,% | Adaptive Acc: 93.142% | clf_exit: 0.253 0.477 0.270
Batch: 100 | Loss: 2.685 | Acc: 50.255,82.805,99.319,% | Adaptive Acc: 93.015% | clf_exit: 0.253 0.475 0.273
Batch: 120 | Loss: 2.696 | Acc: 50.077,82.619,99.329,% | Adaptive Acc: 92.833% | clf_exit: 0.254 0.471 0.275
Batch: 140 | Loss: 2.699 | Acc: 50.122,82.596,99.330,% | Adaptive Acc: 92.786% | clf_exit: 0.254 0.471 0.276
Batch: 160 | Loss: 2.704 | Acc: 49.942,82.643,99.292,% | Adaptive Acc: 92.775% | clf_exit: 0.253 0.470 0.277
Batch: 180 | Loss: 2.709 | Acc: 49.987,82.489,99.314,% | Adaptive Acc: 92.792% | clf_exit: 0.254 0.467 0.278
Batch: 200 | Loss: 2.720 | Acc: 49.911,82.385,99.308,% | Adaptive Acc: 92.809% | clf_exit: 0.253 0.468 0.279
Batch: 220 | Loss: 2.724 | Acc: 49.887,82.268,99.314,% | Adaptive Acc: 92.856% | clf_exit: 0.252 0.467 0.280
Batch: 240 | Loss: 2.725 | Acc: 49.854,82.210,99.310,% | Adaptive Acc: 92.771% | clf_exit: 0.252 0.467 0.280
Batch: 260 | Loss: 2.726 | Acc: 49.746,82.214,99.312,% | Adaptive Acc: 92.714% | clf_exit: 0.252 0.468 0.280
Batch: 280 | Loss: 2.724 | Acc: 49.850,82.184,99.297,% | Adaptive Acc: 92.685% | clf_exit: 0.252 0.467 0.280
Batch: 300 | Loss: 2.724 | Acc: 49.875,82.158,99.294,% | Adaptive Acc: 92.686% | clf_exit: 0.253 0.467 0.281
Batch: 320 | Loss: 2.730 | Acc: 49.737,82.068,99.287,% | Adaptive Acc: 92.611% | clf_exit: 0.253 0.465 0.282
Batch: 340 | Loss: 2.737 | Acc: 49.654,81.992,99.281,% | Adaptive Acc: 92.577% | clf_exit: 0.252 0.466 0.282
Batch: 360 | Loss: 2.741 | Acc: 49.671,81.977,99.275,% | Adaptive Acc: 92.620% | clf_exit: 0.252 0.466 0.282
Batch: 380 | Loss: 2.740 | Acc: 49.688,81.968,99.250,% | Adaptive Acc: 92.565% | clf_exit: 0.252 0.466 0.282
Batch: 0 | Loss: 4.176 | Acc: 49.219,69.531,77.344,% | Adaptive Acc: 69.531% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.551 | Acc: 45.499,66.369,70.722,% | Adaptive Acc: 67.225% | clf_exit: 0.326 0.423 0.251
Batch: 40 | Loss: 4.538 | Acc: 45.694,65.987,70.560,% | Adaptive Acc: 67.054% | clf_exit: 0.329 0.415 0.257
Batch: 60 | Loss: 4.548 | Acc: 45.402,65.894,70.543,% | Adaptive Acc: 66.483% | clf_exit: 0.326 0.412 0.262
Train all parameters

Epoch: 211
Batch: 0 | Loss: 2.659 | Acc: 50.781,82.031,96.875,% | Adaptive Acc: 89.062% | clf_exit: 0.250 0.500 0.250
Batch: 20 | Loss: 2.741 | Acc: 49.330,81.771,98.624,% | Adaptive Acc: 91.183% | clf_exit: 0.245 0.477 0.278
Batch: 40 | Loss: 2.738 | Acc: 49.409,82.088,98.761,% | Adaptive Acc: 91.616% | clf_exit: 0.252 0.475 0.273
Batch: 60 | Loss: 2.727 | Acc: 50.000,82.018,98.847,% | Adaptive Acc: 92.123% | clf_exit: 0.253 0.468 0.279
Batch: 80 | Loss: 2.723 | Acc: 50.010,81.993,98.881,% | Adaptive Acc: 92.004% | clf_exit: 0.254 0.469 0.277
Batch: 100 | Loss: 2.740 | Acc: 49.768,81.799,98.886,% | Adaptive Acc: 92.079% | clf_exit: 0.254 0.467 0.279
Batch: 120 | Loss: 2.740 | Acc: 49.826,81.734,98.909,% | Adaptive Acc: 92.291% | clf_exit: 0.255 0.464 0.282
Batch: 140 | Loss: 2.745 | Acc: 49.773,81.621,98.980,% | Adaptive Acc: 92.260% | clf_exit: 0.256 0.463 0.282
Batch: 160 | Loss: 2.748 | Acc: 49.689,81.653,99.049,% | Adaptive Acc: 92.382% | clf_exit: 0.255 0.461 0.284
Batch: 180 | Loss: 2.744 | Acc: 49.745,81.820,99.068,% | Adaptive Acc: 92.459% | clf_exit: 0.255 0.461 0.284
Batch: 200 | Loss: 2.738 | Acc: 49.907,81.915,99.052,% | Adaptive Acc: 92.495% | clf_exit: 0.256 0.461 0.283
Batch: 220 | Loss: 2.735 | Acc: 49.809,81.879,99.035,% | Adaptive Acc: 92.435% | clf_exit: 0.257 0.461 0.282
Batch: 240 | Loss: 2.735 | Acc: 49.857,81.866,99.053,% | Adaptive Acc: 92.421% | clf_exit: 0.257 0.462 0.281
Batch: 260 | Loss: 2.734 | Acc: 49.883,81.849,99.048,% | Adaptive Acc: 92.457% | clf_exit: 0.257 0.462 0.281
Batch: 280 | Loss: 2.735 | Acc: 49.892,81.789,99.044,% | Adaptive Acc: 92.413% | clf_exit: 0.256 0.463 0.281
Batch: 300 | Loss: 2.738 | Acc: 49.849,81.728,99.040,% | Adaptive Acc: 92.361% | clf_exit: 0.255 0.463 0.281
Batch: 320 | Loss: 2.741 | Acc: 49.796,81.744,99.029,% | Adaptive Acc: 92.290% | clf_exit: 0.255 0.463 0.281
Batch: 340 | Loss: 2.741 | Acc: 49.828,81.775,99.008,% | Adaptive Acc: 92.256% | clf_exit: 0.256 0.463 0.281
Batch: 360 | Loss: 2.750 | Acc: 49.686,81.618,98.992,% | Adaptive Acc: 92.265% | clf_exit: 0.255 0.463 0.283
Batch: 380 | Loss: 2.753 | Acc: 49.627,81.535,98.977,% | Adaptive Acc: 92.204% | clf_exit: 0.255 0.462 0.283
Batch: 0 | Loss: 4.101 | Acc: 48.438,69.531,71.875,% | Adaptive Acc: 67.969% | clf_exit: 0.320 0.430 0.250
Batch: 20 | Loss: 4.535 | Acc: 45.833,66.481,71.205,% | Adaptive Acc: 67.150% | clf_exit: 0.332 0.414 0.253
Batch: 40 | Loss: 4.516 | Acc: 45.903,66.025,70.846,% | Adaptive Acc: 66.864% | clf_exit: 0.333 0.399 0.269
Batch: 60 | Loss: 4.534 | Acc: 45.799,65.702,70.300,% | Adaptive Acc: 66.509% | clf_exit: 0.332 0.398 0.270
Train all parameters

Epoch: 212
Batch: 0 | Loss: 3.073 | Acc: 41.406,78.125,98.438,% | Adaptive Acc: 92.188% | clf_exit: 0.289 0.383 0.328
Batch: 20 | Loss: 2.755 | Acc: 50.298,81.734,99.144,% | Adaptive Acc: 92.634% | clf_exit: 0.260 0.465 0.276
Batch: 40 | Loss: 2.761 | Acc: 49.733,82.127,99.162,% | Adaptive Acc: 92.702% | clf_exit: 0.253 0.470 0.277
Batch: 60 | Loss: 2.750 | Acc: 49.539,82.275,99.219,% | Adaptive Acc: 92.687% | clf_exit: 0.255 0.466 0.279
Batch: 80 | Loss: 2.739 | Acc: 49.643,82.272,99.199,% | Adaptive Acc: 92.872% | clf_exit: 0.255 0.467 0.278
Batch: 100 | Loss: 2.713 | Acc: 49.923,82.604,99.203,% | Adaptive Acc: 92.884% | clf_exit: 0.257 0.469 0.275
Batch: 120 | Loss: 2.712 | Acc: 49.968,82.541,99.238,% | Adaptive Acc: 92.807% | clf_exit: 0.258 0.467 0.275
Batch: 140 | Loss: 2.713 | Acc: 49.895,82.463,99.235,% | Adaptive Acc: 92.803% | clf_exit: 0.257 0.467 0.276
Batch: 160 | Loss: 2.721 | Acc: 49.864,82.284,99.214,% | Adaptive Acc: 92.775% | clf_exit: 0.256 0.467 0.277
Batch: 180 | Loss: 2.724 | Acc: 49.858,82.247,99.171,% | Adaptive Acc: 92.749% | clf_exit: 0.255 0.469 0.276
Batch: 200 | Loss: 2.728 | Acc: 49.813,82.218,99.164,% | Adaptive Acc: 92.755% | clf_exit: 0.254 0.469 0.277
Batch: 220 | Loss: 2.728 | Acc: 49.827,82.166,99.166,% | Adaptive Acc: 92.725% | clf_exit: 0.254 0.469 0.278
Batch: 240 | Loss: 2.726 | Acc: 49.818,82.132,99.157,% | Adaptive Acc: 92.683% | clf_exit: 0.254 0.468 0.278
Batch: 260 | Loss: 2.733 | Acc: 49.764,82.028,99.120,% | Adaptive Acc: 92.619% | clf_exit: 0.252 0.469 0.279
Batch: 280 | Loss: 2.734 | Acc: 49.725,81.937,99.085,% | Adaptive Acc: 92.524% | clf_exit: 0.252 0.468 0.279
Batch: 300 | Loss: 2.738 | Acc: 49.704,81.876,99.092,% | Adaptive Acc: 92.512% | clf_exit: 0.252 0.468 0.280
Batch: 320 | Loss: 2.738 | Acc: 49.647,81.851,99.095,% | Adaptive Acc: 92.521% | clf_exit: 0.252 0.467 0.281
Batch: 340 | Loss: 2.742 | Acc: 49.677,81.738,99.102,% | Adaptive Acc: 92.435% | clf_exit: 0.252 0.467 0.281
Batch: 360 | Loss: 2.745 | Acc: 49.693,81.685,99.093,% | Adaptive Acc: 92.443% | clf_exit: 0.252 0.466 0.282
Batch: 380 | Loss: 2.748 | Acc: 49.573,81.699,99.088,% | Adaptive Acc: 92.448% | clf_exit: 0.252 0.465 0.282
Batch: 0 | Loss: 4.157 | Acc: 49.219,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.600 | Acc: 46.019,66.034,70.759,% | Adaptive Acc: 66.034% | clf_exit: 0.321 0.414 0.265
Batch: 40 | Loss: 4.547 | Acc: 46.284,65.758,70.541,% | Adaptive Acc: 66.425% | clf_exit: 0.319 0.408 0.273
Batch: 60 | Loss: 4.562 | Acc: 45.991,65.651,70.876,% | Adaptive Acc: 66.803% | clf_exit: 0.317 0.408 0.275
Train all parameters

Epoch: 213
Batch: 0 | Loss: 2.897 | Acc: 46.875,81.250,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.227 0.484 0.289
Batch: 20 | Loss: 2.716 | Acc: 50.260,82.812,99.479,% | Adaptive Acc: 93.266% | clf_exit: 0.247 0.474 0.279
Batch: 40 | Loss: 2.718 | Acc: 49.733,82.698,99.409,% | Adaptive Acc: 92.854% | clf_exit: 0.249 0.472 0.279
Batch: 60 | Loss: 2.713 | Acc: 49.949,82.864,99.398,% | Adaptive Acc: 92.866% | clf_exit: 0.248 0.475 0.277
Batch: 80 | Loss: 2.711 | Acc: 49.769,82.735,99.363,% | Adaptive Acc: 92.814% | clf_exit: 0.248 0.476 0.276
Batch: 100 | Loss: 2.719 | Acc: 49.644,82.534,99.327,% | Adaptive Acc: 92.806% | clf_exit: 0.249 0.473 0.278
Batch: 120 | Loss: 2.738 | Acc: 49.438,81.967,99.277,% | Adaptive Acc: 92.556% | clf_exit: 0.249 0.470 0.281
Batch: 140 | Loss: 2.738 | Acc: 49.557,82.070,99.296,% | Adaptive Acc: 92.703% | clf_exit: 0.248 0.470 0.281
Batch: 160 | Loss: 2.740 | Acc: 49.670,81.857,99.306,% | Adaptive Acc: 92.561% | clf_exit: 0.250 0.468 0.282
Batch: 180 | Loss: 2.737 | Acc: 49.715,81.915,99.314,% | Adaptive Acc: 92.598% | clf_exit: 0.250 0.468 0.282
Batch: 200 | Loss: 2.738 | Acc: 49.662,81.915,99.293,% | Adaptive Acc: 92.627% | clf_exit: 0.251 0.467 0.282
Batch: 220 | Loss: 2.738 | Acc: 49.682,81.953,99.272,% | Adaptive Acc: 92.644% | clf_exit: 0.250 0.468 0.282
Batch: 240 | Loss: 2.731 | Acc: 49.857,81.960,99.280,% | Adaptive Acc: 92.632% | clf_exit: 0.252 0.467 0.281
Batch: 260 | Loss: 2.732 | Acc: 49.829,81.956,99.267,% | Adaptive Acc: 92.589% | clf_exit: 0.252 0.467 0.281
Batch: 280 | Loss: 2.731 | Acc: 49.892,81.912,99.258,% | Adaptive Acc: 92.563% | clf_exit: 0.253 0.467 0.281
Batch: 300 | Loss: 2.730 | Acc: 50.008,81.863,99.237,% | Adaptive Acc: 92.520% | clf_exit: 0.254 0.467 0.280
Batch: 320 | Loss: 2.737 | Acc: 49.864,81.776,99.226,% | Adaptive Acc: 92.506% | clf_exit: 0.252 0.467 0.281
Batch: 340 | Loss: 2.741 | Acc: 49.792,81.717,99.228,% | Adaptive Acc: 92.501% | clf_exit: 0.253 0.465 0.282
Batch: 360 | Loss: 2.741 | Acc: 49.812,81.668,99.217,% | Adaptive Acc: 92.510% | clf_exit: 0.253 0.465 0.282
Batch: 380 | Loss: 2.741 | Acc: 49.820,81.635,99.202,% | Adaptive Acc: 92.479% | clf_exit: 0.252 0.466 0.282
Batch: 0 | Loss: 4.220 | Acc: 48.438,68.750,75.000,% | Adaptive Acc: 68.750% | clf_exit: 0.305 0.477 0.219
Batch: 20 | Loss: 4.508 | Acc: 46.354,66.815,70.610,% | Adaptive Acc: 67.671% | clf_exit: 0.326 0.416 0.258
Batch: 40 | Loss: 4.481 | Acc: 46.494,66.502,70.427,% | Adaptive Acc: 67.626% | clf_exit: 0.325 0.404 0.271
Batch: 60 | Loss: 4.489 | Acc: 46.158,66.240,70.594,% | Adaptive Acc: 67.636% | clf_exit: 0.323 0.403 0.274
Train all parameters

Epoch: 214
Batch: 0 | Loss: 2.323 | Acc: 56.250,87.500,99.219,% | Adaptive Acc: 95.312% | clf_exit: 0.297 0.484 0.219
Batch: 20 | Loss: 2.705 | Acc: 50.484,82.292,99.182,% | Adaptive Acc: 92.597% | clf_exit: 0.261 0.475 0.264
Batch: 40 | Loss: 2.678 | Acc: 50.133,83.060,99.181,% | Adaptive Acc: 92.988% | clf_exit: 0.256 0.482 0.262
Batch: 60 | Loss: 2.686 | Acc: 50.192,82.646,99.206,% | Adaptive Acc: 93.148% | clf_exit: 0.256 0.472 0.272
Batch: 80 | Loss: 2.694 | Acc: 49.932,82.571,99.190,% | Adaptive Acc: 92.930% | clf_exit: 0.253 0.475 0.272
Batch: 100 | Loss: 2.714 | Acc: 49.636,82.433,99.165,% | Adaptive Acc: 93.062% | clf_exit: 0.252 0.470 0.278
Batch: 120 | Loss: 2.722 | Acc: 49.496,82.361,99.135,% | Adaptive Acc: 92.995% | clf_exit: 0.249 0.473 0.278
Batch: 140 | Loss: 2.714 | Acc: 49.579,82.441,99.152,% | Adaptive Acc: 92.919% | clf_exit: 0.250 0.473 0.277
Batch: 160 | Loss: 2.717 | Acc: 49.539,82.415,99.151,% | Adaptive Acc: 92.823% | clf_exit: 0.250 0.474 0.277
Batch: 180 | Loss: 2.723 | Acc: 49.542,82.420,99.145,% | Adaptive Acc: 92.835% | clf_exit: 0.250 0.473 0.277
Batch: 200 | Loss: 2.722 | Acc: 49.705,82.334,99.153,% | Adaptive Acc: 92.844% | clf_exit: 0.249 0.474 0.277
Batch: 220 | Loss: 2.727 | Acc: 49.717,82.282,99.141,% | Adaptive Acc: 92.803% | clf_exit: 0.248 0.474 0.278
Batch: 240 | Loss: 2.726 | Acc: 49.861,82.232,99.147,% | Adaptive Acc: 92.794% | clf_exit: 0.250 0.472 0.279
Batch: 260 | Loss: 2.730 | Acc: 49.784,82.127,99.144,% | Adaptive Acc: 92.678% | clf_exit: 0.251 0.470 0.279
Batch: 280 | Loss: 2.734 | Acc: 49.775,82.023,99.144,% | Adaptive Acc: 92.618% | clf_exit: 0.251 0.470 0.279
Batch: 300 | Loss: 2.735 | Acc: 49.787,81.920,99.156,% | Adaptive Acc: 92.543% | clf_exit: 0.252 0.468 0.279
Batch: 320 | Loss: 2.740 | Acc: 49.686,81.800,99.151,% | Adaptive Acc: 92.526% | clf_exit: 0.252 0.468 0.280
Batch: 340 | Loss: 2.742 | Acc: 49.672,81.729,99.145,% | Adaptive Acc: 92.501% | clf_exit: 0.252 0.467 0.281
Batch: 360 | Loss: 2.741 | Acc: 49.708,81.676,99.134,% | Adaptive Acc: 92.473% | clf_exit: 0.253 0.467 0.281
Batch: 380 | Loss: 2.735 | Acc: 49.768,81.699,99.137,% | Adaptive Acc: 92.458% | clf_exit: 0.253 0.467 0.280
Batch: 0 | Loss: 4.292 | Acc: 50.000,67.188,74.219,% | Adaptive Acc: 68.750% | clf_exit: 0.352 0.414 0.234
Batch: 20 | Loss: 4.573 | Acc: 46.466,65.997,70.461,% | Adaptive Acc: 66.890% | clf_exit: 0.329 0.423 0.248
Batch: 40 | Loss: 4.514 | Acc: 46.399,66.292,70.274,% | Adaptive Acc: 66.787% | clf_exit: 0.327 0.414 0.259
Batch: 60 | Loss: 4.537 | Acc: 46.440,66.086,70.377,% | Adaptive Acc: 66.816% | clf_exit: 0.325 0.411 0.264
Train all parameters

Epoch: 215
Batch: 0 | Loss: 2.649 | Acc: 53.906,85.938,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.281 0.438 0.281
Batch: 20 | Loss: 2.669 | Acc: 51.042,82.999,99.219,% | Adaptive Acc: 93.638% | clf_exit: 0.263 0.456 0.282
Batch: 40 | Loss: 2.725 | Acc: 50.343,82.927,99.238,% | Adaptive Acc: 93.064% | clf_exit: 0.255 0.464 0.281
Batch: 60 | Loss: 2.700 | Acc: 50.551,83.145,99.244,% | Adaptive Acc: 93.122% | clf_exit: 0.253 0.470 0.277
Batch: 80 | Loss: 2.698 | Acc: 50.444,83.362,99.180,% | Adaptive Acc: 93.113% | clf_exit: 0.254 0.470 0.276
Batch: 100 | Loss: 2.709 | Acc: 50.271,83.099,99.196,% | Adaptive Acc: 93.085% | clf_exit: 0.256 0.467 0.277
Batch: 120 | Loss: 2.717 | Acc: 50.103,82.877,99.167,% | Adaptive Acc: 92.911% | clf_exit: 0.255 0.468 0.278
Batch: 140 | Loss: 2.726 | Acc: 49.983,82.585,99.158,% | Adaptive Acc: 92.808% | clf_exit: 0.253 0.467 0.279
Batch: 160 | Loss: 2.730 | Acc: 50.112,82.385,99.170,% | Adaptive Acc: 92.707% | clf_exit: 0.253 0.466 0.281
Batch: 180 | Loss: 2.729 | Acc: 50.186,82.351,99.206,% | Adaptive Acc: 92.775% | clf_exit: 0.253 0.466 0.281
Batch: 200 | Loss: 2.729 | Acc: 50.148,82.191,99.242,% | Adaptive Acc: 92.743% | clf_exit: 0.253 0.466 0.281
Batch: 220 | Loss: 2.736 | Acc: 49.993,82.038,99.208,% | Adaptive Acc: 92.608% | clf_exit: 0.252 0.466 0.282
Batch: 240 | Loss: 2.735 | Acc: 49.990,81.999,99.212,% | Adaptive Acc: 92.654% | clf_exit: 0.252 0.465 0.283
Batch: 260 | Loss: 2.741 | Acc: 49.844,81.831,99.192,% | Adaptive Acc: 92.607% | clf_exit: 0.251 0.465 0.284
Batch: 280 | Loss: 2.745 | Acc: 49.847,81.745,99.171,% | Adaptive Acc: 92.527% | clf_exit: 0.251 0.465 0.284
Batch: 300 | Loss: 2.746 | Acc: 49.870,81.746,99.156,% | Adaptive Acc: 92.421% | clf_exit: 0.252 0.465 0.283
Batch: 320 | Loss: 2.740 | Acc: 49.908,81.844,99.141,% | Adaptive Acc: 92.436% | clf_exit: 0.252 0.466 0.282
Batch: 340 | Loss: 2.741 | Acc: 49.881,81.765,99.141,% | Adaptive Acc: 92.428% | clf_exit: 0.252 0.465 0.283
Batch: 360 | Loss: 2.741 | Acc: 49.849,81.782,99.117,% | Adaptive Acc: 92.391% | clf_exit: 0.253 0.464 0.283
Batch: 380 | Loss: 2.745 | Acc: 49.758,81.748,99.108,% | Adaptive Acc: 92.366% | clf_exit: 0.253 0.464 0.283
Batch: 0 | Loss: 3.950 | Acc: 47.656,67.969,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.445 0.219
Batch: 20 | Loss: 4.484 | Acc: 46.057,66.481,71.726,% | Adaptive Acc: 68.452% | clf_exit: 0.315 0.398 0.287
Batch: 40 | Loss: 4.487 | Acc: 46.437,65.911,70.865,% | Adaptive Acc: 67.607% | clf_exit: 0.317 0.391 0.291
Batch: 60 | Loss: 4.513 | Acc: 45.991,65.715,70.505,% | Adaptive Acc: 67.303% | clf_exit: 0.317 0.390 0.293
Train all parameters

Epoch: 216
Batch: 0 | Loss: 2.691 | Acc: 56.250,82.031,98.438,% | Adaptive Acc: 91.406% | clf_exit: 0.258 0.492 0.250
Batch: 20 | Loss: 2.698 | Acc: 50.893,82.812,99.330,% | Adaptive Acc: 93.229% | clf_exit: 0.251 0.470 0.279
Batch: 40 | Loss: 2.695 | Acc: 50.743,83.079,99.333,% | Adaptive Acc: 93.102% | clf_exit: 0.255 0.470 0.275
Batch: 60 | Loss: 2.697 | Acc: 50.564,82.851,99.372,% | Adaptive Acc: 92.918% | clf_exit: 0.257 0.468 0.275
Batch: 80 | Loss: 2.696 | Acc: 50.289,82.764,99.402,% | Adaptive Acc: 92.901% | clf_exit: 0.257 0.466 0.276
Batch: 100 | Loss: 2.690 | Acc: 50.340,82.890,99.312,% | Adaptive Acc: 92.953% | clf_exit: 0.258 0.465 0.277
Batch: 120 | Loss: 2.698 | Acc: 50.239,82.800,99.251,% | Adaptive Acc: 92.930% | clf_exit: 0.258 0.464 0.278
Batch: 140 | Loss: 2.697 | Acc: 50.416,82.524,99.252,% | Adaptive Acc: 92.814% | clf_exit: 0.259 0.464 0.277
Batch: 160 | Loss: 2.710 | Acc: 50.233,82.381,99.262,% | Adaptive Acc: 92.702% | clf_exit: 0.259 0.463 0.278
Batch: 180 | Loss: 2.714 | Acc: 50.155,82.273,99.249,% | Adaptive Acc: 92.541% | clf_exit: 0.260 0.463 0.278
Batch: 200 | Loss: 2.715 | Acc: 50.233,82.187,99.223,% | Adaptive Acc: 92.487% | clf_exit: 0.260 0.461 0.279
Batch: 220 | Loss: 2.716 | Acc: 50.262,82.265,99.240,% | Adaptive Acc: 92.488% | clf_exit: 0.259 0.462 0.279
Batch: 240 | Loss: 2.723 | Acc: 50.237,82.148,99.212,% | Adaptive Acc: 92.427% | clf_exit: 0.259 0.462 0.280
Batch: 260 | Loss: 2.720 | Acc: 50.332,82.244,99.222,% | Adaptive Acc: 92.478% | clf_exit: 0.258 0.463 0.279
Batch: 280 | Loss: 2.713 | Acc: 50.464,82.268,99.222,% | Adaptive Acc: 92.518% | clf_exit: 0.259 0.462 0.279
Batch: 300 | Loss: 2.717 | Acc: 50.376,82.241,99.206,% | Adaptive Acc: 92.457% | clf_exit: 0.259 0.462 0.279
Batch: 320 | Loss: 2.723 | Acc: 50.290,82.141,99.219,% | Adaptive Acc: 92.463% | clf_exit: 0.258 0.462 0.279
Batch: 340 | Loss: 2.727 | Acc: 50.213,82.091,99.196,% | Adaptive Acc: 92.412% | clf_exit: 0.257 0.463 0.279
Batch: 360 | Loss: 2.733 | Acc: 50.074,81.990,99.188,% | Adaptive Acc: 92.380% | clf_exit: 0.257 0.463 0.281
Batch: 380 | Loss: 2.737 | Acc: 49.943,81.951,99.172,% | Adaptive Acc: 92.313% | clf_exit: 0.257 0.463 0.280
Batch: 0 | Loss: 3.940 | Acc: 50.000,68.750,77.344,% | Adaptive Acc: 70.312% | clf_exit: 0.328 0.492 0.180
Batch: 20 | Loss: 4.475 | Acc: 46.391,66.927,70.982,% | Adaptive Acc: 67.336% | clf_exit: 0.329 0.413 0.259
Batch: 40 | Loss: 4.485 | Acc: 46.780,65.968,70.675,% | Adaptive Acc: 67.302% | clf_exit: 0.330 0.401 0.269
Batch: 60 | Loss: 4.478 | Acc: 46.529,65.971,70.453,% | Adaptive Acc: 67.085% | clf_exit: 0.330 0.399 0.271
Train all parameters

Epoch: 217
Batch: 0 | Loss: 2.580 | Acc: 52.344,87.500,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.266 0.461 0.273
Batch: 20 | Loss: 2.717 | Acc: 49.256,83.073,99.144,% | Adaptive Acc: 93.341% | clf_exit: 0.259 0.475 0.266
Batch: 40 | Loss: 2.693 | Acc: 49.638,83.022,99.143,% | Adaptive Acc: 93.064% | clf_exit: 0.258 0.474 0.269
Batch: 60 | Loss: 2.694 | Acc: 49.834,82.761,99.155,% | Adaptive Acc: 92.828% | clf_exit: 0.259 0.474 0.267
Batch: 80 | Loss: 2.689 | Acc: 49.807,83.073,99.151,% | Adaptive Acc: 92.882% | clf_exit: 0.259 0.473 0.268
Batch: 100 | Loss: 2.682 | Acc: 50.023,82.967,99.211,% | Adaptive Acc: 93.038% | clf_exit: 0.256 0.475 0.268
Batch: 120 | Loss: 2.695 | Acc: 50.058,82.735,99.219,% | Adaptive Acc: 92.859% | clf_exit: 0.256 0.475 0.269
Batch: 140 | Loss: 2.710 | Acc: 49.967,82.364,99.141,% | Adaptive Acc: 92.769% | clf_exit: 0.256 0.471 0.273
Batch: 160 | Loss: 2.710 | Acc: 50.005,82.313,99.131,% | Adaptive Acc: 92.687% | clf_exit: 0.254 0.472 0.274
Batch: 180 | Loss: 2.713 | Acc: 50.004,82.131,99.150,% | Adaptive Acc: 92.550% | clf_exit: 0.256 0.471 0.273
Batch: 200 | Loss: 2.728 | Acc: 49.763,81.938,99.145,% | Adaptive Acc: 92.510% | clf_exit: 0.254 0.470 0.276
Batch: 220 | Loss: 2.729 | Acc: 49.742,81.978,99.127,% | Adaptive Acc: 92.421% | clf_exit: 0.255 0.470 0.275
Batch: 240 | Loss: 2.728 | Acc: 49.741,81.999,99.099,% | Adaptive Acc: 92.421% | clf_exit: 0.255 0.470 0.275
Batch: 260 | Loss: 2.725 | Acc: 49.892,81.974,99.096,% | Adaptive Acc: 92.379% | clf_exit: 0.256 0.469 0.275
Batch: 280 | Loss: 2.726 | Acc: 49.839,81.992,99.091,% | Adaptive Acc: 92.404% | clf_exit: 0.256 0.468 0.276
Batch: 300 | Loss: 2.723 | Acc: 49.948,81.904,99.071,% | Adaptive Acc: 92.403% | clf_exit: 0.256 0.468 0.276
Batch: 320 | Loss: 2.724 | Acc: 49.990,81.917,99.061,% | Adaptive Acc: 92.353% | clf_exit: 0.257 0.468 0.276
Batch: 340 | Loss: 2.730 | Acc: 49.970,81.804,99.040,% | Adaptive Acc: 92.332% | clf_exit: 0.256 0.467 0.277
Batch: 360 | Loss: 2.735 | Acc: 49.881,81.704,99.024,% | Adaptive Acc: 92.363% | clf_exit: 0.255 0.466 0.278
Batch: 380 | Loss: 2.733 | Acc: 49.869,81.679,99.024,% | Adaptive Acc: 92.335% | clf_exit: 0.256 0.466 0.278
Batch: 0 | Loss: 4.050 | Acc: 47.656,71.094,75.781,% | Adaptive Acc: 71.875% | clf_exit: 0.320 0.477 0.203
Batch: 20 | Loss: 4.515 | Acc: 45.945,66.481,71.503,% | Adaptive Acc: 67.299% | clf_exit: 0.326 0.411 0.263
Batch: 40 | Loss: 4.499 | Acc: 46.094,66.311,70.960,% | Adaptive Acc: 67.416% | clf_exit: 0.326 0.401 0.273
Batch: 60 | Loss: 4.500 | Acc: 45.978,65.868,70.710,% | Adaptive Acc: 67.226% | clf_exit: 0.324 0.402 0.274
Train all parameters

Epoch: 218
Batch: 0 | Loss: 2.750 | Acc: 46.875,82.812,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.273 0.469 0.258
Batch: 20 | Loss: 2.690 | Acc: 49.926,83.631,99.330,% | Adaptive Acc: 92.857% | clf_exit: 0.251 0.480 0.269
Batch: 40 | Loss: 2.711 | Acc: 49.867,82.851,99.162,% | Adaptive Acc: 92.397% | clf_exit: 0.256 0.469 0.274
Batch: 60 | Loss: 2.699 | Acc: 50.205,82.659,99.129,% | Adaptive Acc: 92.162% | clf_exit: 0.258 0.469 0.273
Batch: 80 | Loss: 2.712 | Acc: 50.029,82.610,99.064,% | Adaptive Acc: 92.342% | clf_exit: 0.256 0.470 0.274
Batch: 100 | Loss: 2.710 | Acc: 49.992,82.573,99.095,% | Adaptive Acc: 92.404% | clf_exit: 0.254 0.470 0.276
Batch: 120 | Loss: 2.727 | Acc: 49.793,82.496,99.077,% | Adaptive Acc: 92.568% | clf_exit: 0.253 0.468 0.280
Batch: 140 | Loss: 2.739 | Acc: 49.568,82.236,99.097,% | Adaptive Acc: 92.514% | clf_exit: 0.252 0.467 0.282
Batch: 160 | Loss: 2.737 | Acc: 49.641,82.206,99.131,% | Adaptive Acc: 92.484% | clf_exit: 0.253 0.466 0.281
Batch: 180 | Loss: 2.731 | Acc: 49.858,82.169,99.085,% | Adaptive Acc: 92.498% | clf_exit: 0.252 0.467 0.280
Batch: 200 | Loss: 2.724 | Acc: 49.938,82.214,99.118,% | Adaptive Acc: 92.498% | clf_exit: 0.253 0.467 0.280
Batch: 220 | Loss: 2.726 | Acc: 50.025,82.060,99.141,% | Adaptive Acc: 92.421% | clf_exit: 0.254 0.465 0.280
Batch: 240 | Loss: 2.721 | Acc: 50.143,82.116,99.151,% | Adaptive Acc: 92.495% | clf_exit: 0.254 0.466 0.280
Batch: 260 | Loss: 2.727 | Acc: 50.024,82.010,99.138,% | Adaptive Acc: 92.463% | clf_exit: 0.255 0.464 0.281
Batch: 280 | Loss: 2.729 | Acc: 49.911,82.037,99.146,% | Adaptive Acc: 92.482% | clf_exit: 0.255 0.464 0.281
Batch: 300 | Loss: 2.726 | Acc: 49.938,82.039,99.149,% | Adaptive Acc: 92.509% | clf_exit: 0.256 0.464 0.281
Batch: 320 | Loss: 2.729 | Acc: 49.927,81.997,99.155,% | Adaptive Acc: 92.514% | clf_exit: 0.256 0.463 0.281
Batch: 340 | Loss: 2.729 | Acc: 49.835,81.997,99.171,% | Adaptive Acc: 92.492% | clf_exit: 0.255 0.464 0.281
Batch: 360 | Loss: 2.726 | Acc: 49.887,81.994,99.162,% | Adaptive Acc: 92.469% | clf_exit: 0.256 0.464 0.280
Batch: 380 | Loss: 2.728 | Acc: 49.834,81.951,99.153,% | Adaptive Acc: 92.470% | clf_exit: 0.256 0.464 0.280
Batch: 0 | Loss: 4.271 | Acc: 46.094,64.062,74.219,% | Adaptive Acc: 71.094% | clf_exit: 0.320 0.406 0.273
Batch: 20 | Loss: 4.643 | Acc: 45.573,64.732,69.866,% | Adaptive Acc: 65.588% | clf_exit: 0.330 0.426 0.244
Batch: 40 | Loss: 4.578 | Acc: 46.094,65.377,70.522,% | Adaptive Acc: 66.521% | clf_exit: 0.326 0.420 0.254
Batch: 60 | Loss: 4.604 | Acc: 45.825,65.292,70.056,% | Adaptive Acc: 66.317% | clf_exit: 0.323 0.422 0.255
Train all parameters

Epoch: 219
Batch: 0 | Loss: 2.456 | Acc: 57.812,85.156,99.219,% | Adaptive Acc: 91.406% | clf_exit: 0.242 0.516 0.242
Batch: 20 | Loss: 2.727 | Acc: 50.521,82.738,99.368,% | Adaptive Acc: 93.118% | clf_exit: 0.242 0.483 0.275
Batch: 40 | Loss: 2.689 | Acc: 50.095,83.327,99.371,% | Adaptive Acc: 92.721% | clf_exit: 0.255 0.474 0.272
Batch: 60 | Loss: 2.699 | Acc: 50.410,82.889,99.360,% | Adaptive Acc: 92.853% | clf_exit: 0.253 0.472 0.274
Batch: 80 | Loss: 2.702 | Acc: 50.183,82.899,99.296,% | Adaptive Acc: 92.795% | clf_exit: 0.253 0.472 0.275
Batch: 100 | Loss: 2.691 | Acc: 50.077,82.689,99.288,% | Adaptive Acc: 92.652% | clf_exit: 0.255 0.472 0.273
Batch: 120 | Loss: 2.695 | Acc: 50.297,82.580,99.264,% | Adaptive Acc: 92.497% | clf_exit: 0.255 0.474 0.272
Batch: 140 | Loss: 2.705 | Acc: 50.233,82.380,99.285,% | Adaptive Acc: 92.559% | clf_exit: 0.253 0.472 0.275
Batch: 160 | Loss: 2.723 | Acc: 49.951,82.342,99.238,% | Adaptive Acc: 92.581% | clf_exit: 0.251 0.471 0.278
Batch: 180 | Loss: 2.725 | Acc: 50.091,82.238,99.214,% | Adaptive Acc: 92.511% | clf_exit: 0.252 0.471 0.278
Batch: 200 | Loss: 2.733 | Acc: 49.895,82.093,99.195,% | Adaptive Acc: 92.502% | clf_exit: 0.252 0.469 0.279
Batch: 220 | Loss: 2.729 | Acc: 49.862,82.123,99.222,% | Adaptive Acc: 92.545% | clf_exit: 0.253 0.468 0.279
Batch: 240 | Loss: 2.735 | Acc: 49.705,82.086,99.216,% | Adaptive Acc: 92.470% | clf_exit: 0.252 0.468 0.280
Batch: 260 | Loss: 2.733 | Acc: 49.695,82.064,99.180,% | Adaptive Acc: 92.457% | clf_exit: 0.253 0.467 0.280
Batch: 280 | Loss: 2.731 | Acc: 49.772,82.042,99.183,% | Adaptive Acc: 92.424% | clf_exit: 0.254 0.467 0.280
Batch: 300 | Loss: 2.736 | Acc: 49.743,81.961,99.180,% | Adaptive Acc: 92.408% | clf_exit: 0.254 0.466 0.281
Batch: 320 | Loss: 2.736 | Acc: 49.761,81.917,99.170,% | Adaptive Acc: 92.380% | clf_exit: 0.254 0.465 0.281
Batch: 340 | Loss: 2.734 | Acc: 49.810,81.951,99.161,% | Adaptive Acc: 92.336% | clf_exit: 0.254 0.466 0.280
Batch: 360 | Loss: 2.733 | Acc: 49.870,81.966,99.152,% | Adaptive Acc: 92.332% | clf_exit: 0.255 0.465 0.279
Batch: 380 | Loss: 2.735 | Acc: 49.805,81.908,99.139,% | Adaptive Acc: 92.325% | clf_exit: 0.256 0.464 0.280
Batch: 0 | Loss: 3.925 | Acc: 42.969,67.188,80.469,% | Adaptive Acc: 72.656% | clf_exit: 0.312 0.438 0.250
Batch: 20 | Loss: 4.555 | Acc: 45.945,65.513,70.647,% | Adaptive Acc: 66.629% | clf_exit: 0.342 0.412 0.247
Batch: 40 | Loss: 4.538 | Acc: 46.151,65.206,70.236,% | Adaptive Acc: 66.540% | clf_exit: 0.337 0.406 0.257
Batch: 60 | Loss: 4.560 | Acc: 45.812,65.305,69.915,% | Adaptive Acc: 66.381% | clf_exit: 0.338 0.404 0.258
Train all parameters

Epoch: 220
Batch: 0 | Loss: 2.331 | Acc: 51.562,85.938,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.336 0.445 0.219
Batch: 20 | Loss: 2.685 | Acc: 50.186,81.957,99.442,% | Adaptive Acc: 92.708% | clf_exit: 0.264 0.468 0.267
Batch: 40 | Loss: 2.684 | Acc: 50.324,81.860,99.333,% | Adaptive Acc: 92.340% | clf_exit: 0.270 0.455 0.275
Batch: 60 | Loss: 2.695 | Acc: 50.243,82.454,99.257,% | Adaptive Acc: 92.892% | clf_exit: 0.259 0.464 0.277
Batch: 80 | Loss: 2.702 | Acc: 50.289,82.369,99.228,% | Adaptive Acc: 92.863% | clf_exit: 0.259 0.467 0.274
Batch: 100 | Loss: 2.694 | Acc: 50.263,82.573,99.196,% | Adaptive Acc: 92.760% | clf_exit: 0.259 0.468 0.273
Batch: 120 | Loss: 2.692 | Acc: 50.200,82.670,99.180,% | Adaptive Acc: 92.762% | clf_exit: 0.260 0.467 0.273
Batch: 140 | Loss: 2.694 | Acc: 50.094,82.774,99.119,% | Adaptive Acc: 92.697% | clf_exit: 0.261 0.464 0.274
Batch: 160 | Loss: 2.685 | Acc: 50.243,82.788,99.131,% | Adaptive Acc: 92.571% | clf_exit: 0.263 0.464 0.272
Batch: 180 | Loss: 2.694 | Acc: 50.078,82.730,99.094,% | Adaptive Acc: 92.524% | clf_exit: 0.262 0.466 0.273
Batch: 200 | Loss: 2.696 | Acc: 50.101,82.649,99.094,% | Adaptive Acc: 92.467% | clf_exit: 0.263 0.464 0.273
Batch: 220 | Loss: 2.702 | Acc: 50.071,82.420,99.070,% | Adaptive Acc: 92.414% | clf_exit: 0.262 0.464 0.274
Batch: 240 | Loss: 2.705 | Acc: 50.042,82.362,99.060,% | Adaptive Acc: 92.405% | clf_exit: 0.262 0.463 0.275
Batch: 260 | Loss: 2.707 | Acc: 50.114,82.349,99.087,% | Adaptive Acc: 92.487% | clf_exit: 0.261 0.464 0.276
Batch: 280 | Loss: 2.711 | Acc: 50.108,82.301,99.102,% | Adaptive Acc: 92.479% | clf_exit: 0.259 0.464 0.276
Batch: 300 | Loss: 2.711 | Acc: 50.070,82.174,99.118,% | Adaptive Acc: 92.450% | clf_exit: 0.258 0.465 0.277
Batch: 320 | Loss: 2.713 | Acc: 49.988,82.185,99.124,% | Adaptive Acc: 92.458% | clf_exit: 0.259 0.464 0.277
Batch: 340 | Loss: 2.713 | Acc: 49.966,82.182,99.106,% | Adaptive Acc: 92.451% | clf_exit: 0.259 0.464 0.277
Batch: 360 | Loss: 2.714 | Acc: 49.900,82.155,99.095,% | Adaptive Acc: 92.454% | clf_exit: 0.258 0.465 0.278
Batch: 380 | Loss: 2.719 | Acc: 49.865,82.087,99.081,% | Adaptive Acc: 92.427% | clf_exit: 0.258 0.464 0.278
Batch: 0 | Loss: 4.531 | Acc: 46.094,67.188,74.219,% | Adaptive Acc: 67.188% | clf_exit: 0.320 0.398 0.281
Batch: 20 | Loss: 4.683 | Acc: 44.085,65.067,70.164,% | Adaptive Acc: 67.113% | clf_exit: 0.298 0.419 0.284
Batch: 40 | Loss: 4.652 | Acc: 43.998,64.806,70.046,% | Adaptive Acc: 66.902% | clf_exit: 0.301 0.406 0.293
Batch: 60 | Loss: 4.659 | Acc: 43.955,64.690,69.954,% | Adaptive Acc: 66.304% | clf_exit: 0.302 0.404 0.294
Train all parameters

Epoch: 221
Batch: 0 | Loss: 2.575 | Acc: 50.000,84.375,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.289 0.445 0.266
Batch: 20 | Loss: 2.594 | Acc: 52.567,84.115,99.330,% | Adaptive Acc: 93.452% | clf_exit: 0.266 0.473 0.261
Batch: 40 | Loss: 2.645 | Acc: 51.353,83.498,99.333,% | Adaptive Acc: 92.797% | clf_exit: 0.259 0.479 0.261
Batch: 60 | Loss: 2.628 | Acc: 51.614,83.338,99.372,% | Adaptive Acc: 92.918% | clf_exit: 0.266 0.473 0.262
Batch: 80 | Loss: 2.648 | Acc: 50.984,83.121,99.257,% | Adaptive Acc: 92.728% | clf_exit: 0.264 0.471 0.264
Batch: 100 | Loss: 2.666 | Acc: 50.665,82.882,99.288,% | Adaptive Acc: 92.744% | clf_exit: 0.262 0.471 0.267
Batch: 120 | Loss: 2.667 | Acc: 50.626,82.767,99.303,% | Adaptive Acc: 92.704% | clf_exit: 0.260 0.473 0.267
Batch: 140 | Loss: 2.670 | Acc: 50.621,82.713,99.296,% | Adaptive Acc: 92.703% | clf_exit: 0.260 0.475 0.265
Batch: 160 | Loss: 2.686 | Acc: 50.277,82.633,99.277,% | Adaptive Acc: 92.682% | clf_exit: 0.258 0.473 0.269
Batch: 180 | Loss: 2.697 | Acc: 50.263,82.437,99.288,% | Adaptive Acc: 92.662% | clf_exit: 0.257 0.471 0.271
Batch: 200 | Loss: 2.698 | Acc: 50.288,82.307,99.269,% | Adaptive Acc: 92.631% | clf_exit: 0.258 0.470 0.272
Batch: 220 | Loss: 2.703 | Acc: 50.329,82.208,99.254,% | Adaptive Acc: 92.509% | clf_exit: 0.259 0.470 0.272
Batch: 240 | Loss: 2.708 | Acc: 50.172,82.167,99.245,% | Adaptive Acc: 92.427% | clf_exit: 0.258 0.470 0.272
Batch: 260 | Loss: 2.707 | Acc: 50.239,82.190,99.237,% | Adaptive Acc: 92.496% | clf_exit: 0.258 0.469 0.273
Batch: 280 | Loss: 2.708 | Acc: 50.270,82.065,99.241,% | Adaptive Acc: 92.502% | clf_exit: 0.258 0.468 0.274
Batch: 300 | Loss: 2.713 | Acc: 50.247,82.060,99.242,% | Adaptive Acc: 92.504% | clf_exit: 0.258 0.468 0.275
Batch: 320 | Loss: 2.716 | Acc: 50.151,82.048,99.226,% | Adaptive Acc: 92.436% | clf_exit: 0.258 0.468 0.274
Batch: 340 | Loss: 2.719 | Acc: 50.069,82.050,99.232,% | Adaptive Acc: 92.435% | clf_exit: 0.257 0.468 0.275
Batch: 360 | Loss: 2.723 | Acc: 50.019,81.986,99.210,% | Adaptive Acc: 92.413% | clf_exit: 0.256 0.468 0.276
Batch: 380 | Loss: 2.724 | Acc: 49.992,81.966,99.194,% | Adaptive Acc: 92.415% | clf_exit: 0.256 0.467 0.277
Batch: 0 | Loss: 4.145 | Acc: 50.781,66.406,73.438,% | Adaptive Acc: 69.531% | clf_exit: 0.305 0.477 0.219
Batch: 20 | Loss: 4.572 | Acc: 47.247,65.960,70.722,% | Adaptive Acc: 67.262% | clf_exit: 0.319 0.434 0.247
Batch: 40 | Loss: 4.534 | Acc: 47.142,65.835,70.122,% | Adaptive Acc: 66.864% | clf_exit: 0.314 0.426 0.260
Batch: 60 | Loss: 4.557 | Acc: 46.452,65.394,69.851,% | Adaptive Acc: 66.598% | clf_exit: 0.314 0.421 0.265
Train all parameters

Epoch: 222
Batch: 0 | Loss: 2.894 | Acc: 47.656,84.375,99.219,% | Adaptive Acc: 97.656% | clf_exit: 0.164 0.523 0.312
Batch: 20 | Loss: 2.633 | Acc: 51.004,84.040,98.996,% | Adaptive Acc: 93.862% | clf_exit: 0.252 0.475 0.273
Batch: 40 | Loss: 2.643 | Acc: 50.210,83.556,99.219,% | Adaptive Acc: 93.064% | clf_exit: 0.257 0.478 0.266
Batch: 60 | Loss: 2.652 | Acc: 50.384,83.350,99.155,% | Adaptive Acc: 92.943% | clf_exit: 0.256 0.479 0.265
Batch: 80 | Loss: 2.657 | Acc: 50.608,83.237,99.132,% | Adaptive Acc: 92.631% | clf_exit: 0.260 0.474 0.266
Batch: 100 | Loss: 2.671 | Acc: 50.526,83.006,99.180,% | Adaptive Acc: 92.659% | clf_exit: 0.258 0.474 0.267
Batch: 120 | Loss: 2.685 | Acc: 50.446,82.806,99.199,% | Adaptive Acc: 92.749% | clf_exit: 0.257 0.469 0.274
Batch: 140 | Loss: 2.685 | Acc: 50.344,82.824,99.163,% | Adaptive Acc: 92.636% | clf_exit: 0.258 0.468 0.274
Batch: 160 | Loss: 2.696 | Acc: 50.073,82.686,99.156,% | Adaptive Acc: 92.619% | clf_exit: 0.255 0.469 0.276
Batch: 180 | Loss: 2.705 | Acc: 49.978,82.459,99.158,% | Adaptive Acc: 92.610% | clf_exit: 0.254 0.467 0.278
Batch: 200 | Loss: 2.712 | Acc: 49.992,82.369,99.157,% | Adaptive Acc: 92.568% | clf_exit: 0.254 0.467 0.278
Batch: 220 | Loss: 2.708 | Acc: 49.993,82.374,99.183,% | Adaptive Acc: 92.615% | clf_exit: 0.254 0.467 0.279
Batch: 240 | Loss: 2.706 | Acc: 50.136,82.372,99.134,% | Adaptive Acc: 92.554% | clf_exit: 0.255 0.466 0.279
Batch: 260 | Loss: 2.707 | Acc: 50.075,82.393,99.147,% | Adaptive Acc: 92.559% | clf_exit: 0.256 0.465 0.279
Batch: 280 | Loss: 2.709 | Acc: 50.081,82.365,99.133,% | Adaptive Acc: 92.538% | clf_exit: 0.256 0.465 0.279
Batch: 300 | Loss: 2.714 | Acc: 50.065,82.249,99.128,% | Adaptive Acc: 92.486% | clf_exit: 0.256 0.465 0.279
Batch: 320 | Loss: 2.714 | Acc: 50.075,82.282,99.124,% | Adaptive Acc: 92.443% | clf_exit: 0.256 0.465 0.278
Batch: 340 | Loss: 2.717 | Acc: 49.991,82.226,99.120,% | Adaptive Acc: 92.467% | clf_exit: 0.257 0.464 0.279
Batch: 360 | Loss: 2.718 | Acc: 50.019,82.178,99.095,% | Adaptive Acc: 92.460% | clf_exit: 0.257 0.463 0.280
Batch: 380 | Loss: 2.720 | Acc: 50.037,82.138,99.075,% | Adaptive Acc: 92.417% | clf_exit: 0.257 0.463 0.279
Batch: 0 | Loss: 4.085 | Acc: 50.000,69.531,75.781,% | Adaptive Acc: 70.312% | clf_exit: 0.352 0.414 0.234
Batch: 20 | Loss: 4.617 | Acc: 45.536,66.146,70.164,% | Adaptive Acc: 67.188% | clf_exit: 0.324 0.419 0.257
Batch: 40 | Loss: 4.533 | Acc: 46.094,66.159,69.893,% | Adaptive Acc: 67.092% | clf_exit: 0.324 0.410 0.266
Batch: 60 | Loss: 4.549 | Acc: 46.107,65.894,69.557,% | Adaptive Acc: 66.547% | clf_exit: 0.322 0.408 0.270
Train all parameters

Epoch: 223
Batch: 0 | Loss: 2.598 | Acc: 52.344,82.031,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.250 0.461 0.289
Batch: 20 | Loss: 2.684 | Acc: 49.182,83.631,99.554,% | Adaptive Acc: 92.932% | clf_exit: 0.245 0.489 0.266
Batch: 40 | Loss: 2.694 | Acc: 50.210,82.793,99.466,% | Adaptive Acc: 92.511% | clf_exit: 0.249 0.485 0.266
Batch: 60 | Loss: 2.701 | Acc: 50.000,82.761,99.424,% | Adaptive Acc: 92.495% | clf_exit: 0.254 0.479 0.267
Batch: 80 | Loss: 2.702 | Acc: 50.010,82.880,99.392,% | Adaptive Acc: 92.564% | clf_exit: 0.254 0.475 0.271
Batch: 100 | Loss: 2.711 | Acc: 49.930,82.789,99.404,% | Adaptive Acc: 92.628% | clf_exit: 0.254 0.471 0.276
Batch: 120 | Loss: 2.702 | Acc: 49.845,82.909,99.432,% | Adaptive Acc: 92.594% | clf_exit: 0.256 0.468 0.275
Batch: 140 | Loss: 2.710 | Acc: 49.607,82.691,99.407,% | Adaptive Acc: 92.575% | clf_exit: 0.256 0.466 0.278
Batch: 160 | Loss: 2.707 | Acc: 49.636,82.648,99.374,% | Adaptive Acc: 92.610% | clf_exit: 0.256 0.465 0.278
Batch: 180 | Loss: 2.701 | Acc: 49.763,82.705,99.374,% | Adaptive Acc: 92.645% | clf_exit: 0.257 0.464 0.279
Batch: 200 | Loss: 2.696 | Acc: 49.825,82.680,99.347,% | Adaptive Acc: 92.576% | clf_exit: 0.258 0.465 0.278
Batch: 220 | Loss: 2.697 | Acc: 49.830,82.675,99.328,% | Adaptive Acc: 92.590% | clf_exit: 0.257 0.465 0.277
Batch: 240 | Loss: 2.696 | Acc: 49.903,82.689,99.290,% | Adaptive Acc: 92.632% | clf_exit: 0.257 0.466 0.277
Batch: 260 | Loss: 2.696 | Acc: 50.021,82.573,99.270,% | Adaptive Acc: 92.642% | clf_exit: 0.258 0.464 0.278
Batch: 280 | Loss: 2.700 | Acc: 50.070,82.507,99.247,% | Adaptive Acc: 92.591% | clf_exit: 0.259 0.463 0.278
Batch: 300 | Loss: 2.703 | Acc: 50.049,82.478,99.237,% | Adaptive Acc: 92.600% | clf_exit: 0.258 0.464 0.278
Batch: 320 | Loss: 2.705 | Acc: 50.015,82.443,99.219,% | Adaptive Acc: 92.557% | clf_exit: 0.258 0.464 0.278
Batch: 340 | Loss: 2.710 | Acc: 49.963,82.336,99.196,% | Adaptive Acc: 92.520% | clf_exit: 0.257 0.466 0.277
Batch: 360 | Loss: 2.715 | Acc: 49.931,82.220,99.195,% | Adaptive Acc: 92.482% | clf_exit: 0.257 0.464 0.279
Batch: 380 | Loss: 2.719 | Acc: 49.871,82.226,99.188,% | Adaptive Acc: 92.409% | clf_exit: 0.257 0.465 0.278
Batch: 0 | Loss: 4.038 | Acc: 49.219,67.969,75.000,% | Adaptive Acc: 70.312% | clf_exit: 0.312 0.469 0.219
Batch: 20 | Loss: 4.512 | Acc: 46.615,66.220,70.499,% | Adaptive Acc: 67.262% | clf_exit: 0.333 0.403 0.265
Batch: 40 | Loss: 4.499 | Acc: 46.322,66.178,70.522,% | Adaptive Acc: 66.959% | clf_exit: 0.329 0.398 0.273
Batch: 60 | Loss: 4.504 | Acc: 46.183,66.060,70.735,% | Adaptive Acc: 66.970% | clf_exit: 0.332 0.395 0.273
Train all parameters

Epoch: 224
Batch: 0 | Loss: 2.494 | Acc: 52.344,85.156,99.219,% | Adaptive Acc: 95.312% | clf_exit: 0.266 0.508 0.227
Batch: 20 | Loss: 2.593 | Acc: 50.484,83.854,99.516,% | Adaptive Acc: 93.899% | clf_exit: 0.261 0.477 0.262
Batch: 40 | Loss: 2.646 | Acc: 50.362,83.670,99.295,% | Adaptive Acc: 93.255% | clf_exit: 0.252 0.484 0.264
Batch: 60 | Loss: 2.664 | Acc: 50.320,83.274,99.129,% | Adaptive Acc: 93.199% | clf_exit: 0.257 0.474 0.269
Batch: 80 | Loss: 2.698 | Acc: 50.087,82.870,99.190,% | Adaptive Acc: 92.988% | clf_exit: 0.255 0.472 0.274
Batch: 100 | Loss: 2.684 | Acc: 50.116,82.851,99.219,% | Adaptive Acc: 92.961% | clf_exit: 0.256 0.471 0.274
Batch: 120 | Loss: 2.684 | Acc: 50.194,82.819,99.174,% | Adaptive Acc: 93.014% | clf_exit: 0.255 0.470 0.275
Batch: 140 | Loss: 2.683 | Acc: 50.172,82.840,99.219,% | Adaptive Acc: 92.996% | clf_exit: 0.255 0.470 0.275
Batch: 160 | Loss: 2.699 | Acc: 49.874,82.696,99.209,% | Adaptive Acc: 92.920% | clf_exit: 0.252 0.471 0.277
Batch: 180 | Loss: 2.702 | Acc: 49.996,82.709,99.214,% | Adaptive Acc: 93.012% | clf_exit: 0.253 0.470 0.278
Batch: 200 | Loss: 2.708 | Acc: 49.977,82.614,99.211,% | Adaptive Acc: 92.980% | clf_exit: 0.252 0.470 0.278
Batch: 220 | Loss: 2.707 | Acc: 49.986,82.586,99.226,% | Adaptive Acc: 92.972% | clf_exit: 0.252 0.469 0.279
Batch: 240 | Loss: 2.709 | Acc: 49.974,82.414,99.193,% | Adaptive Acc: 92.859% | clf_exit: 0.253 0.468 0.278
Batch: 260 | Loss: 2.710 | Acc: 50.045,82.387,99.171,% | Adaptive Acc: 92.789% | clf_exit: 0.253 0.468 0.278
Batch: 280 | Loss: 2.714 | Acc: 49.986,82.281,99.188,% | Adaptive Acc: 92.816% | clf_exit: 0.253 0.467 0.280
Batch: 300 | Loss: 2.721 | Acc: 49.860,82.195,99.185,% | Adaptive Acc: 92.777% | clf_exit: 0.253 0.466 0.281
Batch: 320 | Loss: 2.719 | Acc: 49.864,82.289,99.182,% | Adaptive Acc: 92.759% | clf_exit: 0.254 0.465 0.281
Batch: 340 | Loss: 2.720 | Acc: 49.881,82.237,99.196,% | Adaptive Acc: 92.721% | clf_exit: 0.254 0.466 0.280
Batch: 360 | Loss: 2.722 | Acc: 49.879,82.155,99.167,% | Adaptive Acc: 92.681% | clf_exit: 0.254 0.466 0.280
Batch: 380 | Loss: 2.723 | Acc: 49.969,82.103,99.151,% | Adaptive Acc: 92.624% | clf_exit: 0.254 0.466 0.280
Batch: 0 | Loss: 4.082 | Acc: 52.344,69.531,75.781,% | Adaptive Acc: 67.969% | clf_exit: 0.320 0.477 0.203
Batch: 20 | Loss: 4.495 | Acc: 46.652,66.704,70.499,% | Adaptive Acc: 66.964% | clf_exit: 0.330 0.408 0.262
Batch: 40 | Loss: 4.454 | Acc: 47.275,66.806,70.598,% | Adaptive Acc: 67.550% | clf_exit: 0.333 0.402 0.265
Batch: 60 | Loss: 4.476 | Acc: 46.849,66.522,70.441,% | Adaptive Acc: 67.495% | clf_exit: 0.329 0.401 0.270
Train all parameters

Epoch: 225
Batch: 0 | Loss: 2.644 | Acc: 45.312,82.812,99.219,% | Adaptive Acc: 89.062% | clf_exit: 0.328 0.422 0.250
Batch: 20 | Loss: 2.607 | Acc: 51.153,83.705,99.182,% | Adaptive Acc: 92.150% | clf_exit: 0.280 0.463 0.257
Batch: 40 | Loss: 2.625 | Acc: 50.819,83.441,99.295,% | Adaptive Acc: 92.626% | clf_exit: 0.271 0.467 0.261
Batch: 60 | Loss: 2.628 | Acc: 50.499,83.619,99.462,% | Adaptive Acc: 92.802% | clf_exit: 0.265 0.473 0.261
Batch: 80 | Loss: 2.617 | Acc: 50.521,83.845,99.537,% | Adaptive Acc: 92.834% | clf_exit: 0.266 0.474 0.259
Batch: 100 | Loss: 2.616 | Acc: 50.495,83.888,99.559,% | Adaptive Acc: 92.891% | clf_exit: 0.265 0.477 0.258
Batch: 120 | Loss: 2.614 | Acc: 50.394,84.033,99.580,% | Adaptive Acc: 92.995% | clf_exit: 0.264 0.478 0.258
Batch: 140 | Loss: 2.616 | Acc: 50.471,84.153,99.562,% | Adaptive Acc: 93.002% | clf_exit: 0.264 0.478 0.258
Batch: 160 | Loss: 2.604 | Acc: 50.723,84.268,99.563,% | Adaptive Acc: 93.032% | clf_exit: 0.266 0.476 0.258
Batch: 180 | Loss: 2.604 | Acc: 50.902,84.371,99.568,% | Adaptive Acc: 93.098% | clf_exit: 0.266 0.476 0.258
Batch: 200 | Loss: 2.597 | Acc: 51.077,84.437,99.596,% | Adaptive Acc: 93.151% | clf_exit: 0.266 0.477 0.257
Batch: 220 | Loss: 2.600 | Acc: 50.916,84.460,99.593,% | Adaptive Acc: 93.075% | clf_exit: 0.266 0.477 0.257
Batch: 240 | Loss: 2.598 | Acc: 50.963,84.463,99.611,% | Adaptive Acc: 93.111% | clf_exit: 0.266 0.477 0.256
Batch: 260 | Loss: 2.593 | Acc: 50.973,84.522,99.620,% | Adaptive Acc: 93.133% | clf_exit: 0.267 0.477 0.255
Batch: 280 | Loss: 2.591 | Acc: 51.020,84.592,99.630,% | Adaptive Acc: 93.202% | clf_exit: 0.267 0.479 0.255
Batch: 300 | Loss: 2.588 | Acc: 51.082,84.642,99.637,% | Adaptive Acc: 93.221% | clf_exit: 0.267 0.479 0.255
Batch: 320 | Loss: 2.584 | Acc: 51.134,84.711,99.645,% | Adaptive Acc: 93.300% | clf_exit: 0.266 0.480 0.254
Batch: 340 | Loss: 2.587 | Acc: 51.017,84.716,99.643,% | Adaptive Acc: 93.294% | clf_exit: 0.266 0.480 0.254
Batch: 360 | Loss: 2.582 | Acc: 51.037,84.845,99.658,% | Adaptive Acc: 93.337% | clf_exit: 0.266 0.481 0.253
Batch: 380 | Loss: 2.580 | Acc: 50.988,84.900,99.651,% | Adaptive Acc: 93.365% | clf_exit: 0.266 0.481 0.253
Batch: 0 | Loss: 3.879 | Acc: 50.781,71.094,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.316 | Acc: 47.879,67.783,71.987,% | Adaptive Acc: 67.894% | clf_exit: 0.341 0.409 0.250
Batch: 40 | Loss: 4.271 | Acc: 48.209,67.626,71.932,% | Adaptive Acc: 68.521% | clf_exit: 0.340 0.402 0.258
Batch: 60 | Loss: 4.286 | Acc: 47.964,67.495,71.965,% | Adaptive Acc: 68.519% | clf_exit: 0.337 0.402 0.261
Train all parameters

Epoch: 226
Batch: 0 | Loss: 2.534 | Acc: 56.250,84.375,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.273 0.445 0.281
Batch: 20 | Loss: 2.496 | Acc: 54.427,85.714,99.814,% | Adaptive Acc: 94.271% | clf_exit: 0.280 0.474 0.247
Batch: 40 | Loss: 2.500 | Acc: 53.525,85.671,99.809,% | Adaptive Acc: 94.055% | clf_exit: 0.274 0.488 0.238
Batch: 60 | Loss: 2.513 | Acc: 52.369,86.040,99.795,% | Adaptive Acc: 93.891% | clf_exit: 0.269 0.490 0.241
Batch: 80 | Loss: 2.527 | Acc: 52.141,85.745,99.749,% | Adaptive Acc: 93.866% | clf_exit: 0.268 0.488 0.245
Batch: 100 | Loss: 2.530 | Acc: 51.887,85.667,99.760,% | Adaptive Acc: 93.858% | clf_exit: 0.268 0.490 0.242
Batch: 120 | Loss: 2.534 | Acc: 51.879,85.524,99.780,% | Adaptive Acc: 93.847% | clf_exit: 0.267 0.487 0.245
Batch: 140 | Loss: 2.537 | Acc: 51.845,85.555,99.795,% | Adaptive Acc: 93.756% | clf_exit: 0.269 0.486 0.246
Batch: 160 | Loss: 2.543 | Acc: 51.747,85.598,99.806,% | Adaptive Acc: 93.740% | clf_exit: 0.267 0.485 0.248
Batch: 180 | Loss: 2.545 | Acc: 51.614,85.635,99.797,% | Adaptive Acc: 93.728% | clf_exit: 0.266 0.487 0.247
Batch: 200 | Loss: 2.544 | Acc: 51.590,85.592,99.794,% | Adaptive Acc: 93.703% | clf_exit: 0.267 0.486 0.247
Batch: 220 | Loss: 2.546 | Acc: 51.566,85.485,99.795,% | Adaptive Acc: 93.647% | clf_exit: 0.268 0.484 0.248
Batch: 240 | Loss: 2.541 | Acc: 51.601,85.591,99.802,% | Adaptive Acc: 93.682% | clf_exit: 0.268 0.486 0.246
Batch: 260 | Loss: 2.538 | Acc: 51.565,85.665,99.784,% | Adaptive Acc: 93.753% | clf_exit: 0.268 0.486 0.246
Batch: 280 | Loss: 2.541 | Acc: 51.537,85.626,99.780,% | Adaptive Acc: 93.744% | clf_exit: 0.266 0.487 0.247
Batch: 300 | Loss: 2.542 | Acc: 51.453,85.592,99.785,% | Adaptive Acc: 93.740% | clf_exit: 0.267 0.486 0.247
Batch: 320 | Loss: 2.541 | Acc: 51.528,85.551,99.786,% | Adaptive Acc: 93.731% | clf_exit: 0.268 0.485 0.248
Batch: 340 | Loss: 2.543 | Acc: 51.494,85.514,99.782,% | Adaptive Acc: 93.723% | clf_exit: 0.267 0.485 0.248
Batch: 360 | Loss: 2.542 | Acc: 51.476,85.533,99.781,% | Adaptive Acc: 93.728% | clf_exit: 0.267 0.485 0.248
Batch: 380 | Loss: 2.546 | Acc: 51.405,85.474,99.781,% | Adaptive Acc: 93.686% | clf_exit: 0.266 0.485 0.249
Batch: 0 | Loss: 3.855 | Acc: 51.562,70.312,75.781,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.304 | Acc: 48.028,68.304,71.987,% | Adaptive Acc: 68.899% | clf_exit: 0.337 0.411 0.252
Batch: 40 | Loss: 4.257 | Acc: 48.361,68.045,72.008,% | Adaptive Acc: 69.188% | clf_exit: 0.335 0.403 0.262
Batch: 60 | Loss: 4.278 | Acc: 48.066,67.918,71.965,% | Adaptive Acc: 68.929% | clf_exit: 0.334 0.401 0.265
Train all parameters

Epoch: 227
Batch: 0 | Loss: 2.878 | Acc: 38.281,84.375,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.250 0.516 0.234
Batch: 20 | Loss: 2.531 | Acc: 50.298,85.342,99.777,% | Adaptive Acc: 93.527% | clf_exit: 0.272 0.490 0.238
Batch: 40 | Loss: 2.531 | Acc: 51.029,85.880,99.848,% | Adaptive Acc: 93.864% | clf_exit: 0.267 0.490 0.244
Batch: 60 | Loss: 2.546 | Acc: 50.948,85.835,99.846,% | Adaptive Acc: 94.045% | clf_exit: 0.263 0.490 0.246
Batch: 80 | Loss: 2.540 | Acc: 51.100,85.725,99.817,% | Adaptive Acc: 93.991% | clf_exit: 0.267 0.486 0.247
Batch: 100 | Loss: 2.537 | Acc: 51.269,85.821,99.838,% | Adaptive Acc: 93.804% | clf_exit: 0.267 0.486 0.246
Batch: 120 | Loss: 2.542 | Acc: 51.259,85.834,99.826,% | Adaptive Acc: 93.931% | clf_exit: 0.265 0.487 0.248
Batch: 140 | Loss: 2.533 | Acc: 51.319,85.954,99.828,% | Adaptive Acc: 93.999% | clf_exit: 0.265 0.488 0.248
Batch: 160 | Loss: 2.538 | Acc: 51.165,85.981,99.840,% | Adaptive Acc: 93.978% | clf_exit: 0.263 0.489 0.247
Batch: 180 | Loss: 2.530 | Acc: 51.390,86.002,99.845,% | Adaptive Acc: 94.013% | clf_exit: 0.263 0.490 0.247
Batch: 200 | Loss: 2.533 | Acc: 51.364,85.922,99.837,% | Adaptive Acc: 93.956% | clf_exit: 0.263 0.489 0.248
Batch: 220 | Loss: 2.533 | Acc: 51.393,85.849,99.848,% | Adaptive Acc: 93.927% | clf_exit: 0.263 0.489 0.248
Batch: 240 | Loss: 2.537 | Acc: 51.462,85.714,99.838,% | Adaptive Acc: 93.873% | clf_exit: 0.263 0.489 0.248
Batch: 260 | Loss: 2.539 | Acc: 51.275,85.731,99.832,% | Adaptive Acc: 93.846% | clf_exit: 0.263 0.489 0.248
Batch: 280 | Loss: 2.538 | Acc: 51.368,85.757,99.836,% | Adaptive Acc: 93.892% | clf_exit: 0.264 0.488 0.248
Batch: 300 | Loss: 2.540 | Acc: 51.373,85.681,99.834,% | Adaptive Acc: 93.854% | clf_exit: 0.263 0.489 0.248
Batch: 320 | Loss: 2.539 | Acc: 51.419,85.714,99.837,% | Adaptive Acc: 93.855% | clf_exit: 0.264 0.487 0.249
Batch: 340 | Loss: 2.536 | Acc: 51.386,85.720,99.844,% | Adaptive Acc: 93.869% | clf_exit: 0.264 0.487 0.248
Batch: 360 | Loss: 2.537 | Acc: 51.353,85.725,99.846,% | Adaptive Acc: 93.867% | clf_exit: 0.264 0.488 0.248
Batch: 380 | Loss: 2.533 | Acc: 51.376,85.800,99.850,% | Adaptive Acc: 93.894% | clf_exit: 0.265 0.488 0.247
Batch: 0 | Loss: 3.859 | Acc: 48.438,69.531,77.344,% | Adaptive Acc: 70.312% | clf_exit: 0.336 0.469 0.195
Batch: 20 | Loss: 4.296 | Acc: 48.140,67.857,72.656,% | Adaptive Acc: 69.048% | clf_exit: 0.334 0.414 0.252
Batch: 40 | Loss: 4.252 | Acc: 48.247,67.759,72.351,% | Adaptive Acc: 69.188% | clf_exit: 0.333 0.406 0.261
Batch: 60 | Loss: 4.264 | Acc: 48.002,67.700,72.400,% | Adaptive Acc: 69.032% | clf_exit: 0.333 0.405 0.263
Train all parameters

Epoch: 228
Batch: 0 | Loss: 2.612 | Acc: 46.875,84.375,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.219 0.492 0.289
Batch: 20 | Loss: 2.538 | Acc: 52.344,86.235,99.740,% | Adaptive Acc: 93.378% | clf_exit: 0.269 0.498 0.234
Batch: 40 | Loss: 2.549 | Acc: 51.048,85.995,99.790,% | Adaptive Acc: 93.216% | clf_exit: 0.269 0.495 0.236
Batch: 60 | Loss: 2.524 | Acc: 51.447,85.963,99.808,% | Adaptive Acc: 93.699% | clf_exit: 0.265 0.495 0.240
Batch: 80 | Loss: 2.555 | Acc: 50.974,85.600,99.769,% | Adaptive Acc: 93.586% | clf_exit: 0.262 0.495 0.243
Batch: 100 | Loss: 2.539 | Acc: 51.214,85.713,99.768,% | Adaptive Acc: 93.502% | clf_exit: 0.264 0.494 0.242
Batch: 120 | Loss: 2.540 | Acc: 51.343,85.731,99.774,% | Adaptive Acc: 93.621% | clf_exit: 0.264 0.492 0.243
Batch: 140 | Loss: 2.537 | Acc: 51.507,85.683,99.795,% | Adaptive Acc: 93.711% | clf_exit: 0.265 0.491 0.244
Batch: 160 | Loss: 2.538 | Acc: 51.490,85.734,99.811,% | Adaptive Acc: 93.818% | clf_exit: 0.265 0.490 0.245
Batch: 180 | Loss: 2.537 | Acc: 51.472,85.786,99.814,% | Adaptive Acc: 93.785% | clf_exit: 0.265 0.489 0.246
Batch: 200 | Loss: 2.535 | Acc: 51.524,85.852,99.806,% | Adaptive Acc: 93.727% | clf_exit: 0.265 0.490 0.246
Batch: 220 | Loss: 2.528 | Acc: 51.616,85.920,99.813,% | Adaptive Acc: 93.761% | clf_exit: 0.266 0.488 0.246
Batch: 240 | Loss: 2.527 | Acc: 51.637,85.892,99.825,% | Adaptive Acc: 93.766% | clf_exit: 0.267 0.487 0.246
Batch: 260 | Loss: 2.529 | Acc: 51.607,85.842,99.829,% | Adaptive Acc: 93.741% | clf_exit: 0.268 0.486 0.246
Batch: 280 | Loss: 2.528 | Acc: 51.537,85.829,99.836,% | Adaptive Acc: 93.711% | clf_exit: 0.268 0.486 0.246
Batch: 300 | Loss: 2.528 | Acc: 51.547,85.847,99.836,% | Adaptive Acc: 93.732% | clf_exit: 0.268 0.487 0.246
Batch: 320 | Loss: 2.527 | Acc: 51.621,85.862,99.832,% | Adaptive Acc: 93.755% | clf_exit: 0.268 0.487 0.245
Batch: 340 | Loss: 2.529 | Acc: 51.558,85.846,99.830,% | Adaptive Acc: 93.734% | clf_exit: 0.268 0.487 0.245
Batch: 360 | Loss: 2.530 | Acc: 51.539,85.866,99.833,% | Adaptive Acc: 93.746% | clf_exit: 0.268 0.488 0.245
Batch: 380 | Loss: 2.529 | Acc: 51.526,85.849,99.828,% | Adaptive Acc: 93.744% | clf_exit: 0.268 0.487 0.245
Batch: 0 | Loss: 3.814 | Acc: 52.344,69.531,77.344,% | Adaptive Acc: 70.312% | clf_exit: 0.352 0.414 0.234
Batch: 20 | Loss: 4.276 | Acc: 48.065,68.118,72.768,% | Adaptive Acc: 68.564% | clf_exit: 0.346 0.409 0.246
Batch: 40 | Loss: 4.239 | Acc: 48.457,68.178,72.504,% | Adaptive Acc: 68.883% | clf_exit: 0.345 0.397 0.259
Batch: 60 | Loss: 4.254 | Acc: 48.207,67.982,72.695,% | Adaptive Acc: 68.968% | clf_exit: 0.342 0.399 0.259
Train all parameters

Epoch: 229
Batch: 0 | Loss: 2.699 | Acc: 50.781,82.031,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.281 0.453 0.266
Batch: 20 | Loss: 2.507 | Acc: 51.414,86.086,99.888,% | Adaptive Acc: 93.899% | clf_exit: 0.270 0.497 0.233
Batch: 40 | Loss: 2.486 | Acc: 51.543,86.242,99.924,% | Adaptive Acc: 94.226% | clf_exit: 0.264 0.494 0.243
Batch: 60 | Loss: 2.495 | Acc: 51.345,86.245,99.910,% | Adaptive Acc: 94.429% | clf_exit: 0.264 0.492 0.244
Batch: 80 | Loss: 2.507 | Acc: 51.196,86.044,99.894,% | Adaptive Acc: 94.261% | clf_exit: 0.264 0.490 0.246
Batch: 100 | Loss: 2.510 | Acc: 51.245,86.108,99.869,% | Adaptive Acc: 94.152% | clf_exit: 0.265 0.490 0.245
Batch: 120 | Loss: 2.525 | Acc: 51.136,85.912,99.877,% | Adaptive Acc: 94.053% | clf_exit: 0.267 0.487 0.246
Batch: 140 | Loss: 2.526 | Acc: 51.164,85.932,99.873,% | Adaptive Acc: 94.044% | clf_exit: 0.266 0.487 0.247
Batch: 160 | Loss: 2.511 | Acc: 51.431,86.136,99.874,% | Adaptive Acc: 93.988% | clf_exit: 0.268 0.488 0.244
Batch: 180 | Loss: 2.510 | Acc: 51.502,86.201,99.871,% | Adaptive Acc: 94.039% | clf_exit: 0.269 0.488 0.244
Batch: 200 | Loss: 2.506 | Acc: 51.489,86.241,99.856,% | Adaptive Acc: 94.034% | clf_exit: 0.270 0.487 0.243
Batch: 220 | Loss: 2.509 | Acc: 51.410,86.174,99.866,% | Adaptive Acc: 94.008% | clf_exit: 0.271 0.486 0.243
Batch: 240 | Loss: 2.513 | Acc: 51.352,86.171,99.864,% | Adaptive Acc: 93.996% | clf_exit: 0.271 0.485 0.244
Batch: 260 | Loss: 2.510 | Acc: 51.446,86.165,99.868,% | Adaptive Acc: 94.031% | clf_exit: 0.271 0.485 0.244
Batch: 280 | Loss: 2.509 | Acc: 51.457,86.143,99.867,% | Adaptive Acc: 94.050% | clf_exit: 0.270 0.486 0.244
Batch: 300 | Loss: 2.511 | Acc: 51.503,86.197,99.873,% | Adaptive Acc: 94.072% | clf_exit: 0.269 0.486 0.245
Batch: 320 | Loss: 2.510 | Acc: 51.516,86.193,99.871,% | Adaptive Acc: 94.064% | clf_exit: 0.269 0.487 0.244
Batch: 340 | Loss: 2.515 | Acc: 51.372,86.121,99.879,% | Adaptive Acc: 94.039% | clf_exit: 0.268 0.487 0.246
Batch: 360 | Loss: 2.516 | Acc: 51.368,86.087,99.872,% | Adaptive Acc: 94.021% | clf_exit: 0.267 0.487 0.246
Batch: 380 | Loss: 2.517 | Acc: 51.308,86.108,99.873,% | Adaptive Acc: 94.045% | clf_exit: 0.267 0.487 0.246
Batch: 0 | Loss: 3.806 | Acc: 50.781,70.312,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.344 0.406 0.250
Batch: 20 | Loss: 4.270 | Acc: 48.028,67.932,72.693,% | Adaptive Acc: 68.713% | clf_exit: 0.340 0.414 0.246
Batch: 40 | Loss: 4.241 | Acc: 48.323,67.664,72.332,% | Adaptive Acc: 68.750% | clf_exit: 0.336 0.405 0.259
Batch: 60 | Loss: 4.263 | Acc: 47.874,67.380,72.490,% | Adaptive Acc: 68.776% | clf_exit: 0.334 0.407 0.260
Train all parameters

Epoch: 230
Batch: 0 | Loss: 2.546 | Acc: 50.000,85.156,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.227 0.461 0.312
Batch: 20 | Loss: 2.508 | Acc: 52.865,85.491,99.963,% | Adaptive Acc: 94.420% | clf_exit: 0.269 0.478 0.253
Batch: 40 | Loss: 2.475 | Acc: 52.001,85.995,99.924,% | Adaptive Acc: 94.665% | clf_exit: 0.266 0.491 0.243
Batch: 60 | Loss: 2.490 | Acc: 52.011,85.733,99.872,% | Adaptive Acc: 94.160% | clf_exit: 0.269 0.489 0.243
Batch: 80 | Loss: 2.485 | Acc: 52.334,85.986,99.865,% | Adaptive Acc: 94.039% | clf_exit: 0.271 0.488 0.241
Batch: 100 | Loss: 2.498 | Acc: 51.825,86.139,99.853,% | Adaptive Acc: 93.974% | clf_exit: 0.268 0.491 0.241
Batch: 120 | Loss: 2.513 | Acc: 51.562,86.009,99.839,% | Adaptive Acc: 93.866% | clf_exit: 0.268 0.490 0.243
Batch: 140 | Loss: 2.508 | Acc: 51.562,85.965,99.834,% | Adaptive Acc: 93.905% | clf_exit: 0.268 0.489 0.243
Batch: 160 | Loss: 2.506 | Acc: 51.630,85.938,99.850,% | Adaptive Acc: 93.881% | clf_exit: 0.270 0.487 0.243
Batch: 180 | Loss: 2.502 | Acc: 51.765,85.994,99.849,% | Adaptive Acc: 93.854% | clf_exit: 0.270 0.488 0.242
Batch: 200 | Loss: 2.507 | Acc: 51.737,85.938,99.860,% | Adaptive Acc: 93.851% | clf_exit: 0.270 0.487 0.243
Batch: 220 | Loss: 2.509 | Acc: 51.686,85.976,99.866,% | Adaptive Acc: 93.941% | clf_exit: 0.270 0.486 0.244
Batch: 240 | Loss: 2.509 | Acc: 51.734,86.012,99.867,% | Adaptive Acc: 93.983% | clf_exit: 0.270 0.487 0.243
Batch: 260 | Loss: 2.507 | Acc: 51.745,86.066,99.868,% | Adaptive Acc: 94.040% | clf_exit: 0.268 0.488 0.243
Batch: 280 | Loss: 2.510 | Acc: 51.665,86.032,99.869,% | Adaptive Acc: 94.045% | clf_exit: 0.268 0.488 0.243
Batch: 300 | Loss: 2.510 | Acc: 51.664,86.088,99.875,% | Adaptive Acc: 94.041% | clf_exit: 0.269 0.488 0.243
Batch: 320 | Loss: 2.515 | Acc: 51.589,86.091,99.864,% | Adaptive Acc: 94.020% | clf_exit: 0.269 0.488 0.243
Batch: 340 | Loss: 2.518 | Acc: 51.542,86.031,99.860,% | Adaptive Acc: 93.997% | clf_exit: 0.268 0.488 0.243
Batch: 360 | Loss: 2.518 | Acc: 51.588,86.031,99.859,% | Adaptive Acc: 93.988% | clf_exit: 0.268 0.489 0.243
Batch: 380 | Loss: 2.515 | Acc: 51.634,86.089,99.854,% | Adaptive Acc: 93.984% | clf_exit: 0.268 0.490 0.242
Batch: 0 | Loss: 3.859 | Acc: 51.562,70.312,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.352 0.414 0.234
Batch: 20 | Loss: 4.304 | Acc: 48.065,68.378,72.545,% | Adaptive Acc: 68.490% | clf_exit: 0.342 0.415 0.243
Batch: 40 | Loss: 4.258 | Acc: 48.018,68.140,72.351,% | Adaptive Acc: 68.864% | clf_exit: 0.340 0.403 0.257
Batch: 60 | Loss: 4.269 | Acc: 47.964,67.918,72.464,% | Adaptive Acc: 68.865% | clf_exit: 0.337 0.404 0.259
Train all parameters

Epoch: 231
Batch: 0 | Loss: 2.538 | Acc: 49.219,89.062,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.258 0.477 0.266
Batch: 20 | Loss: 2.470 | Acc: 51.339,86.905,99.926,% | Adaptive Acc: 93.750% | clf_exit: 0.281 0.480 0.239
Batch: 40 | Loss: 2.490 | Acc: 51.772,86.947,99.886,% | Adaptive Acc: 93.826% | clf_exit: 0.275 0.489 0.236
Batch: 60 | Loss: 2.474 | Acc: 52.305,86.770,99.885,% | Adaptive Acc: 94.083% | clf_exit: 0.278 0.487 0.235
Batch: 80 | Loss: 2.500 | Acc: 51.881,86.487,99.894,% | Adaptive Acc: 94.068% | clf_exit: 0.274 0.484 0.241
Batch: 100 | Loss: 2.493 | Acc: 51.887,86.657,99.899,% | Adaptive Acc: 94.183% | clf_exit: 0.274 0.487 0.239
Batch: 120 | Loss: 2.503 | Acc: 51.685,86.460,99.903,% | Adaptive Acc: 94.053% | clf_exit: 0.273 0.487 0.240
Batch: 140 | Loss: 2.513 | Acc: 51.385,86.342,99.917,% | Adaptive Acc: 94.010% | clf_exit: 0.270 0.488 0.242
Batch: 160 | Loss: 2.511 | Acc: 51.509,86.369,99.913,% | Adaptive Acc: 94.007% | clf_exit: 0.269 0.489 0.241
Batch: 180 | Loss: 2.509 | Acc: 51.524,86.486,99.901,% | Adaptive Acc: 94.009% | clf_exit: 0.270 0.489 0.241
Batch: 200 | Loss: 2.504 | Acc: 51.629,86.392,99.880,% | Adaptive Acc: 93.937% | clf_exit: 0.270 0.490 0.240
Batch: 220 | Loss: 2.510 | Acc: 51.616,86.256,99.876,% | Adaptive Acc: 93.934% | clf_exit: 0.269 0.490 0.241
Batch: 240 | Loss: 2.509 | Acc: 51.533,86.265,99.874,% | Adaptive Acc: 93.915% | clf_exit: 0.269 0.491 0.241
Batch: 260 | Loss: 2.515 | Acc: 51.410,86.174,99.865,% | Adaptive Acc: 93.927% | clf_exit: 0.267 0.491 0.242
Batch: 280 | Loss: 2.511 | Acc: 51.485,86.274,99.872,% | Adaptive Acc: 93.975% | clf_exit: 0.267 0.492 0.241
Batch: 300 | Loss: 2.513 | Acc: 51.438,86.265,99.868,% | Adaptive Acc: 93.960% | clf_exit: 0.267 0.492 0.241
Batch: 320 | Loss: 2.513 | Acc: 51.477,86.293,99.859,% | Adaptive Acc: 93.996% | clf_exit: 0.267 0.492 0.241
Batch: 340 | Loss: 2.512 | Acc: 51.501,86.325,99.858,% | Adaptive Acc: 93.938% | clf_exit: 0.267 0.491 0.242
Batch: 360 | Loss: 2.513 | Acc: 51.517,86.284,99.859,% | Adaptive Acc: 93.997% | clf_exit: 0.267 0.490 0.242
Batch: 380 | Loss: 2.513 | Acc: 51.517,86.305,99.852,% | Adaptive Acc: 93.976% | clf_exit: 0.268 0.490 0.243
Batch: 0 | Loss: 3.848 | Acc: 50.781,71.094,77.344,% | Adaptive Acc: 72.656% | clf_exit: 0.336 0.422 0.242
Batch: 20 | Loss: 4.292 | Acc: 48.065,67.411,72.173,% | Adaptive Acc: 67.932% | clf_exit: 0.342 0.407 0.250
Batch: 40 | Loss: 4.255 | Acc: 48.361,67.607,72.085,% | Adaptive Acc: 68.407% | clf_exit: 0.339 0.402 0.259
Batch: 60 | Loss: 4.266 | Acc: 48.053,67.546,72.246,% | Adaptive Acc: 68.532% | clf_exit: 0.338 0.401 0.261
Train all parameters

Epoch: 232
Batch: 0 | Loss: 2.568 | Acc: 51.562,87.500,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.266 0.461 0.273
Batch: 20 | Loss: 2.483 | Acc: 52.790,86.682,99.963,% | Adaptive Acc: 94.568% | clf_exit: 0.271 0.491 0.238
Batch: 40 | Loss: 2.470 | Acc: 52.268,86.738,99.905,% | Adaptive Acc: 94.569% | clf_exit: 0.268 0.493 0.239
Batch: 60 | Loss: 2.492 | Acc: 51.678,86.347,99.898,% | Adaptive Acc: 94.147% | clf_exit: 0.265 0.493 0.241
Batch: 80 | Loss: 2.495 | Acc: 51.939,86.304,99.894,% | Adaptive Acc: 94.155% | clf_exit: 0.266 0.491 0.244
Batch: 100 | Loss: 2.501 | Acc: 51.709,86.170,99.907,% | Adaptive Acc: 94.098% | clf_exit: 0.265 0.489 0.246
Batch: 120 | Loss: 2.503 | Acc: 51.756,86.306,99.890,% | Adaptive Acc: 94.202% | clf_exit: 0.263 0.492 0.245
Batch: 140 | Loss: 2.499 | Acc: 51.928,86.342,99.900,% | Adaptive Acc: 94.166% | clf_exit: 0.265 0.490 0.245
Batch: 160 | Loss: 2.504 | Acc: 51.960,86.272,99.903,% | Adaptive Acc: 94.182% | clf_exit: 0.266 0.489 0.245
Batch: 180 | Loss: 2.503 | Acc: 51.895,86.386,99.909,% | Adaptive Acc: 94.233% | clf_exit: 0.265 0.490 0.245
Batch: 200 | Loss: 2.507 | Acc: 51.854,86.307,99.895,% | Adaptive Acc: 94.205% | clf_exit: 0.264 0.490 0.245
Batch: 220 | Loss: 2.508 | Acc: 51.828,86.330,99.894,% | Adaptive Acc: 94.213% | clf_exit: 0.264 0.490 0.246
Batch: 240 | Loss: 2.501 | Acc: 52.071,86.430,99.890,% | Adaptive Acc: 94.285% | clf_exit: 0.264 0.491 0.245
Batch: 260 | Loss: 2.507 | Acc: 51.889,86.384,99.886,% | Adaptive Acc: 94.220% | clf_exit: 0.264 0.491 0.245
Batch: 280 | Loss: 2.505 | Acc: 51.824,86.421,99.886,% | Adaptive Acc: 94.223% | clf_exit: 0.265 0.490 0.245
Batch: 300 | Loss: 2.504 | Acc: 51.822,86.464,99.888,% | Adaptive Acc: 94.176% | clf_exit: 0.266 0.491 0.243
Batch: 320 | Loss: 2.510 | Acc: 51.696,86.324,99.883,% | Adaptive Acc: 94.105% | clf_exit: 0.266 0.491 0.244
Batch: 340 | Loss: 2.511 | Acc: 51.698,86.263,99.883,% | Adaptive Acc: 94.110% | clf_exit: 0.266 0.490 0.244
Batch: 360 | Loss: 2.517 | Acc: 51.593,86.186,99.885,% | Adaptive Acc: 94.098% | clf_exit: 0.265 0.490 0.244
Batch: 380 | Loss: 2.515 | Acc: 51.722,86.190,99.879,% | Adaptive Acc: 94.133% | clf_exit: 0.266 0.490 0.245
Batch: 0 | Loss: 3.858 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.414 0.250
Batch: 20 | Loss: 4.286 | Acc: 48.512,67.746,72.396,% | Adaptive Acc: 68.601% | clf_exit: 0.334 0.416 0.250
Batch: 40 | Loss: 4.247 | Acc: 48.323,67.702,72.580,% | Adaptive Acc: 69.093% | clf_exit: 0.334 0.403 0.263
Batch: 60 | Loss: 4.259 | Acc: 48.079,67.687,72.503,% | Adaptive Acc: 68.916% | clf_exit: 0.335 0.404 0.261
Train all parameters

Epoch: 233
Batch: 0 | Loss: 2.531 | Acc: 50.000,85.156,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.219 0.547 0.234
Batch: 20 | Loss: 2.520 | Acc: 51.302,85.863,99.888,% | Adaptive Acc: 94.085% | clf_exit: 0.281 0.482 0.237
Batch: 40 | Loss: 2.515 | Acc: 50.572,85.880,99.848,% | Adaptive Acc: 93.655% | clf_exit: 0.275 0.489 0.236
Batch: 60 | Loss: 2.517 | Acc: 50.909,86.283,99.821,% | Adaptive Acc: 93.737% | clf_exit: 0.271 0.496 0.233
Batch: 80 | Loss: 2.525 | Acc: 50.858,86.150,99.836,% | Adaptive Acc: 93.644% | clf_exit: 0.269 0.495 0.235
Batch: 100 | Loss: 2.520 | Acc: 51.075,86.293,99.869,% | Adaptive Acc: 93.773% | clf_exit: 0.270 0.493 0.237
Batch: 120 | Loss: 2.510 | Acc: 51.175,86.493,99.858,% | Adaptive Acc: 93.718% | clf_exit: 0.270 0.494 0.235
Batch: 140 | Loss: 2.500 | Acc: 51.424,86.541,99.856,% | Adaptive Acc: 93.783% | clf_exit: 0.273 0.491 0.236
Batch: 160 | Loss: 2.502 | Acc: 51.339,86.496,99.864,% | Adaptive Acc: 93.837% | clf_exit: 0.270 0.493 0.237
Batch: 180 | Loss: 2.508 | Acc: 51.273,86.365,99.862,% | Adaptive Acc: 93.867% | clf_exit: 0.269 0.491 0.240
Batch: 200 | Loss: 2.499 | Acc: 51.446,86.548,99.852,% | Adaptive Acc: 93.909% | clf_exit: 0.271 0.490 0.239
Batch: 220 | Loss: 2.504 | Acc: 51.410,86.401,99.848,% | Adaptive Acc: 93.948% | clf_exit: 0.269 0.490 0.241
Batch: 240 | Loss: 2.508 | Acc: 51.417,86.333,99.838,% | Adaptive Acc: 93.977% | clf_exit: 0.269 0.490 0.241
Batch: 260 | Loss: 2.506 | Acc: 51.521,86.389,99.838,% | Adaptive Acc: 93.980% | clf_exit: 0.269 0.490 0.241
Batch: 280 | Loss: 2.501 | Acc: 51.665,86.463,99.839,% | Adaptive Acc: 94.064% | clf_exit: 0.268 0.490 0.241
Batch: 300 | Loss: 2.508 | Acc: 51.557,86.361,99.836,% | Adaptive Acc: 94.051% | clf_exit: 0.269 0.490 0.242
Batch: 320 | Loss: 2.508 | Acc: 51.543,86.385,99.839,% | Adaptive Acc: 94.042% | clf_exit: 0.268 0.490 0.241
Batch: 340 | Loss: 2.509 | Acc: 51.448,86.387,99.842,% | Adaptive Acc: 93.997% | clf_exit: 0.268 0.491 0.241
Batch: 360 | Loss: 2.509 | Acc: 51.495,86.327,99.846,% | Adaptive Acc: 94.003% | clf_exit: 0.268 0.491 0.241
Batch: 380 | Loss: 2.506 | Acc: 51.530,86.376,99.846,% | Adaptive Acc: 93.992% | clf_exit: 0.268 0.491 0.241
Batch: 0 | Loss: 3.865 | Acc: 50.781,68.750,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.328 0.445 0.227
Batch: 20 | Loss: 4.279 | Acc: 47.879,67.634,72.656,% | Adaptive Acc: 69.010% | clf_exit: 0.334 0.415 0.251
Batch: 40 | Loss: 4.246 | Acc: 48.133,67.759,72.389,% | Adaptive Acc: 69.055% | clf_exit: 0.337 0.404 0.260
Batch: 60 | Loss: 4.264 | Acc: 47.823,67.649,72.477,% | Adaptive Acc: 68.942% | clf_exit: 0.335 0.404 0.261
Train all parameters

Epoch: 234
Batch: 0 | Loss: 2.594 | Acc: 44.531,81.250,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.273 0.500 0.227
Batch: 20 | Loss: 2.449 | Acc: 52.009,86.458,99.963,% | Adaptive Acc: 94.420% | clf_exit: 0.269 0.505 0.226
Batch: 40 | Loss: 2.493 | Acc: 51.620,86.109,99.962,% | Adaptive Acc: 93.845% | clf_exit: 0.270 0.495 0.236
Batch: 60 | Loss: 2.492 | Acc: 51.639,85.835,99.936,% | Adaptive Acc: 93.916% | clf_exit: 0.268 0.496 0.236
Batch: 80 | Loss: 2.493 | Acc: 51.726,86.314,99.923,% | Adaptive Acc: 94.068% | clf_exit: 0.267 0.495 0.238
Batch: 100 | Loss: 2.499 | Acc: 51.532,86.317,99.899,% | Adaptive Acc: 93.897% | clf_exit: 0.264 0.498 0.238
Batch: 120 | Loss: 2.512 | Acc: 51.362,86.118,99.903,% | Adaptive Acc: 93.821% | clf_exit: 0.267 0.492 0.241
Batch: 140 | Loss: 2.498 | Acc: 51.557,86.464,99.906,% | Adaptive Acc: 93.850% | clf_exit: 0.269 0.493 0.238
Batch: 160 | Loss: 2.499 | Acc: 51.538,86.466,99.908,% | Adaptive Acc: 93.866% | clf_exit: 0.269 0.491 0.240
Batch: 180 | Loss: 2.510 | Acc: 51.433,86.399,99.909,% | Adaptive Acc: 93.905% | clf_exit: 0.268 0.491 0.241
Batch: 200 | Loss: 2.508 | Acc: 51.625,86.412,99.914,% | Adaptive Acc: 93.948% | clf_exit: 0.269 0.489 0.242
Batch: 220 | Loss: 2.507 | Acc: 51.570,86.478,99.897,% | Adaptive Acc: 94.036% | clf_exit: 0.267 0.492 0.241
Batch: 240 | Loss: 2.509 | Acc: 51.618,86.446,99.903,% | Adaptive Acc: 93.957% | clf_exit: 0.268 0.491 0.241
Batch: 260 | Loss: 2.509 | Acc: 51.565,86.428,99.892,% | Adaptive Acc: 93.983% | clf_exit: 0.267 0.491 0.242
Batch: 280 | Loss: 2.508 | Acc: 51.557,86.343,99.889,% | Adaptive Acc: 93.995% | clf_exit: 0.268 0.491 0.241
Batch: 300 | Loss: 2.512 | Acc: 51.544,86.278,99.883,% | Adaptive Acc: 93.929% | clf_exit: 0.267 0.492 0.241
Batch: 320 | Loss: 2.513 | Acc: 51.618,86.239,99.866,% | Adaptive Acc: 93.952% | clf_exit: 0.267 0.491 0.242
Batch: 340 | Loss: 2.512 | Acc: 51.597,86.288,99.872,% | Adaptive Acc: 93.975% | clf_exit: 0.268 0.491 0.241
Batch: 360 | Loss: 2.509 | Acc: 51.597,86.310,99.870,% | Adaptive Acc: 93.960% | clf_exit: 0.268 0.492 0.240
Batch: 380 | Loss: 2.510 | Acc: 51.624,86.247,99.865,% | Adaptive Acc: 93.969% | clf_exit: 0.267 0.492 0.241
Batch: 0 | Loss: 3.812 | Acc: 50.000,72.656,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.344 0.445 0.211
Batch: 20 | Loss: 4.300 | Acc: 47.917,68.043,72.470,% | Adaptive Acc: 68.452% | clf_exit: 0.345 0.407 0.249
Batch: 40 | Loss: 4.267 | Acc: 48.114,67.778,72.389,% | Adaptive Acc: 68.883% | clf_exit: 0.340 0.402 0.258
Batch: 60 | Loss: 4.282 | Acc: 47.823,67.546,72.259,% | Adaptive Acc: 68.532% | clf_exit: 0.338 0.404 0.257
Train classifier parameters

Epoch: 235
Batch: 0 | Loss: 2.581 | Acc: 54.688,85.156,99.219,% | Adaptive Acc: 92.969% | clf_exit: 0.281 0.484 0.234
Batch: 20 | Loss: 2.513 | Acc: 51.228,86.793,99.888,% | Adaptive Acc: 93.936% | clf_exit: 0.269 0.490 0.241
Batch: 40 | Loss: 2.484 | Acc: 51.829,86.662,99.924,% | Adaptive Acc: 94.150% | clf_exit: 0.276 0.481 0.243
Batch: 60 | Loss: 2.486 | Acc: 51.831,86.988,99.923,% | Adaptive Acc: 94.121% | clf_exit: 0.273 0.486 0.241
Batch: 80 | Loss: 2.471 | Acc: 52.160,86.950,99.913,% | Adaptive Acc: 94.117% | clf_exit: 0.276 0.484 0.239
Batch: 100 | Loss: 2.483 | Acc: 51.764,86.750,99.884,% | Adaptive Acc: 94.013% | clf_exit: 0.274 0.485 0.241
Batch: 120 | Loss: 2.496 | Acc: 51.537,86.603,99.890,% | Adaptive Acc: 93.963% | clf_exit: 0.273 0.485 0.242
Batch: 140 | Loss: 2.492 | Acc: 51.701,86.613,99.900,% | Adaptive Acc: 93.999% | clf_exit: 0.274 0.485 0.241
Batch: 160 | Loss: 2.485 | Acc: 51.776,86.704,99.908,% | Adaptive Acc: 94.080% | clf_exit: 0.275 0.485 0.240
Batch: 180 | Loss: 2.479 | Acc: 51.903,86.801,99.892,% | Adaptive Acc: 94.143% | clf_exit: 0.275 0.486 0.239
Batch: 200 | Loss: 2.480 | Acc: 52.033,86.820,99.891,% | Adaptive Acc: 94.178% | clf_exit: 0.274 0.487 0.239
Batch: 220 | Loss: 2.484 | Acc: 51.877,86.814,99.894,% | Adaptive Acc: 94.185% | clf_exit: 0.273 0.488 0.239
Batch: 240 | Loss: 2.483 | Acc: 51.981,86.819,99.896,% | Adaptive Acc: 94.178% | clf_exit: 0.273 0.487 0.240
Batch: 260 | Loss: 2.484 | Acc: 52.017,86.746,99.883,% | Adaptive Acc: 94.187% | clf_exit: 0.273 0.488 0.240
Batch: 280 | Loss: 2.485 | Acc: 51.949,86.741,99.878,% | Adaptive Acc: 94.198% | clf_exit: 0.272 0.489 0.239
Batch: 300 | Loss: 2.488 | Acc: 51.864,86.662,99.883,% | Adaptive Acc: 94.199% | clf_exit: 0.272 0.488 0.240
Batch: 320 | Loss: 2.489 | Acc: 51.833,86.665,99.883,% | Adaptive Acc: 94.144% | clf_exit: 0.272 0.489 0.240
Batch: 340 | Loss: 2.490 | Acc: 51.851,86.641,99.888,% | Adaptive Acc: 94.139% | clf_exit: 0.271 0.488 0.240
Batch: 360 | Loss: 2.493 | Acc: 51.811,86.595,99.887,% | Adaptive Acc: 94.131% | clf_exit: 0.271 0.489 0.240
Batch: 380 | Loss: 2.496 | Acc: 51.802,86.600,99.889,% | Adaptive Acc: 94.185% | clf_exit: 0.270 0.489 0.241
Batch: 0 | Loss: 3.800 | Acc: 51.562,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.287 | Acc: 48.103,67.671,72.582,% | Adaptive Acc: 68.341% | clf_exit: 0.343 0.413 0.245
Batch: 40 | Loss: 4.254 | Acc: 48.361,67.740,72.447,% | Adaptive Acc: 68.807% | clf_exit: 0.337 0.407 0.256
Batch: 60 | Loss: 4.266 | Acc: 48.066,67.533,72.439,% | Adaptive Acc: 68.699% | clf_exit: 0.336 0.406 0.258
Train classifier parameters

Epoch: 236
Batch: 0 | Loss: 2.269 | Acc: 53.125,88.281,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.258 0.500 0.242
Batch: 20 | Loss: 2.464 | Acc: 51.786,86.533,99.963,% | Adaptive Acc: 94.420% | clf_exit: 0.253 0.501 0.246
Batch: 40 | Loss: 2.495 | Acc: 51.391,86.414,99.943,% | Adaptive Acc: 94.284% | clf_exit: 0.254 0.503 0.243
Batch: 60 | Loss: 2.495 | Acc: 51.486,86.488,99.885,% | Adaptive Acc: 94.160% | clf_exit: 0.262 0.496 0.243
Batch: 80 | Loss: 2.496 | Acc: 51.823,86.372,99.913,% | Adaptive Acc: 94.174% | clf_exit: 0.266 0.492 0.242
Batch: 100 | Loss: 2.483 | Acc: 51.996,86.541,99.907,% | Adaptive Acc: 94.307% | clf_exit: 0.268 0.492 0.240
Batch: 120 | Loss: 2.483 | Acc: 51.950,86.532,99.916,% | Adaptive Acc: 94.299% | clf_exit: 0.267 0.493 0.241
Batch: 140 | Loss: 2.487 | Acc: 51.889,86.375,99.900,% | Adaptive Acc: 94.182% | clf_exit: 0.269 0.491 0.240
Batch: 160 | Loss: 2.485 | Acc: 52.057,86.360,99.903,% | Adaptive Acc: 94.221% | clf_exit: 0.270 0.492 0.239
Batch: 180 | Loss: 2.481 | Acc: 52.063,86.503,99.905,% | Adaptive Acc: 94.294% | clf_exit: 0.270 0.492 0.238
Batch: 200 | Loss: 2.481 | Acc: 52.157,86.528,99.907,% | Adaptive Acc: 94.317% | clf_exit: 0.268 0.494 0.238
Batch: 220 | Loss: 2.482 | Acc: 52.114,86.528,99.912,% | Adaptive Acc: 94.330% | clf_exit: 0.268 0.494 0.238
Batch: 240 | Loss: 2.482 | Acc: 52.097,86.654,99.909,% | Adaptive Acc: 94.298% | clf_exit: 0.268 0.495 0.237
Batch: 260 | Loss: 2.484 | Acc: 51.946,86.611,99.907,% | Adaptive Acc: 94.217% | clf_exit: 0.268 0.495 0.237
Batch: 280 | Loss: 2.488 | Acc: 51.896,86.571,99.905,% | Adaptive Acc: 94.181% | clf_exit: 0.268 0.493 0.238
Batch: 300 | Loss: 2.489 | Acc: 51.895,86.589,99.909,% | Adaptive Acc: 94.170% | clf_exit: 0.268 0.493 0.239
Batch: 320 | Loss: 2.489 | Acc: 51.818,86.573,99.912,% | Adaptive Acc: 94.132% | clf_exit: 0.269 0.493 0.238
Batch: 340 | Loss: 2.491 | Acc: 51.824,86.508,99.913,% | Adaptive Acc: 94.034% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.492 | Acc: 51.796,86.565,99.907,% | Adaptive Acc: 94.038% | clf_exit: 0.269 0.493 0.238
Batch: 380 | Loss: 2.491 | Acc: 51.809,86.585,99.912,% | Adaptive Acc: 94.053% | clf_exit: 0.269 0.494 0.238
Batch: 0 | Loss: 3.777 | Acc: 51.562,70.312,78.125,% | Adaptive Acc: 72.656% | clf_exit: 0.344 0.414 0.242
Batch: 20 | Loss: 4.284 | Acc: 48.214,67.485,72.321,% | Adaptive Acc: 68.676% | clf_exit: 0.337 0.414 0.249
Batch: 40 | Loss: 4.252 | Acc: 48.266,67.626,72.389,% | Adaptive Acc: 68.845% | clf_exit: 0.336 0.405 0.258
Batch: 60 | Loss: 4.265 | Acc: 47.989,67.418,72.310,% | Adaptive Acc: 68.686% | clf_exit: 0.335 0.405 0.260
Train classifier parameters

Epoch: 237
Batch: 0 | Loss: 2.737 | Acc: 50.000,83.594,99.219,% | Adaptive Acc: 96.094% | clf_exit: 0.234 0.500 0.266
Batch: 20 | Loss: 2.533 | Acc: 50.856,86.607,99.740,% | Adaptive Acc: 93.899% | clf_exit: 0.259 0.501 0.240
Batch: 40 | Loss: 2.521 | Acc: 51.105,86.547,99.771,% | Adaptive Acc: 94.150% | clf_exit: 0.256 0.502 0.242
Batch: 60 | Loss: 2.521 | Acc: 51.063,86.399,99.846,% | Adaptive Acc: 94.045% | clf_exit: 0.260 0.497 0.243
Batch: 80 | Loss: 2.504 | Acc: 51.321,86.777,99.875,% | Adaptive Acc: 94.088% | clf_exit: 0.261 0.499 0.239
Batch: 100 | Loss: 2.511 | Acc: 51.400,86.618,99.876,% | Adaptive Acc: 93.951% | clf_exit: 0.263 0.497 0.240
Batch: 120 | Loss: 2.509 | Acc: 51.550,86.557,99.884,% | Adaptive Acc: 93.963% | clf_exit: 0.265 0.495 0.241
Batch: 140 | Loss: 2.501 | Acc: 51.795,86.591,99.889,% | Adaptive Acc: 94.077% | clf_exit: 0.265 0.495 0.240
Batch: 160 | Loss: 2.503 | Acc: 51.664,86.549,99.893,% | Adaptive Acc: 94.056% | clf_exit: 0.266 0.494 0.240
Batch: 180 | Loss: 2.490 | Acc: 51.834,86.714,99.901,% | Adaptive Acc: 94.044% | clf_exit: 0.268 0.494 0.238
Batch: 200 | Loss: 2.490 | Acc: 51.749,86.715,99.907,% | Adaptive Acc: 93.995% | clf_exit: 0.268 0.495 0.237
Batch: 220 | Loss: 2.489 | Acc: 51.803,86.740,99.908,% | Adaptive Acc: 94.012% | clf_exit: 0.267 0.496 0.237
Batch: 240 | Loss: 2.489 | Acc: 51.922,86.686,99.890,% | Adaptive Acc: 94.035% | clf_exit: 0.267 0.496 0.237
Batch: 260 | Loss: 2.490 | Acc: 51.946,86.680,99.886,% | Adaptive Acc: 94.031% | clf_exit: 0.267 0.496 0.237
Batch: 280 | Loss: 2.488 | Acc: 51.949,86.702,99.889,% | Adaptive Acc: 94.103% | clf_exit: 0.267 0.495 0.237
Batch: 300 | Loss: 2.494 | Acc: 51.814,86.607,99.886,% | Adaptive Acc: 94.085% | clf_exit: 0.268 0.494 0.239
Batch: 320 | Loss: 2.498 | Acc: 51.794,86.519,99.886,% | Adaptive Acc: 94.088% | clf_exit: 0.267 0.493 0.240
Batch: 340 | Loss: 2.499 | Acc: 51.769,86.531,99.883,% | Adaptive Acc: 94.098% | clf_exit: 0.267 0.493 0.240
Batch: 360 | Loss: 2.498 | Acc: 51.779,86.537,99.885,% | Adaptive Acc: 94.075% | clf_exit: 0.267 0.494 0.240
Batch: 380 | Loss: 2.497 | Acc: 51.772,86.555,99.885,% | Adaptive Acc: 94.076% | clf_exit: 0.267 0.494 0.239
Batch: 0 | Loss: 3.842 | Acc: 51.562,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.295 | Acc: 48.326,67.634,72.433,% | Adaptive Acc: 68.452% | clf_exit: 0.347 0.401 0.252
Batch: 40 | Loss: 4.260 | Acc: 48.438,67.835,72.199,% | Adaptive Acc: 68.617% | clf_exit: 0.344 0.396 0.260
Batch: 60 | Loss: 4.273 | Acc: 48.117,67.700,72.195,% | Adaptive Acc: 68.545% | clf_exit: 0.340 0.398 0.262
Train classifier parameters

Epoch: 238
Batch: 0 | Loss: 2.405 | Acc: 55.469,89.844,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.242 0.547 0.211
Batch: 20 | Loss: 2.466 | Acc: 51.600,87.649,99.777,% | Adaptive Acc: 94.085% | clf_exit: 0.256 0.502 0.243
Batch: 40 | Loss: 2.496 | Acc: 51.753,87.252,99.886,% | Adaptive Acc: 94.455% | clf_exit: 0.256 0.504 0.240
Batch: 60 | Loss: 2.488 | Acc: 52.024,87.077,99.898,% | Adaptive Acc: 94.442% | clf_exit: 0.258 0.504 0.238
Batch: 80 | Loss: 2.488 | Acc: 52.160,86.854,99.884,% | Adaptive Acc: 94.184% | clf_exit: 0.259 0.503 0.238
Batch: 100 | Loss: 2.479 | Acc: 52.197,87.113,99.899,% | Adaptive Acc: 94.183% | clf_exit: 0.261 0.504 0.235
Batch: 120 | Loss: 2.480 | Acc: 52.111,87.177,99.910,% | Adaptive Acc: 94.144% | clf_exit: 0.263 0.502 0.236
Batch: 140 | Loss: 2.474 | Acc: 52.056,87.079,99.911,% | Adaptive Acc: 94.121% | clf_exit: 0.264 0.500 0.236
Batch: 160 | Loss: 2.475 | Acc: 52.106,87.024,99.903,% | Adaptive Acc: 94.138% | clf_exit: 0.266 0.498 0.237
Batch: 180 | Loss: 2.480 | Acc: 52.072,86.982,99.905,% | Adaptive Acc: 94.134% | clf_exit: 0.267 0.496 0.237
Batch: 200 | Loss: 2.478 | Acc: 52.009,86.995,99.903,% | Adaptive Acc: 94.111% | clf_exit: 0.268 0.496 0.236
Batch: 220 | Loss: 2.479 | Acc: 52.075,86.984,99.897,% | Adaptive Acc: 94.107% | clf_exit: 0.268 0.496 0.236
Batch: 240 | Loss: 2.485 | Acc: 51.961,86.900,99.900,% | Adaptive Acc: 94.129% | clf_exit: 0.268 0.495 0.238
Batch: 260 | Loss: 2.485 | Acc: 51.976,86.904,99.892,% | Adaptive Acc: 94.181% | clf_exit: 0.267 0.495 0.238
Batch: 280 | Loss: 2.490 | Acc: 51.854,86.899,99.886,% | Adaptive Acc: 94.159% | clf_exit: 0.268 0.494 0.238
Batch: 300 | Loss: 2.491 | Acc: 51.744,86.874,99.888,% | Adaptive Acc: 94.155% | clf_exit: 0.268 0.494 0.238
Batch: 320 | Loss: 2.491 | Acc: 51.791,86.777,99.895,% | Adaptive Acc: 94.149% | clf_exit: 0.267 0.494 0.239
Batch: 340 | Loss: 2.492 | Acc: 51.762,86.714,99.885,% | Adaptive Acc: 94.091% | clf_exit: 0.267 0.495 0.238
Batch: 360 | Loss: 2.493 | Acc: 51.753,86.727,99.887,% | Adaptive Acc: 94.075% | clf_exit: 0.268 0.494 0.238
Batch: 380 | Loss: 2.492 | Acc: 51.741,86.729,99.891,% | Adaptive Acc: 94.070% | clf_exit: 0.267 0.495 0.238
Batch: 0 | Loss: 3.793 | Acc: 50.781,69.531,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.430 0.227
Batch: 20 | Loss: 4.294 | Acc: 48.512,67.485,72.210,% | Adaptive Acc: 68.341% | clf_exit: 0.341 0.413 0.247
Batch: 40 | Loss: 4.257 | Acc: 48.457,67.550,72.180,% | Adaptive Acc: 68.426% | clf_exit: 0.338 0.407 0.255
Batch: 60 | Loss: 4.269 | Acc: 48.207,67.418,72.067,% | Adaptive Acc: 68.327% | clf_exit: 0.337 0.406 0.257
Train classifier parameters

Epoch: 239
Batch: 0 | Loss: 2.489 | Acc: 54.688,90.625,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.258 0.461 0.281
Batch: 20 | Loss: 2.453 | Acc: 52.902,86.979,100.000,% | Adaptive Acc: 94.159% | clf_exit: 0.266 0.500 0.235
Batch: 40 | Loss: 2.458 | Acc: 52.763,86.947,99.981,% | Adaptive Acc: 93.902% | clf_exit: 0.273 0.499 0.228
Batch: 60 | Loss: 2.459 | Acc: 52.344,86.834,99.949,% | Adaptive Acc: 93.891% | clf_exit: 0.272 0.497 0.230
Batch: 80 | Loss: 2.449 | Acc: 52.431,87.066,99.932,% | Adaptive Acc: 94.097% | clf_exit: 0.271 0.499 0.229
Batch: 100 | Loss: 2.459 | Acc: 52.282,86.804,99.907,% | Adaptive Acc: 94.044% | clf_exit: 0.270 0.501 0.229
Batch: 120 | Loss: 2.471 | Acc: 51.659,86.835,99.903,% | Adaptive Acc: 94.202% | clf_exit: 0.268 0.499 0.233
Batch: 140 | Loss: 2.483 | Acc: 51.540,86.691,99.911,% | Adaptive Acc: 94.166% | clf_exit: 0.268 0.499 0.234
Batch: 160 | Loss: 2.486 | Acc: 51.499,86.699,99.893,% | Adaptive Acc: 94.240% | clf_exit: 0.268 0.498 0.234
Batch: 180 | Loss: 2.481 | Acc: 51.679,86.814,99.896,% | Adaptive Acc: 94.233% | clf_exit: 0.268 0.497 0.235
Batch: 200 | Loss: 2.487 | Acc: 51.574,86.758,99.887,% | Adaptive Acc: 94.166% | clf_exit: 0.269 0.496 0.235
Batch: 220 | Loss: 2.491 | Acc: 51.552,86.761,99.890,% | Adaptive Acc: 94.142% | clf_exit: 0.269 0.494 0.237
Batch: 240 | Loss: 2.491 | Acc: 51.640,86.673,99.890,% | Adaptive Acc: 94.113% | clf_exit: 0.268 0.494 0.237
Batch: 260 | Loss: 2.490 | Acc: 51.592,86.635,99.886,% | Adaptive Acc: 94.082% | clf_exit: 0.270 0.493 0.238
Batch: 280 | Loss: 2.486 | Acc: 51.704,86.708,99.894,% | Adaptive Acc: 94.161% | clf_exit: 0.269 0.494 0.237
Batch: 300 | Loss: 2.489 | Acc: 51.640,86.566,99.899,% | Adaptive Acc: 94.093% | clf_exit: 0.270 0.492 0.238
Batch: 320 | Loss: 2.491 | Acc: 51.677,86.575,99.900,% | Adaptive Acc: 94.086% | clf_exit: 0.270 0.493 0.237
Batch: 340 | Loss: 2.495 | Acc: 51.611,86.579,99.897,% | Adaptive Acc: 94.055% | clf_exit: 0.270 0.492 0.238
Batch: 360 | Loss: 2.498 | Acc: 51.539,86.530,99.894,% | Adaptive Acc: 94.029% | clf_exit: 0.269 0.492 0.239
Batch: 380 | Loss: 2.498 | Acc: 51.476,86.563,99.895,% | Adaptive Acc: 94.064% | clf_exit: 0.269 0.492 0.239
Batch: 0 | Loss: 3.819 | Acc: 52.344,71.094,78.125,% | Adaptive Acc: 70.312% | clf_exit: 0.344 0.477 0.180
Batch: 20 | Loss: 4.304 | Acc: 48.214,67.411,72.098,% | Adaptive Acc: 68.043% | clf_exit: 0.341 0.414 0.244
Batch: 40 | Loss: 4.263 | Acc: 48.323,67.569,72.294,% | Adaptive Acc: 68.502% | clf_exit: 0.337 0.408 0.255
Batch: 60 | Loss: 4.274 | Acc: 48.028,67.405,72.259,% | Adaptive Acc: 68.391% | clf_exit: 0.338 0.404 0.258
Train classifier parameters

Epoch: 240
Batch: 0 | Loss: 2.459 | Acc: 54.688,84.375,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.266 0.484 0.250
Batch: 20 | Loss: 2.539 | Acc: 51.228,85.342,99.963,% | Adaptive Acc: 94.568% | clf_exit: 0.255 0.489 0.256
Batch: 40 | Loss: 2.499 | Acc: 51.543,86.452,99.905,% | Adaptive Acc: 94.646% | clf_exit: 0.261 0.495 0.244
Batch: 60 | Loss: 2.503 | Acc: 51.588,86.616,99.885,% | Adaptive Acc: 94.480% | clf_exit: 0.265 0.492 0.243
Batch: 80 | Loss: 2.510 | Acc: 51.572,86.806,99.875,% | Adaptive Acc: 94.473% | clf_exit: 0.263 0.494 0.242
Batch: 100 | Loss: 2.529 | Acc: 51.640,86.200,99.845,% | Adaptive Acc: 94.183% | clf_exit: 0.266 0.489 0.245
Batch: 120 | Loss: 2.528 | Acc: 51.653,86.170,99.845,% | Adaptive Acc: 94.002% | clf_exit: 0.268 0.489 0.244
Batch: 140 | Loss: 2.518 | Acc: 51.701,86.226,99.861,% | Adaptive Acc: 94.060% | clf_exit: 0.268 0.488 0.244
Batch: 160 | Loss: 2.516 | Acc: 51.698,86.219,99.864,% | Adaptive Acc: 94.027% | clf_exit: 0.267 0.490 0.243
Batch: 180 | Loss: 2.519 | Acc: 51.614,86.214,99.866,% | Adaptive Acc: 94.013% | clf_exit: 0.266 0.491 0.243
Batch: 200 | Loss: 2.517 | Acc: 51.605,86.237,99.868,% | Adaptive Acc: 94.034% | clf_exit: 0.267 0.490 0.243
Batch: 220 | Loss: 2.520 | Acc: 51.545,86.188,99.876,% | Adaptive Acc: 94.008% | clf_exit: 0.268 0.488 0.244
Batch: 240 | Loss: 2.519 | Acc: 51.550,86.138,99.880,% | Adaptive Acc: 93.954% | clf_exit: 0.268 0.488 0.244
Batch: 260 | Loss: 2.511 | Acc: 51.622,86.171,99.877,% | Adaptive Acc: 93.858% | clf_exit: 0.269 0.489 0.242
Batch: 280 | Loss: 2.509 | Acc: 51.568,86.238,99.875,% | Adaptive Acc: 93.892% | clf_exit: 0.269 0.489 0.242
Batch: 300 | Loss: 2.507 | Acc: 51.633,86.324,99.875,% | Adaptive Acc: 93.958% | clf_exit: 0.269 0.490 0.242
Batch: 320 | Loss: 2.502 | Acc: 51.679,86.458,99.881,% | Adaptive Acc: 93.986% | clf_exit: 0.269 0.490 0.241
Batch: 340 | Loss: 2.496 | Acc: 51.819,86.549,99.879,% | Adaptive Acc: 94.039% | clf_exit: 0.270 0.490 0.240
Batch: 360 | Loss: 2.493 | Acc: 51.848,86.556,99.885,% | Adaptive Acc: 94.016% | clf_exit: 0.270 0.490 0.239
Batch: 380 | Loss: 2.497 | Acc: 51.731,86.553,99.889,% | Adaptive Acc: 94.033% | clf_exit: 0.269 0.491 0.240
Batch: 0 | Loss: 3.813 | Acc: 50.000,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.288 | Acc: 48.103,67.597,72.210,% | Adaptive Acc: 68.452% | clf_exit: 0.346 0.406 0.248
Batch: 40 | Loss: 4.254 | Acc: 48.152,67.854,72.275,% | Adaptive Acc: 68.712% | clf_exit: 0.342 0.400 0.258
Batch: 60 | Loss: 4.269 | Acc: 47.951,67.649,72.246,% | Adaptive Acc: 68.584% | clf_exit: 0.342 0.400 0.258
Train classifier parameters

Epoch: 241
Batch: 0 | Loss: 2.466 | Acc: 50.000,91.406,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.273 0.477 0.250
Batch: 20 | Loss: 2.485 | Acc: 53.348,86.570,99.963,% | Adaptive Acc: 94.159% | clf_exit: 0.267 0.499 0.234
Batch: 40 | Loss: 2.494 | Acc: 51.867,86.776,99.924,% | Adaptive Acc: 93.636% | clf_exit: 0.265 0.498 0.237
Batch: 60 | Loss: 2.497 | Acc: 51.767,86.450,99.910,% | Adaptive Acc: 93.968% | clf_exit: 0.265 0.492 0.244
Batch: 80 | Loss: 2.507 | Acc: 51.591,86.603,99.904,% | Adaptive Acc: 94.223% | clf_exit: 0.265 0.490 0.246
Batch: 100 | Loss: 2.519 | Acc: 51.323,86.479,99.907,% | Adaptive Acc: 94.059% | clf_exit: 0.264 0.491 0.245
Batch: 120 | Loss: 2.514 | Acc: 51.369,86.583,99.910,% | Adaptive Acc: 94.176% | clf_exit: 0.263 0.491 0.245
Batch: 140 | Loss: 2.513 | Acc: 51.352,86.541,99.911,% | Adaptive Acc: 94.094% | clf_exit: 0.264 0.491 0.245
Batch: 160 | Loss: 2.506 | Acc: 51.558,86.617,99.903,% | Adaptive Acc: 94.090% | clf_exit: 0.265 0.491 0.243
Batch: 180 | Loss: 2.501 | Acc: 51.679,86.680,99.909,% | Adaptive Acc: 94.156% | clf_exit: 0.264 0.494 0.243
Batch: 200 | Loss: 2.502 | Acc: 51.648,86.703,99.907,% | Adaptive Acc: 94.080% | clf_exit: 0.264 0.494 0.242
Batch: 220 | Loss: 2.499 | Acc: 51.672,86.786,99.905,% | Adaptive Acc: 94.107% | clf_exit: 0.264 0.494 0.243
Batch: 240 | Loss: 2.503 | Acc: 51.572,86.793,99.912,% | Adaptive Acc: 94.035% | clf_exit: 0.264 0.495 0.241
Batch: 260 | Loss: 2.502 | Acc: 51.592,86.773,99.913,% | Adaptive Acc: 94.061% | clf_exit: 0.264 0.494 0.242
Batch: 280 | Loss: 2.500 | Acc: 51.660,86.774,99.917,% | Adaptive Acc: 94.022% | clf_exit: 0.265 0.494 0.241
Batch: 300 | Loss: 2.496 | Acc: 51.630,86.817,99.914,% | Adaptive Acc: 94.025% | clf_exit: 0.267 0.493 0.241
Batch: 320 | Loss: 2.497 | Acc: 51.618,86.828,99.908,% | Adaptive Acc: 94.069% | clf_exit: 0.267 0.493 0.241
Batch: 340 | Loss: 2.496 | Acc: 51.631,86.787,99.911,% | Adaptive Acc: 94.048% | clf_exit: 0.267 0.493 0.239
Batch: 360 | Loss: 2.496 | Acc: 51.686,86.712,99.911,% | Adaptive Acc: 94.070% | clf_exit: 0.268 0.492 0.239
Batch: 380 | Loss: 2.493 | Acc: 51.755,86.756,99.904,% | Adaptive Acc: 94.078% | clf_exit: 0.269 0.493 0.238
Batch: 0 | Loss: 3.785 | Acc: 52.344,69.531,78.906,% | Adaptive Acc: 70.312% | clf_exit: 0.344 0.445 0.211
Batch: 20 | Loss: 4.291 | Acc: 48.103,67.448,72.359,% | Adaptive Acc: 68.564% | clf_exit: 0.345 0.412 0.243
Batch: 40 | Loss: 4.258 | Acc: 48.133,67.530,72.332,% | Adaptive Acc: 68.712% | clf_exit: 0.339 0.408 0.253
Batch: 60 | Loss: 4.273 | Acc: 47.938,67.405,72.234,% | Adaptive Acc: 68.519% | clf_exit: 0.339 0.405 0.256
Train classifier parameters

Epoch: 242
Batch: 0 | Loss: 2.471 | Acc: 50.781,86.719,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.273 0.492 0.234
Batch: 20 | Loss: 2.509 | Acc: 50.893,86.421,99.777,% | Adaptive Acc: 94.382% | clf_exit: 0.262 0.501 0.237
Batch: 40 | Loss: 2.514 | Acc: 51.181,86.395,99.848,% | Adaptive Acc: 93.979% | clf_exit: 0.266 0.498 0.236
Batch: 60 | Loss: 2.502 | Acc: 51.780,86.642,99.872,% | Adaptive Acc: 93.993% | clf_exit: 0.263 0.501 0.236
Batch: 80 | Loss: 2.483 | Acc: 52.209,86.699,99.875,% | Adaptive Acc: 94.088% | clf_exit: 0.264 0.503 0.233
Batch: 100 | Loss: 2.479 | Acc: 52.282,86.765,99.876,% | Adaptive Acc: 94.044% | clf_exit: 0.266 0.501 0.233
Batch: 120 | Loss: 2.483 | Acc: 52.124,86.706,99.884,% | Adaptive Acc: 94.112% | clf_exit: 0.265 0.499 0.235
Batch: 140 | Loss: 2.479 | Acc: 52.000,86.879,99.878,% | Adaptive Acc: 94.177% | clf_exit: 0.265 0.499 0.235
Batch: 160 | Loss: 2.489 | Acc: 51.742,86.777,99.874,% | Adaptive Acc: 94.124% | clf_exit: 0.266 0.499 0.235
Batch: 180 | Loss: 2.487 | Acc: 51.778,86.680,99.875,% | Adaptive Acc: 94.026% | clf_exit: 0.267 0.497 0.235
Batch: 200 | Loss: 2.488 | Acc: 51.749,86.633,99.868,% | Adaptive Acc: 93.960% | clf_exit: 0.268 0.497 0.235
Batch: 220 | Loss: 2.485 | Acc: 51.813,86.645,99.873,% | Adaptive Acc: 93.994% | clf_exit: 0.269 0.495 0.236
Batch: 240 | Loss: 2.482 | Acc: 51.835,86.764,99.883,% | Adaptive Acc: 93.993% | clf_exit: 0.270 0.494 0.236
Batch: 260 | Loss: 2.489 | Acc: 51.784,86.722,99.874,% | Adaptive Acc: 93.960% | clf_exit: 0.270 0.493 0.237
Batch: 280 | Loss: 2.492 | Acc: 51.749,86.638,99.883,% | Adaptive Acc: 93.986% | clf_exit: 0.270 0.492 0.238
Batch: 300 | Loss: 2.488 | Acc: 51.796,86.682,99.883,% | Adaptive Acc: 94.028% | clf_exit: 0.269 0.493 0.238
Batch: 320 | Loss: 2.491 | Acc: 51.796,86.641,99.883,% | Adaptive Acc: 94.079% | clf_exit: 0.268 0.493 0.238
Batch: 340 | Loss: 2.493 | Acc: 51.824,86.590,99.876,% | Adaptive Acc: 94.112% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.495 | Acc: 51.751,86.574,99.881,% | Adaptive Acc: 94.124% | clf_exit: 0.268 0.494 0.238
Batch: 380 | Loss: 2.494 | Acc: 51.747,86.553,99.881,% | Adaptive Acc: 94.109% | clf_exit: 0.268 0.494 0.238
Batch: 0 | Loss: 3.810 | Acc: 53.125,69.531,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.287 | Acc: 48.103,67.671,72.396,% | Adaptive Acc: 68.266% | clf_exit: 0.347 0.407 0.246
Batch: 40 | Loss: 4.258 | Acc: 48.304,67.626,72.237,% | Adaptive Acc: 68.388% | clf_exit: 0.345 0.399 0.256
Batch: 60 | Loss: 4.276 | Acc: 47.989,67.508,72.106,% | Adaptive Acc: 68.199% | clf_exit: 0.344 0.398 0.258
Train classifier parameters

Epoch: 243
Batch: 0 | Loss: 2.688 | Acc: 50.781,85.938,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.203 0.570 0.227
Batch: 20 | Loss: 2.484 | Acc: 51.376,87.351,99.963,% | Adaptive Acc: 94.085% | clf_exit: 0.278 0.479 0.243
Batch: 40 | Loss: 2.465 | Acc: 51.963,87.348,99.905,% | Adaptive Acc: 93.655% | clf_exit: 0.276 0.491 0.232
Batch: 60 | Loss: 2.464 | Acc: 51.947,87.026,99.885,% | Adaptive Acc: 93.929% | clf_exit: 0.271 0.494 0.235
Batch: 80 | Loss: 2.464 | Acc: 51.910,86.883,99.884,% | Adaptive Acc: 94.030% | clf_exit: 0.266 0.498 0.236
Batch: 100 | Loss: 2.476 | Acc: 51.671,86.711,99.884,% | Adaptive Acc: 94.067% | clf_exit: 0.270 0.493 0.237
Batch: 120 | Loss: 2.488 | Acc: 51.595,86.615,99.884,% | Adaptive Acc: 94.112% | clf_exit: 0.268 0.493 0.239
Batch: 140 | Loss: 2.492 | Acc: 51.668,86.497,99.884,% | Adaptive Acc: 94.204% | clf_exit: 0.268 0.491 0.241
Batch: 160 | Loss: 2.484 | Acc: 51.892,86.631,99.888,% | Adaptive Acc: 94.289% | clf_exit: 0.269 0.492 0.239
Batch: 180 | Loss: 2.490 | Acc: 51.809,86.602,99.883,% | Adaptive Acc: 94.225% | clf_exit: 0.268 0.492 0.240
Batch: 200 | Loss: 2.493 | Acc: 51.769,86.575,99.887,% | Adaptive Acc: 94.220% | clf_exit: 0.268 0.493 0.240
Batch: 220 | Loss: 2.492 | Acc: 51.654,86.652,99.880,% | Adaptive Acc: 94.192% | clf_exit: 0.267 0.493 0.239
Batch: 240 | Loss: 2.492 | Acc: 51.618,86.609,99.874,% | Adaptive Acc: 94.100% | clf_exit: 0.268 0.493 0.239
Batch: 260 | Loss: 2.492 | Acc: 51.697,86.605,99.868,% | Adaptive Acc: 94.127% | clf_exit: 0.268 0.494 0.239
Batch: 280 | Loss: 2.497 | Acc: 51.665,86.580,99.864,% | Adaptive Acc: 94.106% | clf_exit: 0.268 0.493 0.239
Batch: 300 | Loss: 2.497 | Acc: 51.620,86.550,99.868,% | Adaptive Acc: 94.100% | clf_exit: 0.268 0.493 0.239
Batch: 320 | Loss: 2.498 | Acc: 51.640,86.558,99.873,% | Adaptive Acc: 94.118% | clf_exit: 0.267 0.494 0.238
Batch: 340 | Loss: 2.501 | Acc: 51.592,86.515,99.872,% | Adaptive Acc: 94.091% | clf_exit: 0.267 0.494 0.239
Batch: 360 | Loss: 2.499 | Acc: 51.692,86.528,99.874,% | Adaptive Acc: 94.094% | clf_exit: 0.268 0.494 0.238
Batch: 380 | Loss: 2.497 | Acc: 51.739,86.643,99.879,% | Adaptive Acc: 94.131% | clf_exit: 0.267 0.495 0.238
Batch: 0 | Loss: 3.828 | Acc: 52.344,70.312,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.352 0.414 0.234
Batch: 20 | Loss: 4.284 | Acc: 48.586,67.708,72.693,% | Adaptive Acc: 68.713% | clf_exit: 0.337 0.414 0.249
Batch: 40 | Loss: 4.255 | Acc: 48.552,67.683,72.409,% | Adaptive Acc: 68.921% | clf_exit: 0.336 0.403 0.260
Batch: 60 | Loss: 4.269 | Acc: 48.117,67.559,72.272,% | Adaptive Acc: 68.737% | clf_exit: 0.335 0.403 0.262
Train classifier parameters

Epoch: 244
Batch: 0 | Loss: 2.307 | Acc: 55.469,85.938,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.336 0.469 0.195
Batch: 20 | Loss: 2.489 | Acc: 52.418,85.379,99.888,% | Adaptive Acc: 94.159% | clf_exit: 0.269 0.482 0.249
Batch: 40 | Loss: 2.473 | Acc: 51.867,86.261,99.867,% | Adaptive Acc: 94.550% | clf_exit: 0.269 0.486 0.245
Batch: 60 | Loss: 2.495 | Acc: 51.627,86.373,99.898,% | Adaptive Acc: 94.249% | clf_exit: 0.263 0.492 0.245
Batch: 80 | Loss: 2.494 | Acc: 51.543,86.487,99.913,% | Adaptive Acc: 94.145% | clf_exit: 0.268 0.487 0.244
Batch: 100 | Loss: 2.497 | Acc: 51.493,86.595,99.915,% | Adaptive Acc: 94.028% | clf_exit: 0.270 0.488 0.242
Batch: 120 | Loss: 2.500 | Acc: 51.575,86.603,99.897,% | Adaptive Acc: 94.105% | clf_exit: 0.271 0.486 0.243
Batch: 140 | Loss: 2.505 | Acc: 51.413,86.575,99.861,% | Adaptive Acc: 94.049% | clf_exit: 0.269 0.488 0.243
Batch: 160 | Loss: 2.508 | Acc: 51.339,86.466,99.864,% | Adaptive Acc: 93.973% | clf_exit: 0.268 0.489 0.243
Batch: 180 | Loss: 2.509 | Acc: 51.291,86.460,99.862,% | Adaptive Acc: 94.031% | clf_exit: 0.268 0.489 0.243
Batch: 200 | Loss: 2.507 | Acc: 51.271,86.497,99.868,% | Adaptive Acc: 93.991% | clf_exit: 0.269 0.490 0.241
Batch: 220 | Loss: 2.510 | Acc: 51.382,86.475,99.866,% | Adaptive Acc: 94.033% | clf_exit: 0.269 0.489 0.242
Batch: 240 | Loss: 2.505 | Acc: 51.540,86.482,99.861,% | Adaptive Acc: 94.006% | clf_exit: 0.270 0.489 0.241
Batch: 260 | Loss: 2.500 | Acc: 51.634,86.536,99.865,% | Adaptive Acc: 94.076% | clf_exit: 0.269 0.491 0.240
Batch: 280 | Loss: 2.498 | Acc: 51.660,86.585,99.875,% | Adaptive Acc: 94.100% | clf_exit: 0.269 0.491 0.240
Batch: 300 | Loss: 2.497 | Acc: 51.726,86.584,99.875,% | Adaptive Acc: 94.090% | clf_exit: 0.269 0.491 0.240
Batch: 320 | Loss: 2.497 | Acc: 51.740,86.587,99.878,% | Adaptive Acc: 94.052% | clf_exit: 0.269 0.492 0.240
Batch: 340 | Loss: 2.494 | Acc: 51.739,86.600,99.876,% | Adaptive Acc: 94.027% | clf_exit: 0.269 0.491 0.239
Batch: 360 | Loss: 2.493 | Acc: 51.779,86.587,99.879,% | Adaptive Acc: 94.014% | clf_exit: 0.269 0.491 0.240
Batch: 380 | Loss: 2.496 | Acc: 51.700,86.542,99.877,% | Adaptive Acc: 94.002% | clf_exit: 0.270 0.491 0.240
Batch: 0 | Loss: 3.796 | Acc: 51.562,70.312,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.453 0.211
Batch: 20 | Loss: 4.290 | Acc: 48.289,67.820,72.247,% | Adaptive Acc: 68.415% | clf_exit: 0.340 0.411 0.249
Batch: 40 | Loss: 4.256 | Acc: 48.323,67.759,72.389,% | Adaptive Acc: 68.712% | clf_exit: 0.337 0.403 0.260
Batch: 60 | Loss: 4.268 | Acc: 48.040,67.559,72.413,% | Adaptive Acc: 68.648% | clf_exit: 0.337 0.402 0.261
Train classifier parameters

Epoch: 245
Batch: 0 | Loss: 2.681 | Acc: 46.875,83.594,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.281 0.508 0.211
Batch: 20 | Loss: 2.475 | Acc: 51.749,86.830,99.963,% | Adaptive Acc: 94.345% | clf_exit: 0.277 0.482 0.241
Batch: 40 | Loss: 2.488 | Acc: 51.734,86.376,99.867,% | Adaptive Acc: 93.788% | clf_exit: 0.275 0.489 0.236
Batch: 60 | Loss: 2.487 | Acc: 51.601,86.463,99.898,% | Adaptive Acc: 93.776% | clf_exit: 0.275 0.488 0.236
Batch: 80 | Loss: 2.491 | Acc: 51.524,86.420,99.894,% | Adaptive Acc: 93.818% | clf_exit: 0.276 0.487 0.237
Batch: 100 | Loss: 2.492 | Acc: 51.408,86.564,99.884,% | Adaptive Acc: 93.967% | clf_exit: 0.269 0.494 0.237
Batch: 120 | Loss: 2.486 | Acc: 51.543,86.667,99.884,% | Adaptive Acc: 93.937% | clf_exit: 0.270 0.492 0.237
Batch: 140 | Loss: 2.483 | Acc: 51.607,86.746,99.889,% | Adaptive Acc: 94.021% | clf_exit: 0.269 0.494 0.237
Batch: 160 | Loss: 2.488 | Acc: 51.553,86.728,99.884,% | Adaptive Acc: 94.056% | clf_exit: 0.268 0.493 0.239
Batch: 180 | Loss: 2.492 | Acc: 51.446,86.710,99.888,% | Adaptive Acc: 93.987% | clf_exit: 0.267 0.494 0.239
Batch: 200 | Loss: 2.487 | Acc: 51.508,86.762,99.887,% | Adaptive Acc: 94.010% | clf_exit: 0.267 0.494 0.238
Batch: 220 | Loss: 2.496 | Acc: 51.322,86.722,99.887,% | Adaptive Acc: 93.937% | clf_exit: 0.267 0.495 0.238
Batch: 240 | Loss: 2.494 | Acc: 51.345,86.719,99.890,% | Adaptive Acc: 93.970% | clf_exit: 0.267 0.494 0.239
Batch: 260 | Loss: 2.492 | Acc: 51.386,86.767,99.895,% | Adaptive Acc: 93.998% | clf_exit: 0.267 0.494 0.239
Batch: 280 | Loss: 2.495 | Acc: 51.376,86.733,99.903,% | Adaptive Acc: 94.020% | clf_exit: 0.266 0.495 0.239
Batch: 300 | Loss: 2.492 | Acc: 51.485,86.812,99.907,% | Adaptive Acc: 94.033% | clf_exit: 0.266 0.495 0.239
Batch: 320 | Loss: 2.492 | Acc: 51.507,86.763,99.903,% | Adaptive Acc: 94.030% | clf_exit: 0.266 0.495 0.239
Batch: 340 | Loss: 2.489 | Acc: 51.599,86.721,99.901,% | Adaptive Acc: 94.041% | clf_exit: 0.266 0.494 0.239
Batch: 360 | Loss: 2.492 | Acc: 51.580,86.719,99.903,% | Adaptive Acc: 94.049% | clf_exit: 0.266 0.495 0.239
Batch: 380 | Loss: 2.489 | Acc: 51.626,86.807,99.902,% | Adaptive Acc: 94.094% | clf_exit: 0.266 0.495 0.238
Batch: 0 | Loss: 3.795 | Acc: 51.562,71.094,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.461 0.203
Batch: 20 | Loss: 4.288 | Acc: 48.698,67.597,72.619,% | Adaptive Acc: 68.638% | clf_exit: 0.340 0.415 0.246
Batch: 40 | Loss: 4.251 | Acc: 48.457,67.683,72.485,% | Adaptive Acc: 68.883% | clf_exit: 0.335 0.408 0.257
Batch: 60 | Loss: 4.262 | Acc: 48.207,67.456,72.298,% | Adaptive Acc: 68.635% | clf_exit: 0.335 0.407 0.258
Train classifier parameters

Epoch: 246
Batch: 0 | Loss: 2.461 | Acc: 58.594,86.719,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.273 0.492 0.234
Batch: 20 | Loss: 2.500 | Acc: 51.414,86.830,99.963,% | Adaptive Acc: 94.308% | clf_exit: 0.262 0.501 0.236
Batch: 40 | Loss: 2.473 | Acc: 51.829,87.081,99.943,% | Adaptive Acc: 94.036% | clf_exit: 0.270 0.498 0.233
Batch: 60 | Loss: 2.487 | Acc: 51.960,87.090,99.936,% | Adaptive Acc: 94.147% | clf_exit: 0.268 0.499 0.233
Batch: 80 | Loss: 2.487 | Acc: 51.736,87.076,99.942,% | Adaptive Acc: 94.030% | clf_exit: 0.269 0.498 0.233
Batch: 100 | Loss: 2.493 | Acc: 51.717,86.881,99.923,% | Adaptive Acc: 93.928% | clf_exit: 0.271 0.493 0.236
Batch: 120 | Loss: 2.490 | Acc: 51.847,86.816,99.935,% | Adaptive Acc: 93.924% | clf_exit: 0.271 0.494 0.235
Batch: 140 | Loss: 2.492 | Acc: 51.701,86.708,99.928,% | Adaptive Acc: 93.983% | clf_exit: 0.270 0.494 0.236
Batch: 160 | Loss: 2.492 | Acc: 51.713,86.680,99.927,% | Adaptive Acc: 93.939% | clf_exit: 0.270 0.495 0.235
Batch: 180 | Loss: 2.499 | Acc: 51.636,86.559,99.927,% | Adaptive Acc: 93.974% | clf_exit: 0.269 0.494 0.237
Batch: 200 | Loss: 2.498 | Acc: 51.586,86.598,99.918,% | Adaptive Acc: 94.065% | clf_exit: 0.269 0.493 0.238
Batch: 220 | Loss: 2.496 | Acc: 51.548,86.627,99.915,% | Adaptive Acc: 94.075% | clf_exit: 0.269 0.493 0.238
Batch: 240 | Loss: 2.498 | Acc: 51.550,86.579,99.912,% | Adaptive Acc: 94.107% | clf_exit: 0.268 0.492 0.240
Batch: 260 | Loss: 2.497 | Acc: 51.557,86.584,99.904,% | Adaptive Acc: 94.058% | clf_exit: 0.268 0.493 0.239
Batch: 280 | Loss: 2.499 | Acc: 51.476,86.544,99.894,% | Adaptive Acc: 94.045% | clf_exit: 0.268 0.492 0.240
Batch: 300 | Loss: 2.498 | Acc: 51.511,86.592,99.899,% | Adaptive Acc: 94.061% | clf_exit: 0.268 0.493 0.239
Batch: 320 | Loss: 2.498 | Acc: 51.558,86.548,99.898,% | Adaptive Acc: 94.071% | clf_exit: 0.269 0.492 0.239
Batch: 340 | Loss: 2.501 | Acc: 51.569,86.497,99.899,% | Adaptive Acc: 94.078% | clf_exit: 0.269 0.491 0.240
Batch: 360 | Loss: 2.499 | Acc: 51.597,86.517,99.903,% | Adaptive Acc: 94.062% | clf_exit: 0.269 0.491 0.240
Batch: 380 | Loss: 2.496 | Acc: 51.642,86.534,99.897,% | Adaptive Acc: 94.068% | clf_exit: 0.269 0.491 0.240
Batch: 0 | Loss: 3.780 | Acc: 50.781,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.283 | Acc: 48.326,67.522,72.321,% | Adaptive Acc: 68.638% | clf_exit: 0.342 0.410 0.248
Batch: 40 | Loss: 4.251 | Acc: 48.418,67.530,72.370,% | Adaptive Acc: 68.864% | clf_exit: 0.339 0.403 0.258
Batch: 60 | Loss: 4.265 | Acc: 48.169,67.392,72.310,% | Adaptive Acc: 68.660% | clf_exit: 0.338 0.403 0.258
Train classifier parameters

Epoch: 247
Batch: 0 | Loss: 2.533 | Acc: 53.125,86.719,99.219,% | Adaptive Acc: 95.312% | clf_exit: 0.234 0.539 0.227
Batch: 20 | Loss: 2.439 | Acc: 52.790,87.351,99.963,% | Adaptive Acc: 94.568% | clf_exit: 0.273 0.487 0.240
Batch: 40 | Loss: 2.478 | Acc: 51.810,86.643,99.962,% | Adaptive Acc: 94.093% | clf_exit: 0.273 0.486 0.241
Batch: 60 | Loss: 2.501 | Acc: 51.230,86.655,99.923,% | Adaptive Acc: 93.916% | clf_exit: 0.269 0.490 0.241
Batch: 80 | Loss: 2.495 | Acc: 51.726,86.593,99.904,% | Adaptive Acc: 93.924% | clf_exit: 0.271 0.491 0.238
Batch: 100 | Loss: 2.487 | Acc: 51.942,86.711,99.923,% | Adaptive Acc: 94.059% | clf_exit: 0.271 0.491 0.237
Batch: 120 | Loss: 2.492 | Acc: 51.847,86.661,99.910,% | Adaptive Acc: 94.183% | clf_exit: 0.268 0.492 0.239
Batch: 140 | Loss: 2.492 | Acc: 51.878,86.641,99.911,% | Adaptive Acc: 94.049% | clf_exit: 0.271 0.491 0.238
Batch: 160 | Loss: 2.485 | Acc: 52.116,86.690,99.918,% | Adaptive Acc: 94.036% | clf_exit: 0.273 0.490 0.237
Batch: 180 | Loss: 2.490 | Acc: 51.852,86.684,99.909,% | Adaptive Acc: 93.962% | clf_exit: 0.272 0.490 0.238
Batch: 200 | Loss: 2.489 | Acc: 51.881,86.676,99.911,% | Adaptive Acc: 93.972% | clf_exit: 0.272 0.490 0.239
Batch: 220 | Loss: 2.490 | Acc: 51.874,86.690,99.894,% | Adaptive Acc: 94.012% | clf_exit: 0.270 0.492 0.238
Batch: 240 | Loss: 2.493 | Acc: 51.926,86.602,99.893,% | Adaptive Acc: 94.064% | clf_exit: 0.270 0.491 0.239
Batch: 260 | Loss: 2.487 | Acc: 52.009,86.665,99.892,% | Adaptive Acc: 94.079% | clf_exit: 0.271 0.491 0.238
Batch: 280 | Loss: 2.484 | Acc: 51.980,86.741,99.892,% | Adaptive Acc: 94.073% | clf_exit: 0.272 0.491 0.237
Batch: 300 | Loss: 2.488 | Acc: 51.845,86.685,99.888,% | Adaptive Acc: 94.061% | clf_exit: 0.271 0.491 0.238
Batch: 320 | Loss: 2.487 | Acc: 51.901,86.687,99.893,% | Adaptive Acc: 94.098% | clf_exit: 0.270 0.492 0.238
Batch: 340 | Loss: 2.488 | Acc: 51.817,86.719,99.888,% | Adaptive Acc: 94.082% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.492 | Acc: 51.768,86.632,99.890,% | Adaptive Acc: 94.031% | clf_exit: 0.269 0.493 0.238
Batch: 380 | Loss: 2.495 | Acc: 51.763,86.653,99.893,% | Adaptive Acc: 94.033% | clf_exit: 0.268 0.494 0.238
Batch: 0 | Loss: 3.857 | Acc: 51.562,71.094,78.125,% | Adaptive Acc: 73.438% | clf_exit: 0.344 0.406 0.250
Batch: 20 | Loss: 4.301 | Acc: 48.214,67.634,72.173,% | Adaptive Acc: 68.452% | clf_exit: 0.345 0.406 0.249
Batch: 40 | Loss: 4.268 | Acc: 48.190,67.530,72.008,% | Adaptive Acc: 68.521% | clf_exit: 0.344 0.398 0.259
Batch: 60 | Loss: 4.282 | Acc: 47.938,67.431,72.067,% | Adaptive Acc: 68.417% | clf_exit: 0.342 0.399 0.259
Train classifier parameters

Epoch: 248
Batch: 0 | Loss: 2.448 | Acc: 53.906,83.594,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.297 0.445 0.258
Batch: 20 | Loss: 2.465 | Acc: 51.153,87.314,99.926,% | Adaptive Acc: 93.862% | clf_exit: 0.283 0.481 0.236
Batch: 40 | Loss: 2.485 | Acc: 51.524,86.890,99.962,% | Adaptive Acc: 93.960% | clf_exit: 0.280 0.487 0.234
Batch: 60 | Loss: 2.461 | Acc: 52.344,87.090,99.936,% | Adaptive Acc: 94.134% | clf_exit: 0.278 0.488 0.234
Batch: 80 | Loss: 2.470 | Acc: 51.997,87.162,99.923,% | Adaptive Acc: 94.232% | clf_exit: 0.276 0.484 0.240
Batch: 100 | Loss: 2.474 | Acc: 51.988,87.129,99.930,% | Adaptive Acc: 94.307% | clf_exit: 0.273 0.488 0.239
Batch: 120 | Loss: 2.480 | Acc: 51.898,87.029,99.910,% | Adaptive Acc: 94.370% | clf_exit: 0.270 0.490 0.240
Batch: 140 | Loss: 2.480 | Acc: 51.873,86.979,99.900,% | Adaptive Acc: 94.310% | clf_exit: 0.270 0.492 0.239
Batch: 160 | Loss: 2.482 | Acc: 51.897,86.835,99.893,% | Adaptive Acc: 94.172% | clf_exit: 0.269 0.493 0.238
Batch: 180 | Loss: 2.489 | Acc: 51.834,86.753,99.896,% | Adaptive Acc: 94.229% | clf_exit: 0.269 0.492 0.238
Batch: 200 | Loss: 2.495 | Acc: 51.796,86.680,99.899,% | Adaptive Acc: 94.236% | clf_exit: 0.270 0.491 0.239
Batch: 220 | Loss: 2.501 | Acc: 51.623,86.606,99.890,% | Adaptive Acc: 94.241% | clf_exit: 0.269 0.490 0.241
Batch: 240 | Loss: 2.502 | Acc: 51.618,86.716,99.896,% | Adaptive Acc: 94.272% | clf_exit: 0.269 0.490 0.241
Batch: 260 | Loss: 2.495 | Acc: 51.691,86.830,99.898,% | Adaptive Acc: 94.289% | clf_exit: 0.268 0.492 0.240
Batch: 280 | Loss: 2.489 | Acc: 51.832,86.844,99.903,% | Adaptive Acc: 94.273% | clf_exit: 0.269 0.492 0.239
Batch: 300 | Loss: 2.485 | Acc: 51.819,86.859,99.904,% | Adaptive Acc: 94.238% | clf_exit: 0.270 0.492 0.238
Batch: 320 | Loss: 2.486 | Acc: 51.777,86.799,99.900,% | Adaptive Acc: 94.166% | clf_exit: 0.270 0.491 0.238
Batch: 340 | Loss: 2.488 | Acc: 51.682,86.781,99.904,% | Adaptive Acc: 94.169% | clf_exit: 0.270 0.491 0.239
Batch: 360 | Loss: 2.494 | Acc: 51.541,86.782,99.903,% | Adaptive Acc: 94.157% | clf_exit: 0.269 0.491 0.239
Batch: 380 | Loss: 2.493 | Acc: 51.542,86.799,99.906,% | Adaptive Acc: 94.183% | clf_exit: 0.268 0.492 0.240
Batch: 0 | Loss: 3.788 | Acc: 51.562,70.312,78.125,% | Adaptive Acc: 72.656% | clf_exit: 0.344 0.406 0.250
Batch: 20 | Loss: 4.288 | Acc: 48.475,67.336,72.359,% | Adaptive Acc: 68.601% | clf_exit: 0.343 0.407 0.250
Batch: 40 | Loss: 4.252 | Acc: 48.514,67.416,72.237,% | Adaptive Acc: 68.769% | clf_exit: 0.340 0.403 0.258
Batch: 60 | Loss: 4.268 | Acc: 48.002,67.341,72.234,% | Adaptive Acc: 68.571% | clf_exit: 0.339 0.403 0.258
Train classifier parameters

Epoch: 249
Batch: 0 | Loss: 2.624 | Acc: 48.438,84.375,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.234 0.500 0.266
Batch: 20 | Loss: 2.504 | Acc: 51.116,86.496,99.851,% | Adaptive Acc: 94.048% | clf_exit: 0.274 0.491 0.235
Batch: 40 | Loss: 2.522 | Acc: 51.353,86.623,99.771,% | Adaptive Acc: 94.303% | clf_exit: 0.262 0.498 0.240
Batch: 60 | Loss: 2.489 | Acc: 51.934,86.962,99.757,% | Adaptive Acc: 93.981% | clf_exit: 0.267 0.501 0.232
Batch: 80 | Loss: 2.491 | Acc: 51.813,87.105,99.788,% | Adaptive Acc: 94.184% | clf_exit: 0.266 0.499 0.235
Batch: 100 | Loss: 2.498 | Acc: 51.516,86.959,99.791,% | Adaptive Acc: 94.199% | clf_exit: 0.263 0.503 0.234
Batch: 120 | Loss: 2.497 | Acc: 51.743,86.861,99.813,% | Adaptive Acc: 94.021% | clf_exit: 0.266 0.498 0.236
Batch: 140 | Loss: 2.497 | Acc: 51.673,86.852,99.823,% | Adaptive Acc: 94.005% | clf_exit: 0.268 0.497 0.235
Batch: 160 | Loss: 2.498 | Acc: 51.747,86.801,99.845,% | Adaptive Acc: 94.002% | clf_exit: 0.267 0.498 0.235
Batch: 180 | Loss: 2.495 | Acc: 51.679,86.814,99.849,% | Adaptive Acc: 93.962% | clf_exit: 0.268 0.499 0.234
Batch: 200 | Loss: 2.495 | Acc: 51.640,86.781,99.848,% | Adaptive Acc: 93.948% | clf_exit: 0.268 0.497 0.235
Batch: 220 | Loss: 2.496 | Acc: 51.693,86.729,99.844,% | Adaptive Acc: 93.888% | clf_exit: 0.268 0.498 0.234
Batch: 240 | Loss: 2.501 | Acc: 51.592,86.777,99.848,% | Adaptive Acc: 93.899% | clf_exit: 0.267 0.497 0.235
Batch: 260 | Loss: 2.499 | Acc: 51.619,86.731,99.850,% | Adaptive Acc: 93.969% | clf_exit: 0.268 0.497 0.236
Batch: 280 | Loss: 2.500 | Acc: 51.615,86.727,99.858,% | Adaptive Acc: 93.903% | clf_exit: 0.268 0.496 0.236
Batch: 300 | Loss: 2.500 | Acc: 51.664,86.701,99.857,% | Adaptive Acc: 93.924% | clf_exit: 0.268 0.495 0.237
Batch: 320 | Loss: 2.498 | Acc: 51.811,86.719,99.861,% | Adaptive Acc: 93.945% | clf_exit: 0.269 0.494 0.237
Batch: 340 | Loss: 2.495 | Acc: 51.826,86.758,99.858,% | Adaptive Acc: 93.931% | clf_exit: 0.269 0.493 0.237
Batch: 360 | Loss: 2.495 | Acc: 51.837,86.749,99.859,% | Adaptive Acc: 93.923% | clf_exit: 0.269 0.494 0.237
Batch: 380 | Loss: 2.496 | Acc: 51.778,86.717,99.856,% | Adaptive Acc: 93.939% | clf_exit: 0.269 0.493 0.238
Batch: 0 | Loss: 3.790 | Acc: 52.344,71.875,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.293 | Acc: 48.065,67.411,72.507,% | Adaptive Acc: 68.601% | clf_exit: 0.341 0.403 0.256
Batch: 40 | Loss: 4.256 | Acc: 48.209,67.626,72.332,% | Adaptive Acc: 68.826% | clf_exit: 0.337 0.401 0.261
Batch: 60 | Loss: 4.268 | Acc: 47.823,67.418,72.246,% | Adaptive Acc: 68.635% | clf_exit: 0.337 0.401 0.262
Train all parameters

Epoch: 250
Batch: 0 | Loss: 2.302 | Acc: 54.688,89.062,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.297 0.500 0.203
Batch: 20 | Loss: 2.443 | Acc: 51.451,87.165,99.963,% | Adaptive Acc: 94.048% | clf_exit: 0.261 0.518 0.221
Batch: 40 | Loss: 2.444 | Acc: 52.344,86.662,99.962,% | Adaptive Acc: 93.998% | clf_exit: 0.271 0.503 0.226
Batch: 60 | Loss: 2.481 | Acc: 51.806,86.347,99.898,% | Adaptive Acc: 93.763% | clf_exit: 0.269 0.500 0.231
Batch: 80 | Loss: 2.499 | Acc: 51.842,86.073,99.884,% | Adaptive Acc: 93.711% | clf_exit: 0.270 0.495 0.235
Batch: 100 | Loss: 2.497 | Acc: 52.042,86.139,99.884,% | Adaptive Acc: 93.820% | clf_exit: 0.269 0.494 0.237
Batch: 120 | Loss: 2.489 | Acc: 51.950,86.377,99.871,% | Adaptive Acc: 93.847% | clf_exit: 0.270 0.494 0.237
Batch: 140 | Loss: 2.495 | Acc: 51.989,86.287,99.884,% | Adaptive Acc: 93.833% | clf_exit: 0.269 0.492 0.238
Batch: 160 | Loss: 2.496 | Acc: 51.946,86.253,99.879,% | Adaptive Acc: 93.876% | clf_exit: 0.269 0.492 0.238
Batch: 180 | Loss: 2.498 | Acc: 51.873,86.227,99.888,% | Adaptive Acc: 93.815% | clf_exit: 0.269 0.492 0.238
Batch: 200 | Loss: 2.495 | Acc: 51.916,86.334,99.887,% | Adaptive Acc: 93.789% | clf_exit: 0.269 0.494 0.237
Batch: 220 | Loss: 2.499 | Acc: 51.824,86.365,99.887,% | Adaptive Acc: 93.810% | clf_exit: 0.268 0.494 0.238
Batch: 240 | Loss: 2.500 | Acc: 51.751,86.375,99.893,% | Adaptive Acc: 93.828% | clf_exit: 0.268 0.494 0.238
Batch: 260 | Loss: 2.501 | Acc: 51.706,86.360,99.880,% | Adaptive Acc: 93.825% | clf_exit: 0.268 0.494 0.238
Batch: 280 | Loss: 2.502 | Acc: 51.688,86.307,99.886,% | Adaptive Acc: 93.833% | clf_exit: 0.268 0.493 0.239
Batch: 300 | Loss: 2.503 | Acc: 51.617,86.301,99.888,% | Adaptive Acc: 93.851% | clf_exit: 0.268 0.492 0.239
Batch: 320 | Loss: 2.505 | Acc: 51.645,86.288,99.886,% | Adaptive Acc: 93.857% | clf_exit: 0.267 0.493 0.239
Batch: 340 | Loss: 2.507 | Acc: 51.583,86.295,99.881,% | Adaptive Acc: 93.839% | clf_exit: 0.267 0.494 0.239
Batch: 360 | Loss: 2.504 | Acc: 51.673,86.372,99.885,% | Adaptive Acc: 93.863% | clf_exit: 0.267 0.494 0.239
Batch: 380 | Loss: 2.502 | Acc: 51.731,86.362,99.881,% | Adaptive Acc: 93.863% | clf_exit: 0.267 0.494 0.239
Batch: 0 | Loss: 3.813 | Acc: 52.344,68.750,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.352 0.422 0.227
Batch: 20 | Loss: 4.269 | Acc: 48.735,67.671,72.619,% | Adaptive Acc: 69.085% | clf_exit: 0.339 0.408 0.253
Batch: 40 | Loss: 4.239 | Acc: 48.838,67.931,72.428,% | Adaptive Acc: 69.150% | clf_exit: 0.339 0.399 0.262
Batch: 60 | Loss: 4.257 | Acc: 48.514,67.879,72.515,% | Adaptive Acc: 69.070% | clf_exit: 0.338 0.401 0.261
Train all parameters

Epoch: 251
Batch: 0 | Loss: 2.663 | Acc: 48.438,86.719,99.219,% | Adaptive Acc: 94.531% | clf_exit: 0.289 0.414 0.297
Batch: 20 | Loss: 2.491 | Acc: 52.753,86.272,99.926,% | Adaptive Acc: 94.903% | clf_exit: 0.264 0.484 0.252
Batch: 40 | Loss: 2.481 | Acc: 52.496,86.890,99.905,% | Adaptive Acc: 94.360% | clf_exit: 0.270 0.491 0.240
Batch: 60 | Loss: 2.482 | Acc: 51.934,87.103,99.898,% | Adaptive Acc: 94.416% | clf_exit: 0.266 0.497 0.237
Batch: 80 | Loss: 2.484 | Acc: 51.929,87.056,99.904,% | Adaptive Acc: 94.387% | clf_exit: 0.267 0.496 0.236
Batch: 100 | Loss: 2.505 | Acc: 51.601,86.719,99.923,% | Adaptive Acc: 94.330% | clf_exit: 0.266 0.494 0.240
Batch: 120 | Loss: 2.502 | Acc: 51.717,86.648,99.910,% | Adaptive Acc: 94.228% | clf_exit: 0.268 0.492 0.240
Batch: 140 | Loss: 2.500 | Acc: 51.690,86.713,99.906,% | Adaptive Acc: 94.254% | clf_exit: 0.268 0.494 0.239
Batch: 160 | Loss: 2.496 | Acc: 51.820,86.656,99.908,% | Adaptive Acc: 94.201% | clf_exit: 0.268 0.494 0.238
Batch: 180 | Loss: 2.486 | Acc: 52.011,86.835,99.905,% | Adaptive Acc: 94.216% | clf_exit: 0.269 0.494 0.237
Batch: 200 | Loss: 2.485 | Acc: 52.079,86.820,99.907,% | Adaptive Acc: 94.259% | clf_exit: 0.270 0.493 0.237
Batch: 220 | Loss: 2.483 | Acc: 52.072,86.804,99.897,% | Adaptive Acc: 94.238% | clf_exit: 0.270 0.492 0.237
Batch: 240 | Loss: 2.486 | Acc: 51.990,86.696,99.900,% | Adaptive Acc: 94.207% | clf_exit: 0.271 0.492 0.237
Batch: 260 | Loss: 2.486 | Acc: 51.979,86.689,99.904,% | Adaptive Acc: 94.220% | clf_exit: 0.270 0.493 0.237
Batch: 280 | Loss: 2.486 | Acc: 51.918,86.658,99.908,% | Adaptive Acc: 94.164% | clf_exit: 0.270 0.493 0.237
Batch: 300 | Loss: 2.490 | Acc: 51.812,86.579,99.914,% | Adaptive Acc: 94.191% | clf_exit: 0.269 0.493 0.238
Batch: 320 | Loss: 2.494 | Acc: 51.757,86.517,99.915,% | Adaptive Acc: 94.159% | clf_exit: 0.269 0.493 0.238
Batch: 340 | Loss: 2.494 | Acc: 51.737,86.538,99.913,% | Adaptive Acc: 94.123% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.498 | Acc: 51.647,86.533,99.911,% | Adaptive Acc: 94.137% | clf_exit: 0.268 0.493 0.239
Batch: 380 | Loss: 2.503 | Acc: 51.595,86.514,99.912,% | Adaptive Acc: 94.158% | clf_exit: 0.268 0.492 0.240
Batch: 0 | Loss: 3.769 | Acc: 53.125,72.656,78.125,% | Adaptive Acc: 72.656% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.290 | Acc: 48.400,67.857,72.619,% | Adaptive Acc: 68.713% | clf_exit: 0.338 0.411 0.250
Batch: 40 | Loss: 4.249 | Acc: 48.609,67.664,72.389,% | Adaptive Acc: 68.902% | clf_exit: 0.338 0.401 0.261
Batch: 60 | Loss: 4.263 | Acc: 48.348,67.482,72.323,% | Adaptive Acc: 68.648% | clf_exit: 0.336 0.403 0.261
Train all parameters

Epoch: 252
Batch: 0 | Loss: 2.419 | Acc: 57.031,82.812,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.273 0.477 0.250
Batch: 20 | Loss: 2.467 | Acc: 52.009,86.756,99.963,% | Adaptive Acc: 93.266% | clf_exit: 0.281 0.482 0.237
Batch: 40 | Loss: 2.492 | Acc: 51.829,86.871,99.829,% | Adaptive Acc: 93.502% | clf_exit: 0.274 0.486 0.240
Batch: 60 | Loss: 2.500 | Acc: 51.806,86.616,99.846,% | Adaptive Acc: 93.750% | clf_exit: 0.271 0.488 0.241
Batch: 80 | Loss: 2.505 | Acc: 51.495,86.719,99.855,% | Adaptive Acc: 93.837% | clf_exit: 0.269 0.490 0.241
Batch: 100 | Loss: 2.504 | Acc: 51.330,86.843,99.830,% | Adaptive Acc: 93.789% | clf_exit: 0.267 0.492 0.241
Batch: 120 | Loss: 2.511 | Acc: 51.214,86.732,99.839,% | Adaptive Acc: 93.763% | clf_exit: 0.267 0.492 0.241
Batch: 140 | Loss: 2.510 | Acc: 51.247,86.752,99.839,% | Adaptive Acc: 93.855% | clf_exit: 0.267 0.491 0.243
Batch: 160 | Loss: 2.514 | Acc: 51.223,86.762,99.830,% | Adaptive Acc: 94.002% | clf_exit: 0.265 0.491 0.245
Batch: 180 | Loss: 2.513 | Acc: 51.243,86.749,99.836,% | Adaptive Acc: 94.048% | clf_exit: 0.265 0.490 0.245
Batch: 200 | Loss: 2.513 | Acc: 51.236,86.688,99.837,% | Adaptive Acc: 94.014% | clf_exit: 0.266 0.490 0.244
Batch: 220 | Loss: 2.511 | Acc: 51.280,86.595,99.848,% | Adaptive Acc: 93.980% | clf_exit: 0.265 0.492 0.244
Batch: 240 | Loss: 2.509 | Acc: 51.313,86.557,99.848,% | Adaptive Acc: 94.029% | clf_exit: 0.265 0.493 0.242
Batch: 260 | Loss: 2.512 | Acc: 51.317,86.494,99.847,% | Adaptive Acc: 94.004% | clf_exit: 0.266 0.491 0.243
Batch: 280 | Loss: 2.513 | Acc: 51.301,86.510,99.847,% | Adaptive Acc: 94.006% | clf_exit: 0.265 0.492 0.243
Batch: 300 | Loss: 2.513 | Acc: 51.339,86.425,99.847,% | Adaptive Acc: 94.004% | clf_exit: 0.265 0.492 0.243
Batch: 320 | Loss: 2.512 | Acc: 51.278,86.397,99.849,% | Adaptive Acc: 93.959% | clf_exit: 0.266 0.492 0.243
Batch: 340 | Loss: 2.510 | Acc: 51.290,86.405,99.856,% | Adaptive Acc: 93.979% | clf_exit: 0.265 0.493 0.242
Batch: 360 | Loss: 2.509 | Acc: 51.316,86.403,99.861,% | Adaptive Acc: 93.979% | clf_exit: 0.266 0.493 0.242
Batch: 380 | Loss: 2.508 | Acc: 51.310,86.423,99.861,% | Adaptive Acc: 94.010% | clf_exit: 0.265 0.493 0.242
Batch: 0 | Loss: 3.806 | Acc: 50.000,71.094,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.344 0.430 0.227
Batch: 20 | Loss: 4.297 | Acc: 48.698,67.634,72.433,% | Adaptive Acc: 68.638% | clf_exit: 0.340 0.416 0.244
Batch: 40 | Loss: 4.256 | Acc: 48.704,67.931,72.275,% | Adaptive Acc: 68.864% | clf_exit: 0.338 0.408 0.254
Batch: 60 | Loss: 4.270 | Acc: 48.335,67.572,72.349,% | Adaptive Acc: 68.840% | clf_exit: 0.335 0.409 0.256
Train all parameters

Epoch: 253
Batch: 0 | Loss: 2.463 | Acc: 51.562,88.281,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.219 0.508 0.273
Batch: 20 | Loss: 2.540 | Acc: 51.972,86.124,99.926,% | Adaptive Acc: 94.010% | clf_exit: 0.267 0.492 0.240
Batch: 40 | Loss: 2.518 | Acc: 52.268,86.795,99.905,% | Adaptive Acc: 94.322% | clf_exit: 0.268 0.492 0.240
Batch: 60 | Loss: 2.482 | Acc: 52.267,86.911,99.910,% | Adaptive Acc: 94.083% | clf_exit: 0.275 0.488 0.236
Batch: 80 | Loss: 2.488 | Acc: 51.977,86.584,99.913,% | Adaptive Acc: 94.126% | clf_exit: 0.274 0.489 0.237
Batch: 100 | Loss: 2.479 | Acc: 51.802,86.634,99.907,% | Adaptive Acc: 94.021% | clf_exit: 0.274 0.489 0.237
Batch: 120 | Loss: 2.482 | Acc: 51.769,86.635,99.916,% | Adaptive Acc: 94.099% | clf_exit: 0.272 0.492 0.235
Batch: 140 | Loss: 2.482 | Acc: 51.740,86.680,99.906,% | Adaptive Acc: 94.171% | clf_exit: 0.271 0.493 0.236
Batch: 160 | Loss: 2.492 | Acc: 51.635,86.627,99.903,% | Adaptive Acc: 94.133% | clf_exit: 0.269 0.494 0.237
Batch: 180 | Loss: 2.489 | Acc: 51.735,86.581,99.914,% | Adaptive Acc: 94.220% | clf_exit: 0.267 0.496 0.237
Batch: 200 | Loss: 2.491 | Acc: 51.679,86.528,99.911,% | Adaptive Acc: 94.174% | clf_exit: 0.268 0.495 0.237
Batch: 220 | Loss: 2.494 | Acc: 51.746,86.422,99.905,% | Adaptive Acc: 94.213% | clf_exit: 0.269 0.492 0.238
Batch: 240 | Loss: 2.500 | Acc: 51.702,86.362,99.896,% | Adaptive Acc: 94.152% | clf_exit: 0.270 0.492 0.238
Batch: 260 | Loss: 2.500 | Acc: 51.607,86.386,99.889,% | Adaptive Acc: 94.067% | clf_exit: 0.270 0.491 0.238
Batch: 280 | Loss: 2.500 | Acc: 51.643,86.452,99.886,% | Adaptive Acc: 94.050% | clf_exit: 0.271 0.492 0.238
Batch: 300 | Loss: 2.501 | Acc: 51.487,86.420,99.878,% | Adaptive Acc: 93.999% | clf_exit: 0.270 0.492 0.238
Batch: 320 | Loss: 2.503 | Acc: 51.443,86.395,99.881,% | Adaptive Acc: 93.989% | clf_exit: 0.270 0.491 0.239
Batch: 340 | Loss: 2.503 | Acc: 51.521,86.428,99.888,% | Adaptive Acc: 94.027% | clf_exit: 0.270 0.491 0.239
Batch: 360 | Loss: 2.500 | Acc: 51.604,86.526,99.887,% | Adaptive Acc: 94.098% | clf_exit: 0.270 0.492 0.239
Batch: 380 | Loss: 2.500 | Acc: 51.565,86.493,99.889,% | Adaptive Acc: 94.056% | clf_exit: 0.269 0.492 0.239
Batch: 0 | Loss: 3.796 | Acc: 51.562,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.295 | Acc: 48.028,67.560,72.396,% | Adaptive Acc: 68.527% | clf_exit: 0.340 0.413 0.247
Batch: 40 | Loss: 4.251 | Acc: 48.266,67.854,72.085,% | Adaptive Acc: 68.636% | clf_exit: 0.339 0.405 0.256
Batch: 60 | Loss: 4.259 | Acc: 48.092,67.725,72.259,% | Adaptive Acc: 68.801% | clf_exit: 0.337 0.404 0.259
Train all parameters

Epoch: 254
Batch: 0 | Loss: 2.414 | Acc: 53.125,89.062,100.000,% | Adaptive Acc: 96.875% | clf_exit: 0.281 0.469 0.250
Batch: 20 | Loss: 2.427 | Acc: 52.679,87.649,99.963,% | Adaptive Acc: 94.085% | clf_exit: 0.278 0.492 0.231
Batch: 40 | Loss: 2.495 | Acc: 51.429,86.376,99.905,% | Adaptive Acc: 93.712% | clf_exit: 0.272 0.492 0.236
Batch: 60 | Loss: 2.481 | Acc: 51.691,86.642,99.910,% | Adaptive Acc: 94.006% | clf_exit: 0.270 0.497 0.232
Batch: 80 | Loss: 2.480 | Acc: 51.640,86.507,99.923,% | Adaptive Acc: 94.059% | clf_exit: 0.270 0.494 0.236
Batch: 100 | Loss: 2.474 | Acc: 52.034,86.587,99.923,% | Adaptive Acc: 94.106% | clf_exit: 0.270 0.495 0.234
Batch: 120 | Loss: 2.478 | Acc: 51.911,86.519,99.923,% | Adaptive Acc: 93.950% | clf_exit: 0.271 0.495 0.234
Batch: 140 | Loss: 2.486 | Acc: 51.762,86.408,99.917,% | Adaptive Acc: 93.944% | clf_exit: 0.271 0.493 0.236
Batch: 160 | Loss: 2.495 | Acc: 51.689,86.428,99.908,% | Adaptive Acc: 93.968% | clf_exit: 0.270 0.494 0.236
Batch: 180 | Loss: 2.498 | Acc: 51.541,86.300,99.909,% | Adaptive Acc: 93.936% | clf_exit: 0.270 0.493 0.237
Batch: 200 | Loss: 2.499 | Acc: 51.496,86.280,99.911,% | Adaptive Acc: 93.940% | clf_exit: 0.270 0.492 0.238
Batch: 220 | Loss: 2.498 | Acc: 51.432,86.323,99.908,% | Adaptive Acc: 93.934% | clf_exit: 0.270 0.492 0.237
Batch: 240 | Loss: 2.492 | Acc: 51.640,86.323,99.909,% | Adaptive Acc: 93.974% | clf_exit: 0.270 0.493 0.237
Batch: 260 | Loss: 2.491 | Acc: 51.652,86.348,99.913,% | Adaptive Acc: 93.969% | clf_exit: 0.270 0.492 0.238
Batch: 280 | Loss: 2.491 | Acc: 51.690,86.321,99.914,% | Adaptive Acc: 93.956% | clf_exit: 0.271 0.491 0.238
Batch: 300 | Loss: 2.495 | Acc: 51.679,86.368,99.907,% | Adaptive Acc: 94.015% | clf_exit: 0.270 0.492 0.239
Batch: 320 | Loss: 2.495 | Acc: 51.782,86.359,99.905,% | Adaptive Acc: 94.010% | clf_exit: 0.269 0.492 0.239
Batch: 340 | Loss: 2.496 | Acc: 51.780,86.364,99.906,% | Adaptive Acc: 94.023% | clf_exit: 0.268 0.493 0.239
Batch: 360 | Loss: 2.498 | Acc: 51.762,86.282,99.909,% | Adaptive Acc: 93.990% | clf_exit: 0.269 0.492 0.239
Batch: 380 | Loss: 2.500 | Acc: 51.718,86.249,99.912,% | Adaptive Acc: 93.990% | clf_exit: 0.269 0.492 0.239
Batch: 0 | Loss: 3.823 | Acc: 51.562,69.531,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.306 | Acc: 47.768,67.783,71.875,% | Adaptive Acc: 68.043% | clf_exit: 0.345 0.406 0.249
Batch: 40 | Loss: 4.265 | Acc: 47.866,67.759,72.123,% | Adaptive Acc: 68.579% | clf_exit: 0.343 0.399 0.258
Batch: 60 | Loss: 4.274 | Acc: 47.784,67.661,72.413,% | Adaptive Acc: 68.609% | clf_exit: 0.341 0.401 0.258
Train all parameters

Epoch: 255
Batch: 0 | Loss: 2.215 | Acc: 54.688,89.062,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.328 0.469 0.203
Batch: 20 | Loss: 2.419 | Acc: 53.906,87.649,99.888,% | Adaptive Acc: 94.903% | clf_exit: 0.278 0.493 0.230
Batch: 40 | Loss: 2.505 | Acc: 51.582,86.433,99.886,% | Adaptive Acc: 94.303% | clf_exit: 0.264 0.489 0.247
Batch: 60 | Loss: 2.500 | Acc: 51.895,86.411,99.910,% | Adaptive Acc: 94.173% | clf_exit: 0.269 0.489 0.242
Batch: 80 | Loss: 2.492 | Acc: 52.160,86.584,99.913,% | Adaptive Acc: 94.174% | clf_exit: 0.266 0.498 0.237
Batch: 100 | Loss: 2.507 | Acc: 51.818,86.595,99.923,% | Adaptive Acc: 94.090% | clf_exit: 0.265 0.496 0.239
Batch: 120 | Loss: 2.517 | Acc: 51.705,86.512,99.916,% | Adaptive Acc: 94.021% | clf_exit: 0.266 0.494 0.240
Batch: 140 | Loss: 2.515 | Acc: 51.690,86.530,99.906,% | Adaptive Acc: 93.988% | clf_exit: 0.267 0.493 0.240
Batch: 160 | Loss: 2.515 | Acc: 51.655,86.398,99.893,% | Adaptive Acc: 93.905% | clf_exit: 0.267 0.493 0.239
Batch: 180 | Loss: 2.505 | Acc: 51.735,86.559,99.896,% | Adaptive Acc: 93.923% | clf_exit: 0.268 0.494 0.238
Batch: 200 | Loss: 2.509 | Acc: 51.574,86.497,99.891,% | Adaptive Acc: 93.859% | clf_exit: 0.267 0.495 0.238
Batch: 220 | Loss: 2.504 | Acc: 51.598,86.602,99.887,% | Adaptive Acc: 93.898% | clf_exit: 0.268 0.494 0.238
Batch: 240 | Loss: 2.502 | Acc: 51.553,86.605,99.887,% | Adaptive Acc: 93.867% | clf_exit: 0.268 0.494 0.238
Batch: 260 | Loss: 2.501 | Acc: 51.554,86.611,99.895,% | Adaptive Acc: 93.900% | clf_exit: 0.268 0.494 0.238
Batch: 280 | Loss: 2.501 | Acc: 51.562,86.563,99.897,% | Adaptive Acc: 93.892% | clf_exit: 0.269 0.493 0.238
Batch: 300 | Loss: 2.503 | Acc: 51.594,86.483,99.899,% | Adaptive Acc: 93.893% | clf_exit: 0.269 0.492 0.238
Batch: 320 | Loss: 2.503 | Acc: 51.524,86.478,99.900,% | Adaptive Acc: 93.884% | clf_exit: 0.269 0.492 0.239
Batch: 340 | Loss: 2.496 | Acc: 51.668,86.565,99.904,% | Adaptive Acc: 93.917% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.496 | Acc: 51.608,86.541,99.905,% | Adaptive Acc: 93.897% | clf_exit: 0.269 0.493 0.238
Batch: 380 | Loss: 2.497 | Acc: 51.636,86.547,99.900,% | Adaptive Acc: 93.877% | clf_exit: 0.270 0.492 0.238
Batch: 0 | Loss: 3.829 | Acc: 50.781,71.094,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.306 | Acc: 48.326,67.857,72.359,% | Adaptive Acc: 68.266% | clf_exit: 0.341 0.417 0.242
Batch: 40 | Loss: 4.258 | Acc: 48.666,67.816,72.218,% | Adaptive Acc: 68.750% | clf_exit: 0.340 0.405 0.256
Batch: 60 | Loss: 4.268 | Acc: 48.425,67.636,72.362,% | Adaptive Acc: 68.596% | clf_exit: 0.338 0.405 0.256
Train all parameters

Epoch: 256
Batch: 0 | Loss: 2.445 | Acc: 50.781,86.719,99.219,% | Adaptive Acc: 92.188% | clf_exit: 0.258 0.484 0.258
Batch: 20 | Loss: 2.463 | Acc: 53.497,87.128,99.777,% | Adaptive Acc: 94.754% | clf_exit: 0.266 0.496 0.239
Batch: 40 | Loss: 2.485 | Acc: 52.229,87.138,99.790,% | Adaptive Acc: 94.303% | clf_exit: 0.263 0.495 0.242
Batch: 60 | Loss: 2.476 | Acc: 52.395,87.039,99.821,% | Adaptive Acc: 94.057% | clf_exit: 0.267 0.494 0.239
Batch: 80 | Loss: 2.475 | Acc: 52.132,86.979,99.826,% | Adaptive Acc: 94.049% | clf_exit: 0.269 0.490 0.241
Batch: 100 | Loss: 2.471 | Acc: 52.421,86.804,99.822,% | Adaptive Acc: 93.959% | clf_exit: 0.270 0.491 0.240
Batch: 120 | Loss: 2.473 | Acc: 52.234,86.725,99.839,% | Adaptive Acc: 94.008% | clf_exit: 0.268 0.492 0.240
Batch: 140 | Loss: 2.481 | Acc: 52.061,86.735,99.845,% | Adaptive Acc: 94.138% | clf_exit: 0.268 0.492 0.241
Batch: 160 | Loss: 2.489 | Acc: 51.970,86.641,99.859,% | Adaptive Acc: 94.090% | clf_exit: 0.267 0.493 0.240
Batch: 180 | Loss: 2.482 | Acc: 51.925,86.701,99.853,% | Adaptive Acc: 94.082% | clf_exit: 0.268 0.493 0.239
Batch: 200 | Loss: 2.486 | Acc: 51.796,86.625,99.852,% | Adaptive Acc: 94.080% | clf_exit: 0.268 0.492 0.240
Batch: 220 | Loss: 2.489 | Acc: 51.690,86.609,99.859,% | Adaptive Acc: 94.040% | clf_exit: 0.268 0.492 0.240
Batch: 240 | Loss: 2.483 | Acc: 51.845,86.664,99.854,% | Adaptive Acc: 94.035% | clf_exit: 0.269 0.494 0.238
Batch: 260 | Loss: 2.485 | Acc: 51.811,86.677,99.853,% | Adaptive Acc: 94.028% | clf_exit: 0.269 0.493 0.237
Batch: 280 | Loss: 2.483 | Acc: 51.816,86.649,99.858,% | Adaptive Acc: 94.031% | clf_exit: 0.269 0.494 0.237
Batch: 300 | Loss: 2.482 | Acc: 51.770,86.677,99.865,% | Adaptive Acc: 93.997% | clf_exit: 0.270 0.495 0.235
Batch: 320 | Loss: 2.488 | Acc: 51.706,86.646,99.866,% | Adaptive Acc: 93.967% | clf_exit: 0.269 0.494 0.236
Batch: 340 | Loss: 2.494 | Acc: 51.663,86.522,99.867,% | Adaptive Acc: 93.933% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.498 | Acc: 51.593,86.468,99.866,% | Adaptive Acc: 93.880% | clf_exit: 0.269 0.493 0.238
Batch: 380 | Loss: 2.498 | Acc: 51.593,86.438,99.871,% | Adaptive Acc: 93.926% | clf_exit: 0.268 0.494 0.238
Batch: 0 | Loss: 3.860 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.352 0.406 0.242
Batch: 20 | Loss: 4.294 | Acc: 48.251,67.708,72.619,% | Adaptive Acc: 69.010% | clf_exit: 0.339 0.410 0.251
Batch: 40 | Loss: 4.255 | Acc: 48.438,67.740,72.332,% | Adaptive Acc: 69.017% | clf_exit: 0.337 0.403 0.261
Batch: 60 | Loss: 4.266 | Acc: 48.092,67.649,72.451,% | Adaptive Acc: 68.929% | clf_exit: 0.336 0.404 0.260
Train all parameters

Epoch: 257
Batch: 0 | Loss: 2.211 | Acc: 52.344,87.500,99.219,% | Adaptive Acc: 90.625% | clf_exit: 0.344 0.477 0.180
Batch: 20 | Loss: 2.477 | Acc: 51.749,86.049,99.777,% | Adaptive Acc: 93.304% | clf_exit: 0.281 0.486 0.233
Batch: 40 | Loss: 2.489 | Acc: 51.258,86.223,99.829,% | Adaptive Acc: 93.788% | clf_exit: 0.274 0.489 0.238
Batch: 60 | Loss: 2.499 | Acc: 51.562,86.693,99.846,% | Adaptive Acc: 93.904% | clf_exit: 0.274 0.488 0.238
Batch: 80 | Loss: 2.515 | Acc: 51.071,86.507,99.855,% | Adaptive Acc: 93.846% | clf_exit: 0.271 0.487 0.242
Batch: 100 | Loss: 2.509 | Acc: 51.207,86.641,99.876,% | Adaptive Acc: 93.912% | clf_exit: 0.270 0.490 0.241
Batch: 120 | Loss: 2.510 | Acc: 51.265,86.609,99.871,% | Adaptive Acc: 93.886% | clf_exit: 0.271 0.488 0.241
Batch: 140 | Loss: 2.509 | Acc: 51.324,86.652,99.873,% | Adaptive Acc: 93.961% | clf_exit: 0.269 0.489 0.242
Batch: 160 | Loss: 2.505 | Acc: 51.364,86.777,99.869,% | Adaptive Acc: 94.031% | clf_exit: 0.268 0.491 0.241
Batch: 180 | Loss: 2.503 | Acc: 51.511,86.714,99.866,% | Adaptive Acc: 94.026% | clf_exit: 0.268 0.490 0.242
Batch: 200 | Loss: 2.499 | Acc: 51.555,86.773,99.880,% | Adaptive Acc: 94.026% | clf_exit: 0.268 0.491 0.240
Batch: 220 | Loss: 2.497 | Acc: 51.637,86.736,99.880,% | Adaptive Acc: 94.114% | clf_exit: 0.269 0.490 0.241
Batch: 240 | Loss: 2.497 | Acc: 51.669,86.712,99.880,% | Adaptive Acc: 94.103% | clf_exit: 0.270 0.488 0.242
Batch: 260 | Loss: 2.500 | Acc: 51.634,86.662,99.877,% | Adaptive Acc: 94.094% | clf_exit: 0.269 0.489 0.242
Batch: 280 | Loss: 2.498 | Acc: 51.729,86.735,99.878,% | Adaptive Acc: 94.123% | clf_exit: 0.269 0.490 0.241
Batch: 300 | Loss: 2.498 | Acc: 51.760,86.698,99.881,% | Adaptive Acc: 94.142% | clf_exit: 0.269 0.491 0.241
Batch: 320 | Loss: 2.498 | Acc: 51.726,86.680,99.881,% | Adaptive Acc: 94.154% | clf_exit: 0.268 0.491 0.241
Batch: 340 | Loss: 2.500 | Acc: 51.764,86.673,99.883,% | Adaptive Acc: 94.153% | clf_exit: 0.268 0.491 0.241
Batch: 360 | Loss: 2.498 | Acc: 51.790,86.647,99.881,% | Adaptive Acc: 94.118% | clf_exit: 0.268 0.491 0.240
Batch: 380 | Loss: 2.496 | Acc: 51.780,86.700,99.883,% | Adaptive Acc: 94.119% | clf_exit: 0.268 0.492 0.240
Batch: 0 | Loss: 3.851 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.328 0.461 0.211
Batch: 20 | Loss: 4.299 | Acc: 48.475,67.783,72.619,% | Adaptive Acc: 68.750% | clf_exit: 0.342 0.417 0.241
Batch: 40 | Loss: 4.263 | Acc: 48.742,67.607,72.351,% | Adaptive Acc: 68.807% | clf_exit: 0.342 0.403 0.256
Batch: 60 | Loss: 4.277 | Acc: 48.476,67.469,72.246,% | Adaptive Acc: 68.776% | clf_exit: 0.338 0.405 0.257
Train all parameters

Epoch: 258
Batch: 0 | Loss: 2.128 | Acc: 60.156,87.500,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.352 0.484 0.164
Batch: 20 | Loss: 2.498 | Acc: 51.637,86.756,99.814,% | Adaptive Acc: 94.085% | clf_exit: 0.265 0.496 0.239
Batch: 40 | Loss: 2.476 | Acc: 52.306,86.719,99.848,% | Adaptive Acc: 93.807% | clf_exit: 0.275 0.494 0.231
Batch: 60 | Loss: 2.466 | Acc: 52.523,86.808,99.859,% | Adaptive Acc: 93.852% | clf_exit: 0.275 0.494 0.231
Batch: 80 | Loss: 2.469 | Acc: 52.469,86.883,99.836,% | Adaptive Acc: 93.789% | clf_exit: 0.275 0.496 0.229
Batch: 100 | Loss: 2.479 | Acc: 52.058,86.618,99.853,% | Adaptive Acc: 93.642% | clf_exit: 0.276 0.493 0.231
Batch: 120 | Loss: 2.494 | Acc: 51.737,86.506,99.864,% | Adaptive Acc: 93.698% | clf_exit: 0.274 0.492 0.234
Batch: 140 | Loss: 2.484 | Acc: 51.851,86.602,99.878,% | Adaptive Acc: 93.778% | clf_exit: 0.275 0.492 0.233
Batch: 160 | Loss: 2.488 | Acc: 51.664,86.573,99.879,% | Adaptive Acc: 93.731% | clf_exit: 0.275 0.491 0.234
Batch: 180 | Loss: 2.483 | Acc: 51.714,86.654,99.892,% | Adaptive Acc: 93.810% | clf_exit: 0.275 0.492 0.234
Batch: 200 | Loss: 2.479 | Acc: 51.753,86.680,99.887,% | Adaptive Acc: 93.816% | clf_exit: 0.274 0.491 0.235
Batch: 220 | Loss: 2.482 | Acc: 51.679,86.772,99.887,% | Adaptive Acc: 93.817% | clf_exit: 0.274 0.491 0.235
Batch: 240 | Loss: 2.484 | Acc: 51.611,86.764,99.887,% | Adaptive Acc: 93.838% | clf_exit: 0.273 0.491 0.236
Batch: 260 | Loss: 2.479 | Acc: 51.706,86.785,99.895,% | Adaptive Acc: 93.900% | clf_exit: 0.273 0.491 0.236
Batch: 280 | Loss: 2.480 | Acc: 51.704,86.741,99.897,% | Adaptive Acc: 93.917% | clf_exit: 0.273 0.491 0.236
Batch: 300 | Loss: 2.481 | Acc: 51.713,86.708,99.899,% | Adaptive Acc: 93.859% | clf_exit: 0.273 0.492 0.235
Batch: 320 | Loss: 2.482 | Acc: 51.640,86.707,99.903,% | Adaptive Acc: 93.913% | clf_exit: 0.273 0.492 0.236
Batch: 340 | Loss: 2.485 | Acc: 51.705,86.595,99.904,% | Adaptive Acc: 93.906% | clf_exit: 0.272 0.491 0.236
Batch: 360 | Loss: 2.490 | Acc: 51.671,86.530,99.898,% | Adaptive Acc: 93.914% | clf_exit: 0.272 0.491 0.237
Batch: 380 | Loss: 2.494 | Acc: 51.585,86.495,99.900,% | Adaptive Acc: 93.889% | clf_exit: 0.271 0.491 0.238
Batch: 0 | Loss: 3.849 | Acc: 53.125,71.875,77.344,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.302 | Acc: 48.103,67.411,72.173,% | Adaptive Acc: 68.564% | clf_exit: 0.341 0.419 0.240
Batch: 40 | Loss: 4.257 | Acc: 48.228,67.797,72.199,% | Adaptive Acc: 68.712% | clf_exit: 0.340 0.410 0.250
Batch: 60 | Loss: 4.270 | Acc: 47.951,67.456,72.170,% | Adaptive Acc: 68.686% | clf_exit: 0.337 0.411 0.252
Train all parameters

Epoch: 259
Batch: 0 | Loss: 2.335 | Acc: 54.688,91.406,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.250 0.492 0.258
Batch: 20 | Loss: 2.528 | Acc: 51.414,86.384,100.000,% | Adaptive Acc: 93.527% | clf_exit: 0.268 0.484 0.249
Batch: 40 | Loss: 2.504 | Acc: 51.105,86.204,99.905,% | Adaptive Acc: 93.407% | clf_exit: 0.270 0.493 0.237
Batch: 60 | Loss: 2.473 | Acc: 52.024,86.424,99.898,% | Adaptive Acc: 93.737% | clf_exit: 0.271 0.496 0.233
Batch: 80 | Loss: 2.484 | Acc: 51.987,86.420,99.894,% | Adaptive Acc: 93.933% | clf_exit: 0.272 0.492 0.236
Batch: 100 | Loss: 2.483 | Acc: 52.189,86.595,99.869,% | Adaptive Acc: 94.013% | clf_exit: 0.268 0.495 0.237
Batch: 120 | Loss: 2.485 | Acc: 52.292,86.609,99.884,% | Adaptive Acc: 93.944% | clf_exit: 0.268 0.495 0.236
Batch: 140 | Loss: 2.484 | Acc: 52.211,86.597,99.889,% | Adaptive Acc: 93.961% | clf_exit: 0.268 0.495 0.237
Batch: 160 | Loss: 2.483 | Acc: 52.135,86.602,99.903,% | Adaptive Acc: 93.959% | clf_exit: 0.267 0.495 0.237
Batch: 180 | Loss: 2.475 | Acc: 52.188,86.676,99.909,% | Adaptive Acc: 93.987% | clf_exit: 0.267 0.496 0.237
Batch: 200 | Loss: 2.477 | Acc: 52.192,86.641,99.907,% | Adaptive Acc: 93.952% | clf_exit: 0.268 0.496 0.237
Batch: 220 | Loss: 2.479 | Acc: 52.142,86.535,99.908,% | Adaptive Acc: 93.930% | clf_exit: 0.268 0.495 0.237
Batch: 240 | Loss: 2.481 | Acc: 52.094,86.628,99.906,% | Adaptive Acc: 93.935% | clf_exit: 0.268 0.495 0.237
Batch: 260 | Loss: 2.483 | Acc: 52.089,86.557,99.913,% | Adaptive Acc: 93.927% | clf_exit: 0.268 0.496 0.237
Batch: 280 | Loss: 2.484 | Acc: 52.043,86.519,99.914,% | Adaptive Acc: 93.961% | clf_exit: 0.267 0.496 0.236
Batch: 300 | Loss: 2.488 | Acc: 51.915,86.496,99.904,% | Adaptive Acc: 93.963% | clf_exit: 0.267 0.496 0.236
Batch: 320 | Loss: 2.489 | Acc: 51.925,86.507,99.905,% | Adaptive Acc: 93.957% | clf_exit: 0.267 0.497 0.236
Batch: 340 | Loss: 2.491 | Acc: 51.833,86.526,99.908,% | Adaptive Acc: 93.972% | clf_exit: 0.268 0.495 0.237
Batch: 360 | Loss: 2.492 | Acc: 51.805,86.463,99.905,% | Adaptive Acc: 93.934% | clf_exit: 0.268 0.495 0.237
Batch: 380 | Loss: 2.494 | Acc: 51.837,86.467,99.897,% | Adaptive Acc: 93.947% | clf_exit: 0.268 0.495 0.237
Batch: 0 | Loss: 3.834 | Acc: 53.125,71.875,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.438 0.219
Batch: 20 | Loss: 4.296 | Acc: 48.214,67.560,72.024,% | Adaptive Acc: 68.527% | clf_exit: 0.346 0.410 0.244
Batch: 40 | Loss: 4.257 | Acc: 48.266,67.873,71.837,% | Adaptive Acc: 68.388% | clf_exit: 0.345 0.402 0.253
Batch: 60 | Loss: 4.271 | Acc: 47.938,67.738,71.913,% | Adaptive Acc: 68.443% | clf_exit: 0.343 0.400 0.256
Train all parameters

Epoch: 260
Batch: 0 | Loss: 2.392 | Acc: 47.656,87.500,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.242 0.531 0.227
Batch: 20 | Loss: 2.475 | Acc: 51.376,87.612,99.963,% | Adaptive Acc: 93.973% | clf_exit: 0.269 0.493 0.238
Batch: 40 | Loss: 2.468 | Acc: 51.772,87.500,99.905,% | Adaptive Acc: 94.322% | clf_exit: 0.272 0.486 0.242
Batch: 60 | Loss: 2.490 | Acc: 51.742,86.924,99.898,% | Adaptive Acc: 94.224% | clf_exit: 0.267 0.495 0.237
Batch: 80 | Loss: 2.482 | Acc: 51.726,86.777,99.894,% | Adaptive Acc: 94.155% | clf_exit: 0.266 0.500 0.234
Batch: 100 | Loss: 2.484 | Acc: 51.740,86.742,99.899,% | Adaptive Acc: 94.168% | clf_exit: 0.266 0.501 0.233
Batch: 120 | Loss: 2.489 | Acc: 51.504,86.822,99.916,% | Adaptive Acc: 94.196% | clf_exit: 0.266 0.499 0.235
Batch: 140 | Loss: 2.483 | Acc: 51.729,86.979,99.928,% | Adaptive Acc: 94.232% | clf_exit: 0.267 0.498 0.235
Batch: 160 | Loss: 2.492 | Acc: 51.398,86.811,99.913,% | Adaptive Acc: 94.090% | clf_exit: 0.266 0.498 0.236
Batch: 180 | Loss: 2.491 | Acc: 51.416,86.714,99.918,% | Adaptive Acc: 94.009% | clf_exit: 0.267 0.498 0.235
Batch: 200 | Loss: 2.490 | Acc: 51.458,86.734,99.914,% | Adaptive Acc: 93.968% | clf_exit: 0.268 0.498 0.234
Batch: 220 | Loss: 2.490 | Acc: 51.492,86.740,99.890,% | Adaptive Acc: 93.895% | clf_exit: 0.268 0.497 0.234
Batch: 240 | Loss: 2.489 | Acc: 51.666,86.664,99.890,% | Adaptive Acc: 93.889% | clf_exit: 0.269 0.496 0.235
Batch: 260 | Loss: 2.490 | Acc: 51.691,86.602,99.889,% | Adaptive Acc: 93.909% | clf_exit: 0.270 0.494 0.235
Batch: 280 | Loss: 2.493 | Acc: 51.724,86.577,99.886,% | Adaptive Acc: 93.886% | clf_exit: 0.270 0.494 0.236
Batch: 300 | Loss: 2.492 | Acc: 51.736,86.602,99.888,% | Adaptive Acc: 93.867% | clf_exit: 0.270 0.494 0.236
Batch: 320 | Loss: 2.495 | Acc: 51.740,86.556,99.888,% | Adaptive Acc: 93.891% | clf_exit: 0.269 0.494 0.236
Batch: 340 | Loss: 2.494 | Acc: 51.684,86.570,99.885,% | Adaptive Acc: 93.910% | clf_exit: 0.269 0.495 0.236
Batch: 360 | Loss: 2.494 | Acc: 51.647,86.621,99.885,% | Adaptive Acc: 93.927% | clf_exit: 0.270 0.494 0.236
Batch: 380 | Loss: 2.491 | Acc: 51.675,86.676,99.883,% | Adaptive Acc: 93.953% | clf_exit: 0.270 0.495 0.235
Batch: 0 | Loss: 3.877 | Acc: 52.344,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.298 | Acc: 48.438,67.597,72.693,% | Adaptive Acc: 68.638% | clf_exit: 0.343 0.411 0.246
Batch: 40 | Loss: 4.252 | Acc: 48.533,67.759,72.523,% | Adaptive Acc: 68.807% | clf_exit: 0.339 0.403 0.258
Batch: 60 | Loss: 4.264 | Acc: 48.271,67.546,72.349,% | Adaptive Acc: 68.622% | clf_exit: 0.337 0.406 0.257
Train all parameters

Epoch: 261
Batch: 0 | Loss: 2.506 | Acc: 55.469,83.594,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.234 0.531 0.234
Batch: 20 | Loss: 2.428 | Acc: 52.939,86.347,99.814,% | Adaptive Acc: 93.899% | clf_exit: 0.279 0.493 0.228
Batch: 40 | Loss: 2.476 | Acc: 51.582,86.223,99.886,% | Adaptive Acc: 93.807% | clf_exit: 0.271 0.494 0.235
Batch: 60 | Loss: 2.478 | Acc: 51.857,86.501,99.885,% | Adaptive Acc: 93.955% | clf_exit: 0.269 0.496 0.235
Batch: 80 | Loss: 2.494 | Acc: 51.659,86.449,99.875,% | Adaptive Acc: 93.885% | clf_exit: 0.269 0.495 0.235
Batch: 100 | Loss: 2.490 | Acc: 51.601,86.618,99.899,% | Adaptive Acc: 93.936% | clf_exit: 0.269 0.496 0.235
Batch: 120 | Loss: 2.490 | Acc: 51.705,86.538,99.916,% | Adaptive Acc: 93.918% | clf_exit: 0.268 0.497 0.236
Batch: 140 | Loss: 2.486 | Acc: 51.801,86.486,99.906,% | Adaptive Acc: 93.850% | clf_exit: 0.269 0.496 0.235
Batch: 160 | Loss: 2.488 | Acc: 51.810,86.525,99.893,% | Adaptive Acc: 93.813% | clf_exit: 0.270 0.495 0.236
Batch: 180 | Loss: 2.482 | Acc: 51.942,86.516,99.905,% | Adaptive Acc: 93.871% | clf_exit: 0.269 0.496 0.235
Batch: 200 | Loss: 2.485 | Acc: 51.803,86.536,99.903,% | Adaptive Acc: 93.851% | clf_exit: 0.269 0.496 0.235
Batch: 220 | Loss: 2.487 | Acc: 51.757,86.599,99.897,% | Adaptive Acc: 93.937% | clf_exit: 0.269 0.495 0.236
Batch: 240 | Loss: 2.486 | Acc: 51.770,86.605,99.896,% | Adaptive Acc: 94.000% | clf_exit: 0.269 0.495 0.236
Batch: 260 | Loss: 2.488 | Acc: 51.784,86.536,99.901,% | Adaptive Acc: 93.963% | clf_exit: 0.269 0.495 0.236
Batch: 280 | Loss: 2.484 | Acc: 51.768,86.619,99.900,% | Adaptive Acc: 93.947% | clf_exit: 0.270 0.495 0.235
Batch: 300 | Loss: 2.489 | Acc: 51.734,86.558,99.899,% | Adaptive Acc: 93.978% | clf_exit: 0.269 0.494 0.236
Batch: 320 | Loss: 2.492 | Acc: 51.682,86.458,99.893,% | Adaptive Acc: 93.976% | clf_exit: 0.269 0.494 0.237
Batch: 340 | Loss: 2.493 | Acc: 51.684,86.400,99.897,% | Adaptive Acc: 94.013% | clf_exit: 0.269 0.493 0.238
Batch: 360 | Loss: 2.493 | Acc: 51.638,86.440,99.898,% | Adaptive Acc: 93.997% | clf_exit: 0.269 0.493 0.238
Batch: 380 | Loss: 2.491 | Acc: 51.616,86.440,99.891,% | Adaptive Acc: 93.965% | clf_exit: 0.269 0.493 0.238
Batch: 0 | Loss: 3.777 | Acc: 52.344,71.094,79.688,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.414 0.242
Batch: 20 | Loss: 4.293 | Acc: 47.991,67.560,72.470,% | Adaptive Acc: 68.341% | clf_exit: 0.343 0.411 0.246
Batch: 40 | Loss: 4.254 | Acc: 48.075,67.740,72.237,% | Adaptive Acc: 68.426% | clf_exit: 0.342 0.403 0.255
Batch: 60 | Loss: 4.268 | Acc: 47.848,67.520,72.285,% | Adaptive Acc: 68.468% | clf_exit: 0.340 0.404 0.256
Train all parameters

Epoch: 262
Batch: 0 | Loss: 2.553 | Acc: 47.656,87.500,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.281 0.500 0.219
Batch: 20 | Loss: 2.461 | Acc: 52.195,87.909,99.888,% | Adaptive Acc: 94.196% | clf_exit: 0.271 0.497 0.232
Batch: 40 | Loss: 2.465 | Acc: 51.715,87.309,99.886,% | Adaptive Acc: 93.921% | clf_exit: 0.268 0.498 0.234
Batch: 60 | Loss: 2.452 | Acc: 51.755,87.180,99.910,% | Adaptive Acc: 94.057% | clf_exit: 0.267 0.500 0.233
Batch: 80 | Loss: 2.454 | Acc: 51.775,87.269,99.904,% | Adaptive Acc: 94.145% | clf_exit: 0.265 0.505 0.231
Batch: 100 | Loss: 2.450 | Acc: 51.872,87.183,99.915,% | Adaptive Acc: 94.028% | clf_exit: 0.268 0.505 0.227
Batch: 120 | Loss: 2.469 | Acc: 51.550,86.971,99.916,% | Adaptive Acc: 94.008% | clf_exit: 0.266 0.503 0.231
Batch: 140 | Loss: 2.472 | Acc: 51.540,86.979,99.922,% | Adaptive Acc: 94.049% | clf_exit: 0.266 0.502 0.232
Batch: 160 | Loss: 2.474 | Acc: 51.504,87.088,99.922,% | Adaptive Acc: 94.167% | clf_exit: 0.265 0.501 0.234
Batch: 180 | Loss: 2.471 | Acc: 51.649,87.124,99.905,% | Adaptive Acc: 94.151% | clf_exit: 0.265 0.503 0.233
Batch: 200 | Loss: 2.475 | Acc: 51.726,87.018,99.911,% | Adaptive Acc: 94.127% | clf_exit: 0.266 0.500 0.234
Batch: 220 | Loss: 2.474 | Acc: 51.796,86.970,99.897,% | Adaptive Acc: 94.093% | clf_exit: 0.267 0.499 0.234
Batch: 240 | Loss: 2.481 | Acc: 51.773,86.861,99.900,% | Adaptive Acc: 94.045% | clf_exit: 0.267 0.499 0.234
Batch: 260 | Loss: 2.482 | Acc: 51.826,86.824,99.895,% | Adaptive Acc: 94.064% | clf_exit: 0.268 0.498 0.234
Batch: 280 | Loss: 2.477 | Acc: 51.882,86.872,99.897,% | Adaptive Acc: 94.064% | clf_exit: 0.268 0.499 0.233
Batch: 300 | Loss: 2.477 | Acc: 51.827,86.859,99.896,% | Adaptive Acc: 94.017% | clf_exit: 0.269 0.498 0.233
Batch: 320 | Loss: 2.474 | Acc: 51.908,86.945,99.893,% | Adaptive Acc: 94.064% | clf_exit: 0.269 0.497 0.233
Batch: 340 | Loss: 2.473 | Acc: 52.009,86.952,99.895,% | Adaptive Acc: 94.078% | clf_exit: 0.270 0.497 0.233
Batch: 360 | Loss: 2.475 | Acc: 51.904,86.950,99.898,% | Adaptive Acc: 94.062% | clf_exit: 0.270 0.496 0.234
Batch: 380 | Loss: 2.475 | Acc: 51.876,86.895,99.897,% | Adaptive Acc: 94.068% | clf_exit: 0.270 0.496 0.234
Batch: 0 | Loss: 3.821 | Acc: 50.781,71.875,78.906,% | Adaptive Acc: 72.656% | clf_exit: 0.344 0.406 0.250
Batch: 20 | Loss: 4.286 | Acc: 48.251,67.634,72.731,% | Adaptive Acc: 68.973% | clf_exit: 0.342 0.409 0.249
Batch: 40 | Loss: 4.241 | Acc: 48.361,67.759,72.561,% | Adaptive Acc: 68.998% | clf_exit: 0.340 0.400 0.260
Batch: 60 | Loss: 4.254 | Acc: 48.143,67.610,72.336,% | Adaptive Acc: 68.686% | clf_exit: 0.338 0.402 0.260
Train all parameters

Epoch: 263
Batch: 0 | Loss: 2.544 | Acc: 50.781,87.500,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.281 0.484 0.234
Batch: 20 | Loss: 2.425 | Acc: 53.051,87.537,99.888,% | Adaptive Acc: 94.048% | clf_exit: 0.266 0.511 0.223
Batch: 40 | Loss: 2.419 | Acc: 52.954,87.729,99.886,% | Adaptive Acc: 94.188% | clf_exit: 0.273 0.502 0.225
Batch: 60 | Loss: 2.434 | Acc: 52.728,87.346,99.898,% | Adaptive Acc: 93.993% | clf_exit: 0.273 0.502 0.225
Batch: 80 | Loss: 2.437 | Acc: 52.392,87.172,99.913,% | Adaptive Acc: 93.798% | clf_exit: 0.276 0.498 0.226
Batch: 100 | Loss: 2.448 | Acc: 52.328,87.028,99.923,% | Adaptive Acc: 93.827% | clf_exit: 0.273 0.499 0.228
Batch: 120 | Loss: 2.461 | Acc: 51.950,86.796,99.923,% | Adaptive Acc: 93.827% | clf_exit: 0.273 0.495 0.232
Batch: 140 | Loss: 2.469 | Acc: 51.856,86.780,99.906,% | Adaptive Acc: 93.794% | clf_exit: 0.271 0.496 0.232
Batch: 160 | Loss: 2.475 | Acc: 51.761,86.753,99.913,% | Adaptive Acc: 93.832% | clf_exit: 0.270 0.496 0.233
Batch: 180 | Loss: 2.475 | Acc: 51.709,86.835,99.901,% | Adaptive Acc: 93.927% | clf_exit: 0.269 0.498 0.233
Batch: 200 | Loss: 2.475 | Acc: 51.679,86.886,99.911,% | Adaptive Acc: 93.937% | clf_exit: 0.268 0.498 0.233
Batch: 220 | Loss: 2.476 | Acc: 51.718,86.853,99.908,% | Adaptive Acc: 93.867% | clf_exit: 0.268 0.498 0.234
Batch: 240 | Loss: 2.477 | Acc: 51.734,86.832,99.916,% | Adaptive Acc: 93.883% | clf_exit: 0.268 0.498 0.234
Batch: 260 | Loss: 2.476 | Acc: 51.802,86.901,99.916,% | Adaptive Acc: 93.936% | clf_exit: 0.268 0.498 0.233
Batch: 280 | Loss: 2.475 | Acc: 51.860,86.933,99.919,% | Adaptive Acc: 93.995% | clf_exit: 0.268 0.498 0.234
Batch: 300 | Loss: 2.472 | Acc: 51.926,86.942,99.925,% | Adaptive Acc: 94.030% | clf_exit: 0.270 0.496 0.234
Batch: 320 | Loss: 2.472 | Acc: 51.986,86.972,99.927,% | Adaptive Acc: 94.044% | clf_exit: 0.270 0.497 0.233
Batch: 340 | Loss: 2.472 | Acc: 51.952,86.934,99.927,% | Adaptive Acc: 94.043% | clf_exit: 0.270 0.496 0.233
Batch: 360 | Loss: 2.474 | Acc: 51.900,86.942,99.926,% | Adaptive Acc: 94.103% | clf_exit: 0.270 0.496 0.234
Batch: 380 | Loss: 2.476 | Acc: 51.930,86.916,99.926,% | Adaptive Acc: 94.088% | clf_exit: 0.270 0.496 0.234
Batch: 0 | Loss: 3.801 | Acc: 53.125,71.875,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.414 0.242
Batch: 20 | Loss: 4.303 | Acc: 48.438,67.522,72.359,% | Adaptive Acc: 68.787% | clf_exit: 0.342 0.417 0.241
Batch: 40 | Loss: 4.253 | Acc: 48.399,67.607,72.485,% | Adaptive Acc: 68.845% | clf_exit: 0.340 0.408 0.252
Batch: 60 | Loss: 4.265 | Acc: 48.092,67.533,72.362,% | Adaptive Acc: 68.724% | clf_exit: 0.339 0.406 0.255
Train all parameters

Epoch: 264
Batch: 0 | Loss: 2.227 | Acc: 52.344,85.938,99.219,% | Adaptive Acc: 91.406% | clf_exit: 0.250 0.578 0.172
Batch: 20 | Loss: 2.472 | Acc: 51.265,86.830,99.888,% | Adaptive Acc: 94.457% | clf_exit: 0.266 0.494 0.240
Batch: 40 | Loss: 2.447 | Acc: 51.829,87.157,99.886,% | Adaptive Acc: 94.341% | clf_exit: 0.274 0.492 0.234
Batch: 60 | Loss: 2.439 | Acc: 52.011,87.334,99.910,% | Adaptive Acc: 94.416% | clf_exit: 0.269 0.495 0.236
Batch: 80 | Loss: 2.452 | Acc: 51.939,87.085,99.904,% | Adaptive Acc: 94.329% | clf_exit: 0.268 0.494 0.238
Batch: 100 | Loss: 2.468 | Acc: 51.771,86.773,99.899,% | Adaptive Acc: 94.175% | clf_exit: 0.267 0.495 0.238
Batch: 120 | Loss: 2.472 | Acc: 51.918,86.732,99.903,% | Adaptive Acc: 94.170% | clf_exit: 0.269 0.492 0.238
Batch: 140 | Loss: 2.470 | Acc: 51.873,86.769,99.911,% | Adaptive Acc: 94.121% | clf_exit: 0.271 0.492 0.237
Batch: 160 | Loss: 2.472 | Acc: 51.868,86.743,99.922,% | Adaptive Acc: 94.153% | clf_exit: 0.270 0.494 0.236
Batch: 180 | Loss: 2.475 | Acc: 51.731,86.844,99.922,% | Adaptive Acc: 94.074% | clf_exit: 0.270 0.496 0.234
Batch: 200 | Loss: 2.474 | Acc: 51.796,86.816,99.922,% | Adaptive Acc: 94.076% | clf_exit: 0.269 0.496 0.235
Batch: 220 | Loss: 2.467 | Acc: 51.973,86.924,99.919,% | Adaptive Acc: 94.104% | clf_exit: 0.270 0.497 0.233
Batch: 240 | Loss: 2.468 | Acc: 52.010,86.881,99.919,% | Adaptive Acc: 94.126% | clf_exit: 0.271 0.495 0.234
Batch: 260 | Loss: 2.473 | Acc: 52.017,86.925,99.898,% | Adaptive Acc: 94.046% | clf_exit: 0.270 0.497 0.234
Batch: 280 | Loss: 2.474 | Acc: 51.918,86.888,99.894,% | Adaptive Acc: 94.011% | clf_exit: 0.270 0.497 0.233
Batch: 300 | Loss: 2.473 | Acc: 51.962,86.921,99.901,% | Adaptive Acc: 94.012% | clf_exit: 0.270 0.497 0.233
Batch: 320 | Loss: 2.472 | Acc: 51.942,86.909,99.903,% | Adaptive Acc: 94.013% | clf_exit: 0.271 0.496 0.233
Batch: 340 | Loss: 2.472 | Acc: 51.968,86.875,99.906,% | Adaptive Acc: 93.995% | clf_exit: 0.271 0.496 0.233
Batch: 360 | Loss: 2.476 | Acc: 51.861,86.844,99.909,% | Adaptive Acc: 94.001% | clf_exit: 0.271 0.495 0.234
Batch: 380 | Loss: 2.475 | Acc: 51.815,86.868,99.912,% | Adaptive Acc: 94.006% | clf_exit: 0.271 0.496 0.233
Batch: 0 | Loss: 3.805 | Acc: 52.344,71.094,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.291 | Acc: 48.289,67.485,72.768,% | Adaptive Acc: 69.010% | clf_exit: 0.339 0.410 0.251
Batch: 40 | Loss: 4.250 | Acc: 48.342,67.588,72.561,% | Adaptive Acc: 69.112% | clf_exit: 0.336 0.405 0.259
Batch: 60 | Loss: 4.262 | Acc: 48.092,67.431,72.490,% | Adaptive Acc: 68.968% | clf_exit: 0.335 0.405 0.260
Train all parameters

Epoch: 265
Batch: 0 | Loss: 2.606 | Acc: 46.875,89.062,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.273 0.461 0.266
Batch: 20 | Loss: 2.496 | Acc: 50.558,87.240,99.851,% | Adaptive Acc: 94.122% | clf_exit: 0.267 0.498 0.235
Batch: 40 | Loss: 2.485 | Acc: 51.658,86.928,99.867,% | Adaptive Acc: 94.569% | clf_exit: 0.263 0.497 0.240
Batch: 60 | Loss: 2.497 | Acc: 51.447,86.924,99.898,% | Adaptive Acc: 94.647% | clf_exit: 0.264 0.496 0.240
Batch: 80 | Loss: 2.487 | Acc: 51.620,86.767,99.875,% | Adaptive Acc: 94.396% | clf_exit: 0.267 0.497 0.236
Batch: 100 | Loss: 2.481 | Acc: 51.849,86.703,99.876,% | Adaptive Acc: 94.338% | clf_exit: 0.269 0.493 0.238
Batch: 120 | Loss: 2.473 | Acc: 52.021,86.848,99.884,% | Adaptive Acc: 94.350% | clf_exit: 0.271 0.493 0.236
Batch: 140 | Loss: 2.476 | Acc: 52.028,86.885,99.884,% | Adaptive Acc: 94.332% | clf_exit: 0.271 0.493 0.236
Batch: 160 | Loss: 2.468 | Acc: 52.014,86.927,99.893,% | Adaptive Acc: 94.226% | clf_exit: 0.272 0.493 0.235
Batch: 180 | Loss: 2.469 | Acc: 52.029,87.034,99.896,% | Adaptive Acc: 94.203% | clf_exit: 0.271 0.495 0.234
Batch: 200 | Loss: 2.470 | Acc: 51.986,87.069,99.891,% | Adaptive Acc: 94.193% | clf_exit: 0.270 0.495 0.234
Batch: 220 | Loss: 2.475 | Acc: 51.895,87.037,99.897,% | Adaptive Acc: 94.256% | clf_exit: 0.268 0.496 0.235
Batch: 240 | Loss: 2.477 | Acc: 51.935,87.004,99.900,% | Adaptive Acc: 94.256% | clf_exit: 0.269 0.496 0.235
Batch: 260 | Loss: 2.478 | Acc: 51.928,86.961,99.904,% | Adaptive Acc: 94.220% | clf_exit: 0.270 0.495 0.236
Batch: 280 | Loss: 2.473 | Acc: 52.038,86.938,99.908,% | Adaptive Acc: 94.189% | clf_exit: 0.271 0.495 0.235
Batch: 300 | Loss: 2.474 | Acc: 52.043,86.945,99.907,% | Adaptive Acc: 94.157% | clf_exit: 0.271 0.494 0.235
Batch: 320 | Loss: 2.470 | Acc: 52.091,86.918,99.900,% | Adaptive Acc: 94.171% | clf_exit: 0.271 0.494 0.234
Batch: 340 | Loss: 2.469 | Acc: 52.108,86.918,99.895,% | Adaptive Acc: 94.172% | clf_exit: 0.271 0.495 0.234
Batch: 360 | Loss: 2.471 | Acc: 52.030,86.924,99.896,% | Adaptive Acc: 94.166% | clf_exit: 0.270 0.496 0.234
Batch: 380 | Loss: 2.472 | Acc: 52.007,86.879,99.895,% | Adaptive Acc: 94.133% | clf_exit: 0.270 0.496 0.235
Batch: 0 | Loss: 3.818 | Acc: 53.125,71.094,78.906,% | Adaptive Acc: 72.656% | clf_exit: 0.336 0.422 0.242
Batch: 20 | Loss: 4.295 | Acc: 48.177,67.708,72.470,% | Adaptive Acc: 68.564% | clf_exit: 0.343 0.414 0.243
Batch: 40 | Loss: 4.251 | Acc: 48.190,67.873,72.599,% | Adaptive Acc: 68.902% | clf_exit: 0.341 0.404 0.255
Batch: 60 | Loss: 4.259 | Acc: 48.002,67.789,72.592,% | Adaptive Acc: 68.827% | clf_exit: 0.340 0.404 0.256
Train all parameters

Epoch: 266
Batch: 0 | Loss: 2.535 | Acc: 53.906,86.719,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.242 0.484 0.273
Batch: 20 | Loss: 2.529 | Acc: 51.042,86.198,99.926,% | Adaptive Acc: 93.899% | clf_exit: 0.265 0.495 0.240
Batch: 40 | Loss: 2.497 | Acc: 51.925,86.623,99.905,% | Adaptive Acc: 94.207% | clf_exit: 0.267 0.497 0.236
Batch: 60 | Loss: 2.479 | Acc: 52.293,87.090,99.923,% | Adaptive Acc: 94.173% | clf_exit: 0.269 0.499 0.232
Batch: 80 | Loss: 2.484 | Acc: 51.968,87.133,99.932,% | Adaptive Acc: 94.309% | clf_exit: 0.268 0.498 0.234
Batch: 100 | Loss: 2.476 | Acc: 51.926,87.245,99.938,% | Adaptive Acc: 94.469% | clf_exit: 0.266 0.500 0.234
Batch: 120 | Loss: 2.474 | Acc: 51.872,87.132,99.942,% | Adaptive Acc: 94.421% | clf_exit: 0.266 0.500 0.234
Batch: 140 | Loss: 2.468 | Acc: 51.967,87.173,99.939,% | Adaptive Acc: 94.365% | clf_exit: 0.267 0.500 0.232
Batch: 160 | Loss: 2.469 | Acc: 51.907,86.986,99.932,% | Adaptive Acc: 94.250% | clf_exit: 0.266 0.502 0.232
Batch: 180 | Loss: 2.470 | Acc: 51.800,86.952,99.935,% | Adaptive Acc: 94.272% | clf_exit: 0.266 0.501 0.233
Batch: 200 | Loss: 2.468 | Acc: 51.838,87.014,99.938,% | Adaptive Acc: 94.197% | clf_exit: 0.268 0.498 0.234
Batch: 220 | Loss: 2.468 | Acc: 51.934,86.917,99.940,% | Adaptive Acc: 94.104% | clf_exit: 0.270 0.496 0.234
Batch: 240 | Loss: 2.472 | Acc: 51.812,86.852,99.929,% | Adaptive Acc: 94.045% | clf_exit: 0.270 0.496 0.234
Batch: 260 | Loss: 2.467 | Acc: 51.904,86.967,99.931,% | Adaptive Acc: 94.085% | clf_exit: 0.271 0.495 0.234
Batch: 280 | Loss: 2.470 | Acc: 51.804,87.019,99.928,% | Adaptive Acc: 94.111% | clf_exit: 0.271 0.495 0.234
Batch: 300 | Loss: 2.467 | Acc: 51.814,87.030,99.933,% | Adaptive Acc: 94.074% | clf_exit: 0.272 0.495 0.233
Batch: 320 | Loss: 2.465 | Acc: 51.850,87.052,99.917,% | Adaptive Acc: 94.071% | clf_exit: 0.271 0.496 0.232
Batch: 340 | Loss: 2.466 | Acc: 51.920,86.987,99.915,% | Adaptive Acc: 94.091% | clf_exit: 0.272 0.496 0.232
Batch: 360 | Loss: 2.468 | Acc: 51.961,86.942,99.916,% | Adaptive Acc: 94.107% | clf_exit: 0.272 0.496 0.233
Batch: 380 | Loss: 2.473 | Acc: 51.962,86.889,99.910,% | Adaptive Acc: 94.133% | clf_exit: 0.271 0.495 0.234
Batch: 0 | Loss: 3.802 | Acc: 53.125,71.094,76.562,% | Adaptive Acc: 71.875% | clf_exit: 0.352 0.414 0.234
Batch: 20 | Loss: 4.285 | Acc: 48.140,67.746,71.987,% | Adaptive Acc: 68.304% | clf_exit: 0.346 0.408 0.246
Batch: 40 | Loss: 4.242 | Acc: 48.266,67.912,72.256,% | Adaptive Acc: 68.617% | clf_exit: 0.345 0.397 0.257
Batch: 60 | Loss: 4.255 | Acc: 47.951,67.879,72.426,% | Adaptive Acc: 68.558% | clf_exit: 0.343 0.401 0.256
Train all parameters

Epoch: 267
Batch: 0 | Loss: 2.790 | Acc: 46.875,85.938,100.000,% | Adaptive Acc: 90.625% | clf_exit: 0.242 0.523 0.234
Batch: 20 | Loss: 2.446 | Acc: 53.162,87.314,99.814,% | Adaptive Acc: 93.750% | clf_exit: 0.284 0.489 0.227
Batch: 40 | Loss: 2.436 | Acc: 52.382,87.214,99.848,% | Adaptive Acc: 94.245% | clf_exit: 0.279 0.490 0.232
Batch: 60 | Loss: 2.441 | Acc: 52.203,87.077,99.859,% | Adaptive Acc: 94.237% | clf_exit: 0.279 0.483 0.238
Batch: 80 | Loss: 2.458 | Acc: 52.189,86.970,99.855,% | Adaptive Acc: 94.213% | clf_exit: 0.274 0.487 0.239
Batch: 100 | Loss: 2.465 | Acc: 52.081,86.703,99.876,% | Adaptive Acc: 94.036% | clf_exit: 0.276 0.486 0.238
Batch: 120 | Loss: 2.468 | Acc: 52.079,86.609,99.884,% | Adaptive Acc: 94.079% | clf_exit: 0.276 0.486 0.238
Batch: 140 | Loss: 2.469 | Acc: 52.089,86.608,99.889,% | Adaptive Acc: 94.082% | clf_exit: 0.274 0.489 0.237
Batch: 160 | Loss: 2.472 | Acc: 52.121,86.631,99.888,% | Adaptive Acc: 93.988% | clf_exit: 0.274 0.491 0.235
Batch: 180 | Loss: 2.468 | Acc: 52.214,86.732,99.883,% | Adaptive Acc: 94.091% | clf_exit: 0.273 0.492 0.236
Batch: 200 | Loss: 2.468 | Acc: 52.204,86.816,99.887,% | Adaptive Acc: 94.139% | clf_exit: 0.273 0.492 0.236
Batch: 220 | Loss: 2.470 | Acc: 52.146,86.853,99.897,% | Adaptive Acc: 94.118% | clf_exit: 0.272 0.493 0.236
Batch: 240 | Loss: 2.468 | Acc: 52.052,86.904,99.903,% | Adaptive Acc: 94.107% | clf_exit: 0.272 0.492 0.235
Batch: 260 | Loss: 2.471 | Acc: 52.006,86.874,99.898,% | Adaptive Acc: 94.028% | clf_exit: 0.272 0.492 0.235
Batch: 280 | Loss: 2.472 | Acc: 51.913,86.894,99.894,% | Adaptive Acc: 94.028% | clf_exit: 0.272 0.492 0.236
Batch: 300 | Loss: 2.468 | Acc: 52.017,86.932,99.901,% | Adaptive Acc: 93.999% | clf_exit: 0.273 0.492 0.235
Batch: 320 | Loss: 2.470 | Acc: 51.923,86.906,99.903,% | Adaptive Acc: 94.020% | clf_exit: 0.272 0.491 0.236
Batch: 340 | Loss: 2.473 | Acc: 51.883,86.886,99.901,% | Adaptive Acc: 94.002% | clf_exit: 0.271 0.493 0.236
Batch: 360 | Loss: 2.477 | Acc: 51.824,86.868,99.903,% | Adaptive Acc: 94.029% | clf_exit: 0.270 0.493 0.236
Batch: 380 | Loss: 2.477 | Acc: 51.876,86.838,99.902,% | Adaptive Acc: 94.035% | clf_exit: 0.271 0.493 0.236
Batch: 0 | Loss: 3.804 | Acc: 53.125,71.875,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.445 0.211
Batch: 20 | Loss: 4.294 | Acc: 48.512,67.708,72.135,% | Adaptive Acc: 68.564% | clf_exit: 0.344 0.408 0.247
Batch: 40 | Loss: 4.251 | Acc: 48.457,67.912,72.123,% | Adaptive Acc: 68.864% | clf_exit: 0.342 0.400 0.258
Batch: 60 | Loss: 4.263 | Acc: 48.156,67.725,72.234,% | Adaptive Acc: 68.699% | clf_exit: 0.341 0.403 0.257
Train all parameters

Epoch: 268
Batch: 0 | Loss: 2.690 | Acc: 46.875,77.344,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.219 0.492 0.289
Batch: 20 | Loss: 2.512 | Acc: 51.265,86.012,99.926,% | Adaptive Acc: 93.415% | clf_exit: 0.265 0.503 0.232
Batch: 40 | Loss: 2.513 | Acc: 50.915,86.090,99.943,% | Adaptive Acc: 93.121% | clf_exit: 0.272 0.490 0.238
Batch: 60 | Loss: 2.514 | Acc: 50.935,86.399,99.936,% | Adaptive Acc: 93.391% | clf_exit: 0.266 0.498 0.236
Batch: 80 | Loss: 2.487 | Acc: 51.524,86.507,99.952,% | Adaptive Acc: 93.567% | clf_exit: 0.267 0.498 0.234
Batch: 100 | Loss: 2.477 | Acc: 51.849,86.742,99.930,% | Adaptive Acc: 93.688% | clf_exit: 0.268 0.498 0.234
Batch: 120 | Loss: 2.483 | Acc: 51.711,86.809,99.929,% | Adaptive Acc: 93.789% | clf_exit: 0.268 0.495 0.237
Batch: 140 | Loss: 2.471 | Acc: 51.912,86.924,99.928,% | Adaptive Acc: 93.805% | clf_exit: 0.270 0.496 0.235
Batch: 160 | Loss: 2.462 | Acc: 52.116,86.893,99.922,% | Adaptive Acc: 93.832% | clf_exit: 0.271 0.497 0.233
Batch: 180 | Loss: 2.462 | Acc: 52.085,86.930,99.914,% | Adaptive Acc: 93.845% | clf_exit: 0.269 0.500 0.231
Batch: 200 | Loss: 2.460 | Acc: 52.153,86.995,99.914,% | Adaptive Acc: 93.964% | clf_exit: 0.269 0.500 0.231
Batch: 220 | Loss: 2.463 | Acc: 52.089,86.977,99.915,% | Adaptive Acc: 94.026% | clf_exit: 0.269 0.500 0.231
Batch: 240 | Loss: 2.468 | Acc: 52.052,87.020,99.922,% | Adaptive Acc: 94.084% | clf_exit: 0.268 0.499 0.232
Batch: 260 | Loss: 2.471 | Acc: 51.985,86.973,99.928,% | Adaptive Acc: 94.103% | clf_exit: 0.268 0.498 0.234
Batch: 280 | Loss: 2.471 | Acc: 52.046,86.986,99.930,% | Adaptive Acc: 94.136% | clf_exit: 0.268 0.497 0.234
Batch: 300 | Loss: 2.473 | Acc: 52.019,86.963,99.922,% | Adaptive Acc: 94.155% | clf_exit: 0.268 0.498 0.235
Batch: 320 | Loss: 2.472 | Acc: 52.103,86.960,99.922,% | Adaptive Acc: 94.159% | clf_exit: 0.268 0.498 0.234
Batch: 340 | Loss: 2.473 | Acc: 52.094,86.996,99.924,% | Adaptive Acc: 94.201% | clf_exit: 0.268 0.497 0.235
Batch: 360 | Loss: 2.471 | Acc: 52.106,87.059,99.926,% | Adaptive Acc: 94.224% | clf_exit: 0.269 0.496 0.235
Batch: 380 | Loss: 2.475 | Acc: 52.114,87.096,99.924,% | Adaptive Acc: 94.273% | clf_exit: 0.268 0.496 0.236
Batch: 0 | Loss: 3.805 | Acc: 53.125,72.656,77.344,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.430 0.227
Batch: 20 | Loss: 4.296 | Acc: 48.363,67.783,72.507,% | Adaptive Acc: 68.601% | clf_exit: 0.342 0.411 0.248
Batch: 40 | Loss: 4.249 | Acc: 48.399,67.893,72.485,% | Adaptive Acc: 68.979% | clf_exit: 0.340 0.403 0.257
Batch: 60 | Loss: 4.259 | Acc: 48.040,67.636,72.426,% | Adaptive Acc: 68.827% | clf_exit: 0.338 0.405 0.257
Train all parameters

Epoch: 269
Batch: 0 | Loss: 2.719 | Acc: 41.406,83.594,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.211 0.523 0.266
Batch: 20 | Loss: 2.498 | Acc: 52.083,86.458,99.888,% | Adaptive Acc: 94.122% | clf_exit: 0.276 0.481 0.243
Batch: 40 | Loss: 2.483 | Acc: 52.134,87.138,99.867,% | Adaptive Acc: 94.379% | clf_exit: 0.277 0.482 0.242
Batch: 60 | Loss: 2.475 | Acc: 52.228,87.436,99.846,% | Adaptive Acc: 94.288% | clf_exit: 0.275 0.487 0.237
Batch: 80 | Loss: 2.477 | Acc: 52.016,87.182,99.865,% | Adaptive Acc: 94.165% | clf_exit: 0.274 0.488 0.237
Batch: 100 | Loss: 2.481 | Acc: 51.570,87.098,99.869,% | Adaptive Acc: 94.098% | clf_exit: 0.272 0.491 0.237
Batch: 120 | Loss: 2.477 | Acc: 51.679,87.093,99.858,% | Adaptive Acc: 94.105% | clf_exit: 0.272 0.491 0.238
Batch: 140 | Loss: 2.478 | Acc: 51.684,87.084,99.867,% | Adaptive Acc: 94.038% | clf_exit: 0.273 0.491 0.236
Batch: 160 | Loss: 2.484 | Acc: 51.538,87.049,99.879,% | Adaptive Acc: 94.075% | clf_exit: 0.271 0.492 0.237
Batch: 180 | Loss: 2.483 | Acc: 51.597,87.034,99.883,% | Adaptive Acc: 94.138% | clf_exit: 0.271 0.492 0.237
Batch: 200 | Loss: 2.479 | Acc: 51.772,87.045,99.887,% | Adaptive Acc: 94.119% | clf_exit: 0.272 0.492 0.236
Batch: 220 | Loss: 2.475 | Acc: 51.753,87.129,99.890,% | Adaptive Acc: 94.125% | clf_exit: 0.272 0.493 0.235
Batch: 240 | Loss: 2.479 | Acc: 51.708,87.069,99.893,% | Adaptive Acc: 94.194% | clf_exit: 0.272 0.493 0.236
Batch: 260 | Loss: 2.476 | Acc: 51.907,86.970,99.889,% | Adaptive Acc: 94.187% | clf_exit: 0.272 0.492 0.236
Batch: 280 | Loss: 2.474 | Acc: 52.043,87.041,99.892,% | Adaptive Acc: 94.206% | clf_exit: 0.271 0.492 0.237
Batch: 300 | Loss: 2.475 | Acc: 51.988,87.059,99.891,% | Adaptive Acc: 94.189% | clf_exit: 0.271 0.492 0.236
Batch: 320 | Loss: 2.477 | Acc: 51.988,87.021,99.886,% | Adaptive Acc: 94.137% | clf_exit: 0.271 0.493 0.236
Batch: 340 | Loss: 2.474 | Acc: 52.037,86.991,99.888,% | Adaptive Acc: 94.119% | clf_exit: 0.271 0.493 0.236
Batch: 360 | Loss: 2.473 | Acc: 52.056,86.972,99.883,% | Adaptive Acc: 94.118% | clf_exit: 0.271 0.494 0.235
Batch: 380 | Loss: 2.473 | Acc: 52.108,86.994,99.887,% | Adaptive Acc: 94.133% | clf_exit: 0.271 0.494 0.234
Batch: 0 | Loss: 3.804 | Acc: 53.125,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.292 | Acc: 48.363,67.634,72.284,% | Adaptive Acc: 68.638% | clf_exit: 0.340 0.410 0.250
Batch: 40 | Loss: 4.249 | Acc: 48.571,67.950,72.332,% | Adaptive Acc: 68.731% | clf_exit: 0.341 0.401 0.259
Batch: 60 | Loss: 4.261 | Acc: 48.271,67.879,72.323,% | Adaptive Acc: 68.635% | clf_exit: 0.340 0.402 0.258
Train all parameters

Epoch: 270
Batch: 0 | Loss: 2.193 | Acc: 57.812,93.750,100.000,% | Adaptive Acc: 96.875% | clf_exit: 0.258 0.523 0.219
Batch: 20 | Loss: 2.516 | Acc: 52.232,87.723,99.926,% | Adaptive Acc: 94.568% | clf_exit: 0.252 0.512 0.236
Batch: 40 | Loss: 2.493 | Acc: 51.582,87.481,99.924,% | Adaptive Acc: 94.112% | clf_exit: 0.258 0.507 0.235
Batch: 60 | Loss: 2.493 | Acc: 52.024,87.039,99.923,% | Adaptive Acc: 93.916% | clf_exit: 0.264 0.502 0.234
Batch: 80 | Loss: 2.505 | Acc: 51.804,86.709,99.932,% | Adaptive Acc: 94.039% | clf_exit: 0.260 0.504 0.236
Batch: 100 | Loss: 2.493 | Acc: 52.081,86.966,99.946,% | Adaptive Acc: 94.245% | clf_exit: 0.263 0.502 0.235
Batch: 120 | Loss: 2.498 | Acc: 51.918,86.874,99.929,% | Adaptive Acc: 94.215% | clf_exit: 0.263 0.502 0.235
Batch: 140 | Loss: 2.496 | Acc: 51.895,86.946,99.928,% | Adaptive Acc: 94.276% | clf_exit: 0.263 0.502 0.236
Batch: 160 | Loss: 2.494 | Acc: 51.791,87.054,99.932,% | Adaptive Acc: 94.148% | clf_exit: 0.266 0.499 0.235
Batch: 180 | Loss: 2.491 | Acc: 51.770,87.012,99.927,% | Adaptive Acc: 94.082% | clf_exit: 0.267 0.497 0.236
Batch: 200 | Loss: 2.487 | Acc: 51.784,86.987,99.922,% | Adaptive Acc: 94.038% | clf_exit: 0.269 0.495 0.235
Batch: 220 | Loss: 2.477 | Acc: 52.029,87.108,99.929,% | Adaptive Acc: 94.047% | clf_exit: 0.271 0.495 0.233
Batch: 240 | Loss: 2.477 | Acc: 52.033,87.137,99.929,% | Adaptive Acc: 94.051% | clf_exit: 0.271 0.495 0.234
Batch: 260 | Loss: 2.476 | Acc: 52.014,87.159,99.928,% | Adaptive Acc: 94.157% | clf_exit: 0.271 0.495 0.234
Batch: 280 | Loss: 2.473 | Acc: 51.991,87.180,99.930,% | Adaptive Acc: 94.161% | clf_exit: 0.271 0.494 0.234
Batch: 300 | Loss: 2.470 | Acc: 52.069,87.246,99.935,% | Adaptive Acc: 94.165% | clf_exit: 0.272 0.494 0.234
Batch: 320 | Loss: 2.468 | Acc: 52.151,87.274,99.939,% | Adaptive Acc: 94.191% | clf_exit: 0.272 0.494 0.234
Batch: 340 | Loss: 2.467 | Acc: 52.101,87.287,99.934,% | Adaptive Acc: 94.190% | clf_exit: 0.271 0.495 0.234
Batch: 360 | Loss: 2.468 | Acc: 51.993,87.281,99.935,% | Adaptive Acc: 94.187% | clf_exit: 0.271 0.495 0.234
Batch: 380 | Loss: 2.467 | Acc: 51.950,87.238,99.930,% | Adaptive Acc: 94.144% | clf_exit: 0.271 0.495 0.234
Batch: 0 | Loss: 3.822 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.279 | Acc: 48.251,67.485,72.619,% | Adaptive Acc: 68.713% | clf_exit: 0.339 0.411 0.250
Batch: 40 | Loss: 4.243 | Acc: 48.457,67.702,72.504,% | Adaptive Acc: 68.788% | clf_exit: 0.339 0.401 0.260
Batch: 60 | Loss: 4.254 | Acc: 48.207,67.649,72.503,% | Adaptive Acc: 68.814% | clf_exit: 0.339 0.402 0.259
Train all parameters

Epoch: 271
Batch: 0 | Loss: 2.558 | Acc: 47.656,85.938,100.000,% | Adaptive Acc: 92.188% | clf_exit: 0.234 0.562 0.203
Batch: 20 | Loss: 2.491 | Acc: 51.860,86.682,99.926,% | Adaptive Acc: 94.048% | clf_exit: 0.267 0.495 0.238
Batch: 40 | Loss: 2.491 | Acc: 51.734,86.585,99.886,% | Adaptive Acc: 94.074% | clf_exit: 0.272 0.491 0.237
Batch: 60 | Loss: 2.472 | Acc: 51.639,86.642,99.872,% | Adaptive Acc: 93.981% | clf_exit: 0.272 0.499 0.229
Batch: 80 | Loss: 2.473 | Acc: 52.016,86.622,99.894,% | Adaptive Acc: 93.972% | clf_exit: 0.274 0.497 0.230
Batch: 100 | Loss: 2.475 | Acc: 52.034,86.641,99.899,% | Adaptive Acc: 93.843% | clf_exit: 0.273 0.496 0.232
Batch: 120 | Loss: 2.478 | Acc: 51.911,86.719,99.903,% | Adaptive Acc: 94.008% | clf_exit: 0.273 0.494 0.233
Batch: 140 | Loss: 2.477 | Acc: 51.900,86.669,99.911,% | Adaptive Acc: 94.027% | clf_exit: 0.274 0.493 0.233
Batch: 160 | Loss: 2.469 | Acc: 52.019,86.792,99.913,% | Adaptive Acc: 94.095% | clf_exit: 0.274 0.495 0.231
Batch: 180 | Loss: 2.468 | Acc: 51.994,86.792,99.909,% | Adaptive Acc: 94.087% | clf_exit: 0.272 0.496 0.231
Batch: 200 | Loss: 2.469 | Acc: 52.165,86.746,99.911,% | Adaptive Acc: 94.100% | clf_exit: 0.272 0.497 0.231
Batch: 220 | Loss: 2.472 | Acc: 52.100,86.729,99.919,% | Adaptive Acc: 94.132% | clf_exit: 0.271 0.497 0.232
Batch: 240 | Loss: 2.473 | Acc: 52.010,86.722,99.916,% | Adaptive Acc: 94.116% | clf_exit: 0.272 0.497 0.232
Batch: 260 | Loss: 2.475 | Acc: 51.847,86.698,99.919,% | Adaptive Acc: 94.088% | clf_exit: 0.272 0.496 0.232
Batch: 280 | Loss: 2.477 | Acc: 51.754,86.663,99.922,% | Adaptive Acc: 94.075% | clf_exit: 0.272 0.495 0.233
Batch: 300 | Loss: 2.479 | Acc: 51.744,86.708,99.925,% | Adaptive Acc: 94.056% | clf_exit: 0.272 0.496 0.232
Batch: 320 | Loss: 2.476 | Acc: 51.786,86.782,99.920,% | Adaptive Acc: 94.120% | clf_exit: 0.271 0.496 0.232
Batch: 340 | Loss: 2.478 | Acc: 51.792,86.799,99.911,% | Adaptive Acc: 94.174% | clf_exit: 0.271 0.496 0.233
Batch: 360 | Loss: 2.477 | Acc: 51.775,86.820,99.903,% | Adaptive Acc: 94.178% | clf_exit: 0.271 0.496 0.233
Batch: 380 | Loss: 2.475 | Acc: 51.815,86.852,99.902,% | Adaptive Acc: 94.150% | clf_exit: 0.270 0.496 0.234
Batch: 0 | Loss: 3.843 | Acc: 54.688,69.531,77.344,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.290 | Acc: 48.214,67.448,72.656,% | Adaptive Acc: 68.936% | clf_exit: 0.344 0.407 0.249
Batch: 40 | Loss: 4.254 | Acc: 48.438,67.664,72.351,% | Adaptive Acc: 68.826% | clf_exit: 0.344 0.399 0.257
Batch: 60 | Loss: 4.267 | Acc: 48.143,67.610,72.464,% | Adaptive Acc: 68.750% | clf_exit: 0.341 0.402 0.257
Train all parameters

Epoch: 272
Batch: 0 | Loss: 2.212 | Acc: 56.250,89.844,100.000,% | Adaptive Acc: 97.656% | clf_exit: 0.281 0.438 0.281
Batch: 20 | Loss: 2.452 | Acc: 54.055,86.942,100.000,% | Adaptive Acc: 94.940% | clf_exit: 0.270 0.494 0.236
Batch: 40 | Loss: 2.414 | Acc: 53.296,87.290,99.981,% | Adaptive Acc: 94.379% | clf_exit: 0.278 0.494 0.228
Batch: 60 | Loss: 2.430 | Acc: 52.779,86.860,99.974,% | Adaptive Acc: 94.262% | clf_exit: 0.277 0.494 0.229
Batch: 80 | Loss: 2.432 | Acc: 52.546,86.912,99.942,% | Adaptive Acc: 94.416% | clf_exit: 0.276 0.492 0.232
Batch: 100 | Loss: 2.442 | Acc: 52.375,86.935,99.954,% | Adaptive Acc: 94.346% | clf_exit: 0.275 0.494 0.231
Batch: 120 | Loss: 2.456 | Acc: 52.221,86.900,99.942,% | Adaptive Acc: 94.267% | clf_exit: 0.275 0.492 0.233
Batch: 140 | Loss: 2.458 | Acc: 52.178,86.918,99.939,% | Adaptive Acc: 94.199% | clf_exit: 0.273 0.494 0.232
Batch: 160 | Loss: 2.456 | Acc: 52.198,87.039,99.942,% | Adaptive Acc: 94.167% | clf_exit: 0.273 0.494 0.233
Batch: 180 | Loss: 2.458 | Acc: 52.037,87.012,99.935,% | Adaptive Acc: 94.177% | clf_exit: 0.272 0.495 0.232
Batch: 200 | Loss: 2.461 | Acc: 51.963,87.065,99.934,% | Adaptive Acc: 94.154% | clf_exit: 0.273 0.494 0.233
Batch: 220 | Loss: 2.463 | Acc: 51.941,87.125,99.929,% | Adaptive Acc: 94.174% | clf_exit: 0.273 0.494 0.233
Batch: 240 | Loss: 2.466 | Acc: 51.854,87.140,99.925,% | Adaptive Acc: 94.239% | clf_exit: 0.271 0.495 0.234
Batch: 260 | Loss: 2.468 | Acc: 51.904,87.126,99.922,% | Adaptive Acc: 94.232% | clf_exit: 0.271 0.495 0.234
Batch: 280 | Loss: 2.466 | Acc: 51.966,87.166,99.922,% | Adaptive Acc: 94.245% | clf_exit: 0.271 0.496 0.233
Batch: 300 | Loss: 2.468 | Acc: 51.892,87.103,99.920,% | Adaptive Acc: 94.264% | clf_exit: 0.271 0.495 0.234
Batch: 320 | Loss: 2.468 | Acc: 51.864,87.111,99.922,% | Adaptive Acc: 94.227% | clf_exit: 0.271 0.495 0.234
Batch: 340 | Loss: 2.470 | Acc: 51.835,87.058,99.924,% | Adaptive Acc: 94.167% | clf_exit: 0.272 0.494 0.234
Batch: 360 | Loss: 2.470 | Acc: 51.835,87.041,99.924,% | Adaptive Acc: 94.142% | clf_exit: 0.272 0.494 0.234
Batch: 380 | Loss: 2.469 | Acc: 51.837,87.086,99.928,% | Adaptive Acc: 94.158% | clf_exit: 0.271 0.495 0.233
Batch: 0 | Loss: 3.817 | Acc: 53.125,70.312,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.422 0.242
Batch: 20 | Loss: 4.305 | Acc: 48.251,66.964,71.949,% | Adaptive Acc: 68.341% | clf_exit: 0.343 0.409 0.248
Batch: 40 | Loss: 4.261 | Acc: 48.476,67.435,71.932,% | Adaptive Acc: 68.502% | clf_exit: 0.344 0.401 0.255
Batch: 60 | Loss: 4.272 | Acc: 48.207,67.392,72.054,% | Adaptive Acc: 68.481% | clf_exit: 0.342 0.402 0.256
Train all parameters

Epoch: 273
Batch: 0 | Loss: 2.310 | Acc: 53.906,91.406,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.258 0.500 0.242
Batch: 20 | Loss: 2.528 | Acc: 50.186,86.198,99.963,% | Adaptive Acc: 93.415% | clf_exit: 0.272 0.494 0.234
Batch: 40 | Loss: 2.525 | Acc: 50.591,86.738,99.924,% | Adaptive Acc: 94.074% | clf_exit: 0.269 0.493 0.238
Batch: 60 | Loss: 2.489 | Acc: 51.447,86.975,99.923,% | Adaptive Acc: 94.249% | clf_exit: 0.269 0.494 0.237
Batch: 80 | Loss: 2.476 | Acc: 51.813,87.037,99.913,% | Adaptive Acc: 94.387% | clf_exit: 0.268 0.494 0.238
Batch: 100 | Loss: 2.470 | Acc: 51.996,87.198,99.915,% | Adaptive Acc: 94.268% | clf_exit: 0.268 0.493 0.238
Batch: 120 | Loss: 2.465 | Acc: 52.105,87.203,99.916,% | Adaptive Acc: 94.344% | clf_exit: 0.270 0.493 0.237
Batch: 140 | Loss: 2.471 | Acc: 51.906,87.096,99.917,% | Adaptive Acc: 94.276% | clf_exit: 0.271 0.492 0.237
Batch: 160 | Loss: 2.478 | Acc: 51.888,86.957,99.903,% | Adaptive Acc: 94.269% | clf_exit: 0.270 0.492 0.238
Batch: 180 | Loss: 2.475 | Acc: 51.934,86.930,99.892,% | Adaptive Acc: 94.277% | clf_exit: 0.270 0.493 0.237
Batch: 200 | Loss: 2.470 | Acc: 52.138,86.944,99.895,% | Adaptive Acc: 94.282% | clf_exit: 0.271 0.494 0.235
Batch: 220 | Loss: 2.468 | Acc: 52.174,87.019,99.890,% | Adaptive Acc: 94.298% | clf_exit: 0.270 0.495 0.235
Batch: 240 | Loss: 2.469 | Acc: 52.133,87.014,99.893,% | Adaptive Acc: 94.334% | clf_exit: 0.270 0.495 0.235
Batch: 260 | Loss: 2.467 | Acc: 52.113,87.060,99.892,% | Adaptive Acc: 94.280% | clf_exit: 0.270 0.496 0.234
Batch: 280 | Loss: 2.469 | Acc: 52.057,87.016,99.892,% | Adaptive Acc: 94.228% | clf_exit: 0.270 0.496 0.234
Batch: 300 | Loss: 2.467 | Acc: 51.967,87.035,99.891,% | Adaptive Acc: 94.222% | clf_exit: 0.270 0.497 0.233
Batch: 320 | Loss: 2.469 | Acc: 52.066,87.064,99.888,% | Adaptive Acc: 94.271% | clf_exit: 0.270 0.496 0.234
Batch: 340 | Loss: 2.471 | Acc: 52.023,87.062,99.892,% | Adaptive Acc: 94.249% | clf_exit: 0.271 0.495 0.234
Batch: 360 | Loss: 2.473 | Acc: 52.026,87.033,99.898,% | Adaptive Acc: 94.209% | clf_exit: 0.271 0.495 0.234
Batch: 380 | Loss: 2.470 | Acc: 52.079,87.057,99.902,% | Adaptive Acc: 94.252% | clf_exit: 0.270 0.496 0.234
Batch: 0 | Loss: 3.783 | Acc: 50.781,71.094,77.344,% | Adaptive Acc: 72.656% | clf_exit: 0.336 0.422 0.242
Batch: 20 | Loss: 4.288 | Acc: 48.177,67.597,72.098,% | Adaptive Acc: 68.378% | clf_exit: 0.342 0.415 0.244
Batch: 40 | Loss: 4.243 | Acc: 48.418,67.740,72.351,% | Adaptive Acc: 68.845% | clf_exit: 0.340 0.404 0.256
Batch: 60 | Loss: 4.253 | Acc: 48.245,67.597,72.387,% | Adaptive Acc: 68.673% | clf_exit: 0.339 0.405 0.256
Train all parameters

Epoch: 274
Batch: 0 | Loss: 2.423 | Acc: 49.219,88.281,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.297 0.461 0.242
Batch: 20 | Loss: 2.494 | Acc: 49.591,86.719,99.814,% | Adaptive Acc: 94.345% | clf_exit: 0.268 0.489 0.243
Batch: 40 | Loss: 2.493 | Acc: 50.534,86.928,99.848,% | Adaptive Acc: 94.379% | clf_exit: 0.266 0.492 0.242
Batch: 60 | Loss: 2.493 | Acc: 50.935,86.834,99.846,% | Adaptive Acc: 94.160% | clf_exit: 0.267 0.494 0.239
Batch: 80 | Loss: 2.489 | Acc: 51.196,86.883,99.865,% | Adaptive Acc: 94.059% | clf_exit: 0.269 0.494 0.238
Batch: 100 | Loss: 2.483 | Acc: 51.462,87.005,99.869,% | Adaptive Acc: 94.121% | clf_exit: 0.268 0.496 0.236
Batch: 120 | Loss: 2.478 | Acc: 51.621,86.932,99.890,% | Adaptive Acc: 94.144% | clf_exit: 0.270 0.494 0.236
Batch: 140 | Loss: 2.487 | Acc: 51.319,86.879,99.867,% | Adaptive Acc: 94.099% | clf_exit: 0.268 0.496 0.236
Batch: 160 | Loss: 2.484 | Acc: 51.465,86.971,99.874,% | Adaptive Acc: 94.128% | clf_exit: 0.268 0.495 0.236
Batch: 180 | Loss: 2.479 | Acc: 51.701,87.055,99.875,% | Adaptive Acc: 94.199% | clf_exit: 0.268 0.497 0.235
Batch: 200 | Loss: 2.474 | Acc: 51.590,87.131,99.876,% | Adaptive Acc: 94.150% | clf_exit: 0.269 0.497 0.233
Batch: 220 | Loss: 2.474 | Acc: 51.612,87.200,99.887,% | Adaptive Acc: 94.167% | clf_exit: 0.269 0.497 0.233
Batch: 240 | Loss: 2.474 | Acc: 51.640,87.163,99.896,% | Adaptive Acc: 94.162% | clf_exit: 0.269 0.497 0.234
Batch: 260 | Loss: 2.472 | Acc: 51.742,87.198,99.895,% | Adaptive Acc: 94.199% | clf_exit: 0.269 0.497 0.234
Batch: 280 | Loss: 2.469 | Acc: 51.704,87.236,99.894,% | Adaptive Acc: 94.225% | clf_exit: 0.270 0.496 0.233
Batch: 300 | Loss: 2.472 | Acc: 51.721,87.194,99.894,% | Adaptive Acc: 94.155% | clf_exit: 0.270 0.496 0.233
Batch: 320 | Loss: 2.472 | Acc: 51.757,87.152,99.890,% | Adaptive Acc: 94.135% | clf_exit: 0.270 0.497 0.233
Batch: 340 | Loss: 2.468 | Acc: 51.842,87.211,99.897,% | Adaptive Acc: 94.151% | clf_exit: 0.271 0.496 0.233
Batch: 360 | Loss: 2.468 | Acc: 51.861,87.247,99.890,% | Adaptive Acc: 94.142% | clf_exit: 0.271 0.497 0.233
Batch: 380 | Loss: 2.469 | Acc: 51.815,87.194,99.889,% | Adaptive Acc: 94.109% | clf_exit: 0.271 0.496 0.233
Batch: 0 | Loss: 3.818 | Acc: 53.125,71.094,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.281 | Acc: 48.549,67.746,72.507,% | Adaptive Acc: 68.862% | clf_exit: 0.343 0.408 0.249
Batch: 40 | Loss: 4.244 | Acc: 48.685,67.912,72.332,% | Adaptive Acc: 68.788% | clf_exit: 0.342 0.400 0.259
Batch: 60 | Loss: 4.253 | Acc: 48.284,67.713,72.439,% | Adaptive Acc: 68.635% | clf_exit: 0.341 0.403 0.257
Train all parameters

Epoch: 275
Batch: 0 | Loss: 2.353 | Acc: 54.688,90.625,100.000,% | Adaptive Acc: 96.875% | clf_exit: 0.305 0.461 0.234
Batch: 20 | Loss: 2.458 | Acc: 53.311,86.682,99.926,% | Adaptive Acc: 94.643% | clf_exit: 0.275 0.470 0.256
Batch: 40 | Loss: 2.462 | Acc: 52.915,86.719,99.924,% | Adaptive Acc: 94.665% | clf_exit: 0.270 0.485 0.245
Batch: 60 | Loss: 2.469 | Acc: 52.626,86.591,99.949,% | Adaptive Acc: 94.506% | clf_exit: 0.272 0.486 0.243
Batch: 80 | Loss: 2.459 | Acc: 52.363,86.941,99.923,% | Adaptive Acc: 94.531% | clf_exit: 0.273 0.489 0.238
Batch: 100 | Loss: 2.465 | Acc: 52.158,86.843,99.892,% | Adaptive Acc: 94.423% | clf_exit: 0.271 0.492 0.238
Batch: 120 | Loss: 2.456 | Acc: 52.286,87.009,99.897,% | Adaptive Acc: 94.344% | clf_exit: 0.273 0.493 0.235
Batch: 140 | Loss: 2.460 | Acc: 52.227,87.179,99.906,% | Adaptive Acc: 94.398% | clf_exit: 0.273 0.494 0.234
Batch: 160 | Loss: 2.461 | Acc: 52.252,87.262,99.903,% | Adaptive Acc: 94.434% | clf_exit: 0.272 0.495 0.233
Batch: 180 | Loss: 2.456 | Acc: 52.184,87.353,99.909,% | Adaptive Acc: 94.484% | clf_exit: 0.272 0.496 0.232
Batch: 200 | Loss: 2.461 | Acc: 52.157,87.306,99.918,% | Adaptive Acc: 94.485% | clf_exit: 0.271 0.496 0.233
Batch: 220 | Loss: 2.464 | Acc: 52.086,87.316,99.915,% | Adaptive Acc: 94.475% | clf_exit: 0.271 0.496 0.233
Batch: 240 | Loss: 2.468 | Acc: 52.016,87.315,99.912,% | Adaptive Acc: 94.350% | clf_exit: 0.271 0.497 0.232
Batch: 260 | Loss: 2.468 | Acc: 52.032,87.267,99.907,% | Adaptive Acc: 94.331% | clf_exit: 0.270 0.498 0.232
Batch: 280 | Loss: 2.468 | Acc: 52.071,87.191,99.908,% | Adaptive Acc: 94.270% | clf_exit: 0.271 0.497 0.233
Batch: 300 | Loss: 2.473 | Acc: 52.043,87.100,99.909,% | Adaptive Acc: 94.235% | clf_exit: 0.270 0.496 0.233
Batch: 320 | Loss: 2.471 | Acc: 52.066,87.118,99.905,% | Adaptive Acc: 94.244% | clf_exit: 0.271 0.496 0.234
Batch: 340 | Loss: 2.471 | Acc: 52.087,87.094,99.901,% | Adaptive Acc: 94.211% | clf_exit: 0.270 0.497 0.234
Batch: 360 | Loss: 2.470 | Acc: 52.121,87.154,99.905,% | Adaptive Acc: 94.204% | clf_exit: 0.270 0.497 0.233
Batch: 380 | Loss: 2.473 | Acc: 52.108,87.137,99.906,% | Adaptive Acc: 94.187% | clf_exit: 0.270 0.496 0.234
Batch: 0 | Loss: 3.794 | Acc: 54.688,71.094,77.344,% | Adaptive Acc: 71.094% | clf_exit: 0.328 0.445 0.227
Batch: 20 | Loss: 4.289 | Acc: 48.438,67.448,72.731,% | Adaptive Acc: 68.638% | clf_exit: 0.339 0.420 0.241
Batch: 40 | Loss: 4.240 | Acc: 48.476,67.683,72.637,% | Adaptive Acc: 68.845% | clf_exit: 0.339 0.409 0.252
Batch: 60 | Loss: 4.255 | Acc: 48.181,67.546,72.503,% | Adaptive Acc: 68.737% | clf_exit: 0.338 0.409 0.253
Train all parameters

Epoch: 276
Batch: 0 | Loss: 2.737 | Acc: 47.656,83.594,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.297 0.414 0.289
Batch: 20 | Loss: 2.545 | Acc: 50.707,86.644,99.926,% | Adaptive Acc: 94.792% | clf_exit: 0.267 0.471 0.262
Batch: 40 | Loss: 2.521 | Acc: 50.610,86.890,99.886,% | Adaptive Acc: 94.950% | clf_exit: 0.267 0.482 0.252
Batch: 60 | Loss: 2.501 | Acc: 51.281,87.154,99.898,% | Adaptive Acc: 94.826% | clf_exit: 0.266 0.488 0.246
Batch: 80 | Loss: 2.482 | Acc: 51.611,87.355,99.904,% | Adaptive Acc: 94.618% | clf_exit: 0.269 0.491 0.240
Batch: 100 | Loss: 2.467 | Acc: 51.849,87.361,99.915,% | Adaptive Acc: 94.554% | clf_exit: 0.273 0.489 0.237
Batch: 120 | Loss: 2.458 | Acc: 51.860,87.487,99.916,% | Adaptive Acc: 94.564% | clf_exit: 0.274 0.490 0.236
Batch: 140 | Loss: 2.470 | Acc: 51.651,87.184,99.928,% | Adaptive Acc: 94.398% | clf_exit: 0.273 0.491 0.236
Batch: 160 | Loss: 2.461 | Acc: 51.859,87.233,99.922,% | Adaptive Acc: 94.458% | clf_exit: 0.270 0.495 0.235
Batch: 180 | Loss: 2.466 | Acc: 51.735,87.155,99.927,% | Adaptive Acc: 94.389% | clf_exit: 0.270 0.494 0.236
Batch: 200 | Loss: 2.465 | Acc: 51.803,87.197,99.926,% | Adaptive Acc: 94.384% | clf_exit: 0.271 0.493 0.236
Batch: 220 | Loss: 2.466 | Acc: 51.683,87.101,99.919,% | Adaptive Acc: 94.305% | clf_exit: 0.270 0.494 0.235
Batch: 240 | Loss: 2.473 | Acc: 51.507,87.069,99.912,% | Adaptive Acc: 94.278% | clf_exit: 0.269 0.495 0.236
Batch: 260 | Loss: 2.471 | Acc: 51.652,87.105,99.913,% | Adaptive Acc: 94.319% | clf_exit: 0.269 0.495 0.235
Batch: 280 | Loss: 2.471 | Acc: 51.621,87.130,99.908,% | Adaptive Acc: 94.275% | clf_exit: 0.270 0.495 0.235
Batch: 300 | Loss: 2.470 | Acc: 51.718,87.116,99.914,% | Adaptive Acc: 94.233% | clf_exit: 0.271 0.494 0.234
Batch: 320 | Loss: 2.469 | Acc: 51.760,87.098,99.908,% | Adaptive Acc: 94.229% | clf_exit: 0.272 0.495 0.234
Batch: 340 | Loss: 2.467 | Acc: 51.792,87.131,99.895,% | Adaptive Acc: 94.265% | clf_exit: 0.272 0.494 0.234
Batch: 360 | Loss: 2.469 | Acc: 51.820,87.087,99.896,% | Adaptive Acc: 94.276% | clf_exit: 0.271 0.495 0.235
Batch: 380 | Loss: 2.471 | Acc: 51.829,87.026,99.887,% | Adaptive Acc: 94.195% | clf_exit: 0.271 0.494 0.234
Batch: 0 | Loss: 3.788 | Acc: 51.562,70.312,77.344,% | Adaptive Acc: 70.312% | clf_exit: 0.344 0.430 0.227
Batch: 20 | Loss: 4.288 | Acc: 48.177,67.597,72.619,% | Adaptive Acc: 68.229% | clf_exit: 0.345 0.412 0.243
Batch: 40 | Loss: 4.244 | Acc: 48.209,67.893,72.637,% | Adaptive Acc: 68.674% | clf_exit: 0.344 0.402 0.254
Batch: 60 | Loss: 4.256 | Acc: 48.040,67.764,72.631,% | Adaptive Acc: 68.699% | clf_exit: 0.342 0.403 0.255
Train all parameters

Epoch: 277
Batch: 0 | Loss: 2.599 | Acc: 46.875,87.500,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.258 0.445 0.297
Batch: 20 | Loss: 2.422 | Acc: 53.609,87.946,100.000,% | Adaptive Acc: 94.159% | clf_exit: 0.266 0.515 0.218
Batch: 40 | Loss: 2.455 | Acc: 52.668,87.919,99.943,% | Adaptive Acc: 94.150% | clf_exit: 0.263 0.512 0.225
Batch: 60 | Loss: 2.429 | Acc: 53.048,87.807,99.949,% | Adaptive Acc: 94.109% | clf_exit: 0.271 0.507 0.222
Batch: 80 | Loss: 2.441 | Acc: 52.662,87.693,99.942,% | Adaptive Acc: 94.117% | clf_exit: 0.272 0.506 0.222
Batch: 100 | Loss: 2.434 | Acc: 52.676,87.871,99.938,% | Adaptive Acc: 94.191% | clf_exit: 0.272 0.507 0.222
Batch: 120 | Loss: 2.440 | Acc: 52.460,87.797,99.910,% | Adaptive Acc: 94.086% | clf_exit: 0.272 0.506 0.222
Batch: 140 | Loss: 2.450 | Acc: 52.493,87.644,99.900,% | Adaptive Acc: 94.016% | clf_exit: 0.273 0.503 0.224
Batch: 160 | Loss: 2.454 | Acc: 52.339,87.524,99.888,% | Adaptive Acc: 94.172% | clf_exit: 0.271 0.503 0.226
Batch: 180 | Loss: 2.459 | Acc: 52.214,87.276,99.901,% | Adaptive Acc: 94.151% | clf_exit: 0.271 0.500 0.228
Batch: 200 | Loss: 2.461 | Acc: 52.099,87.224,99.907,% | Adaptive Acc: 94.143% | clf_exit: 0.271 0.500 0.228
Batch: 220 | Loss: 2.465 | Acc: 52.082,87.203,99.915,% | Adaptive Acc: 94.153% | clf_exit: 0.270 0.501 0.229
Batch: 240 | Loss: 2.468 | Acc: 52.091,87.111,99.916,% | Adaptive Acc: 94.142% | clf_exit: 0.270 0.500 0.230
Batch: 260 | Loss: 2.467 | Acc: 52.017,87.147,99.916,% | Adaptive Acc: 94.172% | clf_exit: 0.269 0.501 0.230
Batch: 280 | Loss: 2.466 | Acc: 52.077,87.086,99.917,% | Adaptive Acc: 94.184% | clf_exit: 0.269 0.500 0.231
Batch: 300 | Loss: 2.471 | Acc: 52.006,87.051,99.917,% | Adaptive Acc: 94.189% | clf_exit: 0.269 0.499 0.232
Batch: 320 | Loss: 2.470 | Acc: 52.018,87.106,99.917,% | Adaptive Acc: 94.188% | clf_exit: 0.270 0.499 0.232
Batch: 340 | Loss: 2.469 | Acc: 52.023,87.065,99.913,% | Adaptive Acc: 94.156% | clf_exit: 0.270 0.499 0.232
Batch: 360 | Loss: 2.467 | Acc: 52.095,87.084,99.918,% | Adaptive Acc: 94.187% | clf_exit: 0.270 0.498 0.232
Batch: 380 | Loss: 2.467 | Acc: 52.102,87.104,99.914,% | Adaptive Acc: 94.187% | clf_exit: 0.270 0.499 0.232
Batch: 0 | Loss: 3.782 | Acc: 53.125,71.875,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.289 | Acc: 48.103,67.522,72.321,% | Adaptive Acc: 68.564% | clf_exit: 0.342 0.409 0.250
Batch: 40 | Loss: 4.248 | Acc: 48.361,67.797,72.313,% | Adaptive Acc: 68.769% | clf_exit: 0.341 0.400 0.259
Batch: 60 | Loss: 4.262 | Acc: 48.053,67.623,72.182,% | Adaptive Acc: 68.519% | clf_exit: 0.340 0.401 0.259
Train all parameters

Epoch: 278
Batch: 0 | Loss: 2.525 | Acc: 47.656,85.938,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.273 0.492 0.234
Batch: 20 | Loss: 2.438 | Acc: 52.232,86.942,99.888,% | Adaptive Acc: 94.606% | clf_exit: 0.279 0.480 0.241
Batch: 40 | Loss: 2.434 | Acc: 51.982,87.252,99.905,% | Adaptive Acc: 94.188% | clf_exit: 0.283 0.486 0.231
Batch: 60 | Loss: 2.463 | Acc: 51.857,86.924,99.936,% | Adaptive Acc: 93.981% | clf_exit: 0.282 0.485 0.233
Batch: 80 | Loss: 2.464 | Acc: 51.871,86.786,99.923,% | Adaptive Acc: 93.885% | clf_exit: 0.278 0.488 0.234
Batch: 100 | Loss: 2.466 | Acc: 51.833,86.680,99.907,% | Adaptive Acc: 93.959% | clf_exit: 0.279 0.486 0.235
Batch: 120 | Loss: 2.477 | Acc: 51.550,86.738,99.916,% | Adaptive Acc: 93.931% | clf_exit: 0.275 0.487 0.238
Batch: 140 | Loss: 2.470 | Acc: 51.806,86.874,99.906,% | Adaptive Acc: 93.999% | clf_exit: 0.276 0.487 0.236
Batch: 160 | Loss: 2.467 | Acc: 51.926,86.976,99.918,% | Adaptive Acc: 94.036% | clf_exit: 0.276 0.488 0.236
Batch: 180 | Loss: 2.464 | Acc: 51.955,87.017,99.922,% | Adaptive Acc: 94.126% | clf_exit: 0.275 0.490 0.236
Batch: 200 | Loss: 2.462 | Acc: 51.982,87.080,99.922,% | Adaptive Acc: 94.069% | clf_exit: 0.275 0.490 0.235
Batch: 220 | Loss: 2.464 | Acc: 52.019,87.097,99.926,% | Adaptive Acc: 94.093% | clf_exit: 0.275 0.491 0.234
Batch: 240 | Loss: 2.459 | Acc: 52.052,87.143,99.929,% | Adaptive Acc: 94.133% | clf_exit: 0.275 0.491 0.233
Batch: 260 | Loss: 2.463 | Acc: 52.023,87.105,99.928,% | Adaptive Acc: 94.154% | clf_exit: 0.274 0.492 0.234
Batch: 280 | Loss: 2.469 | Acc: 51.860,87.041,99.930,% | Adaptive Acc: 94.187% | clf_exit: 0.273 0.493 0.234
Batch: 300 | Loss: 2.470 | Acc: 51.874,87.061,99.917,% | Adaptive Acc: 94.191% | clf_exit: 0.273 0.493 0.234
Batch: 320 | Loss: 2.468 | Acc: 51.842,87.077,99.917,% | Adaptive Acc: 94.186% | clf_exit: 0.273 0.493 0.234
Batch: 340 | Loss: 2.466 | Acc: 51.872,87.111,99.911,% | Adaptive Acc: 94.208% | clf_exit: 0.272 0.495 0.233
Batch: 360 | Loss: 2.469 | Acc: 51.792,87.082,99.916,% | Adaptive Acc: 94.146% | clf_exit: 0.272 0.495 0.234
Batch: 380 | Loss: 2.471 | Acc: 51.813,87.061,99.914,% | Adaptive Acc: 94.156% | clf_exit: 0.271 0.494 0.235
Batch: 0 | Loss: 3.817 | Acc: 53.906,70.312,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.277 | Acc: 48.735,67.783,72.693,% | Adaptive Acc: 69.010% | clf_exit: 0.343 0.412 0.245
Batch: 40 | Loss: 4.240 | Acc: 48.742,67.873,72.618,% | Adaptive Acc: 68.902% | clf_exit: 0.343 0.402 0.255
Batch: 60 | Loss: 4.249 | Acc: 48.412,67.687,72.528,% | Adaptive Acc: 68.737% | clf_exit: 0.341 0.404 0.255
Train all parameters

Epoch: 279
Batch: 0 | Loss: 2.326 | Acc: 58.594,85.156,100.000,% | Adaptive Acc: 91.406% | clf_exit: 0.320 0.500 0.180
Batch: 20 | Loss: 2.435 | Acc: 53.571,87.016,99.888,% | Adaptive Acc: 93.973% | clf_exit: 0.282 0.495 0.223
Batch: 40 | Loss: 2.458 | Acc: 52.649,87.024,99.886,% | Adaptive Acc: 94.379% | clf_exit: 0.275 0.495 0.230
Batch: 60 | Loss: 2.456 | Acc: 52.600,86.847,99.923,% | Adaptive Acc: 94.429% | clf_exit: 0.275 0.495 0.230
Batch: 80 | Loss: 2.464 | Acc: 52.305,86.796,99.913,% | Adaptive Acc: 94.184% | clf_exit: 0.275 0.494 0.231
Batch: 100 | Loss: 2.465 | Acc: 52.065,86.788,99.915,% | Adaptive Acc: 94.230% | clf_exit: 0.274 0.494 0.232
Batch: 120 | Loss: 2.466 | Acc: 52.085,86.906,99.910,% | Adaptive Acc: 94.279% | clf_exit: 0.271 0.496 0.233
Batch: 140 | Loss: 2.467 | Acc: 51.884,86.907,99.906,% | Adaptive Acc: 94.227% | clf_exit: 0.272 0.494 0.234
Batch: 160 | Loss: 2.464 | Acc: 51.907,86.908,99.908,% | Adaptive Acc: 94.158% | clf_exit: 0.272 0.496 0.233
Batch: 180 | Loss: 2.463 | Acc: 51.899,86.900,99.909,% | Adaptive Acc: 94.238% | clf_exit: 0.272 0.496 0.233
Batch: 200 | Loss: 2.464 | Acc: 51.912,86.929,99.911,% | Adaptive Acc: 94.232% | clf_exit: 0.271 0.496 0.233
Batch: 220 | Loss: 2.460 | Acc: 51.969,86.927,99.905,% | Adaptive Acc: 94.199% | clf_exit: 0.273 0.494 0.234
Batch: 240 | Loss: 2.461 | Acc: 52.007,86.959,99.906,% | Adaptive Acc: 94.252% | clf_exit: 0.271 0.494 0.234
Batch: 260 | Loss: 2.460 | Acc: 52.092,87.003,99.913,% | Adaptive Acc: 94.274% | clf_exit: 0.271 0.495 0.234
Batch: 280 | Loss: 2.464 | Acc: 52.002,86.963,99.914,% | Adaptive Acc: 94.250% | clf_exit: 0.271 0.494 0.235
Batch: 300 | Loss: 2.465 | Acc: 52.001,86.958,99.907,% | Adaptive Acc: 94.222% | clf_exit: 0.271 0.494 0.235
Batch: 320 | Loss: 2.465 | Acc: 51.947,87.055,99.908,% | Adaptive Acc: 94.256% | clf_exit: 0.270 0.495 0.235
Batch: 340 | Loss: 2.468 | Acc: 51.879,87.049,99.911,% | Adaptive Acc: 94.275% | clf_exit: 0.270 0.495 0.235
Batch: 360 | Loss: 2.469 | Acc: 51.807,87.050,99.911,% | Adaptive Acc: 94.230% | clf_exit: 0.270 0.495 0.235
Batch: 380 | Loss: 2.467 | Acc: 51.860,87.053,99.914,% | Adaptive Acc: 94.224% | clf_exit: 0.270 0.495 0.235
Batch: 0 | Loss: 3.817 | Acc: 53.906,71.875,79.688,% | Adaptive Acc: 72.656% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.276 | Acc: 48.363,67.671,72.842,% | Adaptive Acc: 68.750% | clf_exit: 0.339 0.414 0.247
Batch: 40 | Loss: 4.239 | Acc: 48.495,67.740,72.771,% | Adaptive Acc: 68.883% | clf_exit: 0.340 0.403 0.256
Batch: 60 | Loss: 4.252 | Acc: 48.297,67.623,72.759,% | Adaptive Acc: 68.763% | clf_exit: 0.340 0.404 0.256
Train all parameters

Epoch: 280
Batch: 0 | Loss: 2.757 | Acc: 50.000,84.375,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.250 0.500 0.250
Batch: 20 | Loss: 2.476 | Acc: 52.046,87.054,99.926,% | Adaptive Acc: 94.308% | clf_exit: 0.278 0.491 0.231
Batch: 40 | Loss: 2.469 | Acc: 51.277,87.614,99.943,% | Adaptive Acc: 94.398% | clf_exit: 0.272 0.495 0.233
Batch: 60 | Loss: 2.466 | Acc: 51.498,87.346,99.936,% | Adaptive Acc: 94.326% | clf_exit: 0.275 0.491 0.234
Batch: 80 | Loss: 2.481 | Acc: 51.312,87.375,99.932,% | Adaptive Acc: 94.280% | clf_exit: 0.272 0.491 0.237
Batch: 100 | Loss: 2.483 | Acc: 51.578,87.237,99.907,% | Adaptive Acc: 94.361% | clf_exit: 0.271 0.492 0.237
Batch: 120 | Loss: 2.480 | Acc: 51.730,87.235,99.903,% | Adaptive Acc: 94.196% | clf_exit: 0.272 0.492 0.235
Batch: 140 | Loss: 2.476 | Acc: 51.734,87.156,99.906,% | Adaptive Acc: 94.260% | clf_exit: 0.271 0.494 0.235
Batch: 160 | Loss: 2.473 | Acc: 51.975,87.151,99.903,% | Adaptive Acc: 94.289% | clf_exit: 0.271 0.494 0.235
Batch: 180 | Loss: 2.478 | Acc: 52.072,87.025,99.896,% | Adaptive Acc: 94.320% | clf_exit: 0.272 0.492 0.236
Batch: 200 | Loss: 2.476 | Acc: 51.982,87.045,99.895,% | Adaptive Acc: 94.275% | clf_exit: 0.272 0.491 0.237
Batch: 220 | Loss: 2.475 | Acc: 51.955,87.012,99.897,% | Adaptive Acc: 94.330% | clf_exit: 0.271 0.492 0.237
Batch: 240 | Loss: 2.479 | Acc: 51.757,86.900,99.903,% | Adaptive Acc: 94.288% | clf_exit: 0.270 0.493 0.237
Batch: 260 | Loss: 2.477 | Acc: 51.769,86.964,99.907,% | Adaptive Acc: 94.289% | clf_exit: 0.270 0.494 0.236
Batch: 280 | Loss: 2.479 | Acc: 51.688,86.963,99.903,% | Adaptive Acc: 94.284% | clf_exit: 0.270 0.492 0.237
Batch: 300 | Loss: 2.473 | Acc: 51.858,87.038,99.904,% | Adaptive Acc: 94.313% | clf_exit: 0.270 0.494 0.236
Batch: 320 | Loss: 2.469 | Acc: 51.998,87.052,99.898,% | Adaptive Acc: 94.261% | clf_exit: 0.272 0.493 0.236
Batch: 340 | Loss: 2.473 | Acc: 51.947,86.985,99.899,% | Adaptive Acc: 94.249% | clf_exit: 0.271 0.492 0.237
Batch: 360 | Loss: 2.472 | Acc: 51.952,86.981,99.903,% | Adaptive Acc: 94.228% | clf_exit: 0.271 0.493 0.236
Batch: 380 | Loss: 2.471 | Acc: 51.917,87.004,99.906,% | Adaptive Acc: 94.232% | clf_exit: 0.270 0.493 0.236
Batch: 0 | Loss: 3.809 | Acc: 54.688,71.875,77.344,% | Adaptive Acc: 72.656% | clf_exit: 0.328 0.438 0.234
Batch: 20 | Loss: 4.291 | Acc: 48.326,68.043,72.247,% | Adaptive Acc: 68.601% | clf_exit: 0.340 0.413 0.247
Batch: 40 | Loss: 4.247 | Acc: 48.247,68.121,72.199,% | Adaptive Acc: 68.712% | clf_exit: 0.339 0.404 0.258
Batch: 60 | Loss: 4.258 | Acc: 47.989,67.815,72.362,% | Adaptive Acc: 68.673% | clf_exit: 0.337 0.406 0.257
Train all parameters

Epoch: 281
Batch: 0 | Loss: 2.652 | Acc: 49.219,89.062,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.266 0.477 0.258
Batch: 20 | Loss: 2.517 | Acc: 50.521,86.235,99.888,% | Adaptive Acc: 94.643% | clf_exit: 0.265 0.493 0.242
Batch: 40 | Loss: 2.505 | Acc: 50.724,87.119,99.943,% | Adaptive Acc: 94.855% | clf_exit: 0.266 0.491 0.243
Batch: 60 | Loss: 2.504 | Acc: 50.845,86.629,99.962,% | Adaptive Acc: 94.390% | clf_exit: 0.268 0.490 0.242
Batch: 80 | Loss: 2.476 | Acc: 51.524,86.815,99.932,% | Adaptive Acc: 94.271% | clf_exit: 0.272 0.493 0.235
Batch: 100 | Loss: 2.472 | Acc: 51.508,86.904,99.930,% | Adaptive Acc: 94.206% | clf_exit: 0.272 0.496 0.233
Batch: 120 | Loss: 2.473 | Acc: 51.537,86.925,99.929,% | Adaptive Acc: 94.234% | clf_exit: 0.270 0.497 0.233
Batch: 140 | Loss: 2.470 | Acc: 51.562,86.963,99.911,% | Adaptive Acc: 94.088% | clf_exit: 0.272 0.496 0.232
Batch: 160 | Loss: 2.477 | Acc: 51.713,86.801,99.918,% | Adaptive Acc: 94.070% | clf_exit: 0.274 0.494 0.232
Batch: 180 | Loss: 2.471 | Acc: 51.891,86.805,99.918,% | Adaptive Acc: 94.091% | clf_exit: 0.274 0.495 0.231
Batch: 200 | Loss: 2.468 | Acc: 51.947,86.808,99.907,% | Adaptive Acc: 94.096% | clf_exit: 0.274 0.495 0.231
Batch: 220 | Loss: 2.465 | Acc: 52.029,86.850,99.905,% | Adaptive Acc: 94.104% | clf_exit: 0.273 0.496 0.230
Batch: 240 | Loss: 2.468 | Acc: 52.000,86.810,99.912,% | Adaptive Acc: 94.175% | clf_exit: 0.271 0.497 0.231
Batch: 260 | Loss: 2.470 | Acc: 52.026,86.889,99.910,% | Adaptive Acc: 94.220% | clf_exit: 0.272 0.496 0.232
Batch: 280 | Loss: 2.471 | Acc: 52.107,86.936,99.905,% | Adaptive Acc: 94.262% | clf_exit: 0.272 0.496 0.233
Batch: 300 | Loss: 2.472 | Acc: 52.061,86.924,99.907,% | Adaptive Acc: 94.321% | clf_exit: 0.272 0.495 0.233
Batch: 320 | Loss: 2.475 | Acc: 52.013,86.940,99.910,% | Adaptive Acc: 94.351% | clf_exit: 0.271 0.495 0.234
Batch: 340 | Loss: 2.474 | Acc: 51.998,86.909,99.908,% | Adaptive Acc: 94.343% | clf_exit: 0.271 0.495 0.234
Batch: 360 | Loss: 2.473 | Acc: 51.935,86.950,99.905,% | Adaptive Acc: 94.323% | clf_exit: 0.271 0.495 0.235
Batch: 380 | Loss: 2.470 | Acc: 51.942,86.965,99.904,% | Adaptive Acc: 94.357% | clf_exit: 0.271 0.495 0.234
Batch: 0 | Loss: 3.818 | Acc: 53.125,71.875,77.344,% | Adaptive Acc: 72.656% | clf_exit: 0.328 0.422 0.250
Batch: 20 | Loss: 4.289 | Acc: 48.363,67.560,72.433,% | Adaptive Acc: 68.713% | clf_exit: 0.337 0.419 0.244
Batch: 40 | Loss: 4.249 | Acc: 48.380,67.664,72.275,% | Adaptive Acc: 68.845% | clf_exit: 0.338 0.406 0.256
Batch: 60 | Loss: 4.258 | Acc: 48.130,67.559,72.374,% | Adaptive Acc: 68.724% | clf_exit: 0.336 0.408 0.256
Train all parameters

Epoch: 282
Batch: 0 | Loss: 2.313 | Acc: 57.812,88.281,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.305 0.453 0.242
Batch: 20 | Loss: 2.399 | Acc: 53.162,87.426,100.000,% | Adaptive Acc: 93.899% | clf_exit: 0.280 0.493 0.228
Batch: 40 | Loss: 2.463 | Acc: 52.420,86.890,99.943,% | Adaptive Acc: 94.379% | clf_exit: 0.267 0.488 0.245
Batch: 60 | Loss: 2.453 | Acc: 52.728,86.885,99.949,% | Adaptive Acc: 94.621% | clf_exit: 0.271 0.488 0.240
Batch: 80 | Loss: 2.452 | Acc: 52.730,87.027,99.942,% | Adaptive Acc: 94.406% | clf_exit: 0.274 0.490 0.237
Batch: 100 | Loss: 2.455 | Acc: 52.506,87.067,99.954,% | Adaptive Acc: 94.516% | clf_exit: 0.269 0.497 0.234
Batch: 120 | Loss: 2.458 | Acc: 52.454,87.164,99.935,% | Adaptive Acc: 94.667% | clf_exit: 0.268 0.497 0.235
Batch: 140 | Loss: 2.455 | Acc: 52.554,87.317,99.917,% | Adaptive Acc: 94.598% | clf_exit: 0.269 0.497 0.234
Batch: 160 | Loss: 2.467 | Acc: 52.383,87.175,99.918,% | Adaptive Acc: 94.507% | clf_exit: 0.269 0.496 0.235
Batch: 180 | Loss: 2.464 | Acc: 52.503,87.181,99.922,% | Adaptive Acc: 94.518% | clf_exit: 0.270 0.495 0.235
Batch: 200 | Loss: 2.460 | Acc: 52.309,87.321,99.930,% | Adaptive Acc: 94.485% | clf_exit: 0.270 0.497 0.233
Batch: 220 | Loss: 2.465 | Acc: 52.178,87.224,99.926,% | Adaptive Acc: 94.453% | clf_exit: 0.269 0.496 0.234
Batch: 240 | Loss: 2.462 | Acc: 52.240,87.263,99.919,% | Adaptive Acc: 94.476% | clf_exit: 0.270 0.496 0.234
Batch: 260 | Loss: 2.458 | Acc: 52.326,87.276,99.922,% | Adaptive Acc: 94.519% | clf_exit: 0.270 0.497 0.234
Batch: 280 | Loss: 2.456 | Acc: 52.333,87.297,99.922,% | Adaptive Acc: 94.506% | clf_exit: 0.271 0.497 0.232
Batch: 300 | Loss: 2.455 | Acc: 52.375,87.256,99.917,% | Adaptive Acc: 94.474% | clf_exit: 0.270 0.498 0.232
Batch: 320 | Loss: 2.463 | Acc: 52.164,87.196,99.915,% | Adaptive Acc: 94.456% | clf_exit: 0.269 0.499 0.232
Batch: 340 | Loss: 2.461 | Acc: 52.163,87.218,99.913,% | Adaptive Acc: 94.453% | clf_exit: 0.270 0.499 0.232
Batch: 360 | Loss: 2.462 | Acc: 52.136,87.223,99.911,% | Adaptive Acc: 94.388% | clf_exit: 0.270 0.499 0.232
Batch: 380 | Loss: 2.466 | Acc: 52.133,87.139,99.910,% | Adaptive Acc: 94.365% | clf_exit: 0.269 0.498 0.232
Batch: 0 | Loss: 3.839 | Acc: 53.125,69.531,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.328 0.430 0.242
Batch: 20 | Loss: 4.276 | Acc: 48.363,67.560,72.768,% | Adaptive Acc: 68.713% | clf_exit: 0.342 0.411 0.247
Batch: 40 | Loss: 4.247 | Acc: 48.418,67.740,72.237,% | Adaptive Acc: 68.712% | clf_exit: 0.342 0.404 0.255
Batch: 60 | Loss: 4.261 | Acc: 48.092,67.725,72.310,% | Adaptive Acc: 68.699% | clf_exit: 0.339 0.407 0.255
Train all parameters

Epoch: 283
Batch: 0 | Loss: 2.339 | Acc: 52.344,92.969,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.242 0.531 0.227
Batch: 20 | Loss: 2.430 | Acc: 52.976,87.277,99.888,% | Adaptive Acc: 94.829% | clf_exit: 0.278 0.492 0.230
Batch: 40 | Loss: 2.429 | Acc: 51.925,87.633,99.905,% | Adaptive Acc: 93.921% | clf_exit: 0.275 0.501 0.224
Batch: 60 | Loss: 2.455 | Acc: 51.460,87.065,99.910,% | Adaptive Acc: 93.814% | clf_exit: 0.274 0.497 0.230
Batch: 80 | Loss: 2.458 | Acc: 51.630,87.182,99.884,% | Adaptive Acc: 93.827% | clf_exit: 0.275 0.493 0.232
Batch: 100 | Loss: 2.468 | Acc: 51.609,86.827,99.884,% | Adaptive Acc: 93.742% | clf_exit: 0.275 0.493 0.232
Batch: 120 | Loss: 2.471 | Acc: 51.601,86.757,99.897,% | Adaptive Acc: 93.931% | clf_exit: 0.272 0.494 0.234
Batch: 140 | Loss: 2.475 | Acc: 51.585,86.780,99.884,% | Adaptive Acc: 93.944% | clf_exit: 0.272 0.492 0.235
Batch: 160 | Loss: 2.473 | Acc: 51.640,86.811,99.884,% | Adaptive Acc: 94.031% | clf_exit: 0.271 0.494 0.235
Batch: 180 | Loss: 2.475 | Acc: 51.632,86.866,99.879,% | Adaptive Acc: 94.074% | clf_exit: 0.270 0.495 0.235
Batch: 200 | Loss: 2.466 | Acc: 51.737,86.968,99.883,% | Adaptive Acc: 94.076% | clf_exit: 0.272 0.494 0.234
Batch: 220 | Loss: 2.461 | Acc: 51.891,86.903,99.890,% | Adaptive Acc: 94.047% | clf_exit: 0.272 0.496 0.233
Batch: 240 | Loss: 2.464 | Acc: 51.828,86.926,99.890,% | Adaptive Acc: 94.071% | clf_exit: 0.272 0.495 0.233
Batch: 260 | Loss: 2.468 | Acc: 51.709,86.928,99.889,% | Adaptive Acc: 94.067% | clf_exit: 0.272 0.495 0.233
Batch: 280 | Loss: 2.466 | Acc: 51.843,86.947,99.897,% | Adaptive Acc: 94.089% | clf_exit: 0.272 0.496 0.232
Batch: 300 | Loss: 2.462 | Acc: 51.926,86.965,99.896,% | Adaptive Acc: 94.087% | clf_exit: 0.272 0.496 0.232
Batch: 320 | Loss: 2.463 | Acc: 51.954,86.930,99.895,% | Adaptive Acc: 94.054% | clf_exit: 0.272 0.496 0.231
Batch: 340 | Loss: 2.463 | Acc: 52.055,86.950,99.897,% | Adaptive Acc: 94.020% | clf_exit: 0.272 0.497 0.231
Batch: 360 | Loss: 2.466 | Acc: 52.015,86.933,99.898,% | Adaptive Acc: 94.031% | clf_exit: 0.272 0.496 0.232
Batch: 380 | Loss: 2.468 | Acc: 51.966,86.916,99.900,% | Adaptive Acc: 94.033% | clf_exit: 0.272 0.497 0.232
Batch: 0 | Loss: 3.849 | Acc: 53.906,71.094,76.562,% | Adaptive Acc: 72.656% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.296 | Acc: 48.289,67.225,72.247,% | Adaptive Acc: 68.787% | clf_exit: 0.343 0.407 0.250
Batch: 40 | Loss: 4.252 | Acc: 48.323,67.511,72.370,% | Adaptive Acc: 68.902% | clf_exit: 0.341 0.402 0.257
Batch: 60 | Loss: 4.259 | Acc: 48.117,67.469,72.515,% | Adaptive Acc: 68.865% | clf_exit: 0.340 0.403 0.257
Train all parameters

Epoch: 284
Batch: 0 | Loss: 2.353 | Acc: 52.344,85.938,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.242 0.539 0.219
Batch: 20 | Loss: 2.548 | Acc: 51.525,86.979,99.963,% | Adaptive Acc: 94.792% | clf_exit: 0.251 0.496 0.253
Batch: 40 | Loss: 2.501 | Acc: 51.696,86.814,99.943,% | Adaptive Acc: 94.226% | clf_exit: 0.264 0.496 0.240
Batch: 60 | Loss: 2.497 | Acc: 51.806,87.167,99.936,% | Adaptive Acc: 94.403% | clf_exit: 0.264 0.497 0.239
Batch: 80 | Loss: 2.487 | Acc: 52.016,87.143,99.923,% | Adaptive Acc: 94.145% | clf_exit: 0.267 0.498 0.234
Batch: 100 | Loss: 2.484 | Acc: 51.903,87.013,99.938,% | Adaptive Acc: 94.028% | clf_exit: 0.269 0.496 0.235
Batch: 120 | Loss: 2.480 | Acc: 52.040,87.003,99.923,% | Adaptive Acc: 94.163% | clf_exit: 0.269 0.497 0.235
Batch: 140 | Loss: 2.483 | Acc: 52.045,86.913,99.911,% | Adaptive Acc: 94.221% | clf_exit: 0.267 0.498 0.236
Batch: 160 | Loss: 2.488 | Acc: 51.941,86.947,99.918,% | Adaptive Acc: 94.240% | clf_exit: 0.266 0.497 0.237
Batch: 180 | Loss: 2.492 | Acc: 52.003,86.896,99.914,% | Adaptive Acc: 94.264% | clf_exit: 0.266 0.498 0.237
Batch: 200 | Loss: 2.495 | Acc: 51.842,86.793,99.922,% | Adaptive Acc: 94.232% | clf_exit: 0.266 0.497 0.238
Batch: 220 | Loss: 2.490 | Acc: 51.803,86.871,99.919,% | Adaptive Acc: 94.195% | clf_exit: 0.267 0.496 0.237
Batch: 240 | Loss: 2.492 | Acc: 51.773,86.897,99.916,% | Adaptive Acc: 94.223% | clf_exit: 0.266 0.496 0.238
Batch: 260 | Loss: 2.490 | Acc: 51.841,86.892,99.916,% | Adaptive Acc: 94.223% | clf_exit: 0.267 0.495 0.238
Batch: 280 | Loss: 2.486 | Acc: 51.868,86.944,99.914,% | Adaptive Acc: 94.228% | clf_exit: 0.266 0.497 0.236
Batch: 300 | Loss: 2.477 | Acc: 51.967,87.025,99.914,% | Adaptive Acc: 94.269% | clf_exit: 0.267 0.498 0.235
Batch: 320 | Loss: 2.477 | Acc: 51.867,87.059,99.910,% | Adaptive Acc: 94.178% | clf_exit: 0.268 0.498 0.234
Batch: 340 | Loss: 2.477 | Acc: 51.833,87.060,99.908,% | Adaptive Acc: 94.119% | clf_exit: 0.268 0.498 0.234
Batch: 360 | Loss: 2.473 | Acc: 51.965,87.128,99.911,% | Adaptive Acc: 94.146% | clf_exit: 0.269 0.497 0.234
Batch: 380 | Loss: 2.470 | Acc: 51.964,87.131,99.916,% | Adaptive Acc: 94.162% | clf_exit: 0.269 0.497 0.234
Batch: 0 | Loss: 3.806 | Acc: 52.344,71.875,77.344,% | Adaptive Acc: 72.656% | clf_exit: 0.344 0.422 0.234
Batch: 20 | Loss: 4.283 | Acc: 48.326,67.448,72.619,% | Adaptive Acc: 68.936% | clf_exit: 0.340 0.410 0.249
Batch: 40 | Loss: 4.240 | Acc: 48.533,67.759,72.618,% | Adaptive Acc: 69.207% | clf_exit: 0.339 0.402 0.259
Batch: 60 | Loss: 4.252 | Acc: 48.297,67.649,72.682,% | Adaptive Acc: 69.070% | clf_exit: 0.338 0.404 0.258
Train classifier parameters

Epoch: 285
Batch: 0 | Loss: 2.302 | Acc: 52.344,89.062,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.227 0.609 0.164
Batch: 20 | Loss: 2.387 | Acc: 53.385,88.244,99.851,% | Adaptive Acc: 94.122% | clf_exit: 0.279 0.507 0.214
Batch: 40 | Loss: 2.441 | Acc: 52.782,87.271,99.886,% | Adaptive Acc: 94.322% | clf_exit: 0.276 0.498 0.227
Batch: 60 | Loss: 2.435 | Acc: 53.266,87.641,99.898,% | Adaptive Acc: 94.442% | clf_exit: 0.276 0.495 0.228
Batch: 80 | Loss: 2.447 | Acc: 52.681,87.471,99.913,% | Adaptive Acc: 94.329% | clf_exit: 0.278 0.492 0.230
Batch: 100 | Loss: 2.453 | Acc: 52.483,87.345,99.923,% | Adaptive Acc: 94.261% | clf_exit: 0.274 0.496 0.230
Batch: 120 | Loss: 2.456 | Acc: 52.324,87.455,99.910,% | Adaptive Acc: 94.273% | clf_exit: 0.275 0.494 0.232
Batch: 140 | Loss: 2.451 | Acc: 52.305,87.472,99.922,% | Adaptive Acc: 94.299% | clf_exit: 0.276 0.494 0.230
Batch: 160 | Loss: 2.451 | Acc: 52.412,87.432,99.927,% | Adaptive Acc: 94.230% | clf_exit: 0.276 0.495 0.230
Batch: 180 | Loss: 2.460 | Acc: 52.244,87.289,99.931,% | Adaptive Acc: 94.238% | clf_exit: 0.274 0.495 0.231
Batch: 200 | Loss: 2.455 | Acc: 52.208,87.329,99.922,% | Adaptive Acc: 94.216% | clf_exit: 0.274 0.496 0.230
Batch: 220 | Loss: 2.456 | Acc: 52.135,87.366,99.922,% | Adaptive Acc: 94.234% | clf_exit: 0.274 0.496 0.230
Batch: 240 | Loss: 2.460 | Acc: 52.107,87.364,99.929,% | Adaptive Acc: 94.249% | clf_exit: 0.273 0.496 0.231
Batch: 260 | Loss: 2.461 | Acc: 52.089,87.299,99.928,% | Adaptive Acc: 94.271% | clf_exit: 0.273 0.496 0.232
Batch: 280 | Loss: 2.465 | Acc: 52.035,87.269,99.925,% | Adaptive Acc: 94.287% | clf_exit: 0.271 0.497 0.232
Batch: 300 | Loss: 2.466 | Acc: 52.139,87.227,99.922,% | Adaptive Acc: 94.290% | clf_exit: 0.271 0.498 0.232
Batch: 320 | Loss: 2.468 | Acc: 52.069,87.201,99.920,% | Adaptive Acc: 94.295% | clf_exit: 0.270 0.498 0.232
Batch: 340 | Loss: 2.465 | Acc: 52.103,87.211,99.911,% | Adaptive Acc: 94.259% | clf_exit: 0.271 0.497 0.232
Batch: 360 | Loss: 2.459 | Acc: 52.149,87.297,99.907,% | Adaptive Acc: 94.248% | clf_exit: 0.272 0.497 0.231
Batch: 380 | Loss: 2.464 | Acc: 52.079,87.260,99.910,% | Adaptive Acc: 94.271% | clf_exit: 0.271 0.496 0.233
Batch: 0 | Loss: 3.821 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.287 | Acc: 48.735,67.671,72.991,% | Adaptive Acc: 69.234% | clf_exit: 0.340 0.411 0.249
Batch: 40 | Loss: 4.245 | Acc: 48.761,67.912,72.561,% | Adaptive Acc: 68.960% | clf_exit: 0.341 0.403 0.256
Batch: 60 | Loss: 4.258 | Acc: 48.348,67.713,72.567,% | Adaptive Acc: 68.916% | clf_exit: 0.338 0.407 0.256
Train classifier parameters

Epoch: 286
Batch: 0 | Loss: 2.312 | Acc: 55.469,87.500,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.289 0.523 0.188
Batch: 20 | Loss: 2.457 | Acc: 52.121,87.016,99.963,% | Adaptive Acc: 93.862% | clf_exit: 0.275 0.504 0.221
Batch: 40 | Loss: 2.442 | Acc: 52.782,87.138,99.943,% | Adaptive Acc: 94.226% | clf_exit: 0.276 0.501 0.224
Batch: 60 | Loss: 2.455 | Acc: 52.421,87.065,99.949,% | Adaptive Acc: 94.057% | clf_exit: 0.270 0.503 0.228
Batch: 80 | Loss: 2.472 | Acc: 51.939,87.066,99.932,% | Adaptive Acc: 94.145% | clf_exit: 0.271 0.498 0.231
Batch: 100 | Loss: 2.469 | Acc: 52.027,86.928,99.938,% | Adaptive Acc: 94.098% | clf_exit: 0.272 0.496 0.231
Batch: 120 | Loss: 2.488 | Acc: 51.698,86.835,99.916,% | Adaptive Acc: 94.060% | clf_exit: 0.269 0.496 0.235
Batch: 140 | Loss: 2.485 | Acc: 51.834,86.835,99.911,% | Adaptive Acc: 94.132% | clf_exit: 0.270 0.494 0.236
Batch: 160 | Loss: 2.472 | Acc: 52.077,86.952,99.908,% | Adaptive Acc: 94.143% | clf_exit: 0.271 0.494 0.235
Batch: 180 | Loss: 2.470 | Acc: 52.158,86.917,99.905,% | Adaptive Acc: 94.091% | clf_exit: 0.272 0.494 0.235
Batch: 200 | Loss: 2.471 | Acc: 52.056,86.882,99.903,% | Adaptive Acc: 94.034% | clf_exit: 0.272 0.494 0.234
Batch: 220 | Loss: 2.474 | Acc: 52.011,86.927,99.908,% | Adaptive Acc: 94.047% | clf_exit: 0.271 0.495 0.234
Batch: 240 | Loss: 2.473 | Acc: 51.994,86.913,99.912,% | Adaptive Acc: 94.087% | clf_exit: 0.271 0.495 0.234
Batch: 260 | Loss: 2.474 | Acc: 51.919,86.889,99.901,% | Adaptive Acc: 94.073% | clf_exit: 0.271 0.495 0.234
Batch: 280 | Loss: 2.470 | Acc: 51.996,86.950,99.903,% | Adaptive Acc: 94.109% | clf_exit: 0.270 0.496 0.234
Batch: 300 | Loss: 2.468 | Acc: 51.928,86.960,99.909,% | Adaptive Acc: 94.108% | clf_exit: 0.271 0.495 0.234
Batch: 320 | Loss: 2.464 | Acc: 52.035,86.999,99.910,% | Adaptive Acc: 94.132% | clf_exit: 0.272 0.495 0.233
Batch: 340 | Loss: 2.468 | Acc: 51.927,87.005,99.911,% | Adaptive Acc: 94.130% | clf_exit: 0.271 0.495 0.234
Batch: 360 | Loss: 2.465 | Acc: 52.047,87.065,99.907,% | Adaptive Acc: 94.168% | clf_exit: 0.271 0.495 0.233
Batch: 380 | Loss: 2.465 | Acc: 51.997,87.069,99.910,% | Adaptive Acc: 94.187% | clf_exit: 0.271 0.495 0.233
Batch: 0 | Loss: 3.787 | Acc: 52.344,69.531,78.906,% | Adaptive Acc: 70.312% | clf_exit: 0.336 0.453 0.211
Batch: 20 | Loss: 4.295 | Acc: 48.214,67.560,72.321,% | Adaptive Acc: 68.490% | clf_exit: 0.343 0.413 0.245
Batch: 40 | Loss: 4.250 | Acc: 48.342,67.588,72.332,% | Adaptive Acc: 68.731% | clf_exit: 0.341 0.404 0.254
Batch: 60 | Loss: 4.264 | Acc: 48.053,67.495,72.413,% | Adaptive Acc: 68.686% | clf_exit: 0.338 0.406 0.256
Train classifier parameters

Epoch: 287
Batch: 0 | Loss: 2.362 | Acc: 53.906,85.938,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.297 0.523 0.180
Batch: 20 | Loss: 2.435 | Acc: 52.232,87.686,100.000,% | Adaptive Acc: 94.680% | clf_exit: 0.281 0.491 0.228
Batch: 40 | Loss: 2.442 | Acc: 51.963,87.633,99.943,% | Adaptive Acc: 94.569% | clf_exit: 0.276 0.494 0.230
Batch: 60 | Loss: 2.453 | Acc: 51.819,87.474,99.949,% | Adaptive Acc: 94.352% | clf_exit: 0.272 0.497 0.231
Batch: 80 | Loss: 2.441 | Acc: 52.045,87.635,99.952,% | Adaptive Acc: 94.425% | clf_exit: 0.274 0.498 0.229
Batch: 100 | Loss: 2.451 | Acc: 51.934,87.631,99.954,% | Adaptive Acc: 94.415% | clf_exit: 0.270 0.501 0.229
Batch: 120 | Loss: 2.448 | Acc: 52.034,87.558,99.961,% | Adaptive Acc: 94.421% | clf_exit: 0.272 0.500 0.228
Batch: 140 | Loss: 2.449 | Acc: 52.117,87.450,99.950,% | Adaptive Acc: 94.354% | clf_exit: 0.273 0.499 0.228
Batch: 160 | Loss: 2.450 | Acc: 52.159,87.563,99.947,% | Adaptive Acc: 94.361% | clf_exit: 0.273 0.499 0.228
Batch: 180 | Loss: 2.454 | Acc: 52.124,87.535,99.948,% | Adaptive Acc: 94.376% | clf_exit: 0.273 0.499 0.229
Batch: 200 | Loss: 2.459 | Acc: 51.963,87.453,99.938,% | Adaptive Acc: 94.372% | clf_exit: 0.272 0.499 0.229
Batch: 220 | Loss: 2.460 | Acc: 51.955,87.489,99.933,% | Adaptive Acc: 94.330% | clf_exit: 0.273 0.498 0.229
Batch: 240 | Loss: 2.465 | Acc: 51.838,87.351,99.929,% | Adaptive Acc: 94.282% | clf_exit: 0.272 0.497 0.230
Batch: 260 | Loss: 2.466 | Acc: 51.868,87.317,99.931,% | Adaptive Acc: 94.310% | clf_exit: 0.272 0.497 0.231
Batch: 280 | Loss: 2.466 | Acc: 51.874,87.289,99.928,% | Adaptive Acc: 94.359% | clf_exit: 0.272 0.497 0.231
Batch: 300 | Loss: 2.466 | Acc: 51.944,87.256,99.922,% | Adaptive Acc: 94.363% | clf_exit: 0.271 0.498 0.231
Batch: 320 | Loss: 2.468 | Acc: 51.886,87.266,99.915,% | Adaptive Acc: 94.281% | clf_exit: 0.272 0.497 0.232
Batch: 340 | Loss: 2.467 | Acc: 51.957,87.248,99.918,% | Adaptive Acc: 94.270% | clf_exit: 0.272 0.497 0.232
Batch: 360 | Loss: 2.465 | Acc: 51.965,87.284,99.909,% | Adaptive Acc: 94.248% | clf_exit: 0.272 0.496 0.231
Batch: 380 | Loss: 2.465 | Acc: 51.991,87.276,99.908,% | Adaptive Acc: 94.279% | clf_exit: 0.272 0.496 0.232
Batch: 0 | Loss: 3.815 | Acc: 51.562,71.875,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.279 | Acc: 48.698,67.671,72.507,% | Adaptive Acc: 68.899% | clf_exit: 0.337 0.414 0.249
Batch: 40 | Loss: 4.243 | Acc: 48.761,67.912,72.523,% | Adaptive Acc: 69.150% | clf_exit: 0.336 0.405 0.259
Batch: 60 | Loss: 4.261 | Acc: 48.476,67.713,72.503,% | Adaptive Acc: 68.993% | clf_exit: 0.335 0.407 0.258
Train classifier parameters

Epoch: 288
Batch: 0 | Loss: 2.602 | Acc: 46.875,84.375,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.273 0.445 0.281
Batch: 20 | Loss: 2.446 | Acc: 52.121,87.984,99.963,% | Adaptive Acc: 95.052% | clf_exit: 0.268 0.494 0.238
Batch: 40 | Loss: 2.474 | Acc: 51.467,87.233,99.962,% | Adaptive Acc: 94.398% | clf_exit: 0.268 0.495 0.238
Batch: 60 | Loss: 2.483 | Acc: 51.281,87.269,99.962,% | Adaptive Acc: 94.224% | clf_exit: 0.270 0.491 0.239
Batch: 80 | Loss: 2.489 | Acc: 51.157,87.297,99.971,% | Adaptive Acc: 94.252% | clf_exit: 0.268 0.491 0.241
Batch: 100 | Loss: 2.496 | Acc: 51.423,87.167,99.961,% | Adaptive Acc: 94.369% | clf_exit: 0.267 0.491 0.242
Batch: 120 | Loss: 2.489 | Acc: 51.375,87.113,99.961,% | Adaptive Acc: 94.305% | clf_exit: 0.269 0.492 0.239
Batch: 140 | Loss: 2.491 | Acc: 51.053,87.068,99.961,% | Adaptive Acc: 94.293% | clf_exit: 0.267 0.494 0.239
Batch: 160 | Loss: 2.481 | Acc: 51.320,87.175,99.961,% | Adaptive Acc: 94.318% | clf_exit: 0.269 0.493 0.238
Batch: 180 | Loss: 2.480 | Acc: 51.476,87.159,99.961,% | Adaptive Acc: 94.337% | clf_exit: 0.269 0.492 0.239
Batch: 200 | Loss: 2.482 | Acc: 51.570,87.076,99.946,% | Adaptive Acc: 94.310% | clf_exit: 0.269 0.491 0.240
Batch: 220 | Loss: 2.479 | Acc: 51.654,87.072,99.940,% | Adaptive Acc: 94.362% | clf_exit: 0.270 0.490 0.240
Batch: 240 | Loss: 2.480 | Acc: 51.747,87.014,99.942,% | Adaptive Acc: 94.288% | clf_exit: 0.270 0.491 0.239
Batch: 260 | Loss: 2.479 | Acc: 51.676,87.009,99.937,% | Adaptive Acc: 94.199% | clf_exit: 0.271 0.492 0.238
Batch: 280 | Loss: 2.474 | Acc: 51.841,87.075,99.933,% | Adaptive Acc: 94.259% | clf_exit: 0.270 0.494 0.236
Batch: 300 | Loss: 2.470 | Acc: 51.900,87.069,99.933,% | Adaptive Acc: 94.261% | clf_exit: 0.270 0.493 0.236
Batch: 320 | Loss: 2.470 | Acc: 51.874,87.050,99.932,% | Adaptive Acc: 94.256% | clf_exit: 0.270 0.493 0.236
Batch: 340 | Loss: 2.470 | Acc: 51.837,87.021,99.934,% | Adaptive Acc: 94.220% | clf_exit: 0.271 0.493 0.236
Batch: 360 | Loss: 2.470 | Acc: 51.859,87.015,99.933,% | Adaptive Acc: 94.213% | clf_exit: 0.270 0.494 0.235
Batch: 380 | Loss: 2.468 | Acc: 51.829,87.067,99.930,% | Adaptive Acc: 94.232% | clf_exit: 0.271 0.494 0.235
Batch: 0 | Loss: 3.794 | Acc: 53.125,70.312,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.414 0.242
Batch: 20 | Loss: 4.287 | Acc: 48.735,68.043,72.582,% | Adaptive Acc: 68.936% | clf_exit: 0.339 0.414 0.247
Batch: 40 | Loss: 4.246 | Acc: 48.704,67.988,72.561,% | Adaptive Acc: 69.131% | clf_exit: 0.337 0.404 0.259
Batch: 60 | Loss: 4.258 | Acc: 48.476,67.764,72.515,% | Adaptive Acc: 68.929% | clf_exit: 0.336 0.405 0.259
Train classifier parameters

Epoch: 289
Batch: 0 | Loss: 2.387 | Acc: 53.125,89.062,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.258 0.570 0.172
Batch: 20 | Loss: 2.471 | Acc: 50.856,87.686,99.888,% | Adaptive Acc: 94.085% | clf_exit: 0.260 0.513 0.227
Batch: 40 | Loss: 2.462 | Acc: 51.105,87.671,99.905,% | Adaptive Acc: 94.245% | clf_exit: 0.262 0.505 0.233
Batch: 60 | Loss: 2.454 | Acc: 52.024,87.577,99.910,% | Adaptive Acc: 94.570% | clf_exit: 0.260 0.504 0.236
Batch: 80 | Loss: 2.448 | Acc: 51.968,87.596,99.913,% | Adaptive Acc: 94.464% | clf_exit: 0.268 0.497 0.235
Batch: 100 | Loss: 2.446 | Acc: 51.779,87.353,99.930,% | Adaptive Acc: 94.369% | clf_exit: 0.268 0.497 0.235
Batch: 120 | Loss: 2.445 | Acc: 52.150,87.274,99.942,% | Adaptive Acc: 94.350% | clf_exit: 0.268 0.498 0.234
Batch: 140 | Loss: 2.452 | Acc: 52.017,87.262,99.934,% | Adaptive Acc: 94.276% | clf_exit: 0.267 0.500 0.233
Batch: 160 | Loss: 2.444 | Acc: 52.140,87.456,99.937,% | Adaptive Acc: 94.162% | clf_exit: 0.268 0.501 0.231
Batch: 180 | Loss: 2.451 | Acc: 51.985,87.383,99.935,% | Adaptive Acc: 94.208% | clf_exit: 0.267 0.501 0.232
Batch: 200 | Loss: 2.452 | Acc: 51.994,87.325,99.934,% | Adaptive Acc: 94.158% | clf_exit: 0.268 0.499 0.232
Batch: 220 | Loss: 2.453 | Acc: 52.043,87.316,99.929,% | Adaptive Acc: 94.224% | clf_exit: 0.268 0.499 0.233
Batch: 240 | Loss: 2.455 | Acc: 52.023,87.309,99.932,% | Adaptive Acc: 94.201% | clf_exit: 0.268 0.499 0.233
Batch: 260 | Loss: 2.454 | Acc: 52.029,87.344,99.934,% | Adaptive Acc: 94.187% | clf_exit: 0.268 0.499 0.233
Batch: 280 | Loss: 2.460 | Acc: 51.955,87.328,99.930,% | Adaptive Acc: 94.225% | clf_exit: 0.268 0.498 0.234
Batch: 300 | Loss: 2.462 | Acc: 52.037,87.285,99.922,% | Adaptive Acc: 94.256% | clf_exit: 0.268 0.497 0.235
Batch: 320 | Loss: 2.465 | Acc: 51.974,87.296,99.922,% | Adaptive Acc: 94.254% | clf_exit: 0.268 0.497 0.235
Batch: 340 | Loss: 2.466 | Acc: 51.899,87.298,99.924,% | Adaptive Acc: 94.208% | clf_exit: 0.267 0.498 0.235
Batch: 360 | Loss: 2.466 | Acc: 51.872,87.318,99.920,% | Adaptive Acc: 94.204% | clf_exit: 0.268 0.497 0.235
Batch: 380 | Loss: 2.465 | Acc: 51.862,87.315,99.916,% | Adaptive Acc: 94.209% | clf_exit: 0.268 0.497 0.235
Batch: 0 | Loss: 3.819 | Acc: 55.469,70.312,78.125,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.288 | Acc: 48.400,67.336,72.582,% | Adaptive Acc: 68.676% | clf_exit: 0.344 0.408 0.248
Batch: 40 | Loss: 4.250 | Acc: 48.533,67.702,72.409,% | Adaptive Acc: 68.960% | clf_exit: 0.342 0.402 0.256
Batch: 60 | Loss: 4.264 | Acc: 48.271,67.649,72.605,% | Adaptive Acc: 68.878% | clf_exit: 0.339 0.404 0.257
Train classifier parameters

Epoch: 290
Batch: 0 | Loss: 2.396 | Acc: 55.469,89.062,100.000,% | Adaptive Acc: 96.875% | clf_exit: 0.250 0.531 0.219
Batch: 20 | Loss: 2.474 | Acc: 51.935,87.054,99.888,% | Adaptive Acc: 94.308% | clf_exit: 0.272 0.489 0.239
Batch: 40 | Loss: 2.508 | Acc: 51.391,86.338,99.886,% | Adaptive Acc: 94.264% | clf_exit: 0.269 0.486 0.245
Batch: 60 | Loss: 2.502 | Acc: 51.422,86.860,99.910,% | Adaptive Acc: 94.288% | clf_exit: 0.268 0.492 0.240
Batch: 80 | Loss: 2.491 | Acc: 51.688,87.008,99.913,% | Adaptive Acc: 94.261% | clf_exit: 0.272 0.491 0.238
Batch: 100 | Loss: 2.491 | Acc: 51.733,86.843,99.923,% | Adaptive Acc: 94.175% | clf_exit: 0.272 0.489 0.239
Batch: 120 | Loss: 2.496 | Acc: 51.614,87.003,99.929,% | Adaptive Acc: 94.215% | clf_exit: 0.270 0.491 0.239
Batch: 140 | Loss: 2.501 | Acc: 51.607,86.907,99.928,% | Adaptive Acc: 94.287% | clf_exit: 0.267 0.493 0.240
Batch: 160 | Loss: 2.496 | Acc: 51.694,86.889,99.918,% | Adaptive Acc: 94.192% | clf_exit: 0.266 0.495 0.239
Batch: 180 | Loss: 2.483 | Acc: 51.990,86.986,99.909,% | Adaptive Acc: 94.220% | clf_exit: 0.267 0.496 0.238
Batch: 200 | Loss: 2.484 | Acc: 52.025,86.971,99.918,% | Adaptive Acc: 94.244% | clf_exit: 0.267 0.495 0.238
Batch: 220 | Loss: 2.483 | Acc: 51.948,87.065,99.908,% | Adaptive Acc: 94.266% | clf_exit: 0.268 0.494 0.238
Batch: 240 | Loss: 2.479 | Acc: 52.143,87.075,99.906,% | Adaptive Acc: 94.291% | clf_exit: 0.268 0.495 0.237
Batch: 260 | Loss: 2.482 | Acc: 51.970,87.048,99.901,% | Adaptive Acc: 94.262% | clf_exit: 0.267 0.495 0.237
Batch: 280 | Loss: 2.486 | Acc: 51.888,87.016,99.908,% | Adaptive Acc: 94.203% | clf_exit: 0.268 0.495 0.237
Batch: 300 | Loss: 2.482 | Acc: 51.890,87.069,99.907,% | Adaptive Acc: 94.178% | clf_exit: 0.268 0.496 0.237
Batch: 320 | Loss: 2.473 | Acc: 52.025,87.159,99.905,% | Adaptive Acc: 94.229% | clf_exit: 0.268 0.496 0.235
Batch: 340 | Loss: 2.470 | Acc: 52.103,87.204,99.908,% | Adaptive Acc: 94.231% | clf_exit: 0.269 0.497 0.234
Batch: 360 | Loss: 2.468 | Acc: 52.093,87.249,99.907,% | Adaptive Acc: 94.191% | clf_exit: 0.269 0.496 0.234
Batch: 380 | Loss: 2.463 | Acc: 52.122,87.352,99.906,% | Adaptive Acc: 94.228% | clf_exit: 0.270 0.496 0.234
Batch: 0 | Loss: 3.785 | Acc: 54.688,71.094,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.295 | Acc: 48.251,67.225,72.470,% | Adaptive Acc: 68.638% | clf_exit: 0.343 0.409 0.248
Batch: 40 | Loss: 4.249 | Acc: 48.399,67.626,72.523,% | Adaptive Acc: 68.960% | clf_exit: 0.341 0.400 0.258
Batch: 60 | Loss: 4.263 | Acc: 48.207,67.623,72.528,% | Adaptive Acc: 68.763% | clf_exit: 0.339 0.402 0.259
Train classifier parameters

Epoch: 291
Batch: 0 | Loss: 2.564 | Acc: 54.688,85.156,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.227 0.516 0.258
Batch: 20 | Loss: 2.487 | Acc: 50.595,87.426,99.926,% | Adaptive Acc: 94.829% | clf_exit: 0.260 0.491 0.249
Batch: 40 | Loss: 2.463 | Acc: 51.982,87.367,99.943,% | Adaptive Acc: 94.569% | clf_exit: 0.272 0.492 0.236
Batch: 60 | Loss: 2.464 | Acc: 52.177,87.116,99.923,% | Adaptive Acc: 94.365% | clf_exit: 0.271 0.495 0.235
Batch: 80 | Loss: 2.466 | Acc: 52.344,87.085,99.923,% | Adaptive Acc: 94.261% | clf_exit: 0.271 0.496 0.233
Batch: 100 | Loss: 2.470 | Acc: 52.220,86.959,99.923,% | Adaptive Acc: 94.253% | clf_exit: 0.271 0.494 0.234
Batch: 120 | Loss: 2.457 | Acc: 52.428,87.042,99.923,% | Adaptive Acc: 94.176% | clf_exit: 0.273 0.495 0.232
Batch: 140 | Loss: 2.464 | Acc: 52.216,87.035,99.917,% | Adaptive Acc: 94.232% | clf_exit: 0.271 0.496 0.233
Batch: 160 | Loss: 2.462 | Acc: 52.227,87.126,99.913,% | Adaptive Acc: 94.293% | clf_exit: 0.270 0.498 0.232
Batch: 180 | Loss: 2.462 | Acc: 52.309,87.081,99.914,% | Adaptive Acc: 94.251% | clf_exit: 0.270 0.498 0.232
Batch: 200 | Loss: 2.453 | Acc: 52.581,87.107,99.911,% | Adaptive Acc: 94.294% | clf_exit: 0.271 0.498 0.231
Batch: 220 | Loss: 2.453 | Acc: 52.556,87.164,99.897,% | Adaptive Acc: 94.316% | clf_exit: 0.271 0.496 0.232
Batch: 240 | Loss: 2.457 | Acc: 52.366,87.134,99.900,% | Adaptive Acc: 94.269% | clf_exit: 0.271 0.496 0.232
Batch: 260 | Loss: 2.460 | Acc: 52.308,87.123,99.889,% | Adaptive Acc: 94.247% | clf_exit: 0.271 0.496 0.233
Batch: 280 | Loss: 2.457 | Acc: 52.383,87.158,99.886,% | Adaptive Acc: 94.231% | clf_exit: 0.271 0.497 0.232
Batch: 300 | Loss: 2.456 | Acc: 52.403,87.207,99.891,% | Adaptive Acc: 94.259% | clf_exit: 0.271 0.498 0.231
Batch: 320 | Loss: 2.461 | Acc: 52.327,87.128,99.895,% | Adaptive Acc: 94.191% | clf_exit: 0.271 0.497 0.232
Batch: 340 | Loss: 2.464 | Acc: 52.259,87.099,99.890,% | Adaptive Acc: 94.146% | clf_exit: 0.270 0.498 0.231
Batch: 360 | Loss: 2.466 | Acc: 52.171,87.113,99.894,% | Adaptive Acc: 94.142% | clf_exit: 0.270 0.498 0.232
Batch: 380 | Loss: 2.465 | Acc: 52.231,87.125,99.891,% | Adaptive Acc: 94.154% | clf_exit: 0.270 0.498 0.232
Batch: 0 | Loss: 3.801 | Acc: 53.125,71.875,78.906,% | Adaptive Acc: 71.094% | clf_exit: 0.336 0.445 0.219
Batch: 20 | Loss: 4.289 | Acc: 48.512,67.634,72.656,% | Adaptive Acc: 68.415% | clf_exit: 0.344 0.416 0.240
Batch: 40 | Loss: 4.242 | Acc: 48.476,67.969,72.675,% | Adaptive Acc: 68.750% | clf_exit: 0.343 0.405 0.252
Batch: 60 | Loss: 4.254 | Acc: 48.207,67.674,72.631,% | Adaptive Acc: 68.609% | clf_exit: 0.341 0.406 0.253
Train classifier parameters

Epoch: 292
Batch: 0 | Loss: 2.520 | Acc: 52.344,86.719,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.234 0.492 0.273
Batch: 20 | Loss: 2.500 | Acc: 50.558,86.533,99.851,% | Adaptive Acc: 94.271% | clf_exit: 0.250 0.507 0.243
Batch: 40 | Loss: 2.467 | Acc: 51.505,87.157,99.905,% | Adaptive Acc: 94.588% | clf_exit: 0.258 0.502 0.240
Batch: 60 | Loss: 2.463 | Acc: 51.691,87.295,99.885,% | Adaptive Acc: 94.518% | clf_exit: 0.262 0.501 0.238
Batch: 80 | Loss: 2.458 | Acc: 51.987,87.683,99.894,% | Adaptive Acc: 94.502% | clf_exit: 0.264 0.501 0.236
Batch: 100 | Loss: 2.461 | Acc: 52.150,87.454,99.884,% | Adaptive Acc: 94.446% | clf_exit: 0.264 0.503 0.233
Batch: 120 | Loss: 2.452 | Acc: 52.240,87.487,99.884,% | Adaptive Acc: 94.344% | clf_exit: 0.268 0.500 0.232
Batch: 140 | Loss: 2.450 | Acc: 52.327,87.373,99.900,% | Adaptive Acc: 94.282% | clf_exit: 0.269 0.500 0.231
Batch: 160 | Loss: 2.446 | Acc: 52.353,87.485,99.913,% | Adaptive Acc: 94.298% | clf_exit: 0.271 0.500 0.230
Batch: 180 | Loss: 2.448 | Acc: 52.167,87.448,99.918,% | Adaptive Acc: 94.285% | clf_exit: 0.272 0.497 0.231
Batch: 200 | Loss: 2.459 | Acc: 52.033,87.337,99.911,% | Adaptive Acc: 94.127% | clf_exit: 0.273 0.496 0.231
Batch: 220 | Loss: 2.460 | Acc: 51.941,87.344,99.912,% | Adaptive Acc: 94.118% | clf_exit: 0.273 0.496 0.231
Batch: 240 | Loss: 2.463 | Acc: 51.893,87.289,99.909,% | Adaptive Acc: 94.142% | clf_exit: 0.273 0.495 0.232
Batch: 260 | Loss: 2.465 | Acc: 51.964,87.177,99.910,% | Adaptive Acc: 94.088% | clf_exit: 0.273 0.495 0.232
Batch: 280 | Loss: 2.464 | Acc: 51.938,87.177,99.914,% | Adaptive Acc: 94.086% | clf_exit: 0.273 0.495 0.232
Batch: 300 | Loss: 2.465 | Acc: 51.910,87.207,99.912,% | Adaptive Acc: 94.067% | clf_exit: 0.273 0.495 0.232
Batch: 320 | Loss: 2.467 | Acc: 51.876,87.184,99.910,% | Adaptive Acc: 94.103% | clf_exit: 0.272 0.496 0.232
Batch: 340 | Loss: 2.467 | Acc: 51.881,87.193,99.908,% | Adaptive Acc: 94.121% | clf_exit: 0.272 0.495 0.233
Batch: 360 | Loss: 2.466 | Acc: 51.857,87.234,99.911,% | Adaptive Acc: 94.127% | clf_exit: 0.272 0.495 0.233
Batch: 380 | Loss: 2.465 | Acc: 51.868,87.209,99.908,% | Adaptive Acc: 94.166% | clf_exit: 0.272 0.494 0.234
Batch: 0 | Loss: 3.817 | Acc: 53.906,71.094,76.562,% | Adaptive Acc: 71.875% | clf_exit: 0.344 0.430 0.227
Batch: 20 | Loss: 4.288 | Acc: 48.624,67.411,72.433,% | Adaptive Acc: 68.713% | clf_exit: 0.345 0.408 0.247
Batch: 40 | Loss: 4.253 | Acc: 48.704,67.435,72.294,% | Adaptive Acc: 68.750% | clf_exit: 0.344 0.400 0.256
Batch: 60 | Loss: 4.267 | Acc: 48.322,67.341,72.362,% | Adaptive Acc: 68.609% | clf_exit: 0.342 0.402 0.257
Train classifier parameters

Epoch: 293
Batch: 0 | Loss: 2.612 | Acc: 51.562,85.938,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.258 0.469 0.273
Batch: 20 | Loss: 2.448 | Acc: 51.749,87.649,99.814,% | Adaptive Acc: 93.824% | clf_exit: 0.265 0.512 0.224
Batch: 40 | Loss: 2.479 | Acc: 51.467,87.481,99.867,% | Adaptive Acc: 94.017% | clf_exit: 0.267 0.500 0.233
Batch: 60 | Loss: 2.466 | Acc: 51.639,87.257,99.898,% | Adaptive Acc: 94.109% | clf_exit: 0.269 0.497 0.234
Batch: 80 | Loss: 2.459 | Acc: 51.736,87.442,99.923,% | Adaptive Acc: 94.261% | clf_exit: 0.269 0.499 0.232
Batch: 100 | Loss: 2.470 | Acc: 51.377,87.283,99.907,% | Adaptive Acc: 94.121% | clf_exit: 0.268 0.500 0.232
Batch: 120 | Loss: 2.485 | Acc: 51.227,87.190,99.890,% | Adaptive Acc: 94.202% | clf_exit: 0.267 0.497 0.235
Batch: 140 | Loss: 2.478 | Acc: 51.430,87.251,99.889,% | Adaptive Acc: 94.315% | clf_exit: 0.268 0.496 0.235
Batch: 160 | Loss: 2.475 | Acc: 51.485,87.214,99.903,% | Adaptive Acc: 94.298% | clf_exit: 0.270 0.495 0.235
Batch: 180 | Loss: 2.460 | Acc: 51.847,87.358,99.905,% | Adaptive Acc: 94.320% | clf_exit: 0.270 0.497 0.233
Batch: 200 | Loss: 2.457 | Acc: 51.811,87.337,99.907,% | Adaptive Acc: 94.248% | clf_exit: 0.272 0.497 0.231
Batch: 220 | Loss: 2.459 | Acc: 51.845,87.330,99.905,% | Adaptive Acc: 94.309% | clf_exit: 0.271 0.497 0.232
Batch: 240 | Loss: 2.462 | Acc: 51.864,87.289,99.906,% | Adaptive Acc: 94.304% | clf_exit: 0.271 0.496 0.233
Batch: 260 | Loss: 2.462 | Acc: 51.910,87.278,99.904,% | Adaptive Acc: 94.262% | clf_exit: 0.272 0.495 0.233
Batch: 280 | Loss: 2.463 | Acc: 51.907,87.283,99.905,% | Adaptive Acc: 94.242% | clf_exit: 0.272 0.496 0.232
Batch: 300 | Loss: 2.466 | Acc: 51.817,87.264,99.909,% | Adaptive Acc: 94.259% | clf_exit: 0.271 0.497 0.233
Batch: 320 | Loss: 2.466 | Acc: 51.881,87.254,99.903,% | Adaptive Acc: 94.232% | clf_exit: 0.271 0.496 0.233
Batch: 340 | Loss: 2.465 | Acc: 51.920,87.275,99.901,% | Adaptive Acc: 94.211% | clf_exit: 0.271 0.497 0.232
Batch: 360 | Loss: 2.468 | Acc: 51.868,87.216,99.903,% | Adaptive Acc: 94.194% | clf_exit: 0.270 0.496 0.234
Batch: 380 | Loss: 2.468 | Acc: 51.884,87.207,99.900,% | Adaptive Acc: 94.205% | clf_exit: 0.270 0.497 0.233
Batch: 0 | Loss: 3.797 | Acc: 51.562,71.875,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.438 0.227
Batch: 20 | Loss: 4.284 | Acc: 48.363,67.671,72.619,% | Adaptive Acc: 68.378% | clf_exit: 0.342 0.411 0.247
Batch: 40 | Loss: 4.243 | Acc: 48.533,67.797,72.409,% | Adaptive Acc: 68.750% | clf_exit: 0.342 0.398 0.260
Batch: 60 | Loss: 4.255 | Acc: 48.297,67.738,72.528,% | Adaptive Acc: 68.737% | clf_exit: 0.340 0.402 0.258
Train classifier parameters

Epoch: 294
Batch: 0 | Loss: 2.366 | Acc: 53.125,90.625,100.000,% | Adaptive Acc: 95.312% | clf_exit: 0.203 0.570 0.227
Batch: 20 | Loss: 2.527 | Acc: 50.074,86.458,99.888,% | Adaptive Acc: 93.973% | clf_exit: 0.275 0.481 0.244
Batch: 40 | Loss: 2.467 | Acc: 51.620,86.871,99.867,% | Adaptive Acc: 94.607% | clf_exit: 0.265 0.501 0.234
Batch: 60 | Loss: 2.462 | Acc: 52.024,87.321,99.898,% | Adaptive Acc: 94.544% | clf_exit: 0.265 0.503 0.232
Batch: 80 | Loss: 2.451 | Acc: 52.373,87.432,99.923,% | Adaptive Acc: 94.570% | clf_exit: 0.266 0.501 0.232
Batch: 100 | Loss: 2.453 | Acc: 52.382,87.407,99.892,% | Adaptive Acc: 94.485% | clf_exit: 0.268 0.501 0.231
Batch: 120 | Loss: 2.451 | Acc: 52.318,87.403,99.897,% | Adaptive Acc: 94.589% | clf_exit: 0.267 0.502 0.231
Batch: 140 | Loss: 2.441 | Acc: 52.460,87.439,99.895,% | Adaptive Acc: 94.481% | clf_exit: 0.270 0.501 0.229
Batch: 160 | Loss: 2.451 | Acc: 52.208,87.432,99.908,% | Adaptive Acc: 94.478% | clf_exit: 0.267 0.501 0.231
Batch: 180 | Loss: 2.451 | Acc: 52.171,87.371,99.914,% | Adaptive Acc: 94.441% | clf_exit: 0.268 0.501 0.232
Batch: 200 | Loss: 2.452 | Acc: 52.087,87.352,99.911,% | Adaptive Acc: 94.368% | clf_exit: 0.268 0.501 0.231
Batch: 220 | Loss: 2.454 | Acc: 52.107,87.302,99.912,% | Adaptive Acc: 94.347% | clf_exit: 0.269 0.500 0.231
Batch: 240 | Loss: 2.458 | Acc: 52.007,87.257,99.909,% | Adaptive Acc: 94.337% | clf_exit: 0.269 0.499 0.233
Batch: 260 | Loss: 2.459 | Acc: 51.967,87.210,99.910,% | Adaptive Acc: 94.310% | clf_exit: 0.269 0.498 0.233
Batch: 280 | Loss: 2.462 | Acc: 51.929,87.266,99.914,% | Adaptive Acc: 94.295% | clf_exit: 0.270 0.497 0.233
Batch: 300 | Loss: 2.464 | Acc: 51.892,87.235,99.912,% | Adaptive Acc: 94.298% | clf_exit: 0.270 0.497 0.233
Batch: 320 | Loss: 2.463 | Acc: 52.001,87.210,99.912,% | Adaptive Acc: 94.266% | clf_exit: 0.270 0.497 0.233
Batch: 340 | Loss: 2.462 | Acc: 52.071,87.218,99.913,% | Adaptive Acc: 94.224% | clf_exit: 0.270 0.497 0.233
Batch: 360 | Loss: 2.464 | Acc: 52.032,87.212,99.905,% | Adaptive Acc: 94.207% | clf_exit: 0.270 0.497 0.233
Batch: 380 | Loss: 2.466 | Acc: 52.034,87.176,99.900,% | Adaptive Acc: 94.179% | clf_exit: 0.270 0.497 0.233
Batch: 0 | Loss: 3.787 | Acc: 51.562,71.875,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.422 0.242
Batch: 20 | Loss: 4.270 | Acc: 48.624,67.560,72.507,% | Adaptive Acc: 68.415% | clf_exit: 0.346 0.409 0.244
Batch: 40 | Loss: 4.236 | Acc: 48.552,67.835,72.504,% | Adaptive Acc: 68.693% | clf_exit: 0.347 0.398 0.255
Batch: 60 | Loss: 4.251 | Acc: 48.348,67.713,72.541,% | Adaptive Acc: 68.596% | clf_exit: 0.344 0.400 0.256
Train classifier parameters

Epoch: 295
Batch: 0 | Loss: 2.199 | Acc: 57.031,88.281,100.000,% | Adaptive Acc: 96.094% | clf_exit: 0.336 0.484 0.180
Batch: 20 | Loss: 2.436 | Acc: 51.711,87.537,99.888,% | Adaptive Acc: 94.159% | clf_exit: 0.274 0.495 0.231
Batch: 40 | Loss: 2.467 | Acc: 51.601,86.928,99.886,% | Adaptive Acc: 93.979% | clf_exit: 0.274 0.497 0.229
Batch: 60 | Loss: 2.447 | Acc: 51.934,87.026,99.910,% | Adaptive Acc: 94.173% | clf_exit: 0.274 0.496 0.230
Batch: 80 | Loss: 2.457 | Acc: 52.141,86.902,99.913,% | Adaptive Acc: 94.136% | clf_exit: 0.273 0.495 0.231
Batch: 100 | Loss: 2.458 | Acc: 52.143,87.136,99.899,% | Adaptive Acc: 94.284% | clf_exit: 0.271 0.496 0.233
Batch: 120 | Loss: 2.453 | Acc: 52.260,87.087,99.897,% | Adaptive Acc: 94.312% | clf_exit: 0.271 0.496 0.233
Batch: 140 | Loss: 2.462 | Acc: 52.089,86.968,99.895,% | Adaptive Acc: 94.221% | clf_exit: 0.271 0.496 0.233
Batch: 160 | Loss: 2.462 | Acc: 52.155,86.981,99.898,% | Adaptive Acc: 94.138% | clf_exit: 0.272 0.495 0.232
Batch: 180 | Loss: 2.459 | Acc: 52.240,87.025,99.909,% | Adaptive Acc: 94.164% | clf_exit: 0.272 0.497 0.232
Batch: 200 | Loss: 2.462 | Acc: 52.083,86.995,99.914,% | Adaptive Acc: 94.026% | clf_exit: 0.272 0.496 0.232
Batch: 220 | Loss: 2.460 | Acc: 52.160,87.065,99.919,% | Adaptive Acc: 94.107% | clf_exit: 0.273 0.494 0.233
Batch: 240 | Loss: 2.464 | Acc: 51.935,87.033,99.916,% | Adaptive Acc: 94.042% | clf_exit: 0.273 0.494 0.233
Batch: 260 | Loss: 2.467 | Acc: 51.916,87.039,99.913,% | Adaptive Acc: 94.016% | clf_exit: 0.272 0.495 0.233
Batch: 280 | Loss: 2.470 | Acc: 51.827,87.061,99.914,% | Adaptive Acc: 93.997% | clf_exit: 0.272 0.495 0.233
Batch: 300 | Loss: 2.469 | Acc: 51.869,87.069,99.914,% | Adaptive Acc: 94.017% | clf_exit: 0.271 0.496 0.233
Batch: 320 | Loss: 2.467 | Acc: 51.845,87.132,99.905,% | Adaptive Acc: 94.052% | clf_exit: 0.271 0.496 0.233
Batch: 340 | Loss: 2.467 | Acc: 51.803,87.072,99.908,% | Adaptive Acc: 94.023% | clf_exit: 0.270 0.496 0.233
Batch: 360 | Loss: 2.468 | Acc: 51.842,87.039,99.903,% | Adaptive Acc: 94.021% | clf_exit: 0.271 0.496 0.233
Batch: 380 | Loss: 2.467 | Acc: 51.839,87.065,99.908,% | Adaptive Acc: 94.023% | clf_exit: 0.271 0.496 0.232
Batch: 0 | Loss: 3.807 | Acc: 52.344,71.094,78.125,% | Adaptive Acc: 72.656% | clf_exit: 0.328 0.430 0.242
Batch: 20 | Loss: 4.303 | Acc: 48.251,67.411,72.247,% | Adaptive Acc: 68.564% | clf_exit: 0.343 0.409 0.248
Batch: 40 | Loss: 4.261 | Acc: 48.304,67.473,72.294,% | Adaptive Acc: 68.826% | clf_exit: 0.343 0.400 0.257
Batch: 60 | Loss: 4.276 | Acc: 48.002,67.533,72.413,% | Adaptive Acc: 68.763% | clf_exit: 0.342 0.401 0.257
Train classifier parameters

Epoch: 296
Batch: 0 | Loss: 2.419 | Acc: 50.781,83.594,100.000,% | Adaptive Acc: 92.969% | clf_exit: 0.242 0.547 0.211
Batch: 20 | Loss: 2.456 | Acc: 51.674,87.240,99.814,% | Adaptive Acc: 94.122% | clf_exit: 0.277 0.492 0.231
Batch: 40 | Loss: 2.424 | Acc: 52.306,87.462,99.905,% | Adaptive Acc: 94.341% | clf_exit: 0.278 0.494 0.228
Batch: 60 | Loss: 2.443 | Acc: 52.549,87.116,99.898,% | Adaptive Acc: 94.339% | clf_exit: 0.278 0.491 0.231
Batch: 80 | Loss: 2.445 | Acc: 52.141,87.143,99.904,% | Adaptive Acc: 94.522% | clf_exit: 0.272 0.496 0.232
Batch: 100 | Loss: 2.456 | Acc: 51.748,87.152,99.907,% | Adaptive Acc: 94.454% | clf_exit: 0.269 0.499 0.233
Batch: 120 | Loss: 2.457 | Acc: 51.634,87.274,99.910,% | Adaptive Acc: 94.447% | clf_exit: 0.267 0.499 0.234
Batch: 140 | Loss: 2.455 | Acc: 51.812,87.251,99.900,% | Adaptive Acc: 94.310% | clf_exit: 0.269 0.498 0.232
Batch: 160 | Loss: 2.459 | Acc: 51.883,87.248,99.908,% | Adaptive Acc: 94.357% | clf_exit: 0.270 0.497 0.233
Batch: 180 | Loss: 2.458 | Acc: 51.925,87.276,99.909,% | Adaptive Acc: 94.346% | clf_exit: 0.269 0.497 0.233
Batch: 200 | Loss: 2.454 | Acc: 52.111,87.290,99.914,% | Adaptive Acc: 94.333% | clf_exit: 0.270 0.498 0.232
Batch: 220 | Loss: 2.451 | Acc: 52.206,87.214,99.919,% | Adaptive Acc: 94.347% | clf_exit: 0.271 0.498 0.231
Batch: 240 | Loss: 2.457 | Acc: 52.020,87.221,99.909,% | Adaptive Acc: 94.298% | clf_exit: 0.271 0.498 0.231
Batch: 260 | Loss: 2.455 | Acc: 52.143,87.246,99.913,% | Adaptive Acc: 94.313% | clf_exit: 0.271 0.498 0.231
Batch: 280 | Loss: 2.460 | Acc: 52.102,87.286,99.917,% | Adaptive Acc: 94.303% | clf_exit: 0.270 0.498 0.232
Batch: 300 | Loss: 2.463 | Acc: 52.084,87.168,99.914,% | Adaptive Acc: 94.238% | clf_exit: 0.271 0.497 0.232
Batch: 320 | Loss: 2.468 | Acc: 51.954,87.125,99.920,% | Adaptive Acc: 94.246% | clf_exit: 0.271 0.496 0.233
Batch: 340 | Loss: 2.467 | Acc: 51.918,87.097,99.920,% | Adaptive Acc: 94.190% | clf_exit: 0.271 0.496 0.233
Batch: 360 | Loss: 2.469 | Acc: 51.950,87.106,99.918,% | Adaptive Acc: 94.161% | clf_exit: 0.271 0.497 0.233
Batch: 380 | Loss: 2.468 | Acc: 51.927,87.096,99.916,% | Adaptive Acc: 94.193% | clf_exit: 0.270 0.497 0.233
Batch: 0 | Loss: 3.818 | Acc: 53.906,70.312,78.125,% | Adaptive Acc: 71.875% | clf_exit: 0.359 0.398 0.242
Batch: 20 | Loss: 4.288 | Acc: 48.549,67.225,72.284,% | Adaptive Acc: 68.601% | clf_exit: 0.343 0.408 0.249
Batch: 40 | Loss: 4.244 | Acc: 48.552,67.511,72.313,% | Adaptive Acc: 69.017% | clf_exit: 0.340 0.401 0.259
Batch: 60 | Loss: 4.253 | Acc: 48.207,67.559,72.451,% | Adaptive Acc: 68.891% | clf_exit: 0.338 0.403 0.259
Train classifier parameters

Epoch: 297
Batch: 0 | Loss: 2.544 | Acc: 44.531,82.812,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.242 0.516 0.242
Batch: 20 | Loss: 2.491 | Acc: 51.525,86.905,99.963,% | Adaptive Acc: 93.490% | clf_exit: 0.260 0.516 0.224
Batch: 40 | Loss: 2.492 | Acc: 51.562,86.814,99.943,% | Adaptive Acc: 93.921% | clf_exit: 0.267 0.502 0.231
Batch: 60 | Loss: 2.460 | Acc: 52.152,87.334,99.923,% | Adaptive Acc: 94.249% | clf_exit: 0.271 0.500 0.229
Batch: 80 | Loss: 2.452 | Acc: 52.160,87.307,99.894,% | Adaptive Acc: 94.203% | clf_exit: 0.275 0.496 0.229
Batch: 100 | Loss: 2.439 | Acc: 52.243,87.500,99.899,% | Adaptive Acc: 94.183% | clf_exit: 0.276 0.495 0.229
Batch: 120 | Loss: 2.452 | Acc: 52.150,87.326,99.897,% | Adaptive Acc: 94.228% | clf_exit: 0.273 0.496 0.231
Batch: 140 | Loss: 2.458 | Acc: 52.128,87.356,99.906,% | Adaptive Acc: 94.188% | clf_exit: 0.271 0.496 0.233
Batch: 160 | Loss: 2.458 | Acc: 52.198,87.286,99.903,% | Adaptive Acc: 94.085% | clf_exit: 0.271 0.496 0.232
Batch: 180 | Loss: 2.458 | Acc: 52.227,87.280,99.905,% | Adaptive Acc: 94.147% | clf_exit: 0.271 0.495 0.234
Batch: 200 | Loss: 2.464 | Acc: 52.048,87.337,99.911,% | Adaptive Acc: 94.158% | clf_exit: 0.270 0.495 0.235
Batch: 220 | Loss: 2.464 | Acc: 51.987,87.267,99.901,% | Adaptive Acc: 94.157% | clf_exit: 0.271 0.494 0.235
Batch: 240 | Loss: 2.467 | Acc: 51.906,87.267,99.906,% | Adaptive Acc: 94.175% | clf_exit: 0.271 0.493 0.236
Batch: 260 | Loss: 2.460 | Acc: 51.988,87.380,99.907,% | Adaptive Acc: 94.166% | clf_exit: 0.271 0.494 0.234
Batch: 280 | Loss: 2.464 | Acc: 51.982,87.333,99.905,% | Adaptive Acc: 94.189% | clf_exit: 0.271 0.495 0.234
Batch: 300 | Loss: 2.467 | Acc: 51.970,87.316,99.896,% | Adaptive Acc: 94.199% | clf_exit: 0.270 0.496 0.234
Batch: 320 | Loss: 2.470 | Acc: 51.945,87.203,99.898,% | Adaptive Acc: 94.159% | clf_exit: 0.271 0.495 0.235
Batch: 340 | Loss: 2.466 | Acc: 51.993,87.253,99.901,% | Adaptive Acc: 94.188% | clf_exit: 0.272 0.494 0.234
Batch: 360 | Loss: 2.465 | Acc: 52.008,87.253,99.907,% | Adaptive Acc: 94.211% | clf_exit: 0.272 0.494 0.234
Batch: 380 | Loss: 2.466 | Acc: 51.983,87.231,99.908,% | Adaptive Acc: 94.234% | clf_exit: 0.272 0.494 0.234
Batch: 0 | Loss: 3.828 | Acc: 54.688,71.875,77.344,% | Adaptive Acc: 71.875% | clf_exit: 0.352 0.406 0.242
Batch: 20 | Loss: 4.285 | Acc: 48.735,67.485,72.842,% | Adaptive Acc: 68.936% | clf_exit: 0.342 0.406 0.253
Batch: 40 | Loss: 4.246 | Acc: 48.876,67.569,72.732,% | Adaptive Acc: 68.960% | clf_exit: 0.344 0.396 0.260
Batch: 60 | Loss: 4.260 | Acc: 48.463,67.469,72.605,% | Adaptive Acc: 68.648% | clf_exit: 0.341 0.399 0.260
Train classifier parameters

Epoch: 298
Batch: 0 | Loss: 2.428 | Acc: 57.812,85.938,100.000,% | Adaptive Acc: 93.750% | clf_exit: 0.242 0.500 0.258
Batch: 20 | Loss: 2.458 | Acc: 53.311,87.723,99.888,% | Adaptive Acc: 95.089% | clf_exit: 0.271 0.490 0.239
Batch: 40 | Loss: 2.448 | Acc: 52.763,87.138,99.752,% | Adaptive Acc: 94.455% | clf_exit: 0.270 0.495 0.235
Batch: 60 | Loss: 2.435 | Acc: 52.907,87.334,99.795,% | Adaptive Acc: 94.326% | clf_exit: 0.273 0.495 0.232
Batch: 80 | Loss: 2.461 | Acc: 52.382,87.220,99.826,% | Adaptive Acc: 94.271% | clf_exit: 0.273 0.491 0.236
Batch: 100 | Loss: 2.466 | Acc: 52.437,87.237,99.830,% | Adaptive Acc: 94.261% | clf_exit: 0.273 0.491 0.235
Batch: 120 | Loss: 2.469 | Acc: 52.163,87.255,99.832,% | Adaptive Acc: 94.267% | clf_exit: 0.271 0.494 0.235
Batch: 140 | Loss: 2.472 | Acc: 52.072,87.273,99.850,% | Adaptive Acc: 94.188% | clf_exit: 0.270 0.493 0.236
Batch: 160 | Loss: 2.465 | Acc: 52.329,87.296,99.850,% | Adaptive Acc: 94.153% | clf_exit: 0.272 0.492 0.236
Batch: 180 | Loss: 2.469 | Acc: 52.162,87.301,99.849,% | Adaptive Acc: 94.156% | clf_exit: 0.271 0.493 0.236
Batch: 200 | Loss: 2.467 | Acc: 52.157,87.356,99.860,% | Adaptive Acc: 94.205% | clf_exit: 0.271 0.494 0.235
Batch: 220 | Loss: 2.464 | Acc: 52.142,87.359,99.866,% | Adaptive Acc: 94.245% | clf_exit: 0.272 0.493 0.235
Batch: 240 | Loss: 2.465 | Acc: 52.117,87.322,99.867,% | Adaptive Acc: 94.230% | clf_exit: 0.272 0.493 0.235
Batch: 260 | Loss: 2.468 | Acc: 52.107,87.246,99.877,% | Adaptive Acc: 94.223% | clf_exit: 0.271 0.494 0.235
Batch: 280 | Loss: 2.464 | Acc: 52.157,87.308,99.878,% | Adaptive Acc: 94.262% | clf_exit: 0.272 0.494 0.234
Batch: 300 | Loss: 2.464 | Acc: 52.159,87.355,99.883,% | Adaptive Acc: 94.264% | clf_exit: 0.273 0.493 0.234
Batch: 320 | Loss: 2.463 | Acc: 52.149,87.369,99.881,% | Adaptive Acc: 94.283% | clf_exit: 0.272 0.493 0.235
Batch: 340 | Loss: 2.468 | Acc: 52.087,87.305,99.883,% | Adaptive Acc: 94.318% | clf_exit: 0.272 0.493 0.236
Batch: 360 | Loss: 2.465 | Acc: 52.108,87.342,99.883,% | Adaptive Acc: 94.334% | clf_exit: 0.271 0.494 0.235
Batch: 380 | Loss: 2.466 | Acc: 52.106,87.246,99.877,% | Adaptive Acc: 94.306% | clf_exit: 0.271 0.494 0.235
Batch: 0 | Loss: 3.798 | Acc: 53.906,71.094,78.906,% | Adaptive Acc: 71.875% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.289 | Acc: 48.698,67.820,72.731,% | Adaptive Acc: 68.750% | clf_exit: 0.343 0.413 0.244
Batch: 40 | Loss: 4.243 | Acc: 48.438,68.045,72.599,% | Adaptive Acc: 68.826% | clf_exit: 0.339 0.407 0.254
Batch: 60 | Loss: 4.254 | Acc: 48.207,67.725,72.503,% | Adaptive Acc: 68.712% | clf_exit: 0.338 0.407 0.256
Train classifier parameters

Epoch: 299
Batch: 0 | Loss: 2.359 | Acc: 50.781,89.844,100.000,% | Adaptive Acc: 94.531% | clf_exit: 0.281 0.492 0.227
Batch: 20 | Loss: 2.431 | Acc: 51.786,88.393,100.000,% | Adaptive Acc: 94.829% | clf_exit: 0.268 0.497 0.235
Batch: 40 | Loss: 2.456 | Acc: 51.715,87.881,99.886,% | Adaptive Acc: 94.569% | clf_exit: 0.265 0.504 0.231
Batch: 60 | Loss: 2.456 | Acc: 51.703,87.654,99.910,% | Adaptive Acc: 94.301% | clf_exit: 0.268 0.499 0.234
Batch: 80 | Loss: 2.469 | Acc: 51.620,87.404,99.913,% | Adaptive Acc: 94.387% | clf_exit: 0.268 0.496 0.236
Batch: 100 | Loss: 2.468 | Acc: 51.686,87.222,99.907,% | Adaptive Acc: 94.315% | clf_exit: 0.270 0.495 0.235
Batch: 120 | Loss: 2.464 | Acc: 51.666,87.255,99.903,% | Adaptive Acc: 94.331% | clf_exit: 0.273 0.493 0.235
Batch: 140 | Loss: 2.463 | Acc: 51.612,87.201,99.911,% | Adaptive Acc: 94.343% | clf_exit: 0.271 0.494 0.235
Batch: 160 | Loss: 2.462 | Acc: 51.689,87.209,99.918,% | Adaptive Acc: 94.357% | clf_exit: 0.271 0.493 0.235
Batch: 180 | Loss: 2.457 | Acc: 51.800,87.280,99.905,% | Adaptive Acc: 94.333% | clf_exit: 0.272 0.494 0.234
Batch: 200 | Loss: 2.460 | Acc: 51.811,87.174,99.883,% | Adaptive Acc: 94.244% | clf_exit: 0.273 0.494 0.233
Batch: 220 | Loss: 2.461 | Acc: 51.771,87.161,99.883,% | Adaptive Acc: 94.238% | clf_exit: 0.271 0.496 0.233
Batch: 240 | Loss: 2.459 | Acc: 51.909,87.092,99.887,% | Adaptive Acc: 94.207% | clf_exit: 0.271 0.497 0.232
Batch: 260 | Loss: 2.458 | Acc: 51.979,87.186,99.895,% | Adaptive Acc: 94.259% | clf_exit: 0.272 0.496 0.232
Batch: 280 | Loss: 2.458 | Acc: 51.991,87.211,99.900,% | Adaptive Acc: 94.262% | clf_exit: 0.271 0.496 0.232
Batch: 300 | Loss: 2.459 | Acc: 52.043,87.196,99.899,% | Adaptive Acc: 94.269% | clf_exit: 0.271 0.496 0.233
Batch: 320 | Loss: 2.460 | Acc: 51.991,87.188,99.893,% | Adaptive Acc: 94.261% | clf_exit: 0.272 0.495 0.233
Batch: 340 | Loss: 2.464 | Acc: 51.924,87.168,99.885,% | Adaptive Acc: 94.247% | clf_exit: 0.271 0.496 0.233
Batch: 360 | Loss: 2.464 | Acc: 51.909,87.169,99.890,% | Adaptive Acc: 94.198% | clf_exit: 0.271 0.496 0.233
Batch: 380 | Loss: 2.464 | Acc: 51.946,87.129,99.893,% | Adaptive Acc: 94.207% | clf_exit: 0.271 0.496 0.233
Batch: 0 | Loss: 3.796 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 70.312% | clf_exit: 0.336 0.430 0.234
Batch: 20 | Loss: 4.272 | Acc: 48.586,67.634,72.433,% | Adaptive Acc: 68.452% | clf_exit: 0.342 0.414 0.244
Batch: 40 | Loss: 4.233 | Acc: 48.647,67.988,72.370,% | Adaptive Acc: 68.731% | clf_exit: 0.341 0.406 0.254
Batch: 60 | Loss: 4.246 | Acc: 48.386,67.853,72.477,% | Adaptive Acc: 68.724% | clf_exit: 0.338 0.407 0.255
