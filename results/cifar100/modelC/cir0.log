
Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=0 | lr=1.0e-02 | circles=0 
Training: Epoch=0 | Loss: 32.496 |  Acc: 3.624,5.160,6.558,7.986,9.722,10.454,10.996,11.186,% 
Testing: Epoch=0 | Loss: 30.636 |  Acc: 4.800,7.030,8.900,11.650,14.250,14.980,15.500,15.800,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=1 | lr=1.0e-02 | circles=0 
Training: Epoch=1 | Loss: 29.097 |  Acc: 5.820,8.208,11.040,14.682,17.650,19.002,20.270,20.654,% 
Testing: Epoch=1 | Loss: 27.802 |  Acc: 6.740,8.000,11.980,16.380,20.690,23.530,25.150,24.850,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=2 | lr=1.0e-02 | circles=0 
Training: Epoch=2 | Loss: 26.904 |  Acc: 7.204,9.562,14.226,19.144,23.634,26.482,28.348,28.656,% 
Testing: Epoch=2 | Loss: 26.211 |  Acc: 7.170,10.260,15.250,20.790,25.200,28.620,31.220,31.730,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=3 | lr=1.0e-02 | circles=0 
Training: Epoch=3 | Loss: 25.224 |  Acc: 8.006,10.766,16.808,22.696,28.414,32.264,34.526,34.924,% 
Testing: Epoch=3 | Loss: 25.219 |  Acc: 8.180,11.220,17.070,22.780,28.680,32.470,34.710,34.920,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=4 | lr=1.0e-02 | circles=0 
Training: Epoch=4 | Loss: 23.924 |  Acc: 8.686,11.622,18.950,25.666,32.850,37.310,39.966,40.494,% 
Testing: Epoch=4 | Loss: 24.128 |  Acc: 8.290,10.110,17.560,25.030,31.740,36.760,40.360,40.760,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=5 | lr=1.0e-02 | circles=0 
Training: Epoch=5 | Loss: 22.863 |  Acc: 9.216,12.804,21.108,28.250,36.412,41.248,44.268,44.862,% 
Testing: Epoch=5 | Loss: 23.374 |  Acc: 8.960,12.340,20.390,26.660,34.550,39.480,42.520,43.320,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=6 | lr=1.0e-02 | circles=0 
Training: Epoch=6 | Loss: 21.973 |  Acc: 9.944,13.758,22.540,30.156,39.466,44.632,47.718,48.306,% 
Testing: Epoch=6 | Loss: 22.087 |  Acc: 9.940,12.950,20.620,28.830,39.240,43.710,47.350,47.370,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=7 | lr=1.0e-02 | circles=0 
Training: Epoch=7 | Loss: 21.247 |  Acc: 10.494,14.580,23.764,32.132,42.094,47.546,50.768,51.344,% 
Testing: Epoch=7 | Loss: 21.770 |  Acc: 9.890,13.750,21.850,31.170,39.990,45.690,48.550,48.790,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=8 | lr=1.0e-02 | circles=0 
Training: Epoch=8 | Loss: 20.544 |  Acc: 11.128,15.516,24.880,33.848,44.840,50.668,53.702,54.310,% 
Testing: Epoch=8 | Loss: 20.787 |  Acc: 11.480,14.460,22.960,33.770,44.280,49.990,52.380,52.090,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=9 | lr=1.0e-02 | circles=0 
Training: Epoch=9 | Loss: 19.979 |  Acc: 11.616,16.162,25.972,35.608,46.906,52.952,56.282,56.538,% 
Testing: Epoch=9 | Loss: 20.748 |  Acc: 10.780,14.280,23.310,32.700,44.420,50.270,52.410,52.710,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=10 | lr=1.0e-02 | circles=0 
Training: Epoch=10 | Loss: 19.472 |  Acc: 11.946,16.774,27.086,36.938,49.078,55.376,58.550,59.164,% 
Testing: Epoch=10 | Loss: 20.325 |  Acc: 11.370,14.490,22.970,33.640,46.130,51.830,54.410,54.570,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=11 | lr=1.0e-02 | circles=0 
Training: Epoch=11 | Loss: 19.018 |  Acc: 12.762,17.400,27.544,38.016,50.608,57.024,60.412,60.874,% 
Testing: Epoch=11 | Loss: 19.844 |  Acc: 12.130,16.260,25.960,35.950,47.850,53.910,55.620,55.900,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=12 | lr=1.0e-02 | circles=0 
Training: Epoch=12 | Loss: 18.586 |  Acc: 12.898,17.990,28.628,39.326,52.122,58.320,61.778,62.440,% 
Testing: Epoch=12 | Loss: 19.718 |  Acc: 12.110,15.260,26.080,36.430,49.020,54.520,57.310,56.910,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=13 | lr=1.0e-02 | circles=0 
Training: Epoch=13 | Loss: 18.299 |  Acc: 13.184,18.360,29.190,40.300,53.284,59.776,63.234,63.712,% 
Testing: Epoch=13 | Loss: 19.697 |  Acc: 11.990,16.270,26.870,36.780,49.670,55.020,56.860,56.690,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=14 | lr=1.0e-02 | circles=0 
Training: Epoch=14 | Loss: 17.905 |  Acc: 13.482,18.850,29.712,41.182,54.652,61.132,64.678,65.370,% 
Testing: Epoch=14 | Loss: 19.192 |  Acc: 13.110,17.410,27.640,38.450,50.130,55.960,58.480,58.380,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=15 | lr=1.0e-02 | circles=0 
Training: Epoch=15 | Loss: 17.651 |  Acc: 13.856,19.184,30.252,42.088,55.250,62.336,65.918,66.664,% 
Testing: Epoch=15 | Loss: 19.312 |  Acc: 13.210,18.090,28.340,37.870,49.830,54.720,57.610,57.580,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=16 | lr=1.0e-02 | circles=0 
Training: Epoch=16 | Loss: 17.376 |  Acc: 14.134,19.606,30.960,42.866,56.448,63.402,67.370,67.878,% 
Testing: Epoch=16 | Loss: 19.177 |  Acc: 12.430,16.720,28.100,39.400,51.280,56.630,58.980,58.970,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=17 | lr=1.0e-02 | circles=0 
Training: Epoch=17 | Loss: 17.161 |  Acc: 14.380,20.156,31.368,43.186,57.024,64.170,68.022,68.598,% 
Testing: Epoch=17 | Loss: 18.515 |  Acc: 13.440,18.370,28.680,40.610,52.620,57.610,59.900,60.340,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=18 | lr=1.0e-02 | circles=0 
Training: Epoch=18 | Loss: 16.932 |  Acc: 14.644,20.570,31.888,43.994,58.226,65.472,69.396,69.868,% 
Testing: Epoch=18 | Loss: 18.827 |  Acc: 13.430,19.270,29.720,40.950,52.740,57.490,59.970,59.800,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=19 | lr=1.0e-02 | circles=0 
Training: Epoch=19 | Loss: 16.694 |  Acc: 14.858,20.942,32.232,44.432,58.756,66.184,70.356,70.712,% 
Testing: Epoch=19 | Loss: 18.486 |  Acc: 14.340,19.280,29.840,40.520,52.270,58.150,60.110,59.730,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=20 | lr=1.0e-02 | circles=0 
Training: Epoch=20 | Loss: 16.503 |  Acc: 14.888,21.148,32.740,44.966,59.514,67.036,71.108,71.596,% 
Testing: Epoch=20 | Loss: 18.241 |  Acc: 13.090,17.640,29.450,42.330,54.700,59.700,61.760,61.710,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=21 | lr=1.0e-02 | circles=0 
Training: Epoch=21 | Loss: 16.340 |  Acc: 15.012,21.518,33.220,45.616,59.952,67.668,71.792,72.256,% 
Testing: Epoch=21 | Loss: 18.210 |  Acc: 14.630,19.500,30.770,42.150,53.960,58.960,61.200,61.280,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=22 | lr=1.0e-02 | circles=0 
Training: Epoch=22 | Loss: 16.169 |  Acc: 15.246,21.810,33.526,46.046,60.580,68.676,72.728,73.108,% 
Testing: Epoch=22 | Loss: 18.026 |  Acc: 14.730,20.660,31.800,43.010,55.340,60.380,62.010,61.940,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=23 | lr=1.0e-02 | circles=0 
Training: Epoch=23 | Loss: 15.986 |  Acc: 15.282,21.944,33.880,46.518,61.340,69.392,73.392,73.900,% 
Testing: Epoch=23 | Loss: 18.070 |  Acc: 14.370,19.660,29.750,43.160,54.660,60.870,62.650,62.730,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=24 | lr=1.0e-02 | circles=0 
Training: Epoch=24 | Loss: 15.821 |  Acc: 15.528,22.256,34.322,46.728,61.630,69.922,74.092,74.560,% 
Testing: Epoch=24 | Loss: 18.157 |  Acc: 14.450,20.270,31.690,42.530,54.180,60.290,62.090,62.130,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=25 | lr=1.0e-02 | circles=0 
Training: Epoch=25 | Loss: 15.719 |  Acc: 15.596,22.324,34.396,47.146,62.158,70.392,74.508,75.188,% 
Testing: Epoch=25 | Loss: 17.962 |  Acc: 13.550,17.630,30.560,44.180,55.910,61.720,63.600,63.640,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=26 | lr=1.0e-02 | circles=0 
Training: Epoch=26 | Loss: 15.594 |  Acc: 15.766,22.964,34.948,47.596,62.522,70.944,75.018,75.556,% 
Testing: Epoch=26 | Loss: 18.841 |  Acc: 14.810,19.320,30.440,41.390,53.290,58.480,60.230,60.110,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=27 | lr=1.0e-02 | circles=0 
Training: Epoch=27 | Loss: 15.490 |  Acc: 15.888,22.946,35.154,47.776,63.110,71.436,75.506,75.932,% 
Testing: Epoch=27 | Loss: 17.678 |  Acc: 14.750,19.020,32.480,44.870,57.130,61.780,63.620,63.420,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=28 | lr=1.0e-02 | circles=0 
Training: Epoch=28 | Loss: 15.337 |  Acc: 16.130,23.404,35.508,48.368,63.668,72.368,76.400,76.650,% 
Testing: Epoch=28 | Loss: 18.075 |  Acc: 14.710,20.290,30.400,42.460,55.800,60.910,63.150,63.090,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=29 | lr=1.0e-02 | circles=0 
Training: Epoch=29 | Loss: 15.257 |  Acc: 16.000,23.486,35.728,48.662,63.818,72.268,76.436,76.910,% 
Testing: Epoch=29 | Loss: 17.369 |  Acc: 15.780,22.030,33.480,46.150,57.310,61.880,63.510,63.260,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=30 | lr=1.0e-02 | circles=0 
Training: Epoch=30 | Loss: 15.142 |  Acc: 16.090,23.626,35.858,48.806,64.154,72.944,77.106,77.512,% 
Testing: Epoch=30 | Loss: 17.985 |  Acc: 15.290,19.670,30.710,43.260,56.150,61.150,62.590,62.140,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=31 | lr=1.0e-02 | circles=0 
Training: Epoch=31 | Loss: 15.020 |  Acc: 16.446,23.940,36.214,49.328,64.644,73.600,77.626,77.950,% 
Testing: Epoch=31 | Loss: 17.819 |  Acc: 15.850,21.690,30.320,43.580,57.050,62.350,63.310,62.980,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=32 | lr=1.0e-02 | circles=0 
Training: Epoch=32 | Loss: 14.950 |  Acc: 16.490,24.008,36.260,49.048,64.774,73.690,77.976,78.536,% 
Testing: Epoch=32 | Loss: 18.165 |  Acc: 15.400,19.480,31.600,44.650,56.620,60.510,62.050,61.610,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=33 | lr=1.0e-02 | circles=0 
Training: Epoch=33 | Loss: 14.902 |  Acc: 16.534,24.044,36.652,49.708,64.972,73.968,78.042,78.388,% 
Testing: Epoch=33 | Loss: 17.323 |  Acc: 15.630,22.060,32.950,45.930,57.780,62.900,64.320,63.780,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=34 | lr=1.0e-02 | circles=0 
Training: Epoch=34 | Loss: 14.782 |  Acc: 16.562,24.234,36.960,49.608,65.486,74.660,79.088,79.224,% 
Testing: Epoch=34 | Loss: 17.545 |  Acc: 15.910,21.800,33.460,44.690,56.910,62.450,63.690,63.480,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=35 | lr=1.0e-02 | circles=0 
Training: Epoch=35 | Loss: 14.703 |  Acc: 16.540,24.340,37.066,49.926,65.812,74.864,79.262,79.694,% 
Testing: Epoch=35 | Loss: 17.391 |  Acc: 16.070,22.050,34.330,46.610,57.410,62.310,63.520,63.210,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=36 | lr=1.0e-02 | circles=0 
Training: Epoch=36 | Loss: 14.646 |  Acc: 16.756,24.614,37.376,50.258,65.936,75.044,79.388,79.524,% 
Testing: Epoch=36 | Loss: 17.867 |  Acc: 16.060,20.660,31.710,44.260,55.660,61.790,63.320,63.170,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=37 | lr=1.0e-02 | circles=0 
Training: Epoch=37 | Loss: 14.549 |  Acc: 16.834,24.768,37.536,50.296,66.386,75.612,79.764,80.128,% 
Testing: Epoch=37 | Loss: 17.467 |  Acc: 15.670,21.840,33.630,45.940,58.150,62.990,63.380,63.010,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=38 | lr=1.0e-02 | circles=0 


==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=64, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=64, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=164, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=164, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (3): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (4): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (5): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (6): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (7): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Batch: 0 | Loss: 37.070 | Acc: 0.000,0.000,0.000,0.781,1.562,1.562,2.344,1.562,%
Batch: 20 | Loss: 36.458 | Acc: 0.670,0.893,1.860,1.600,2.641,2.902,3.051,3.869,%
Batch: 40 | Loss: 35.933 | Acc: 1.029,1.601,2.496,2.706,3.982,3.868,4.249,4.878,%
Batch: 60 | Loss: 35.508 | Acc: 1.306,2.024,2.805,3.189,4.265,4.483,5.046,5.353,%
Batch: 80 | Loss: 35.156 | Acc: 1.591,2.180,3.173,3.617,4.861,5.141,5.642,5.912,%
Batch: 100 | Loss: 34.822 | Acc: 1.833,2.491,3.465,3.953,5.430,5.933,6.397,6.559,%
Batch: 120 | Loss: 34.572 | Acc: 2.060,2.634,3.803,4.339,5.733,6.315,6.812,6.973,%
Batch: 140 | Loss: 34.347 | Acc: 2.144,2.892,3.995,4.604,5.995,6.677,7.209,7.369,%
Batch: 160 | Loss: 34.124 | Acc: 2.295,3.076,4.280,4.896,6.400,7.138,7.580,7.798,%
Batch: 180 | Loss: 33.937 | Acc: 2.499,3.280,4.519,5.292,6.790,7.467,7.925,8.153,%
Batch: 200 | Loss: 33.749 | Acc: 2.647,3.556,4.703,5.546,7.000,7.754,8.186,8.407,%
Batch: 220 | Loss: 33.605 | Acc: 2.754,3.761,4.857,5.847,7.346,8.113,8.445,8.714,%
Batch: 240 | Loss: 33.438 | Acc: 2.856,3.991,5.099,6.111,7.764,8.532,8.808,9.067,%
Batch: 260 | Loss: 33.293 | Acc: 2.978,4.209,5.367,6.403,8.058,8.839,9.133,9.393,%
Batch: 280 | Loss: 33.182 | Acc: 3.019,4.318,5.452,6.561,8.288,8.977,9.317,9.611,%
Batch: 300 | Loss: 33.043 | Acc: 3.138,4.527,5.689,6.878,8.576,9.263,9.661,9.907,%
Batch: 320 | Loss: 32.905 | Acc: 3.254,4.683,5.943,7.165,8.861,9.562,9.996,10.224,%
Batch: 340 | Loss: 32.800 | Acc: 3.345,4.830,6.140,7.430,9.130,9.797,10.211,10.427,%
Batch: 360 | Loss: 32.685 | Acc: 3.452,4.954,6.311,7.637,9.371,10.081,10.511,10.738,%
Batch: 380 | Loss: 32.559 | Acc: 3.588,5.124,6.486,7.880,9.623,10.324,10.851,11.040,%
Batch: 0 | Loss: 30.787 | Acc: 3.125,8.594,10.156,12.500,13.281,14.844,14.844,14.062,%
Batch: 20 | Loss: 30.715 | Acc: 4.874,7.217,9.263,11.496,14.286,14.881,15.216,15.737,%
Batch: 40 | Loss: 30.709 | Acc: 4.688,6.879,8.861,11.261,13.910,14.844,15.454,15.682,%
Batch: 60 | Loss: 30.688 | Acc: 4.688,6.878,8.760,11.642,14.383,15.010,15.356,15.740,%

Epoch: 1
Batch: 0 | Loss: 31.673 | Acc: 5.469,9.375,12.500,10.156,12.500,14.062,11.719,14.062,%
Batch: 20 | Loss: 30.120 | Acc: 5.357,8.036,10.305,12.537,15.551,15.885,15.699,16.555,%
Batch: 40 | Loss: 30.147 | Acc: 5.373,8.079,10.194,12.633,15.492,16.082,15.854,16.387,%
Batch: 60 | Loss: 30.109 | Acc: 5.405,7.864,9.785,12.923,15.343,15.971,16.124,16.598,%
Batch: 80 | Loss: 30.068 | Acc: 5.382,7.716,9.819,12.731,15.162,15.885,16.291,16.744,%
Batch: 100 | Loss: 30.018 | Acc: 5.360,7.557,9.785,12.987,15.486,16.360,16.878,17.280,%
Batch: 120 | Loss: 29.972 | Acc: 5.475,7.503,9.853,13.055,15.509,16.393,16.871,17.368,%
Batch: 140 | Loss: 29.861 | Acc: 5.458,7.580,10.062,13.359,15.880,16.744,17.437,17.913,%
Batch: 160 | Loss: 29.778 | Acc: 5.537,7.711,10.171,13.519,16.033,16.959,17.687,18.124,%
Batch: 180 | Loss: 29.770 | Acc: 5.529,7.718,10.148,13.463,16.083,16.894,17.714,18.163,%
Batch: 200 | Loss: 29.720 | Acc: 5.605,7.840,10.308,13.561,16.227,17.083,17.934,18.311,%
Batch: 220 | Loss: 29.658 | Acc: 5.575,7.798,10.386,13.695,16.300,17.244,18.061,18.474,%
Batch: 240 | Loss: 29.573 | Acc: 5.618,7.861,10.484,13.793,16.403,17.525,18.397,18.825,%
Batch: 260 | Loss: 29.507 | Acc: 5.621,7.857,10.506,13.922,16.598,17.690,18.684,19.127,%
Batch: 280 | Loss: 29.442 | Acc: 5.661,7.896,10.612,14.043,16.776,17.899,18.961,19.314,%
Batch: 300 | Loss: 29.364 | Acc: 5.708,7.989,10.694,14.172,16.951,18.150,19.215,19.645,%
Batch: 320 | Loss: 29.310 | Acc: 5.732,8.032,10.765,14.282,17.056,18.309,19.475,19.901,%
Batch: 340 | Loss: 29.242 | Acc: 5.714,8.087,10.857,14.413,17.231,18.514,19.710,20.095,%
Batch: 360 | Loss: 29.182 | Acc: 5.761,8.133,10.903,14.485,17.374,18.744,19.973,20.347,%
Batch: 380 | Loss: 29.125 | Acc: 5.813,8.198,10.999,14.592,17.532,18.877,20.153,20.534,%
Batch: 0 | Loss: 27.853 | Acc: 8.594,7.031,14.062,21.094,25.000,24.219,27.344,27.344,%
Batch: 20 | Loss: 27.867 | Acc: 6.734,8.259,12.649,17.336,21.243,24.033,26.228,25.372,%
Batch: 40 | Loss: 27.861 | Acc: 6.326,7.812,12.062,16.654,20.675,23.685,25.171,24.943,%
Batch: 60 | Loss: 27.848 | Acc: 6.481,8.056,11.885,16.650,20.914,23.655,25.205,24.795,%

Epoch: 2
Batch: 0 | Loss: 27.586 | Acc: 3.906,8.594,10.938,15.625,21.875,23.438,24.219,23.438,%
Batch: 20 | Loss: 27.644 | Acc: 7.403,9.449,12.165,16.629,19.903,23.103,25.930,25.818,%
Batch: 40 | Loss: 27.535 | Acc: 6.936,8.994,13.014,17.530,20.808,23.723,26.124,26.353,%
Batch: 60 | Loss: 27.648 | Acc: 6.429,8.799,12.526,17.252,20.774,23.783,25.551,25.628,%
Batch: 80 | Loss: 27.561 | Acc: 6.549,8.893,12.722,17.458,21.219,23.900,25.762,26.022,%
Batch: 100 | Loss: 27.502 | Acc: 6.598,9.027,12.941,17.628,21.759,24.489,26.191,26.470,%
Batch: 120 | Loss: 27.451 | Acc: 6.760,9.136,13.210,17.898,22.062,24.638,26.246,26.504,%
Batch: 140 | Loss: 27.413 | Acc: 6.915,9.198,13.231,17.858,22.219,24.778,26.391,26.679,%
Batch: 160 | Loss: 27.341 | Acc: 7.031,9.380,13.398,18.207,22.433,25.082,26.679,27.033,%
Batch: 180 | Loss: 27.331 | Acc: 7.113,9.405,13.415,18.111,22.402,25.009,26.653,26.981,%
Batch: 200 | Loss: 27.310 | Acc: 7.078,9.422,13.417,18.078,22.341,25.105,26.807,27.048,%
Batch: 220 | Loss: 27.269 | Acc: 6.982,9.382,13.497,18.248,22.444,25.194,26.874,27.107,%
Batch: 240 | Loss: 27.218 | Acc: 7.041,9.404,13.541,18.299,22.569,25.415,27.088,27.328,%
Batch: 260 | Loss: 27.190 | Acc: 6.995,9.402,13.590,18.334,22.626,25.452,27.140,27.413,%
Batch: 280 | Loss: 27.153 | Acc: 7.079,9.445,13.779,18.528,22.826,25.564,27.277,27.555,%
Batch: 300 | Loss: 27.102 | Acc: 7.055,9.424,13.894,18.706,22.999,25.797,27.502,27.834,%
Batch: 320 | Loss: 27.068 | Acc: 7.082,9.429,13.946,18.784,23.162,25.939,27.660,27.989,%
Batch: 340 | Loss: 27.029 | Acc: 7.114,9.421,13.996,18.867,23.266,26.029,27.818,28.148,%
Batch: 360 | Loss: 26.987 | Acc: 7.161,9.483,14.073,18.971,23.403,26.231,28.062,28.359,%
Batch: 380 | Loss: 26.926 | Acc: 7.169,9.539,14.175,19.084,23.591,26.433,28.264,28.562,%
Batch: 0 | Loss: 26.142 | Acc: 6.250,10.938,15.625,21.094,26.562,26.562,31.250,32.031,%
Batch: 20 | Loss: 26.460 | Acc: 6.994,10.305,14.435,20.647,25.446,28.832,31.324,31.882,%
Batch: 40 | Loss: 26.312 | Acc: 6.555,10.252,14.977,20.960,25.267,28.716,31.269,32.050,%
Batch: 60 | Loss: 26.272 | Acc: 6.865,10.348,15.292,20.940,25.282,28.740,31.224,31.634,%

Epoch: 3
Batch: 0 | Loss: 26.229 | Acc: 7.031,10.156,13.281,20.312,23.438,26.562,30.469,32.812,%
Batch: 20 | Loss: 25.626 | Acc: 7.143,10.751,16.890,21.726,27.195,30.618,34.263,34.301,%
Batch: 40 | Loss: 25.648 | Acc: 7.031,10.232,16.273,21.246,27.191,30.716,33.841,34.051,%
Batch: 60 | Loss: 25.688 | Acc: 7.095,10.105,15.932,21.094,26.895,30.904,33.607,34.093,%
Batch: 80 | Loss: 25.744 | Acc: 7.243,10.060,16.136,21.393,26.823,30.835,33.227,33.642,%
Batch: 100 | Loss: 25.762 | Acc: 7.317,10.063,16.004,21.272,26.709,30.593,32.843,33.408,%
Batch: 120 | Loss: 25.729 | Acc: 7.425,10.124,16.058,21.352,26.756,30.514,32.787,33.252,%
Batch: 140 | Loss: 25.665 | Acc: 7.447,10.206,16.068,21.515,26.973,30.790,32.907,33.428,%
Batch: 160 | Loss: 25.667 | Acc: 7.395,10.156,15.926,21.472,26.990,30.808,32.885,33.453,%
Batch: 180 | Loss: 25.579 | Acc: 7.579,10.307,16.212,21.737,27.283,31.051,33.041,33.641,%
Batch: 200 | Loss: 25.505 | Acc: 7.754,10.386,16.325,21.961,27.538,31.266,33.318,33.858,%
Batch: 220 | Loss: 25.473 | Acc: 7.858,10.499,16.427,22.066,27.627,31.353,33.410,33.961,%
Batch: 240 | Loss: 25.437 | Acc: 7.926,10.623,16.513,22.112,27.720,31.490,33.535,34.080,%
Batch: 260 | Loss: 25.427 | Acc: 7.932,10.659,16.577,22.207,27.790,31.549,33.651,34.186,%
Batch: 280 | Loss: 25.387 | Acc: 7.924,10.671,16.593,22.286,27.919,31.712,33.855,34.383,%
Batch: 300 | Loss: 25.352 | Acc: 7.963,10.745,16.676,22.412,28.016,31.857,34.001,34.494,%
Batch: 320 | Loss: 25.341 | Acc: 7.997,10.753,16.662,22.408,28.008,31.851,34.032,34.516,%
Batch: 340 | Loss: 25.329 | Acc: 8.010,10.770,16.635,22.482,28.114,31.985,34.185,34.595,%
Batch: 360 | Loss: 25.284 | Acc: 8.029,10.808,16.774,22.600,28.307,32.181,34.369,34.745,%
Batch: 380 | Loss: 25.232 | Acc: 8.038,10.812,16.812,22.691,28.404,32.273,34.516,34.914,%
Batch: 0 | Loss: 25.236 | Acc: 7.031,13.281,17.969,27.344,28.906,28.906,34.375,32.031,%
Batch: 20 | Loss: 25.273 | Acc: 8.408,11.533,17.336,23.140,28.237,32.106,34.524,34.412,%
Batch: 40 | Loss: 25.247 | Acc: 8.041,11.052,16.730,22.580,28.563,32.489,34.699,34.718,%
Batch: 60 | Loss: 25.241 | Acc: 8.056,11.245,16.983,22.925,28.714,32.633,34.810,35.118,%

Epoch: 4
Batch: 0 | Loss: 25.507 | Acc: 9.375,8.594,15.625,19.531,25.781,33.594,32.812,35.156,%
Batch: 20 | Loss: 24.132 | Acc: 8.966,12.054,18.973,25.298,31.845,36.496,39.658,39.955,%
Batch: 40 | Loss: 24.359 | Acc: 8.841,11.814,18.274,24.638,31.441,35.614,38.758,39.710,%
Batch: 60 | Loss: 24.409 | Acc: 8.427,11.386,17.982,24.193,31.135,35.207,38.256,39.127,%
Batch: 80 | Loss: 24.327 | Acc: 8.468,11.420,18.036,24.614,31.491,35.532,38.387,39.506,%
Batch: 100 | Loss: 24.274 | Acc: 8.617,11.595,18.162,24.776,31.644,35.775,38.738,39.728,%
Batch: 120 | Loss: 24.250 | Acc: 8.632,11.577,18.233,24.864,31.696,35.815,38.849,39.644,%
Batch: 140 | Loss: 24.247 | Acc: 8.549,11.442,18.285,24.812,31.738,35.965,38.952,39.788,%
Batch: 160 | Loss: 24.241 | Acc: 8.516,11.398,18.294,24.869,31.740,36.015,38.912,39.645,%
Batch: 180 | Loss: 24.213 | Acc: 8.555,11.451,18.370,25.043,31.712,36.114,39.006,39.753,%
Batch: 200 | Loss: 24.182 | Acc: 8.598,11.482,18.544,25.241,31.911,36.198,39.078,39.789,%
Batch: 220 | Loss: 24.155 | Acc: 8.604,11.570,18.520,25.198,31.953,36.295,39.066,39.727,%
Batch: 240 | Loss: 24.136 | Acc: 8.565,11.495,18.555,25.256,32.180,36.498,39.195,39.782,%
Batch: 260 | Loss: 24.107 | Acc: 8.639,11.605,18.657,25.374,32.277,36.587,39.278,39.832,%
Batch: 280 | Loss: 24.069 | Acc: 8.602,11.535,18.672,25.367,32.362,36.785,39.452,39.955,%
Batch: 300 | Loss: 24.027 | Acc: 8.648,11.620,18.807,25.483,32.509,36.898,39.519,40.049,%
Batch: 320 | Loss: 23.998 | Acc: 8.718,11.651,18.847,25.594,32.572,36.989,39.630,40.172,%
Batch: 340 | Loss: 23.987 | Acc: 8.715,11.641,18.876,25.593,32.554,36.991,39.635,40.210,%
Batch: 360 | Loss: 23.967 | Acc: 8.682,11.630,18.901,25.593,32.644,37.126,39.798,40.324,%
Batch: 380 | Loss: 23.946 | Acc: 8.688,11.606,18.914,25.623,32.782,37.213,39.852,40.377,%
Batch: 0 | Loss: 22.941 | Acc: 9.375,15.625,22.656,32.812,35.938,39.062,42.188,43.750,%
Batch: 20 | Loss: 24.223 | Acc: 8.222,10.900,17.857,25.595,31.845,37.240,41.034,41.220,%
Batch: 40 | Loss: 24.109 | Acc: 8.251,10.118,18.159,25.705,32.069,36.928,40.530,40.835,%
Batch: 60 | Loss: 24.149 | Acc: 8.286,10.143,17.585,25.231,31.929,36.911,40.394,40.843,%

Epoch: 5
Batch: 0 | Loss: 24.170 | Acc: 7.031,4.688,13.281,23.438,32.031,35.156,39.062,41.406,%
Batch: 20 | Loss: 23.364 | Acc: 9.338,11.830,20.461,27.195,36.124,39.435,43.043,43.638,%
Batch: 40 | Loss: 23.233 | Acc: 9.470,12.157,20.579,27.248,35.061,39.577,43.350,44.455,%
Batch: 60 | Loss: 23.244 | Acc: 9.426,12.116,20.479,27.382,35.438,40.151,43.584,44.339,%
Batch: 80 | Loss: 23.183 | Acc: 9.385,12.413,20.959,27.807,35.523,40.162,43.557,44.107,%
Batch: 100 | Loss: 23.187 | Acc: 9.259,12.392,20.761,27.491,35.311,40.060,43.464,43.990,%
Batch: 120 | Loss: 23.146 | Acc: 9.117,12.410,20.823,27.544,35.466,40.070,43.285,44.002,%
Batch: 140 | Loss: 23.107 | Acc: 9.115,12.522,20.783,27.648,35.721,40.354,43.451,44.132,%
Batch: 160 | Loss: 23.077 | Acc: 9.123,12.621,20.885,27.756,35.797,40.460,43.546,44.128,%
Batch: 180 | Loss: 23.049 | Acc: 9.228,12.733,20.977,27.728,35.700,40.426,43.530,44.147,%
Batch: 200 | Loss: 23.056 | Acc: 9.185,12.694,21.035,27.857,35.689,40.411,43.571,44.193,%
Batch: 220 | Loss: 23.062 | Acc: 9.128,12.606,20.853,27.655,35.584,40.515,43.573,44.231,%
Batch: 240 | Loss: 23.046 | Acc: 9.129,12.636,20.864,27.687,35.607,40.492,43.555,44.201,%
Batch: 260 | Loss: 23.032 | Acc: 9.165,12.701,20.890,27.724,35.677,40.556,43.567,44.190,%
Batch: 280 | Loss: 22.992 | Acc: 9.192,12.720,20.991,27.869,35.790,40.711,43.703,44.334,%
Batch: 300 | Loss: 22.959 | Acc: 9.118,12.728,21.052,27.878,35.867,40.807,43.807,44.456,%
Batch: 320 | Loss: 22.951 | Acc: 9.134,12.692,21.009,27.974,35.967,40.871,43.903,44.529,%
Batch: 340 | Loss: 22.929 | Acc: 9.139,12.686,21.023,28.077,36.059,40.992,43.997,44.575,%
Batch: 360 | Loss: 22.899 | Acc: 9.208,12.784,21.079,28.125,36.167,41.084,44.064,44.626,%
Batch: 380 | Loss: 22.879 | Acc: 9.223,12.818,21.122,28.203,36.272,41.172,44.150,44.736,%
Batch: 0 | Loss: 22.497 | Acc: 6.250,14.844,25.781,30.469,39.062,42.969,42.188,43.750,%
Batch: 20 | Loss: 23.410 | Acc: 8.929,12.463,21.168,26.637,35.007,40.104,43.490,44.085,%
Batch: 40 | Loss: 23.431 | Acc: 8.632,12.462,21.018,26.448,34.909,39.939,42.607,43.121,%
Batch: 60 | Loss: 23.412 | Acc: 8.824,12.615,20.684,26.806,34.670,39.472,42.636,43.238,%

Epoch: 6
Batch: 0 | Loss: 22.381 | Acc: 9.375,14.062,16.406,24.219,33.594,42.969,47.656,53.906,%
Batch: 20 | Loss: 22.332 | Acc: 9.449,13.170,21.689,29.092,37.537,43.490,47.098,47.545,%
Batch: 40 | Loss: 22.220 | Acc: 9.680,13.529,21.856,28.944,37.671,43.579,47.085,47.809,%
Batch: 60 | Loss: 22.277 | Acc: 9.746,13.550,21.785,29.214,37.948,43.276,46.773,47.720,%
Batch: 80 | Loss: 22.296 | Acc: 9.790,13.754,21.943,29.041,37.568,43.084,46.730,47.618,%
Batch: 100 | Loss: 22.282 | Acc: 9.769,13.714,22.184,29.417,38.003,43.402,47.030,47.989,%
Batch: 120 | Loss: 22.244 | Acc: 9.756,13.682,22.237,29.791,38.385,43.589,47.340,48.069,%
Batch: 140 | Loss: 22.261 | Acc: 9.813,13.586,22.141,29.577,38.248,43.523,47.130,47.800,%
Batch: 160 | Loss: 22.229 | Acc: 9.831,13.635,22.093,29.556,38.262,43.551,47.127,47.812,%
Batch: 180 | Loss: 22.199 | Acc: 9.815,13.691,22.263,29.662,38.376,43.720,47.246,47.894,%
Batch: 200 | Loss: 22.188 | Acc: 9.752,13.623,22.334,29.586,38.410,43.703,47.108,47.769,%
Batch: 220 | Loss: 22.126 | Acc: 9.926,13.681,22.426,29.765,38.713,43.976,47.313,48.013,%
Batch: 240 | Loss: 22.105 | Acc: 9.884,13.579,22.416,29.869,38.865,44.032,47.342,48.039,%
Batch: 260 | Loss: 22.074 | Acc: 9.938,13.643,22.444,29.900,38.985,44.073,47.378,48.042,%
Batch: 280 | Loss: 22.070 | Acc: 10.001,13.662,22.392,29.924,38.971,44.111,47.381,48.009,%
Batch: 300 | Loss: 22.065 | Acc: 9.949,13.619,22.355,29.931,39.070,44.264,47.459,48.077,%
Batch: 320 | Loss: 22.034 | Acc: 9.979,13.668,22.403,29.997,39.218,44.402,47.605,48.231,%
Batch: 340 | Loss: 22.014 | Acc: 9.957,13.707,22.455,30.068,39.331,44.483,47.624,48.270,%
Batch: 360 | Loss: 22.000 | Acc: 9.892,13.671,22.448,30.071,39.314,44.533,47.678,48.280,%
Batch: 380 | Loss: 21.974 | Acc: 9.912,13.712,22.517,30.178,39.471,44.648,47.744,48.321,%
Batch: 0 | Loss: 21.733 | Acc: 8.594,14.062,24.219,29.688,39.844,43.750,48.438,47.656,%
Batch: 20 | Loss: 22.196 | Acc: 9.859,13.207,21.243,29.055,38.765,43.564,47.693,47.433,%
Batch: 40 | Loss: 22.147 | Acc: 9.870,12.671,20.789,28.887,39.444,43.979,47.294,47.370,%
Batch: 60 | Loss: 22.132 | Acc: 9.887,12.974,20.876,28.932,39.203,43.827,47.336,47.310,%

Epoch: 7
Batch: 0 | Loss: 23.002 | Acc: 6.250,12.500,19.531,26.562,35.938,39.062,39.844,41.406,%
Batch: 20 | Loss: 21.344 | Acc: 10.305,15.179,24.107,31.957,41.629,47.061,50.781,51.600,%
Batch: 40 | Loss: 21.444 | Acc: 10.690,14.768,24.257,31.612,41.463,46.551,50.610,50.934,%
Batch: 60 | Loss: 21.341 | Acc: 10.412,14.395,24.091,31.801,41.483,46.875,50.948,51.281,%
Batch: 80 | Loss: 21.292 | Acc: 10.523,14.294,24.238,31.944,41.869,47.058,51.071,51.562,%
Batch: 100 | Loss: 21.298 | Acc: 10.821,14.596,24.312,32.054,41.901,47.138,50.998,51.562,%
Batch: 120 | Loss: 21.311 | Acc: 10.621,14.585,24.219,32.122,41.852,47.224,51.085,51.730,%
Batch: 140 | Loss: 21.326 | Acc: 10.644,14.495,23.969,31.937,41.611,47.130,50.831,51.524,%
Batch: 160 | Loss: 21.344 | Acc: 10.525,14.470,23.913,31.852,41.639,47.190,50.728,51.485,%
Batch: 180 | Loss: 21.333 | Acc: 10.601,14.473,23.822,31.923,41.531,47.099,50.565,51.291,%
Batch: 200 | Loss: 21.297 | Acc: 10.557,14.498,23.908,32.016,41.729,47.264,50.711,51.384,%
Batch: 220 | Loss: 21.316 | Acc: 10.513,14.504,23.886,32.010,41.686,47.168,50.583,51.290,%
Batch: 240 | Loss: 21.311 | Acc: 10.565,14.614,23.898,32.106,41.750,47.215,50.626,51.352,%
Batch: 260 | Loss: 21.325 | Acc: 10.521,14.574,23.788,32.100,41.724,47.252,50.691,51.413,%
Batch: 280 | Loss: 21.319 | Acc: 10.515,14.582,23.763,32.006,41.676,47.289,50.753,51.443,%
Batch: 300 | Loss: 21.308 | Acc: 10.457,14.576,23.710,32.003,41.738,47.358,50.755,51.433,%
Batch: 320 | Loss: 21.291 | Acc: 10.453,14.557,23.678,32.036,41.835,47.420,50.752,51.399,%
Batch: 340 | Loss: 21.276 | Acc: 10.447,14.514,23.618,31.972,41.876,47.505,50.784,51.404,%
Batch: 360 | Loss: 21.271 | Acc: 10.435,14.502,23.602,31.979,41.969,47.531,50.762,51.331,%
Batch: 380 | Loss: 21.252 | Acc: 10.495,14.573,23.720,32.085,42.060,47.556,50.765,51.351,%
Batch: 0 | Loss: 21.253 | Acc: 8.594,15.625,20.312,36.719,42.969,46.875,52.344,53.125,%
Batch: 20 | Loss: 21.831 | Acc: 10.007,13.802,22.061,31.287,40.067,45.759,48.103,48.438,%
Batch: 40 | Loss: 21.837 | Acc: 9.699,13.681,22.161,31.479,40.282,45.732,48.114,48.304,%
Batch: 60 | Loss: 21.829 | Acc: 9.695,13.973,22.118,31.352,40.138,45.607,48.297,48.630,%

Epoch: 8
Batch: 0 | Loss: 19.830 | Acc: 9.375,14.062,29.688,46.094,55.469,54.688,56.250,53.906,%
Batch: 20 | Loss: 20.500 | Acc: 11.272,14.397,24.926,33.445,44.568,51.004,54.650,55.097,%
Batch: 40 | Loss: 20.505 | Acc: 11.376,15.282,25.210,34.204,44.950,50.915,54.402,55.126,%
Batch: 60 | Loss: 20.574 | Acc: 11.296,15.459,24.987,33.863,44.659,50.269,53.560,54.431,%
Batch: 80 | Loss: 20.569 | Acc: 11.314,15.461,24.913,33.951,44.772,50.376,53.771,54.552,%
Batch: 100 | Loss: 20.606 | Acc: 11.208,15.486,24.776,33.516,44.369,50.193,53.481,54.208,%
Batch: 120 | Loss: 20.599 | Acc: 11.377,15.444,24.826,33.536,44.350,50.465,53.848,54.507,%
Batch: 140 | Loss: 20.580 | Acc: 11.292,15.586,25.006,33.644,44.487,50.554,53.923,54.460,%
Batch: 160 | Loss: 20.567 | Acc: 11.282,15.557,25.015,33.793,44.556,50.602,53.955,54.493,%
Batch: 180 | Loss: 20.568 | Acc: 11.179,15.504,24.927,33.835,44.596,50.617,53.902,54.454,%
Batch: 200 | Loss: 20.549 | Acc: 11.175,15.365,24.841,33.776,44.768,50.634,53.922,54.497,%
Batch: 220 | Loss: 20.562 | Acc: 11.164,15.356,24.897,33.693,44.750,50.647,53.846,54.398,%
Batch: 240 | Loss: 20.570 | Acc: 11.077,15.262,24.861,33.594,44.645,50.548,53.764,54.308,%
Batch: 260 | Loss: 20.581 | Acc: 11.078,15.380,24.922,33.618,44.633,50.566,53.712,54.295,%
Batch: 280 | Loss: 20.592 | Acc: 11.079,15.430,24.836,33.610,44.573,50.439,53.653,54.282,%
Batch: 300 | Loss: 20.586 | Acc: 11.047,15.381,24.803,33.604,44.565,50.423,53.603,54.280,%
Batch: 320 | Loss: 20.571 | Acc: 11.062,15.401,24.817,33.611,44.648,50.518,53.663,54.339,%
Batch: 340 | Loss: 20.562 | Acc: 11.036,15.419,24.787,33.660,44.742,50.616,53.753,54.406,%
Batch: 360 | Loss: 20.563 | Acc: 11.031,15.448,24.797,33.687,44.758,50.595,53.655,54.296,%
Batch: 380 | Loss: 20.542 | Acc: 11.095,15.490,24.844,33.780,44.794,50.625,53.668,54.275,%
Batch: 0 | Loss: 20.519 | Acc: 14.844,15.625,25.781,37.500,47.656,53.125,55.469,52.344,%
Batch: 20 | Loss: 20.879 | Acc: 11.124,14.397,22.693,33.966,43.787,51.004,53.013,52.641,%
Batch: 40 | Loss: 20.809 | Acc: 11.147,14.215,23.380,33.861,44.093,50.267,52.172,51.677,%
Batch: 60 | Loss: 20.839 | Acc: 11.347,14.280,23.028,33.786,44.185,50.256,52.357,52.062,%

Epoch: 9
Batch: 0 | Loss: 20.421 | Acc: 10.938,15.625,25.781,36.719,47.656,48.438,60.938,57.812,%
Batch: 20 | Loss: 19.846 | Acc: 10.938,16.555,26.600,35.342,46.726,53.274,56.622,57.775,%
Batch: 40 | Loss: 19.914 | Acc: 11.528,16.235,26.258,35.175,46.456,52.992,56.745,57.832,%
Batch: 60 | Loss: 19.865 | Acc: 11.668,16.342,25.909,35.233,46.644,53.163,56.878,57.736,%
Batch: 80 | Loss: 19.853 | Acc: 11.825,16.310,26.051,35.407,46.740,53.405,57.089,57.803,%
Batch: 100 | Loss: 19.862 | Acc: 11.796,16.368,25.913,35.187,46.643,53.481,57.054,57.642,%
Batch: 120 | Loss: 19.887 | Acc: 11.648,16.245,25.775,35.105,46.481,53.351,57.038,57.683,%
Batch: 140 | Loss: 19.949 | Acc: 11.525,16.223,25.881,35.001,46.415,53.014,56.643,57.214,%
Batch: 160 | Loss: 20.008 | Acc: 11.549,16.101,25.742,35.001,46.322,52.839,56.512,57.051,%
Batch: 180 | Loss: 19.978 | Acc: 11.585,16.091,25.747,35.139,46.741,53.090,56.781,57.290,%
Batch: 200 | Loss: 20.014 | Acc: 11.521,16.056,25.669,34.989,46.537,52.900,56.553,57.055,%
Batch: 220 | Loss: 19.994 | Acc: 11.577,16.077,25.795,35.206,46.674,52.924,56.565,57.088,%
Batch: 240 | Loss: 20.015 | Acc: 11.524,15.962,25.694,35.159,46.554,52.788,56.425,56.869,%
Batch: 260 | Loss: 20.024 | Acc: 11.527,15.984,25.641,35.210,46.627,52.781,56.376,56.780,%
Batch: 280 | Loss: 20.010 | Acc: 11.591,16.067,25.798,35.440,46.811,52.878,56.339,56.742,%
Batch: 300 | Loss: 20.009 | Acc: 11.618,16.126,25.828,35.488,46.870,52.842,56.255,56.655,%
Batch: 320 | Loss: 19.994 | Acc: 11.653,16.139,25.896,35.592,46.885,52.884,56.267,56.656,%
Batch: 340 | Loss: 20.005 | Acc: 11.636,16.111,25.857,35.486,46.825,52.910,56.250,56.617,%
Batch: 360 | Loss: 19.976 | Acc: 11.688,16.170,25.978,35.557,46.871,52.911,56.248,56.581,%
Batch: 380 | Loss: 19.978 | Acc: 11.655,16.183,26.017,35.589,46.916,52.963,56.295,56.578,%
Batch: 0 | Loss: 20.477 | Acc: 10.156,14.062,24.219,30.469,50.000,52.344,53.906,55.469,%
Batch: 20 | Loss: 20.856 | Acc: 10.714,14.509,23.810,32.738,44.457,50.409,52.753,52.641,%
Batch: 40 | Loss: 20.850 | Acc: 10.575,14.310,23.704,32.812,44.284,50.152,52.344,52.268,%
Batch: 60 | Loss: 20.787 | Acc: 10.886,14.331,23.425,32.761,44.518,50.435,52.421,52.613,%

Epoch: 10
Batch: 0 | Loss: 20.012 | Acc: 10.938,16.406,24.219,29.688,43.750,54.688,58.594,58.594,%
Batch: 20 | Loss: 19.192 | Acc: 12.500,16.853,28.646,38.170,49.851,56.176,59.896,60.491,%
Batch: 40 | Loss: 19.165 | Acc: 12.100,16.787,28.030,37.900,49.638,56.117,59.489,60.118,%
Batch: 60 | Loss: 19.329 | Acc: 11.821,16.624,27.766,37.141,49.360,55.610,58.837,59.618,%
Batch: 80 | Loss: 19.454 | Acc: 11.728,16.696,26.958,36.593,48.640,55.025,58.353,59.144,%
Batch: 100 | Loss: 19.421 | Acc: 11.912,16.739,27.019,36.781,48.855,55.082,58.315,59.019,%
Batch: 120 | Loss: 19.377 | Acc: 11.874,16.755,27.027,36.816,49.012,55.307,58.626,59.433,%
Batch: 140 | Loss: 19.419 | Acc: 11.951,16.772,27.022,36.785,48.942,55.214,58.511,59.297,%
Batch: 160 | Loss: 19.454 | Acc: 11.855,16.722,26.883,36.748,48.894,55.105,58.312,59.132,%
Batch: 180 | Loss: 19.436 | Acc: 11.930,16.834,26.916,36.771,48.968,55.188,58.365,59.138,%
Batch: 200 | Loss: 19.465 | Acc: 11.948,16.822,26.959,36.800,48.943,55.092,58.345,59.091,%
Batch: 220 | Loss: 19.470 | Acc: 11.973,16.880,27.019,36.878,48.961,55.073,58.403,59.170,%
Batch: 240 | Loss: 19.455 | Acc: 12.017,16.918,27.081,36.972,49.066,55.287,58.607,59.352,%
Batch: 260 | Loss: 19.461 | Acc: 11.967,16.882,27.003,36.883,49.012,55.202,58.513,59.201,%
Batch: 280 | Loss: 19.453 | Acc: 11.997,16.871,27.018,36.905,49.030,55.252,58.499,59.125,%
Batch: 300 | Loss: 19.476 | Acc: 11.947,16.811,26.949,36.864,49.009,55.209,58.389,59.019,%
Batch: 320 | Loss: 19.477 | Acc: 11.933,16.764,26.974,36.882,48.980,55.179,58.409,59.015,%
Batch: 340 | Loss: 19.461 | Acc: 11.968,16.796,27.076,36.998,49.090,55.260,58.440,59.073,%
Batch: 360 | Loss: 19.472 | Acc: 11.937,16.776,27.069,36.914,49.028,55.265,58.412,59.057,%
Batch: 380 | Loss: 19.466 | Acc: 11.971,16.794,27.102,36.969,49.090,55.350,58.524,59.145,%
Batch: 0 | Loss: 20.757 | Acc: 11.719,11.719,21.875,28.906,45.312,52.344,52.344,51.562,%
Batch: 20 | Loss: 20.465 | Acc: 11.161,14.621,23.326,33.594,45.796,51.525,54.353,53.534,%
Batch: 40 | Loss: 20.453 | Acc: 11.376,14.367,23.190,33.289,45.636,50.991,53.906,53.620,%
Batch: 60 | Loss: 20.423 | Acc: 11.296,14.434,22.797,33.325,45.927,51.691,54.111,54.137,%

Epoch: 11
Batch: 0 | Loss: 19.256 | Acc: 13.281,16.406,22.656,35.156,53.906,62.500,65.625,64.062,%
Batch: 20 | Loss: 19.084 | Acc: 12.760,17.113,26.637,38.207,51.004,57.887,61.310,61.012,%
Batch: 40 | Loss: 18.912 | Acc: 12.843,17.721,27.515,38.186,50.877,57.851,61.280,61.338,%
Batch: 60 | Loss: 18.897 | Acc: 12.820,17.520,27.754,38.422,50.845,57.736,61.181,61.386,%
Batch: 80 | Loss: 18.983 | Acc: 12.587,17.342,27.450,38.329,50.762,57.195,60.899,61.034,%
Batch: 100 | Loss: 18.967 | Acc: 12.546,17.218,27.545,38.181,50.657,57.047,60.783,60.999,%
Batch: 120 | Loss: 19.005 | Acc: 12.681,17.355,27.460,37.784,50.342,56.663,60.369,60.763,%
Batch: 140 | Loss: 19.024 | Acc: 12.639,17.309,27.565,37.805,50.388,56.643,60.450,60.832,%
Batch: 160 | Loss: 19.024 | Acc: 12.777,17.299,27.514,37.849,50.524,56.706,60.486,60.865,%
Batch: 180 | Loss: 19.043 | Acc: 12.720,17.270,27.460,37.932,50.458,56.708,60.480,60.868,%
Batch: 200 | Loss: 19.021 | Acc: 12.729,17.331,27.546,38.122,50.614,56.930,60.514,60.949,%
Batch: 220 | Loss: 19.008 | Acc: 12.793,17.407,27.612,38.059,50.626,56.936,60.549,60.980,%
Batch: 240 | Loss: 19.031 | Acc: 12.789,17.333,27.580,37.873,50.499,56.872,60.429,60.918,%
Batch: 260 | Loss: 19.026 | Acc: 12.814,17.313,27.511,37.868,50.563,56.879,60.399,60.845,%
Batch: 280 | Loss: 19.041 | Acc: 12.778,17.263,27.524,37.809,50.553,56.839,60.370,60.871,%
Batch: 300 | Loss: 19.030 | Acc: 12.723,17.260,27.559,37.879,50.602,56.894,60.408,60.966,%
Batch: 320 | Loss: 19.017 | Acc: 12.724,17.282,27.577,37.906,50.635,57.002,60.453,61.011,%
Batch: 340 | Loss: 19.022 | Acc: 12.750,17.352,27.527,37.972,50.612,57.006,60.406,60.908,%
Batch: 360 | Loss: 19.012 | Acc: 12.773,17.417,27.588,38.043,50.617,57.042,60.409,60.927,%
Batch: 380 | Loss: 19.016 | Acc: 12.795,17.403,27.553,38.043,50.601,56.998,60.400,60.870,%
Batch: 0 | Loss: 19.111 | Acc: 11.719,19.531,28.906,41.406,57.031,54.688,61.719,63.281,%
Batch: 20 | Loss: 20.155 | Acc: 12.351,15.923,25.707,36.384,47.731,53.274,55.543,55.543,%
Batch: 40 | Loss: 20.011 | Acc: 12.138,15.854,26.067,35.652,47.523,53.430,55.354,55.183,%
Batch: 60 | Loss: 19.945 | Acc: 12.065,16.112,25.961,35.861,47.707,53.420,55.213,55.366,%

Epoch: 12
Batch: 0 | Loss: 18.655 | Acc: 14.844,14.844,25.781,34.375,49.219,60.938,61.719,63.281,%
Batch: 20 | Loss: 18.719 | Acc: 13.802,18.229,27.567,38.132,52.307,57.738,61.458,62.798,%
Batch: 40 | Loss: 18.556 | Acc: 12.424,17.607,27.973,38.262,52.058,58.518,62.748,63.605,%
Batch: 60 | Loss: 18.628 | Acc: 12.065,17.290,27.856,38.717,52.075,58.696,62.590,63.384,%
Batch: 80 | Loss: 18.762 | Acc: 12.153,17.120,27.537,38.310,51.726,58.478,62.317,63.021,%
Batch: 100 | Loss: 18.711 | Acc: 12.268,17.389,27.792,38.490,51.849,58.354,62.214,62.964,%
Batch: 120 | Loss: 18.683 | Acc: 12.423,17.349,27.983,38.636,51.801,58.529,62.164,63.010,%
Batch: 140 | Loss: 18.645 | Acc: 12.483,17.548,28.092,38.797,51.878,58.599,62.245,63.065,%
Batch: 160 | Loss: 18.638 | Acc: 12.650,17.634,28.241,38.922,51.825,58.424,62.112,62.976,%
Batch: 180 | Loss: 18.658 | Acc: 12.595,17.563,28.289,38.985,51.852,58.326,61.887,62.720,%
Batch: 200 | Loss: 18.654 | Acc: 12.764,17.720,28.382,39.047,51.967,58.322,61.894,62.749,%
Batch: 220 | Loss: 18.605 | Acc: 12.885,17.905,28.638,39.296,52.160,58.488,62.030,62.811,%
Batch: 240 | Loss: 18.607 | Acc: 12.892,17.943,28.530,39.257,52.055,58.445,61.968,62.727,%
Batch: 260 | Loss: 18.592 | Acc: 12.847,17.897,28.586,39.359,52.068,58.423,62.009,62.751,%
Batch: 280 | Loss: 18.580 | Acc: 12.884,18.002,28.662,39.335,52.046,58.344,61.955,62.703,%
Batch: 300 | Loss: 18.594 | Acc: 12.874,18.026,28.709,39.361,52.063,58.308,61.867,62.599,%
Batch: 320 | Loss: 18.589 | Acc: 12.816,17.966,28.719,39.391,52.078,58.314,61.853,62.541,%
Batch: 340 | Loss: 18.594 | Acc: 12.894,17.937,28.661,39.340,52.034,58.300,61.840,62.505,%
Batch: 360 | Loss: 18.591 | Acc: 12.892,17.956,28.666,39.294,52.056,58.308,61.805,62.494,%
Batch: 380 | Loss: 18.588 | Acc: 12.935,17.975,28.636,39.313,52.096,58.331,61.829,62.461,%
Batch: 0 | Loss: 18.471 | Acc: 12.500,16.406,31.250,41.406,51.562,57.812,64.844,67.188,%
Batch: 20 | Loss: 19.576 | Acc: 11.979,15.662,25.967,37.016,50.112,55.283,57.961,58.296,%
Batch: 40 | Loss: 19.681 | Acc: 12.024,15.454,26.200,36.757,49.524,54.954,57.603,57.260,%
Batch: 60 | Loss: 19.748 | Acc: 12.039,15.241,26.025,36.463,48.975,54.585,57.467,57.082,%

Epoch: 13
Batch: 0 | Loss: 17.886 | Acc: 10.156,18.750,29.688,41.406,56.250,61.719,70.312,67.188,%
Batch: 20 | Loss: 17.886 | Acc: 12.202,17.932,30.618,41.518,54.576,62.016,64.397,64.323,%
Batch: 40 | Loss: 18.117 | Acc: 12.748,18.121,29.878,40.625,53.639,60.518,64.101,64.234,%
Batch: 60 | Loss: 18.177 | Acc: 12.846,18.071,29.752,40.369,53.317,60.207,63.922,63.998,%
Batch: 80 | Loss: 18.205 | Acc: 12.905,18.007,29.495,40.268,52.874,59.992,63.493,63.947,%
Batch: 100 | Loss: 18.265 | Acc: 12.902,18.054,29.247,40.207,53.133,60.218,63.583,63.861,%
Batch: 120 | Loss: 18.296 | Acc: 13.023,18.033,29.242,40.296,53.022,59.956,63.630,63.811,%
Batch: 140 | Loss: 18.307 | Acc: 13.043,18.207,29.422,40.370,53.042,59.813,63.536,63.774,%
Batch: 160 | Loss: 18.294 | Acc: 12.976,18.260,29.459,40.421,53.076,59.695,63.369,63.587,%
Batch: 180 | Loss: 18.306 | Acc: 13.022,18.163,29.394,40.366,53.108,59.720,63.329,63.605,%
Batch: 200 | Loss: 18.311 | Acc: 12.990,18.175,29.516,40.384,53.020,59.659,63.188,63.557,%
Batch: 220 | Loss: 18.300 | Acc: 13.104,18.269,29.521,40.342,53.100,59.683,63.214,63.561,%
Batch: 240 | Loss: 18.300 | Acc: 13.067,18.218,29.418,40.372,53.106,59.625,63.184,63.583,%
Batch: 260 | Loss: 18.278 | Acc: 13.102,18.271,29.466,40.433,53.290,59.743,63.308,63.721,%
Batch: 280 | Loss: 18.279 | Acc: 13.059,18.202,29.412,40.450,53.292,59.756,63.278,63.718,%
Batch: 300 | Loss: 18.268 | Acc: 13.128,18.244,29.475,40.552,53.423,59.868,63.344,63.754,%
Batch: 320 | Loss: 18.280 | Acc: 13.130,18.222,29.371,40.491,53.293,59.781,63.286,63.751,%
Batch: 340 | Loss: 18.292 | Acc: 13.130,18.267,29.348,40.501,53.288,59.774,63.235,63.730,%
Batch: 360 | Loss: 18.308 | Acc: 13.147,18.293,29.246,40.391,53.229,59.715,63.167,63.649,%
Batch: 380 | Loss: 18.297 | Acc: 13.168,18.348,29.197,40.332,53.322,59.804,63.250,63.745,%
Batch: 0 | Loss: 19.201 | Acc: 12.500,17.188,26.562,35.156,50.000,57.031,59.375,60.938,%
Batch: 20 | Loss: 19.701 | Acc: 12.463,16.071,27.455,38.207,49.777,54.539,56.473,55.878,%
Batch: 40 | Loss: 19.754 | Acc: 11.966,16.292,27.287,37.309,49.314,54.154,56.002,55.888,%
Batch: 60 | Loss: 19.769 | Acc: 11.936,16.393,27.139,36.732,49.424,54.547,56.442,56.224,%

Epoch: 14
Batch: 0 | Loss: 18.827 | Acc: 10.938,17.969,26.562,39.062,50.781,54.688,57.031,56.250,%
Batch: 20 | Loss: 17.682 | Acc: 13.393,17.894,29.501,41.815,53.981,61.347,64.360,65.290,%
Batch: 40 | Loss: 17.809 | Acc: 13.243,17.969,29.611,41.940,54.459,61.376,64.748,65.187,%
Batch: 60 | Loss: 17.930 | Acc: 13.064,18.033,29.342,41.419,54.329,61.091,64.652,65.625,%
Batch: 80 | Loss: 17.933 | Acc: 13.040,18.403,29.032,41.233,54.398,61.188,64.921,65.683,%
Batch: 100 | Loss: 17.845 | Acc: 13.103,18.363,29.100,41.282,54.664,61.371,65.362,66.136,%
Batch: 120 | Loss: 17.878 | Acc: 13.055,18.479,29.106,41.077,54.681,61.228,65.147,65.987,%
Batch: 140 | Loss: 17.895 | Acc: 13.104,18.551,29.100,41.079,54.538,61.059,65.076,65.896,%
Batch: 160 | Loss: 17.900 | Acc: 13.155,18.697,29.178,41.042,54.435,61.175,65.096,65.751,%
Batch: 180 | Loss: 17.897 | Acc: 13.165,18.582,29.316,41.083,54.441,61.123,65.038,65.720,%
Batch: 200 | Loss: 17.892 | Acc: 13.207,18.591,29.384,41.142,54.563,61.140,65.038,65.679,%
Batch: 220 | Loss: 17.884 | Acc: 13.214,18.630,29.518,41.113,54.543,61.220,65.052,65.625,%
Batch: 240 | Loss: 17.847 | Acc: 13.421,18.773,29.688,41.244,54.720,61.343,65.116,65.732,%
Batch: 260 | Loss: 17.860 | Acc: 13.449,18.786,29.562,41.149,54.717,61.354,65.014,65.604,%
Batch: 280 | Loss: 17.862 | Acc: 13.392,18.703,29.593,41.220,54.760,61.324,64.972,65.578,%
Batch: 300 | Loss: 17.889 | Acc: 13.372,18.636,29.560,41.154,54.667,61.202,64.839,65.521,%
Batch: 320 | Loss: 17.889 | Acc: 13.391,18.689,29.634,41.197,54.644,61.164,64.776,65.455,%
Batch: 340 | Loss: 17.911 | Acc: 13.387,18.713,29.651,41.074,54.566,61.089,64.651,65.355,%
Batch: 360 | Loss: 17.908 | Acc: 13.452,18.776,29.655,41.142,54.670,61.147,64.688,65.367,%
Batch: 380 | Loss: 17.909 | Acc: 13.462,18.820,29.692,41.160,54.636,61.138,64.704,65.375,%
Batch: 0 | Loss: 18.044 | Acc: 13.281,18.750,32.031,43.750,51.562,57.812,60.938,60.938,%
Batch: 20 | Loss: 19.250 | Acc: 13.690,17.560,28.274,38.616,49.926,56.399,58.594,58.594,%
Batch: 40 | Loss: 19.262 | Acc: 13.262,17.683,28.087,38.167,49.809,55.678,58.022,58.270,%
Batch: 60 | Loss: 19.272 | Acc: 13.179,17.456,27.882,38.397,49.936,55.597,58.030,57.992,%

Epoch: 15
Batch: 0 | Loss: 16.772 | Acc: 18.750,22.656,34.375,48.438,59.375,66.406,71.875,69.531,%
Batch: 20 | Loss: 17.473 | Acc: 14.546,18.824,30.134,42.336,56.250,63.765,67.597,67.857,%
Batch: 40 | Loss: 17.560 | Acc: 13.148,17.912,29.421,41.578,55.755,63.510,67.816,68.445,%
Batch: 60 | Loss: 17.541 | Acc: 13.704,18.942,29.854,42.226,55.930,63.563,67.777,68.238,%
Batch: 80 | Loss: 17.573 | Acc: 13.474,18.625,29.977,41.956,55.874,63.542,67.554,67.998,%
Batch: 100 | Loss: 17.599 | Acc: 13.637,18.959,30.051,42.095,55.964,63.420,67.327,68.000,%
Batch: 120 | Loss: 17.629 | Acc: 13.630,18.873,29.791,41.832,55.662,63.165,67.084,67.865,%
Batch: 140 | Loss: 17.636 | Acc: 13.603,18.983,30.098,41.861,55.474,62.794,66.705,67.498,%
Batch: 160 | Loss: 17.668 | Acc: 13.689,18.896,30.090,41.901,55.333,62.505,66.290,67.192,%
Batch: 180 | Loss: 17.674 | Acc: 13.687,18.953,30.132,41.941,55.253,62.383,66.199,66.967,%
Batch: 200 | Loss: 17.675 | Acc: 13.685,19.049,30.173,41.981,55.251,62.329,66.208,66.970,%
Batch: 220 | Loss: 17.654 | Acc: 13.713,19.104,30.228,42.060,55.296,62.475,66.251,66.968,%
Batch: 240 | Loss: 17.656 | Acc: 13.764,19.064,30.226,41.993,55.248,62.432,66.176,66.863,%
Batch: 260 | Loss: 17.650 | Acc: 13.805,19.088,30.062,41.954,55.259,62.347,66.089,66.876,%
Batch: 280 | Loss: 17.654 | Acc: 13.790,19.073,30.074,41.934,55.280,62.333,66.059,66.776,%
Batch: 300 | Loss: 17.647 | Acc: 13.821,19.119,30.100,41.962,55.230,62.360,66.069,66.811,%
Batch: 320 | Loss: 17.637 | Acc: 13.824,19.173,30.225,42.066,55.238,62.376,66.073,66.798,%
Batch: 340 | Loss: 17.642 | Acc: 13.872,19.146,30.235,42.103,55.201,62.344,65.987,66.738,%
Batch: 360 | Loss: 17.652 | Acc: 13.863,19.176,30.248,42.084,55.257,62.329,65.915,66.679,%
Batch: 380 | Loss: 17.640 | Acc: 13.882,19.209,30.268,42.124,55.307,62.416,65.972,66.736,%
Batch: 0 | Loss: 19.243 | Acc: 15.625,17.969,25.781,40.625,53.906,56.250,59.375,59.375,%
Batch: 20 | Loss: 19.371 | Acc: 13.132,17.857,28.571,38.132,50.446,55.208,57.924,58.817,%
Batch: 40 | Loss: 19.389 | Acc: 13.186,18.426,28.887,38.205,49.790,54.459,57.489,57.832,%
Batch: 60 | Loss: 19.375 | Acc: 13.358,18.186,28.445,37.999,49.795,54.598,57.505,57.454,%

Epoch: 16
Batch: 0 | Loss: 18.355 | Acc: 11.719,9.375,21.875,32.031,52.344,62.500,69.531,70.312,%
Batch: 20 | Loss: 17.282 | Acc: 13.914,18.676,31.324,43.043,58.073,64.509,68.862,69.382,%
Batch: 40 | Loss: 17.082 | Acc: 14.539,19.817,31.822,44.036,58.365,65.072,69.169,69.779,%
Batch: 60 | Loss: 17.092 | Acc: 14.293,19.736,31.455,44.032,57.774,64.472,68.724,69.467,%
Batch: 80 | Loss: 17.152 | Acc: 14.178,19.705,31.501,44.117,57.803,64.468,68.702,69.416,%
Batch: 100 | Loss: 17.212 | Acc: 13.931,19.647,31.103,43.518,57.480,64.318,68.448,69.137,%
Batch: 120 | Loss: 17.282 | Acc: 14.062,19.699,31.076,43.363,57.193,64.004,68.201,68.873,%
Batch: 140 | Loss: 17.278 | Acc: 14.118,19.598,30.973,43.379,57.292,64.123,68.174,68.833,%
Batch: 160 | Loss: 17.322 | Acc: 14.155,19.517,30.770,43.105,56.949,63.805,68.046,68.648,%
Batch: 180 | Loss: 17.331 | Acc: 14.084,19.471,30.857,43.059,56.781,63.652,67.956,68.603,%
Batch: 200 | Loss: 17.311 | Acc: 14.117,19.485,30.943,43.148,56.860,63.720,67.957,68.618,%
Batch: 220 | Loss: 17.312 | Acc: 14.243,19.499,30.978,43.167,56.819,63.705,67.926,68.612,%
Batch: 240 | Loss: 17.310 | Acc: 14.289,19.541,31.072,43.179,56.795,63.712,67.894,68.543,%
Batch: 260 | Loss: 17.323 | Acc: 14.311,19.603,31.064,43.148,56.618,63.560,67.696,68.319,%
Batch: 280 | Loss: 17.329 | Acc: 14.282,19.604,31.105,43.138,56.609,63.554,67.582,68.197,%
Batch: 300 | Loss: 17.361 | Acc: 14.166,19.523,30.996,43.015,56.478,63.411,67.452,68.036,%
Batch: 320 | Loss: 17.376 | Acc: 14.143,19.458,30.987,42.959,56.442,63.386,67.331,67.964,%
Batch: 340 | Loss: 17.370 | Acc: 14.134,19.538,30.998,42.879,56.495,63.368,67.357,67.973,%
Batch: 360 | Loss: 17.358 | Acc: 14.162,19.642,31.064,42.908,56.510,63.409,67.343,67.923,%
Batch: 380 | Loss: 17.369 | Acc: 14.136,19.587,30.983,42.881,56.512,63.435,67.391,67.928,%
Batch: 0 | Loss: 18.745 | Acc: 10.156,18.750,24.219,45.312,53.906,60.938,60.156,60.156,%
Batch: 20 | Loss: 19.358 | Acc: 12.388,16.295,28.125,39.769,51.637,56.287,58.929,58.296,%
Batch: 40 | Loss: 19.285 | Acc: 12.309,17.111,28.049,39.329,51.505,56.269,58.518,58.213,%
Batch: 60 | Loss: 19.270 | Acc: 12.218,16.662,27.728,39.344,51.140,56.301,58.581,58.389,%

Epoch: 17
Batch: 0 | Loss: 16.436 | Acc: 17.188,24.219,32.812,46.875,59.375,69.531,73.438,73.438,%
Batch: 20 | Loss: 16.926 | Acc: 14.397,20.238,31.101,44.345,57.292,65.216,69.754,70.982,%
Batch: 40 | Loss: 16.920 | Acc: 14.691,20.103,31.136,43.921,57.946,65.625,69.817,70.827,%
Batch: 60 | Loss: 16.839 | Acc: 14.754,20.210,31.673,44.403,58.632,65.715,70.184,70.991,%
Batch: 80 | Loss: 16.906 | Acc: 14.593,20.293,31.703,44.078,58.044,65.355,69.811,70.592,%
Batch: 100 | Loss: 16.945 | Acc: 14.395,20.065,31.467,43.789,57.805,65.308,69.717,70.521,%
Batch: 120 | Loss: 16.995 | Acc: 14.424,19.932,31.302,43.827,57.716,65.141,69.363,70.151,%
Batch: 140 | Loss: 16.995 | Acc: 14.561,20.191,31.438,43.767,57.718,65.110,69.227,70.035,%
Batch: 160 | Loss: 17.019 | Acc: 14.470,20.138,31.464,43.653,57.434,64.727,68.949,69.667,%
Batch: 180 | Loss: 17.047 | Acc: 14.477,20.192,31.418,43.539,57.472,64.641,68.810,69.406,%
Batch: 200 | Loss: 17.070 | Acc: 14.455,20.114,31.398,43.466,57.292,64.506,68.731,69.391,%
Batch: 220 | Loss: 17.060 | Acc: 14.497,20.107,31.381,43.368,57.204,64.462,68.630,69.270,%
Batch: 240 | Loss: 17.088 | Acc: 14.409,20.030,31.302,43.316,57.132,64.351,68.526,69.178,%
Batch: 260 | Loss: 17.092 | Acc: 14.500,20.121,31.373,43.310,57.178,64.428,68.552,69.133,%
Batch: 280 | Loss: 17.104 | Acc: 14.391,20.132,31.475,43.327,57.112,64.388,68.491,69.000,%
Batch: 300 | Loss: 17.102 | Acc: 14.504,20.274,31.489,43.335,57.125,64.431,68.418,68.963,%
Batch: 320 | Loss: 17.110 | Acc: 14.515,20.264,31.520,43.285,57.206,64.374,68.292,68.889,%
Batch: 340 | Loss: 17.147 | Acc: 14.431,20.232,31.426,43.221,57.050,64.285,68.191,68.798,%
Batch: 360 | Loss: 17.152 | Acc: 14.333,20.144,31.389,43.222,57.023,64.249,68.103,68.702,%
Batch: 380 | Loss: 17.161 | Acc: 14.358,20.120,31.375,43.196,57.037,64.179,68.022,68.609,%
Batch: 0 | Loss: 18.273 | Acc: 11.719,19.531,28.125,42.969,57.031,59.375,61.719,62.500,%
Batch: 20 | Loss: 18.554 | Acc: 13.579,18.452,29.129,40.662,53.683,57.924,59.747,60.007,%
Batch: 40 | Loss: 18.576 | Acc: 13.072,18.655,29.135,40.644,52.858,57.698,59.775,60.099,%
Batch: 60 | Loss: 18.611 | Acc: 13.345,18.571,28.778,40.574,52.305,57.134,59.388,59.734,%

Epoch: 18
Batch: 0 | Loss: 18.113 | Acc: 10.156,17.188,28.125,39.062,54.688,61.719,63.281,61.719,%
Batch: 20 | Loss: 16.905 | Acc: 15.365,21.280,31.957,43.006,57.775,66.741,71.243,71.280,%
Batch: 40 | Loss: 16.861 | Acc: 14.844,20.636,31.707,43.350,58.041,66.482,70.713,71.284,%
Batch: 60 | Loss: 16.880 | Acc: 14.536,20.940,31.903,43.635,58.376,66.278,70.530,71.209,%
Batch: 80 | Loss: 16.868 | Acc: 14.603,20.640,32.089,44.155,58.642,66.194,70.419,71.161,%
Batch: 100 | Loss: 16.833 | Acc: 14.573,20.661,32.186,44.353,58.601,65.903,70.135,70.746,%
Batch: 120 | Loss: 16.791 | Acc: 14.657,20.732,32.290,44.493,58.652,66.103,70.248,70.965,%
Batch: 140 | Loss: 16.788 | Acc: 14.722,20.889,32.342,44.515,58.682,66.035,70.157,70.783,%
Batch: 160 | Loss: 16.779 | Acc: 14.722,20.880,32.114,44.468,58.754,66.183,70.235,70.905,%
Batch: 180 | Loss: 16.772 | Acc: 14.714,20.869,32.161,44.410,58.633,66.013,70.213,70.882,%
Batch: 200 | Loss: 16.813 | Acc: 14.611,20.756,32.101,44.178,58.434,65.924,70.079,70.655,%
Batch: 220 | Loss: 16.826 | Acc: 14.593,20.765,32.088,44.107,58.368,65.915,70.037,70.581,%
Batch: 240 | Loss: 16.846 | Acc: 14.571,20.663,31.889,43.983,58.279,65.839,69.995,70.539,%
Batch: 260 | Loss: 16.860 | Acc: 14.511,20.591,31.909,44.013,58.241,65.736,69.846,70.432,%
Batch: 280 | Loss: 16.865 | Acc: 14.646,20.629,32.045,44.103,58.241,65.650,69.765,70.318,%
Batch: 300 | Loss: 16.878 | Acc: 14.641,20.556,31.995,44.121,58.210,65.615,69.677,70.216,%
Batch: 320 | Loss: 16.879 | Acc: 14.647,20.558,32.048,44.178,58.367,65.662,69.651,70.176,%
Batch: 340 | Loss: 16.897 | Acc: 14.640,20.544,31.949,44.057,58.362,65.689,69.623,70.148,%
Batch: 360 | Loss: 16.895 | Acc: 14.653,20.548,31.945,44.077,58.369,65.705,69.624,70.118,%
Batch: 380 | Loss: 16.922 | Acc: 14.622,20.507,31.933,44.035,58.290,65.566,69.464,69.952,%
Batch: 0 | Loss: 17.370 | Acc: 16.406,20.312,31.250,46.094,56.250,65.625,69.531,67.969,%
Batch: 20 | Loss: 18.916 | Acc: 13.653,20.164,30.097,41.146,53.162,57.106,60.268,60.119,%
Batch: 40 | Loss: 18.955 | Acc: 13.472,19.569,29.649,40.835,52.572,57.107,59.832,59.680,%
Batch: 60 | Loss: 18.911 | Acc: 13.665,19.173,29.726,40.920,52.318,56.865,59.324,59.221,%

Epoch: 19
Batch: 0 | Loss: 16.077 | Acc: 14.844,21.875,30.469,47.656,59.375,67.969,75.781,73.438,%
Batch: 20 | Loss: 16.560 | Acc: 13.876,20.796,33.296,46.205,59.338,67.708,72.061,71.689,%
Batch: 40 | Loss: 16.554 | Acc: 14.672,20.960,32.908,45.770,59.108,67.778,71.970,72.123,%
Batch: 60 | Loss: 16.556 | Acc: 14.575,21.043,32.915,45.312,59.541,67.943,71.580,71.798,%
Batch: 80 | Loss: 16.613 | Acc: 14.477,20.853,32.330,44.878,59.221,67.323,71.190,71.566,%
Batch: 100 | Loss: 16.604 | Acc: 14.411,20.869,32.263,44.794,59.244,67.033,71.218,71.496,%
Batch: 120 | Loss: 16.646 | Acc: 14.611,20.894,32.270,44.447,58.968,66.619,70.997,71.346,%
Batch: 140 | Loss: 16.669 | Acc: 14.666,20.950,32.297,44.276,58.644,66.417,70.822,71.254,%
Batch: 160 | Loss: 16.629 | Acc: 14.737,20.963,32.361,44.468,58.836,66.547,70.885,71.254,%
Batch: 180 | Loss: 16.606 | Acc: 14.757,21.111,32.454,44.713,58.982,66.557,70.852,71.215,%
Batch: 200 | Loss: 16.618 | Acc: 14.844,21.094,32.389,44.714,58.936,66.597,70.888,71.238,%
Batch: 220 | Loss: 16.617 | Acc: 14.897,21.016,32.378,44.740,59.018,66.583,70.822,71.189,%
Batch: 240 | Loss: 16.655 | Acc: 14.831,20.899,32.323,44.674,58.895,66.387,70.611,70.987,%
Batch: 260 | Loss: 16.673 | Acc: 14.775,20.836,32.211,44.546,58.794,66.307,70.489,70.947,%
Batch: 280 | Loss: 16.680 | Acc: 14.852,20.852,32.201,44.573,58.786,66.289,70.518,70.969,%
Batch: 300 | Loss: 16.679 | Acc: 14.748,20.806,32.158,44.479,58.700,66.323,70.507,70.943,%
Batch: 320 | Loss: 16.671 | Acc: 14.778,20.838,32.209,44.449,58.672,66.289,70.517,70.933,%
Batch: 340 | Loss: 16.654 | Acc: 14.819,20.913,32.247,44.492,58.754,66.340,70.558,70.963,%
Batch: 360 | Loss: 16.675 | Acc: 14.811,20.897,32.215,44.468,58.728,66.292,70.473,70.886,%
Batch: 380 | Loss: 16.680 | Acc: 14.895,20.942,32.245,44.480,58.795,66.242,70.421,70.805,%
Batch: 0 | Loss: 17.256 | Acc: 13.281,21.094,32.812,45.312,57.812,59.375,64.062,66.406,%
Batch: 20 | Loss: 18.467 | Acc: 13.988,19.234,29.799,41.853,52.976,58.371,60.119,60.082,%
Batch: 40 | Loss: 18.504 | Acc: 14.082,19.474,29.649,40.892,52.344,57.812,59.889,59.585,%
Batch: 60 | Loss: 18.557 | Acc: 14.203,19.160,29.662,40.382,51.960,57.556,59.708,59.388,%

Epoch: 20
Batch: 0 | Loss: 17.898 | Acc: 10.156,15.625,25.781,42.188,57.812,62.500,67.188,67.188,%
Batch: 20 | Loss: 16.555 | Acc: 13.728,20.610,31.808,42.857,58.891,67.001,71.652,72.805,%
Batch: 40 | Loss: 16.380 | Acc: 14.139,20.884,32.050,43.807,59.508,67.607,72.046,72.999,%
Batch: 60 | Loss: 16.351 | Acc: 13.896,20.914,32.185,44.301,59.593,67.520,71.965,72.848,%
Batch: 80 | Loss: 16.342 | Acc: 14.236,21.055,32.176,44.290,59.481,67.245,71.721,72.521,%
Batch: 100 | Loss: 16.385 | Acc: 14.411,20.784,32.070,44.369,59.530,67.211,71.844,72.687,%
Batch: 120 | Loss: 16.456 | Acc: 14.728,20.881,32.096,44.428,59.291,66.916,71.436,72.198,%
Batch: 140 | Loss: 16.441 | Acc: 14.827,21.016,32.192,44.509,59.441,66.988,71.504,72.274,%
Batch: 160 | Loss: 16.452 | Acc: 14.684,20.943,32.308,44.556,59.419,67.037,71.613,72.394,%
Batch: 180 | Loss: 16.436 | Acc: 14.684,20.887,32.415,44.553,59.474,67.231,71.664,72.410,%
Batch: 200 | Loss: 16.456 | Acc: 14.684,20.833,32.408,44.605,59.480,67.133,71.479,72.132,%
Batch: 220 | Loss: 16.443 | Acc: 14.653,20.906,32.501,44.712,59.615,67.219,71.557,72.250,%
Batch: 240 | Loss: 16.462 | Acc: 14.756,21.022,32.530,44.680,59.514,67.197,71.434,72.057,%
Batch: 260 | Loss: 16.439 | Acc: 14.790,21.049,32.645,44.843,59.546,67.286,71.525,72.094,%
Batch: 280 | Loss: 16.456 | Acc: 14.810,21.049,32.623,44.812,59.489,67.254,71.455,72.011,%
Batch: 300 | Loss: 16.460 | Acc: 14.880,21.161,32.740,44.882,59.549,67.252,71.442,71.948,%
Batch: 320 | Loss: 16.467 | Acc: 14.846,21.067,32.666,44.938,59.511,67.178,71.388,71.873,%
Batch: 340 | Loss: 16.494 | Acc: 14.853,21.128,32.686,44.859,59.414,67.071,71.240,71.731,%
Batch: 360 | Loss: 16.494 | Acc: 14.878,21.152,32.717,44.940,59.501,67.058,71.180,71.650,%
Batch: 380 | Loss: 16.493 | Acc: 14.887,21.163,32.769,44.984,59.545,67.067,71.129,71.615,%
Batch: 0 | Loss: 17.346 | Acc: 13.281,21.875,30.469,40.625,56.250,62.500,66.406,64.062,%
Batch: 20 | Loss: 18.414 | Acc: 12.909,17.783,28.571,41.778,55.208,59.561,61.012,60.789,%
Batch: 40 | Loss: 18.353 | Acc: 12.843,17.854,29.192,42.016,54.630,59.566,61.433,61.471,%
Batch: 60 | Loss: 18.344 | Acc: 13.089,17.444,29.226,41.829,54.508,59.529,61.411,61.283,%

Epoch: 21
Batch: 0 | Loss: 16.402 | Acc: 16.406,21.094,28.906,46.094,64.844,73.438,76.562,79.688,%
Batch: 20 | Loss: 16.154 | Acc: 15.402,22.284,33.891,47.433,60.454,69.048,73.996,74.368,%
Batch: 40 | Loss: 16.215 | Acc: 15.263,21.799,33.613,46.341,60.366,68.998,73.380,74.333,%
Batch: 60 | Loss: 16.216 | Acc: 15.151,21.632,33.222,46.068,60.272,69.070,73.578,74.232,%
Batch: 80 | Loss: 16.252 | Acc: 14.940,21.528,33.015,45.891,60.108,68.673,73.129,73.949,%
Batch: 100 | Loss: 16.233 | Acc: 14.867,21.457,32.890,45.792,60.311,68.642,73.097,73.809,%
Batch: 120 | Loss: 16.234 | Acc: 15.083,21.494,32.974,45.700,60.311,68.556,72.895,73.515,%
Batch: 140 | Loss: 16.226 | Acc: 15.082,21.365,32.940,45.850,60.295,68.545,72.856,73.521,%
Batch: 160 | Loss: 16.267 | Acc: 15.091,21.322,32.871,45.715,60.113,68.464,72.812,73.433,%
Batch: 180 | Loss: 16.259 | Acc: 15.085,21.430,33.084,45.822,60.182,68.383,72.730,73.317,%
Batch: 200 | Loss: 16.267 | Acc: 15.108,21.405,33.026,45.775,60.020,68.229,72.477,73.041,%
Batch: 220 | Loss: 16.286 | Acc: 15.109,21.440,33.088,45.673,59.937,68.089,72.366,72.865,%
Batch: 240 | Loss: 16.278 | Acc: 15.155,21.551,33.221,45.695,59.959,68.008,72.309,72.815,%
Batch: 260 | Loss: 16.278 | Acc: 15.149,21.432,33.148,45.684,59.986,67.951,72.216,72.725,%
Batch: 280 | Loss: 16.290 | Acc: 15.166,21.475,33.188,45.688,60.048,67.877,72.125,72.626,%
Batch: 300 | Loss: 16.289 | Acc: 15.096,21.530,33.186,45.671,60.071,67.886,72.129,72.623,%
Batch: 320 | Loss: 16.304 | Acc: 15.073,21.503,33.185,45.639,60.035,67.849,72.089,72.571,%
Batch: 340 | Loss: 16.306 | Acc: 15.061,21.513,33.236,45.743,60.053,67.868,72.063,72.542,%
Batch: 360 | Loss: 16.320 | Acc: 15.002,21.514,33.260,45.773,60.074,67.800,71.959,72.436,%
Batch: 380 | Loss: 16.341 | Acc: 15.030,21.479,33.188,45.624,59.922,67.684,71.820,72.299,%
Batch: 0 | Loss: 17.395 | Acc: 14.062,24.219,28.125,45.312,57.031,60.156,64.062,65.625,%
Batch: 20 | Loss: 18.362 | Acc: 15.067,19.196,30.469,42.374,54.762,58.631,61.272,61.682,%
Batch: 40 | Loss: 18.305 | Acc: 14.558,19.379,30.640,42.816,54.040,58.479,60.842,60.957,%
Batch: 60 | Loss: 18.322 | Acc: 14.664,19.121,30.546,42.239,53.842,58.491,60.873,60.912,%

Epoch: 22
Batch: 0 | Loss: 16.768 | Acc: 12.500,17.969,28.125,40.625,57.812,65.625,67.969,68.750,%
Batch: 20 | Loss: 16.358 | Acc: 15.476,21.949,33.333,44.940,60.193,68.787,73.103,74.405,%
Batch: 40 | Loss: 16.086 | Acc: 15.758,22.218,34.184,46.399,61.166,69.874,74.200,75.781,%
Batch: 60 | Loss: 16.152 | Acc: 15.510,21.593,33.709,46.311,61.091,69.941,74.091,75.269,%
Batch: 80 | Loss: 16.148 | Acc: 15.422,21.595,33.517,46.152,61.073,69.898,74.132,75.068,%
Batch: 100 | Loss: 16.117 | Acc: 15.316,21.705,33.524,45.985,60.852,69.833,74.203,74.915,%
Batch: 120 | Loss: 16.129 | Acc: 15.334,21.746,33.561,46.016,60.757,69.654,73.935,74.567,%
Batch: 140 | Loss: 16.078 | Acc: 15.326,21.903,33.738,46.166,61.004,69.720,73.875,74.573,%
Batch: 160 | Loss: 16.080 | Acc: 15.416,21.948,33.691,46.128,60.923,69.609,73.831,74.486,%
Batch: 180 | Loss: 16.105 | Acc: 15.349,21.763,33.615,46.007,60.864,69.458,73.696,74.340,%
Batch: 200 | Loss: 16.076 | Acc: 15.388,21.953,33.823,46.090,60.945,69.523,73.787,74.359,%
Batch: 220 | Loss: 16.081 | Acc: 15.385,21.949,33.816,46.168,60.906,69.358,73.653,74.141,%
Batch: 240 | Loss: 16.072 | Acc: 15.418,21.937,33.860,46.233,61.015,69.385,73.567,74.027,%
Batch: 260 | Loss: 16.085 | Acc: 15.377,21.836,33.788,46.216,61.030,69.340,73.494,73.940,%
Batch: 280 | Loss: 16.086 | Acc: 15.378,21.861,33.825,46.202,60.938,69.264,73.404,73.913,%
Batch: 300 | Loss: 16.110 | Acc: 15.298,21.792,33.653,46.096,60.847,69.093,73.238,73.676,%
Batch: 320 | Loss: 16.124 | Acc: 15.318,21.848,33.647,46.084,60.743,68.976,73.089,73.552,%
Batch: 340 | Loss: 16.134 | Acc: 15.265,21.815,33.617,46.110,60.729,68.947,73.039,73.449,%
Batch: 360 | Loss: 16.142 | Acc: 15.251,21.804,33.572,46.107,60.663,68.817,72.920,73.321,%
Batch: 380 | Loss: 16.156 | Acc: 15.229,21.779,33.569,46.063,60.626,68.760,72.853,73.230,%
Batch: 0 | Loss: 17.193 | Acc: 13.281,18.750,29.688,48.438,60.156,68.750,71.094,68.750,%
Batch: 20 | Loss: 18.175 | Acc: 15.290,20.685,31.808,43.080,55.804,60.305,60.975,61.384,%
Batch: 40 | Loss: 18.212 | Acc: 14.520,20.522,31.879,42.721,54.764,59.966,61.280,61.395,%
Batch: 60 | Loss: 18.167 | Acc: 14.793,20.620,31.839,42.713,54.995,59.785,61.373,61.322,%

Epoch: 23
Batch: 0 | Loss: 14.968 | Acc: 20.312,25.000,35.156,44.531,67.969,71.094,75.000,78.125,%
Batch: 20 | Loss: 15.765 | Acc: 15.067,22.470,35.789,48.177,62.909,70.833,74.628,75.335,%
Batch: 40 | Loss: 15.788 | Acc: 15.111,21.704,34.318,47.294,62.500,70.408,75.133,75.705,%
Batch: 60 | Loss: 15.756 | Acc: 15.010,21.593,34.029,46.606,62.013,70.389,74.885,75.448,%
Batch: 80 | Loss: 15.744 | Acc: 15.278,22.020,34.230,46.827,62.442,70.544,74.942,75.502,%
Batch: 100 | Loss: 15.754 | Acc: 15.122,21.728,34.004,46.620,62.299,70.282,74.683,75.456,%
Batch: 120 | Loss: 15.787 | Acc: 15.251,21.785,34.046,46.559,62.048,70.022,74.548,75.271,%
Batch: 140 | Loss: 15.829 | Acc: 15.331,21.953,34.043,46.476,61.863,69.875,74.335,75.039,%
Batch: 160 | Loss: 15.823 | Acc: 15.232,21.860,34.026,46.584,61.932,69.963,74.364,75.019,%
Batch: 180 | Loss: 15.837 | Acc: 15.448,21.974,34.017,46.521,61.870,69.807,74.094,74.754,%
Batch: 200 | Loss: 15.866 | Acc: 15.481,21.945,33.998,46.412,61.660,69.753,74.024,74.650,%
Batch: 220 | Loss: 15.865 | Acc: 15.448,21.917,34.092,46.490,61.736,69.747,74.003,74.618,%
Batch: 240 | Loss: 15.867 | Acc: 15.515,22.057,34.112,46.593,61.806,69.781,73.985,74.621,%
Batch: 260 | Loss: 15.897 | Acc: 15.571,22.052,34.055,46.525,61.674,69.705,73.878,74.467,%
Batch: 280 | Loss: 15.912 | Acc: 15.525,22.017,33.986,46.452,61.555,69.584,73.768,74.347,%
Batch: 300 | Loss: 15.917 | Acc: 15.529,22.080,33.957,46.470,61.534,69.664,73.765,74.351,%
Batch: 320 | Loss: 15.927 | Acc: 15.408,22.023,33.988,46.561,61.509,69.616,73.730,74.287,%
Batch: 340 | Loss: 15.947 | Acc: 15.387,22.024,33.967,46.559,61.421,69.554,73.618,74.139,%
Batch: 360 | Loss: 15.952 | Acc: 15.329,22.001,33.968,46.546,61.385,69.481,73.539,74.050,%
Batch: 380 | Loss: 15.969 | Acc: 15.285,21.996,33.934,46.531,61.364,69.412,73.479,73.989,%
Batch: 0 | Loss: 17.492 | Acc: 12.500,21.875,34.375,45.312,59.375,66.406,66.406,66.406,%
Batch: 20 | Loss: 18.281 | Acc: 14.732,19.792,29.725,42.969,55.394,60.677,62.649,62.091,%
Batch: 40 | Loss: 18.229 | Acc: 14.253,19.341,29.211,42.969,54.306,60.023,62.024,61.966,%
Batch: 60 | Loss: 18.193 | Acc: 14.255,19.582,29.355,42.943,54.547,60.284,62.129,62.218,%

Epoch: 24
Batch: 0 | Loss: 15.371 | Acc: 14.844,22.656,31.250,47.656,66.406,71.094,78.906,78.125,%
Batch: 20 | Loss: 15.377 | Acc: 15.551,23.065,35.007,48.103,63.132,71.912,76.674,77.418,%
Batch: 40 | Loss: 15.493 | Acc: 15.244,22.504,34.242,47.466,62.633,71.818,76.734,77.439,%
Batch: 60 | Loss: 15.542 | Acc: 15.318,22.374,34.234,47.515,62.218,71.516,76.165,76.755,%
Batch: 80 | Loss: 15.604 | Acc: 15.355,22.348,34.066,47.049,61.777,70.785,75.540,76.206,%
Batch: 100 | Loss: 15.586 | Acc: 15.563,22.409,34.035,47.068,61.928,71.078,75.758,76.269,%
Batch: 120 | Loss: 15.609 | Acc: 15.302,22.107,34.026,46.914,61.990,71.074,75.620,76.175,%
Batch: 140 | Loss: 15.653 | Acc: 15.198,22.063,34.087,46.997,62.201,71.149,75.587,76.108,%
Batch: 160 | Loss: 15.662 | Acc: 15.232,21.953,34.030,46.957,62.131,71.128,75.543,76.024,%
Batch: 180 | Loss: 15.709 | Acc: 15.327,21.892,34.034,46.789,61.939,70.822,75.302,75.811,%
Batch: 200 | Loss: 15.717 | Acc: 15.442,22.116,34.216,46.821,61.968,70.713,75.117,75.676,%
Batch: 220 | Loss: 15.716 | Acc: 15.522,22.165,34.258,46.864,61.878,70.592,75.042,75.559,%
Batch: 240 | Loss: 15.727 | Acc: 15.469,22.050,34.252,46.830,61.845,70.572,74.997,75.493,%
Batch: 260 | Loss: 15.727 | Acc: 15.550,22.138,34.330,46.887,61.767,70.456,74.778,75.290,%
Batch: 280 | Loss: 15.749 | Acc: 15.586,22.239,34.328,46.842,61.730,70.296,74.569,75.061,%
Batch: 300 | Loss: 15.753 | Acc: 15.581,22.231,34.297,46.836,61.747,70.287,74.559,75.062,%
Batch: 320 | Loss: 15.785 | Acc: 15.581,22.240,34.309,46.751,61.668,70.110,74.396,74.942,%
Batch: 340 | Loss: 15.809 | Acc: 15.536,22.232,34.247,46.728,61.595,70.001,74.255,74.780,%
Batch: 360 | Loss: 15.818 | Acc: 15.506,22.241,34.245,46.719,61.582,69.984,74.160,74.652,%
Batch: 380 | Loss: 15.823 | Acc: 15.475,22.242,34.289,46.699,61.626,69.931,74.102,74.584,%
