Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=0 | lr=1.0e-02 | circles=3 
Training: Epoch=0 | Loss: nan |  Acc: 1.000,0.990,0.998,0.994,1.006,0.990,0.990,0.994,% 
Testing: Epoch=0 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=1 | lr=1.0e-02 | circles=3 
Training: Epoch=1 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=1 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=2 | lr=1.0e-02 | circles=3 
Training: Epoch=2 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=2 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=3 | lr=1.0e-02 | circles=3 
Training: Epoch=3 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=3 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=4 | lr=1.0e-02 | circles=3 
Training: Epoch=4 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=4 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=5 | lr=1.0e-02 | circles=3 
Training: Epoch=5 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=5 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=6 | lr=1.0e-02 | circles=3 
Training: Epoch=6 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=6 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=7 | lr=1.0e-02 | circles=3 
Training: Epoch=7 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=7 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=8 | lr=1.0e-02 | circles=3 
Training: Epoch=8 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=8 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=9 | lr=1.0e-02 | circles=3 
Training: Epoch=9 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=9 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=10 | lr=1.0e-02 | circles=3 
Training: Epoch=10 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=10 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=11 | lr=1.0e-02 | circles=3 
Training: Epoch=11 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=11 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=12 | lr=1.0e-02 | circles=3 
Training: Epoch=12 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=12 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=13 | lr=1.0e-02 | circles=3 
Training: Epoch=13 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=13 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=14 | lr=1.0e-02 | circles=3 
Training: Epoch=14 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=14 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=15 | lr=1.0e-02 | circles=3 
Training: Epoch=15 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=15 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=16 | lr=1.0e-02 | circles=3 
Training: Epoch=16 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=16 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=17 | lr=1.0e-02 | circles=3 
Training: Epoch=17 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=17 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=18 | lr=1.0e-02 | circles=3 
Training: Epoch=18 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=18 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=19 | lr=1.0e-02 | circles=3 
Training: Epoch=19 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=19 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=20 | lr=1.0e-02 | circles=3 
Training: Epoch=20 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=20 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=21 | lr=1.0e-02 | circles=3 
Training: Epoch=21 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=21 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=22 | lr=1.0e-02 | circles=3 
Training: Epoch=22 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=22 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=23 | lr=1.0e-02 | circles=3 
Training: Epoch=23 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=23 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=24 | lr=1.0e-02 | circles=3 
Training: Epoch=24 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=24 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=25 | lr=1.0e-02 | circles=3 
Training: Epoch=25 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=25 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=26 | lr=1.0e-02 | circles=3 
Training: Epoch=26 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=26 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=27 | lr=1.0e-02 | circles=3 
Training: Epoch=27 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=27 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=28 | lr=1.0e-02 | circles=3 
Training: Epoch=28 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=28 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=29 | lr=1.0e-02 | circles=3 
Training: Epoch=29 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=29 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=30 | lr=1.0e-02 | circles=3 
Training: Epoch=30 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=30 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=31 | lr=1.0e-02 | circles=3 
Training: Epoch=31 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=31 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=32 | lr=1.0e-02 | circles=3 
Training: Epoch=32 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=32 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=33 | lr=1.0e-02 | circles=3 
Training: Epoch=33 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=33 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=34 | lr=1.0e-02 | circles=3 
Training: Epoch=34 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=34 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=35 | lr=1.0e-02 | circles=3 
Training: Epoch=35 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=35 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=36 | lr=1.0e-02 | circles=3 
Training: Epoch=36 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=36 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=37 | lr=1.0e-02 | circles=3 
Training: Epoch=37 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=37 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=38 | lr=1.0e-02 | circles=3 
Training: Epoch=38 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=38 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=39 | lr=1.0e-02 | circles=3 
Training: Epoch=39 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=39 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=40 | lr=1.0e-02 | circles=3 
Training: Epoch=40 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=40 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=41 | lr=1.0e-02 | circles=3 
Training: Epoch=41 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=41 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=42 | lr=1.0e-02 | circles=3 
Training: Epoch=42 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=42 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=43 | lr=1.0e-02 | circles=3 
Training: Epoch=43 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=43 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=44 | lr=1.0e-02 | circles=3 
Training: Epoch=44 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=44 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=45 | lr=1.0e-02 | circles=3 
Training: Epoch=45 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=45 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=46 | lr=1.0e-02 | circles=3 
Training: Epoch=46 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=46 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=47 | lr=1.0e-02 | circles=3 
Training: Epoch=47 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=47 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=48 | lr=1.0e-02 | circles=3 
Training: Epoch=48 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=48 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=49 | lr=1.0e-02 | circles=3 
Training: Epoch=49 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=49 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=50 | lr=1.0e-02 | circles=3 
Training: Epoch=50 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=50 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=51 | lr=1.0e-02 | circles=3 
Training: Epoch=51 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=51 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=52 | lr=1.0e-02 | circles=3 
Training: Epoch=52 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=52 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=53 | lr=1.0e-02 | circles=3 
Training: Epoch=53 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=53 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=54 | lr=1.0e-02 | circles=3 
Training: Epoch=54 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=54 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=55 | lr=1.0e-02 | circles=3 
Training: Epoch=55 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=55 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=56 | lr=1.0e-02 | circles=3 
Training: Epoch=56 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=56 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=57 | lr=1.0e-02 | circles=3 
Training: Epoch=57 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=57 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=58 | lr=1.0e-02 | circles=3 
Training: Epoch=58 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=58 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=59 | lr=1.0e-02 | circles=3 
Training: Epoch=59 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=59 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=60 | lr=1.0e-02 | circles=3 
Training: Epoch=60 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=60 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=61 | lr=1.0e-02 | circles=3 
Training: Epoch=61 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=61 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=62 | lr=1.0e-02 | circles=3 
Training: Epoch=62 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=62 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=63 | lr=1.0e-02 | circles=3 
Training: Epoch=63 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=63 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=64 | lr=1.0e-02 | circles=3 
Training: Epoch=64 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=64 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=65 | lr=1.0e-02 | circles=3 
Training: Epoch=65 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=65 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=66 | lr=1.0e-02 | circles=3 
Training: Epoch=66 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=66 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=67 | lr=1.0e-02 | circles=3 
Training: Epoch=67 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=67 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=68 | lr=1.0e-02 | circles=3 
Training: Epoch=68 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=68 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=69 | lr=1.0e-02 | circles=3 
Training: Epoch=69 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=69 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=70 | lr=1.0e-02 | circles=3 
Training: Epoch=70 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=70 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=71 | lr=1.0e-02 | circles=3 
Training: Epoch=71 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=71 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=72 | lr=1.0e-02 | circles=3 
Training: Epoch=72 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=72 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=73 | lr=1.0e-02 | circles=3 
Training: Epoch=73 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=73 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=74 | lr=1.0e-02 | circles=3 
Training: Epoch=74 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=74 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=75 | lr=1.0e-02 | circles=3 
Training: Epoch=75 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=75 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=76 | lr=1.0e-02 | circles=3 
Training: Epoch=76 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=76 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=77 | lr=1.0e-02 | circles=3 
Training: Epoch=77 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=77 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=78 | lr=1.0e-02 | circles=3 
Training: Epoch=78 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=78 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=79 | lr=1.0e-02 | circles=3 
Training: Epoch=79 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=79 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=80 | lr=1.0e-02 | circles=3 
Training: Epoch=80 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=80 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=81 | lr=1.0e-02 | circles=3 
Training: Epoch=81 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=81 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=82 | lr=1.0e-02 | circles=3 
Training: Epoch=82 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=82 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=83 | lr=1.0e-02 | circles=3 
Training: Epoch=83 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=83 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=84 | lr=1.0e-02 | circles=3 
Training: Epoch=84 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=84 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=85 | lr=1.0e-02 | circles=3 
Training: Epoch=85 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=85 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=86 | lr=1.0e-02 | circles=3 
Training: Epoch=86 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=86 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=87 | lr=1.0e-02 | circles=3 
Training: Epoch=87 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=87 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=88 | lr=1.0e-02 | circles=3 
Training: Epoch=88 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=88 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=89 | lr=1.0e-02 | circles=3 
Training: Epoch=89 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=89 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=90 | lr=1.0e-02 | circles=3 
Training: Epoch=90 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=90 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=91 | lr=1.0e-02 | circles=3 
Training: Epoch=91 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=91 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=92 | lr=1.0e-02 | circles=3 
Training: Epoch=92 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=92 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=93 | lr=1.0e-02 | circles=3 
Training: Epoch=93 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=93 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=94 | lr=1.0e-02 | circles=3 
Training: Epoch=94 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=94 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=95 | lr=1.0e-02 | circles=3 
Training: Epoch=95 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=95 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=96 | lr=1.0e-02 | circles=3 
Training: Epoch=96 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=96 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=97 | lr=1.0e-02 | circles=3 
Training: Epoch=97 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=97 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=98 | lr=1.0e-02 | circles=3 
Training: Epoch=98 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=98 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=99 | lr=1.0e-02 | circles=3 
Training: Epoch=99 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=99 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 


==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=64, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=64, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=164, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=164, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (3): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (4): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (5): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (6): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (7): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Batch: 0 | Loss: 63.271 | Acc: 1.562,0.781,0.781,1.562,1.562,2.344,0.000,1.562,%
Batch: 20 | Loss: nan | Acc: 0.818,0.632,0.781,0.707,0.930,0.632,0.632,0.707,%
Batch: 40 | Loss: nan | Acc: 0.896,0.800,0.877,0.838,0.953,0.800,0.800,0.838,%
Batch: 60 | Loss: nan | Acc: 0.948,0.884,0.935,0.909,0.986,0.884,0.884,0.909,%
Batch: 80 | Loss: nan | Acc: 1.032,0.984,1.022,1.003,1.061,0.984,0.984,1.003,%
Batch: 100 | Loss: nan | Acc: 1.037,0.998,1.029,1.013,1.060,0.998,0.998,1.013,%
Batch: 120 | Loss: nan | Acc: 1.085,1.052,1.078,1.065,1.104,1.052,1.052,1.065,%
Batch: 140 | Loss: nan | Acc: 1.036,1.008,1.031,1.020,1.053,1.008,1.008,1.020,%
Batch: 160 | Loss: nan | Acc: 1.043,1.019,1.038,1.029,1.058,1.019,1.019,1.029,%
Batch: 180 | Loss: nan | Acc: 1.014,0.993,1.010,1.001,1.027,0.993,0.993,1.001,%
Batch: 200 | Loss: nan | Acc: 1.042,1.022,1.038,1.030,1.053,1.022,1.022,1.030,%
Batch: 220 | Loss: nan | Acc: 1.046,1.029,1.043,1.036,1.057,1.029,1.029,1.036,%
Batch: 240 | Loss: nan | Acc: 1.063,1.047,1.060,1.054,1.073,1.047,1.047,1.054,%
Batch: 260 | Loss: nan | Acc: 1.066,1.051,1.063,1.057,1.075,1.051,1.051,1.057,%
Batch: 280 | Loss: nan | Acc: 1.059,1.045,1.056,1.051,1.068,1.045,1.045,1.051,%
Batch: 300 | Loss: nan | Acc: 1.054,1.041,1.051,1.046,1.062,1.041,1.041,1.046,%
Batch: 320 | Loss: nan | Acc: 1.039,1.027,1.037,1.032,1.047,1.027,1.027,1.032,%
Batch: 340 | Loss: nan | Acc: 1.038,1.026,1.036,1.031,1.045,1.026,1.026,1.031,%
Batch: 360 | Loss: nan | Acc: 1.019,1.008,1.017,1.013,1.026,1.008,1.008,1.013,%
Batch: 380 | Loss: nan | Acc: 1.005,0.995,1.003,0.999,1.011,0.995,0.995,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 1
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 80 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 140 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 160 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 180 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 280 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 300 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 320 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 2
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.076,1.076,1.076,1.076,1.076,1.076,1.076,1.076,%
Batch: 80 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 100 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 160 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 180 | Loss: nan | Acc: 1.049,1.049,1.049,1.049,1.049,1.049,1.049,1.049,%
Batch: 200 | Loss: nan | Acc: 1.069,1.069,1.069,1.069,1.069,1.069,1.069,1.069,%
Batch: 220 | Loss: nan | Acc: 1.071,1.071,1.071,1.071,1.071,1.071,1.071,1.071,%
Batch: 240 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 260 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 280 | Loss: nan | Acc: 1.062,1.062,1.062,1.062,1.062,1.062,1.062,1.062,%
Batch: 300 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 320 | Loss: nan | Acc: 1.049,1.049,1.049,1.049,1.049,1.049,1.049,1.049,%
Batch: 340 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 360 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 380 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 3
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.877,0.877,0.877,0.877,0.877,0.877,0.877,0.877,%
Batch: 60 | Loss: nan | Acc: 0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,%
Batch: 80 | Loss: nan | Acc: 0.839,0.839,0.839,0.839,0.839,0.839,0.839,0.839,%
Batch: 100 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 120 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 180 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 200 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 220 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 320 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 4
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.857,0.857,0.857,0.857,0.857,0.857,0.857,0.857,%
Batch: 60 | Loss: nan | Acc: 0.909,0.909,0.909,0.909,0.909,0.909,0.909,0.909,%
Batch: 80 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 100 | Loss: nan | Acc: 0.928,0.928,0.928,0.928,0.928,0.928,0.928,0.928,%
Batch: 120 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 180 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 200 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 220 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 240 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 260 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 280 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 300 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 320 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 340 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 360 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 5
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 80 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 100 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 120 | Loss: nan | Acc: 1.059,1.059,1.059,1.059,1.059,1.059,1.059,1.059,%
Batch: 140 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 160 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 180 | Loss: nan | Acc: 1.066,1.066,1.066,1.066,1.066,1.066,1.066,1.066,%
Batch: 200 | Loss: nan | Acc: 1.065,1.065,1.065,1.065,1.065,1.065,1.065,1.065,%
Batch: 220 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 240 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 260 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 280 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 300 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 320 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 340 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 6
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 40 | Loss: nan | Acc: 1.181,1.181,1.181,1.181,1.181,1.181,1.181,1.181,%
Batch: 60 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 80 | Loss: nan | Acc: 1.109,1.109,1.109,1.109,1.109,1.109,1.109,1.109,%
Batch: 100 | Loss: nan | Acc: 1.083,1.083,1.083,1.083,1.083,1.083,1.083,1.083,%
Batch: 120 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 140 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 160 | Loss: nan | Acc: 1.077,1.077,1.077,1.077,1.077,1.077,1.077,1.077,%
Batch: 180 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 200 | Loss: nan | Acc: 1.038,1.038,1.038,1.038,1.038,1.038,1.038,1.038,%
Batch: 220 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 320 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 7
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.190,1.190,1.190,1.190,1.190,1.190,1.190,1.190,%
Batch: 40 | Loss: nan | Acc: 1.105,1.105,1.105,1.105,1.105,1.105,1.105,1.105,%
Batch: 60 | Loss: nan | Acc: 1.127,1.127,1.127,1.127,1.127,1.127,1.127,1.127,%
Batch: 80 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 100 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 120 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 140 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 160 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 180 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 200 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 220 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 240 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 260 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 280 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 8
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 1.105,1.105,1.105,1.105,1.105,1.105,1.105,1.105,%
Batch: 60 | Loss: nan | Acc: 1.089,1.089,1.089,1.089,1.089,1.089,1.089,1.089,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 0.944,0.944,0.944,0.944,0.944,0.944,0.944,0.944,%
Batch: 120 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 180 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 200 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 220 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 280 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 300 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 320 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 340 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 360 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 9
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 160 | Loss: nan | Acc: 1.077,1.077,1.077,1.077,1.077,1.077,1.077,1.077,%
Batch: 180 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 200 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 220 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 240 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 260 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 280 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 300 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 320 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 340 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 10
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 100 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 120 | Loss: nan | Acc: 0.917,0.917,0.917,0.917,0.917,0.917,0.917,0.917,%
Batch: 140 | Loss: nan | Acc: 0.925,0.925,0.925,0.925,0.925,0.925,0.925,0.925,%
Batch: 160 | Loss: nan | Acc: 0.946,0.946,0.946,0.946,0.946,0.946,0.946,0.946,%
Batch: 180 | Loss: nan | Acc: 0.958,0.958,0.958,0.958,0.958,0.958,0.958,0.958,%
Batch: 200 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 220 | Loss: nan | Acc: 0.940,0.940,0.940,0.940,0.940,0.940,0.940,0.940,%
Batch: 240 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 260 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 280 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 300 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 320 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 340 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 360 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 380 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 11
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 120 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 140 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 160 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 180 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 200 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 220 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 240 | Loss: nan | Acc: 1.054,1.054,1.054,1.054,1.054,1.054,1.054,1.054,%
Batch: 260 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 280 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 300 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 320 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 12
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.896,0.896,0.896,0.896,0.896,0.896,0.896,0.896,%
Batch: 60 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 80 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 100 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 120 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 140 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 200 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 220 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 240 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 300 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 320 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 340 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 360 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 13
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 1.143,1.143,1.143,1.143,1.143,1.143,1.143,1.143,%
Batch: 60 | Loss: nan | Acc: 1.140,1.140,1.140,1.140,1.140,1.140,1.140,1.140,%
Batch: 80 | Loss: nan | Acc: 1.071,1.071,1.071,1.071,1.071,1.071,1.071,1.071,%
Batch: 100 | Loss: nan | Acc: 1.106,1.106,1.106,1.106,1.106,1.106,1.106,1.106,%
Batch: 120 | Loss: nan | Acc: 1.091,1.091,1.091,1.091,1.091,1.091,1.091,1.091,%
Batch: 140 | Loss: nan | Acc: 1.108,1.108,1.108,1.108,1.108,1.108,1.108,1.108,%
Batch: 160 | Loss: nan | Acc: 1.116,1.116,1.116,1.116,1.116,1.116,1.116,1.116,%
Batch: 180 | Loss: nan | Acc: 1.083,1.083,1.083,1.083,1.083,1.083,1.083,1.083,%
Batch: 200 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 220 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 240 | Loss: nan | Acc: 1.041,1.041,1.041,1.041,1.041,1.041,1.041,1.041,%
Batch: 260 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 280 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 300 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 320 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 14
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.670,0.670,0.670,0.670,0.670,0.670,0.670,0.670,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 100 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 120 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 140 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 200 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 220 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 240 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 15
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.724,0.724,0.724,0.724,0.724,0.724,0.724,0.724,%
Batch: 60 | Loss: nan | Acc: 0.768,0.768,0.768,0.768,0.768,0.768,0.768,0.768,%
Batch: 80 | Loss: nan | Acc: 0.839,0.839,0.839,0.839,0.839,0.839,0.839,0.839,%
Batch: 100 | Loss: nan | Acc: 0.797,0.797,0.797,0.797,0.797,0.797,0.797,0.797,%
Batch: 120 | Loss: nan | Acc: 0.865,0.865,0.865,0.865,0.865,0.865,0.865,0.865,%
Batch: 140 | Loss: nan | Acc: 0.875,0.875,0.875,0.875,0.875,0.875,0.875,0.875,%
Batch: 160 | Loss: nan | Acc: 0.912,0.912,0.912,0.912,0.912,0.912,0.912,0.912,%
Batch: 180 | Loss: nan | Acc: 0.924,0.924,0.924,0.924,0.924,0.924,0.924,0.924,%
Batch: 200 | Loss: nan | Acc: 0.933,0.933,0.933,0.933,0.933,0.933,0.933,0.933,%
Batch: 220 | Loss: nan | Acc: 0.947,0.947,0.947,0.947,0.947,0.947,0.947,0.947,%
Batch: 240 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 260 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 280 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 300 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 320 | Loss: nan | Acc: 0.978,0.978,0.978,0.978,0.978,0.978,0.978,0.978,%
Batch: 340 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 360 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 16
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 60 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 120 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 140 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 160 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 180 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 200 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 220 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 240 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 260 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 280 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 340 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 17
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 40 | Loss: nan | Acc: 0.896,0.896,0.896,0.896,0.896,0.896,0.896,0.896,%
Batch: 60 | Loss: nan | Acc: 0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,%
Batch: 80 | Loss: nan | Acc: 0.916,0.916,0.916,0.916,0.916,0.916,0.916,0.916,%
Batch: 100 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 120 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 180 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 200 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 220 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 240 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.049,1.049,1.049,1.049,1.049,1.049,1.049,1.049,%
Batch: 320 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 340 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 360 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 18
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.707,0.707,0.707,0.707,0.707,0.707,0.707,0.707,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 80 | Loss: nan | Acc: 1.100,1.100,1.100,1.100,1.100,1.100,1.100,1.100,%
Batch: 100 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 120 | Loss: nan | Acc: 1.123,1.123,1.123,1.123,1.123,1.123,1.123,1.123,%
Batch: 140 | Loss: nan | Acc: 1.092,1.092,1.092,1.092,1.092,1.092,1.092,1.092,%
Batch: 160 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 180 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 200 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 220 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 240 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 260 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 280 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 300 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 320 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 340 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 19
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 180 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 200 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 220 | Loss: nan | Acc: 0.947,0.947,0.947,0.947,0.947,0.947,0.947,0.947,%
Batch: 240 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 260 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 280 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 300 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 320 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 340 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 360 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 20
Batch: 0 | Loss: nan | Acc: 3.125,3.125,3.125,3.125,3.125,3.125,3.125,3.125,%
Batch: 20 | Loss: nan | Acc: 1.525,1.525,1.525,1.525,1.525,1.525,1.525,1.525,%
Batch: 40 | Loss: nan | Acc: 1.162,1.162,1.162,1.162,1.162,1.162,1.162,1.162,%
Batch: 60 | Loss: nan | Acc: 1.076,1.076,1.076,1.076,1.076,1.076,1.076,1.076,%
Batch: 80 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 100 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 120 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 140 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 160 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 180 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 200 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 220 | Loss: nan | Acc: 1.071,1.071,1.071,1.071,1.071,1.071,1.071,1.071,%
Batch: 240 | Loss: nan | Acc: 1.099,1.099,1.099,1.099,1.099,1.099,1.099,1.099,%
Batch: 260 | Loss: nan | Acc: 1.084,1.084,1.084,1.084,1.084,1.084,1.084,1.084,%
Batch: 280 | Loss: nan | Acc: 1.059,1.059,1.059,1.059,1.059,1.059,1.059,1.059,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 340 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 360 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 380 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 21
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 80 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 100 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 120 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 140 | Loss: nan | Acc: 0.947,0.947,0.947,0.947,0.947,0.947,0.947,0.947,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 0.950,0.950,0.950,0.950,0.950,0.950,0.950,0.950,%
Batch: 200 | Loss: nan | Acc: 0.917,0.917,0.917,0.917,0.917,0.917,0.917,0.917,%
Batch: 220 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 240 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 260 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 280 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 300 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 320 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 340 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 360 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 22
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 80 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 100 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 120 | Loss: nan | Acc: 1.065,1.065,1.065,1.065,1.065,1.065,1.065,1.065,%
Batch: 140 | Loss: nan | Acc: 1.092,1.092,1.092,1.092,1.092,1.092,1.092,1.092,%
Batch: 160 | Loss: nan | Acc: 1.106,1.106,1.106,1.106,1.106,1.106,1.106,1.106,%
Batch: 180 | Loss: nan | Acc: 1.062,1.062,1.062,1.062,1.062,1.062,1.062,1.062,%
Batch: 200 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 220 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 240 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 260 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 280 | Loss: nan | Acc: 1.062,1.062,1.062,1.062,1.062,1.062,1.062,1.062,%
Batch: 300 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 320 | Loss: nan | Acc: 1.056,1.056,1.056,1.056,1.056,1.056,1.056,1.056,%
Batch: 340 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 23
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 120 | Loss: nan | Acc: 0.962,0.962,0.962,0.962,0.962,0.962,0.962,0.962,%
Batch: 140 | Loss: nan | Acc: 0.914,0.914,0.914,0.914,0.914,0.914,0.914,0.914,%
Batch: 160 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 220 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 240 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 260 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 320 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 340 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 24
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.558,0.558,0.558,0.558,0.558,0.558,0.558,0.558,%
Batch: 40 | Loss: nan | Acc: 0.857,0.857,0.857,0.857,0.857,0.857,0.857,0.857,%
Batch: 60 | Loss: nan | Acc: 0.909,0.909,0.909,0.909,0.909,0.909,0.909,0.909,%
Batch: 80 | Loss: nan | Acc: 0.907,0.907,0.907,0.907,0.907,0.907,0.907,0.907,%
Batch: 100 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 200 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 300 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 320 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 25
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.190,1.190,1.190,1.190,1.190,1.190,1.190,1.190,%
Batch: 40 | Loss: nan | Acc: 1.181,1.181,1.181,1.181,1.181,1.181,1.181,1.181,%
Batch: 60 | Loss: nan | Acc: 1.101,1.101,1.101,1.101,1.101,1.101,1.101,1.101,%
Batch: 80 | Loss: nan | Acc: 1.090,1.090,1.090,1.090,1.090,1.090,1.090,1.090,%
Batch: 100 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 120 | Loss: nan | Acc: 1.072,1.072,1.072,1.072,1.072,1.072,1.072,1.072,%
Batch: 140 | Loss: nan | Acc: 1.069,1.069,1.069,1.069,1.069,1.069,1.069,1.069,%
Batch: 160 | Loss: nan | Acc: 1.072,1.072,1.072,1.072,1.072,1.072,1.072,1.072,%
Batch: 180 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 240 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 260 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 280 | Loss: nan | Acc: 1.059,1.059,1.059,1.059,1.059,1.059,1.059,1.059,%
Batch: 300 | Loss: nan | Acc: 1.064,1.064,1.064,1.064,1.064,1.064,1.064,1.064,%
Batch: 320 | Loss: nan | Acc: 1.066,1.066,1.066,1.066,1.066,1.066,1.066,1.066,%
Batch: 340 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 360 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 26
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 40 | Loss: nan | Acc: 0.857,0.857,0.857,0.857,0.857,0.857,0.857,0.857,%
Batch: 60 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 80 | Loss: nan | Acc: 0.916,0.916,0.916,0.916,0.916,0.916,0.916,0.916,%
Batch: 100 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 120 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 140 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 160 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 180 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 200 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 280 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 300 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 320 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 340 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 27
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.896,0.896,0.896,0.896,0.896,0.896,0.896,0.896,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.936,0.936,0.936,0.936,0.936,0.936,0.936,0.936,%
Batch: 120 | Loss: nan | Acc: 0.910,0.910,0.910,0.910,0.910,0.910,0.910,0.910,%
Batch: 140 | Loss: nan | Acc: 0.942,0.942,0.942,0.942,0.942,0.942,0.942,0.942,%
Batch: 160 | Loss: nan | Acc: 0.927,0.927,0.927,0.927,0.927,0.927,0.927,0.927,%
Batch: 180 | Loss: nan | Acc: 0.928,0.928,0.928,0.928,0.928,0.928,0.928,0.928,%
Batch: 200 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 220 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 240 | Loss: nan | Acc: 0.947,0.947,0.947,0.947,0.947,0.947,0.947,0.947,%
Batch: 260 | Loss: nan | Acc: 0.952,0.952,0.952,0.952,0.952,0.952,0.952,0.952,%
Batch: 280 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 300 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 320 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 28
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.934,0.934,0.934,0.934,0.934,0.934,0.934,0.934,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 0.926,0.926,0.926,0.926,0.926,0.926,0.926,0.926,%
Batch: 100 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 120 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 140 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 0.941,0.941,0.941,0.941,0.941,0.941,0.941,0.941,%
Batch: 200 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 220 | Loss: nan | Acc: 0.954,0.954,0.954,0.954,0.954,0.954,0.954,0.954,%
Batch: 240 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 300 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 29
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 120 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 140 | Loss: nan | Acc: 0.942,0.942,0.942,0.942,0.942,0.942,0.942,0.942,%
Batch: 160 | Loss: nan | Acc: 0.946,0.946,0.946,0.946,0.946,0.946,0.946,0.946,%
Batch: 180 | Loss: nan | Acc: 0.924,0.924,0.924,0.924,0.924,0.924,0.924,0.924,%
Batch: 200 | Loss: nan | Acc: 0.941,0.941,0.941,0.941,0.941,0.941,0.941,0.941,%
Batch: 220 | Loss: nan | Acc: 0.940,0.940,0.940,0.940,0.940,0.940,0.940,0.940,%
Batch: 240 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 260 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 280 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 300 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 320 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 340 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 360 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 30
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.819,0.819,0.819,0.819,0.819,0.819,0.819,0.819,%
Batch: 60 | Loss: nan | Acc: 0.756,0.756,0.756,0.756,0.756,0.756,0.756,0.756,%
Batch: 80 | Loss: nan | Acc: 0.791,0.791,0.791,0.791,0.791,0.791,0.791,0.791,%
Batch: 100 | Loss: nan | Acc: 0.828,0.828,0.828,0.828,0.828,0.828,0.828,0.828,%
Batch: 120 | Loss: nan | Acc: 0.859,0.859,0.859,0.859,0.859,0.859,0.859,0.859,%
Batch: 140 | Loss: nan | Acc: 0.887,0.887,0.887,0.887,0.887,0.887,0.887,0.887,%
Batch: 160 | Loss: nan | Acc: 0.946,0.946,0.946,0.946,0.946,0.946,0.946,0.946,%
Batch: 180 | Loss: nan | Acc: 0.932,0.932,0.932,0.932,0.932,0.932,0.932,0.932,%
Batch: 200 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 220 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 240 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 300 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 320 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 31
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 80 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 100 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 160 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 180 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 200 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 220 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 240 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 260 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 280 | Loss: nan | Acc: 1.054,1.054,1.054,1.054,1.054,1.054,1.054,1.054,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 32
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 80 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 100 | Loss: nan | Acc: 0.913,0.913,0.913,0.913,0.913,0.913,0.913,0.913,%
Batch: 120 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 200 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 220 | Loss: nan | Acc: 0.944,0.944,0.944,0.944,0.944,0.944,0.944,0.944,%
Batch: 240 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 260 | Loss: nan | Acc: 0.949,0.949,0.949,0.949,0.949,0.949,0.949,0.949,%
Batch: 280 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 340 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 360 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 33
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 0.922,0.922,0.922,0.922,0.922,0.922,0.922,0.922,%
Batch: 80 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 100 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 160 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 180 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 200 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 260 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 280 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 300 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 320 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 340 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 34
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 0.744,0.744,0.744,0.744,0.744,0.744,0.744,0.744,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 120 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 140 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 160 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 180 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.958,0.958,0.958,0.958,0.958,0.958,0.958,0.958,%
Batch: 240 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 260 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 280 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 380 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 35
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.857,0.857,0.857,0.857,0.857,0.857,0.857,0.857,%
Batch: 60 | Loss: nan | Acc: 0.807,0.807,0.807,0.807,0.807,0.807,0.807,0.807,%
Batch: 80 | Loss: nan | Acc: 0.868,0.868,0.868,0.868,0.868,0.868,0.868,0.868,%
Batch: 100 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 120 | Loss: nan | Acc: 0.936,0.936,0.936,0.936,0.936,0.936,0.936,0.936,%
Batch: 140 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 160 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 180 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 340 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 360 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 36
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 120 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 180 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 200 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 220 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 240 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 260 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 280 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 300 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 320 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 340 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 360 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 37
Batch: 0 | Loss: nan | Acc: 3.125,3.125,3.125,3.125,3.125,3.125,3.125,3.125,%
Batch: 20 | Loss: nan | Acc: 1.265,1.265,1.265,1.265,1.265,1.265,1.265,1.265,%
Batch: 40 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 60 | Loss: nan | Acc: 1.089,1.089,1.089,1.089,1.089,1.089,1.089,1.089,%
Batch: 80 | Loss: nan | Acc: 1.119,1.119,1.119,1.119,1.119,1.119,1.119,1.119,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 140 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 160 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 180 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 240 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 320 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 340 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 360 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 38
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 120 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 180 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 280 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 300 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 320 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 39
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.190,1.190,1.190,1.190,1.190,1.190,1.190,1.190,%
Batch: 40 | Loss: nan | Acc: 1.162,1.162,1.162,1.162,1.162,1.162,1.162,1.162,%
Batch: 60 | Loss: nan | Acc: 1.140,1.140,1.140,1.140,1.140,1.140,1.140,1.140,%
Batch: 80 | Loss: nan | Acc: 1.090,1.090,1.090,1.090,1.090,1.090,1.090,1.090,%
Batch: 100 | Loss: nan | Acc: 1.091,1.091,1.091,1.091,1.091,1.091,1.091,1.091,%
Batch: 120 | Loss: nan | Acc: 1.078,1.078,1.078,1.078,1.078,1.078,1.078,1.078,%
Batch: 140 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 160 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 180 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 200 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 220 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 240 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 300 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 320 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 340 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 40
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.339,1.339,1.339,1.339,1.339,1.339,1.339,1.339,%
Batch: 40 | Loss: nan | Acc: 1.105,1.105,1.105,1.105,1.105,1.105,1.105,1.105,%
Batch: 60 | Loss: nan | Acc: 1.127,1.127,1.127,1.127,1.127,1.127,1.127,1.127,%
Batch: 80 | Loss: nan | Acc: 1.100,1.100,1.100,1.100,1.100,1.100,1.100,1.100,%
Batch: 100 | Loss: nan | Acc: 1.083,1.083,1.083,1.083,1.083,1.083,1.083,1.083,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 160 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 180 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 200 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 220 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 240 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 260 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 280 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 300 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 320 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 41
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.856,0.856,0.856,0.856,0.856,0.856,0.856,0.856,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 80 | Loss: nan | Acc: 1.090,1.090,1.090,1.090,1.090,1.090,1.090,1.090,%
Batch: 100 | Loss: nan | Acc: 1.098,1.098,1.098,1.098,1.098,1.098,1.098,1.098,%
Batch: 120 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 140 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 160 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 180 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 200 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 260 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 280 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 300 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 320 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 340 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 360 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 380 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 42
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 1.228,1.228,1.228,1.228,1.228,1.228,1.228,1.228,%
Batch: 40 | Loss: nan | Acc: 1.143,1.143,1.143,1.143,1.143,1.143,1.143,1.143,%
Batch: 60 | Loss: nan | Acc: 1.165,1.165,1.165,1.165,1.165,1.165,1.165,1.165,%
Batch: 80 | Loss: nan | Acc: 1.196,1.196,1.196,1.196,1.196,1.196,1.196,1.196,%
Batch: 100 | Loss: nan | Acc: 1.122,1.122,1.122,1.122,1.122,1.122,1.122,1.122,%
Batch: 120 | Loss: nan | Acc: 1.091,1.091,1.091,1.091,1.091,1.091,1.091,1.091,%
Batch: 140 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 160 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 180 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 200 | Loss: nan | Acc: 1.069,1.069,1.069,1.069,1.069,1.069,1.069,1.069,%
Batch: 220 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 240 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 260 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 340 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 360 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 380 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 43
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 1.116,1.116,1.116,1.116,1.116,1.116,1.116,1.116,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 80 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 140 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 160 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 180 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 200 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 220 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 240 | Loss: nan | Acc: 0.947,0.947,0.947,0.947,0.947,0.947,0.947,0.947,%
Batch: 260 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 280 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 300 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 44
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 80 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.962,0.962,0.962,0.962,0.962,0.962,0.962,0.962,%
Batch: 140 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 160 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 180 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 200 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 220 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 240 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 260 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 320 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 45
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.858,0.858,0.858,0.858,0.858,0.858,0.858,0.858,%
Batch: 80 | Loss: nan | Acc: 0.772,0.772,0.772,0.772,0.772,0.772,0.772,0.772,%
Batch: 100 | Loss: nan | Acc: 0.774,0.774,0.774,0.774,0.774,0.774,0.774,0.774,%
Batch: 120 | Loss: nan | Acc: 0.820,0.820,0.820,0.820,0.820,0.820,0.820,0.820,%
Batch: 140 | Loss: nan | Acc: 0.875,0.875,0.875,0.875,0.875,0.875,0.875,0.875,%
Batch: 160 | Loss: nan | Acc: 0.869,0.869,0.869,0.869,0.869,0.869,0.869,0.869,%
Batch: 180 | Loss: nan | Acc: 0.881,0.881,0.881,0.881,0.881,0.881,0.881,0.881,%
Batch: 200 | Loss: nan | Acc: 0.890,0.890,0.890,0.890,0.890,0.890,0.890,0.890,%
Batch: 220 | Loss: nan | Acc: 0.898,0.898,0.898,0.898,0.898,0.898,0.898,0.898,%
Batch: 240 | Loss: nan | Acc: 0.921,0.921,0.921,0.921,0.921,0.921,0.921,0.921,%
Batch: 260 | Loss: nan | Acc: 0.946,0.946,0.946,0.946,0.946,0.946,0.946,0.946,%
Batch: 280 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 300 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 320 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 340 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 360 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 380 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 46
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 40 | Loss: nan | Acc: 1.124,1.124,1.124,1.124,1.124,1.124,1.124,1.124,%
Batch: 60 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 80 | Loss: nan | Acc: 1.080,1.080,1.080,1.080,1.080,1.080,1.080,1.080,%
Batch: 100 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 120 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 140 | Loss: nan | Acc: 1.064,1.064,1.064,1.064,1.064,1.064,1.064,1.064,%
Batch: 160 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 180 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 200 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 220 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 280 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 300 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 320 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 340 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 47
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 100 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 120 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 140 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 160 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 180 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 200 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 260 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 280 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 340 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 360 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 48
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 80 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 100 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 180 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 200 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 280 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 300 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 320 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 340 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 360 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 49
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.265,1.265,1.265,1.265,1.265,1.265,1.265,1.265,%
Batch: 40 | Loss: nan | Acc: 1.124,1.124,1.124,1.124,1.124,1.124,1.124,1.124,%
Batch: 60 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 80 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 140 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 220 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 240 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 260 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 280 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 300 | Loss: nan | Acc: 1.038,1.038,1.038,1.038,1.038,1.038,1.038,1.038,%
Batch: 320 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 340 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 360 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 50
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 40 | Loss: nan | Acc: 1.124,1.124,1.124,1.124,1.124,1.124,1.124,1.124,%
Batch: 60 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 120 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 180 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 200 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 220 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 280 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 300 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 320 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 340 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 51
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.521,0.521,0.521,0.521,0.521,0.521,0.521,0.521,%
Batch: 40 | Loss: nan | Acc: 0.934,0.934,0.934,0.934,0.934,0.934,0.934,0.934,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 0.916,0.916,0.916,0.916,0.916,0.916,0.916,0.916,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 140 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 160 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 220 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 240 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 260 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 52
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.707,0.707,0.707,0.707,0.707,0.707,0.707,0.707,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 100 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 120 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 140 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 160 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 180 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 200 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 220 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 340 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 360 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 380 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 53
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.838,0.838,0.838,0.838,0.838,0.838,0.838,0.838,%
Batch: 60 | Loss: nan | Acc: 0.884,0.884,0.884,0.884,0.884,0.884,0.884,0.884,%
Batch: 80 | Loss: nan | Acc: 0.849,0.849,0.849,0.849,0.849,0.849,0.849,0.849,%
Batch: 100 | Loss: nan | Acc: 0.859,0.859,0.859,0.859,0.859,0.859,0.859,0.859,%
Batch: 120 | Loss: nan | Acc: 0.923,0.923,0.923,0.923,0.923,0.923,0.923,0.923,%
Batch: 140 | Loss: nan | Acc: 0.887,0.887,0.887,0.887,0.887,0.887,0.887,0.887,%
Batch: 160 | Loss: nan | Acc: 0.903,0.903,0.903,0.903,0.903,0.903,0.903,0.903,%
Batch: 180 | Loss: nan | Acc: 0.919,0.919,0.919,0.919,0.919,0.919,0.919,0.919,%
Batch: 200 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 220 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 240 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 260 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 280 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 300 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 320 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 54
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 0.905,0.905,0.905,0.905,0.905,0.905,0.905,0.905,%
Batch: 120 | Loss: nan | Acc: 0.917,0.917,0.917,0.917,0.917,0.917,0.917,0.917,%
Batch: 140 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 160 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 180 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 200 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 220 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 240 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 260 | Loss: nan | Acc: 0.952,0.952,0.952,0.952,0.952,0.952,0.952,0.952,%
Batch: 280 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 300 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 320 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 340 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 55
Batch: 0 | Loss: nan | Acc: 3.906,3.906,3.906,3.906,3.906,3.906,3.906,3.906,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.076,1.076,1.076,1.076,1.076,1.076,1.076,1.076,%
Batch: 80 | Loss: nan | Acc: 1.128,1.128,1.128,1.128,1.128,1.128,1.128,1.128,%
Batch: 100 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 120 | Loss: nan | Acc: 1.130,1.130,1.130,1.130,1.130,1.130,1.130,1.130,%
Batch: 140 | Loss: nan | Acc: 1.097,1.097,1.097,1.097,1.097,1.097,1.097,1.097,%
Batch: 160 | Loss: nan | Acc: 1.087,1.087,1.087,1.087,1.087,1.087,1.087,1.087,%
Batch: 180 | Loss: nan | Acc: 1.062,1.062,1.062,1.062,1.062,1.062,1.062,1.062,%
Batch: 200 | Loss: nan | Acc: 1.077,1.077,1.077,1.077,1.077,1.077,1.077,1.077,%
Batch: 220 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 340 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 56
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.707,0.707,0.707,0.707,0.707,0.707,0.707,0.707,%
Batch: 40 | Loss: nan | Acc: 0.877,0.877,0.877,0.877,0.877,0.877,0.877,0.877,%
Batch: 60 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 140 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 160 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 320 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 57
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 1.101,1.101,1.101,1.101,1.101,1.101,1.101,1.101,%
Batch: 80 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 100 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 120 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 140 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 200 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 220 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 240 | Loss: nan | Acc: 1.044,1.044,1.044,1.044,1.044,1.044,1.044,1.044,%
Batch: 260 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 280 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 300 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 320 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 340 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 58
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 1.200,1.200,1.200,1.200,1.200,1.200,1.200,1.200,%
Batch: 60 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 80 | Loss: nan | Acc: 1.109,1.109,1.109,1.109,1.109,1.109,1.109,1.109,%
Batch: 100 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 120 | Loss: nan | Acc: 1.194,1.194,1.194,1.194,1.194,1.194,1.194,1.194,%
Batch: 140 | Loss: nan | Acc: 1.180,1.180,1.180,1.180,1.180,1.180,1.180,1.180,%
Batch: 160 | Loss: nan | Acc: 1.140,1.140,1.140,1.140,1.140,1.140,1.140,1.140,%
Batch: 180 | Loss: nan | Acc: 1.083,1.083,1.083,1.083,1.083,1.083,1.083,1.083,%
Batch: 200 | Loss: nan | Acc: 1.084,1.084,1.084,1.084,1.084,1.084,1.084,1.084,%
Batch: 220 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 240 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 260 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 280 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 340 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 360 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 59
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.707,0.707,0.707,0.707,0.707,0.707,0.707,0.707,%
Batch: 40 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 60 | Loss: nan | Acc: 0.794,0.794,0.794,0.794,0.794,0.794,0.794,0.794,%
Batch: 80 | Loss: nan | Acc: 0.801,0.801,0.801,0.801,0.801,0.801,0.801,0.801,%
Batch: 100 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 120 | Loss: nan | Acc: 0.904,0.904,0.904,0.904,0.904,0.904,0.904,0.904,%
Batch: 140 | Loss: nan | Acc: 0.931,0.931,0.931,0.931,0.931,0.931,0.931,0.931,%
Batch: 160 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 180 | Loss: nan | Acc: 0.954,0.954,0.954,0.954,0.954,0.954,0.954,0.954,%
Batch: 200 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 220 | Loss: nan | Acc: 0.940,0.940,0.940,0.940,0.940,0.940,0.940,0.940,%
Batch: 240 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 260 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 280 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 300 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 360 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 60
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 40 | Loss: nan | Acc: 1.181,1.181,1.181,1.181,1.181,1.181,1.181,1.181,%
Batch: 60 | Loss: nan | Acc: 1.089,1.089,1.089,1.089,1.089,1.089,1.089,1.089,%
Batch: 80 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 100 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 160 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 180 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 200 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 220 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 240 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 300 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 320 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 340 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 61
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.896,0.896,0.896,0.896,0.896,0.896,0.896,0.896,%
Batch: 60 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 0.944,0.944,0.944,0.944,0.944,0.944,0.944,0.944,%
Batch: 120 | Loss: nan | Acc: 0.949,0.949,0.949,0.949,0.949,0.949,0.949,0.949,%
Batch: 140 | Loss: nan | Acc: 0.942,0.942,0.942,0.942,0.942,0.942,0.942,0.942,%
Batch: 160 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 180 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 200 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 280 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 340 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 360 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 380 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 62
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 1.239,1.239,1.239,1.239,1.239,1.239,1.239,1.239,%
Batch: 60 | Loss: nan | Acc: 1.165,1.165,1.165,1.165,1.165,1.165,1.165,1.165,%
Batch: 80 | Loss: nan | Acc: 1.177,1.177,1.177,1.177,1.177,1.177,1.177,1.177,%
Batch: 100 | Loss: nan | Acc: 1.122,1.122,1.122,1.122,1.122,1.122,1.122,1.122,%
Batch: 120 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 140 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 160 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 180 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 200 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 260 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 280 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 300 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 320 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 380 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 63
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.856,0.856,0.856,0.856,0.856,0.856,0.856,0.856,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.101,1.101,1.101,1.101,1.101,1.101,1.101,1.101,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 140 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 220 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 280 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 64
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 0.857,0.857,0.857,0.857,0.857,0.857,0.857,0.857,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 0.936,0.936,0.936,0.936,0.936,0.936,0.936,0.936,%
Batch: 120 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 140 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 160 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 220 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 240 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 260 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 280 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 300 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 320 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 65
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.928,0.928,0.928,0.928,0.928,0.928,0.928,0.928,%
Batch: 120 | Loss: nan | Acc: 0.910,0.910,0.910,0.910,0.910,0.910,0.910,0.910,%
Batch: 140 | Loss: nan | Acc: 0.920,0.920,0.920,0.920,0.920,0.920,0.920,0.920,%
Batch: 160 | Loss: nan | Acc: 0.946,0.946,0.946,0.946,0.946,0.946,0.946,0.946,%
Batch: 180 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 280 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 300 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 320 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 66
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.724,0.724,0.724,0.724,0.724,0.724,0.724,0.724,%
Batch: 60 | Loss: nan | Acc: 0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,%
Batch: 80 | Loss: nan | Acc: 0.878,0.878,0.878,0.878,0.878,0.878,0.878,0.878,%
Batch: 100 | Loss: nan | Acc: 0.890,0.890,0.890,0.890,0.890,0.890,0.890,0.890,%
Batch: 120 | Loss: nan | Acc: 0.962,0.962,0.962,0.962,0.962,0.962,0.962,0.962,%
Batch: 140 | Loss: nan | Acc: 0.925,0.925,0.925,0.925,0.925,0.925,0.925,0.925,%
Batch: 160 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 180 | Loss: nan | Acc: 0.911,0.911,0.911,0.911,0.911,0.911,0.911,0.911,%
Batch: 200 | Loss: nan | Acc: 0.917,0.917,0.917,0.917,0.917,0.917,0.917,0.917,%
Batch: 220 | Loss: nan | Acc: 0.923,0.923,0.923,0.923,0.923,0.923,0.923,0.923,%
Batch: 240 | Loss: nan | Acc: 0.911,0.911,0.911,0.911,0.911,0.911,0.911,0.911,%
Batch: 260 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 280 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 300 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 320 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 67
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 0.934,0.934,0.934,0.934,0.934,0.934,0.934,0.934,%
Batch: 60 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 80 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 100 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 120 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 140 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 160 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 180 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 200 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 260 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 280 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 360 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 68
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 80 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 160 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 180 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 320 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 69
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 80 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 100 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 120 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 140 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 160 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 220 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 240 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 260 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 280 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 340 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 70
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.744,0.744,0.744,0.744,0.744,0.744,0.744,0.744,%
Batch: 40 | Loss: nan | Acc: 0.857,0.857,0.857,0.857,0.857,0.857,0.857,0.857,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 100 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 120 | Loss: nan | Acc: 1.098,1.098,1.098,1.098,1.098,1.098,1.098,1.098,%
Batch: 140 | Loss: nan | Acc: 1.080,1.080,1.080,1.080,1.080,1.080,1.080,1.080,%
Batch: 160 | Loss: nan | Acc: 1.111,1.111,1.111,1.111,1.111,1.111,1.111,1.111,%
Batch: 180 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 200 | Loss: nan | Acc: 1.092,1.092,1.092,1.092,1.092,1.092,1.092,1.092,%
Batch: 220 | Loss: nan | Acc: 1.064,1.064,1.064,1.064,1.064,1.064,1.064,1.064,%
Batch: 240 | Loss: nan | Acc: 1.054,1.054,1.054,1.054,1.054,1.054,1.054,1.054,%
Batch: 260 | Loss: nan | Acc: 1.057,1.057,1.057,1.057,1.057,1.057,1.057,1.057,%
Batch: 280 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 300 | Loss: nan | Acc: 1.038,1.038,1.038,1.038,1.038,1.038,1.038,1.038,%
Batch: 320 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 340 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 360 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 380 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 71
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 80 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 100 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 120 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 180 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 200 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 220 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 240 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 72
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 0.936,0.936,0.936,0.936,0.936,0.936,0.936,0.936,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 180 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 240 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 260 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 280 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 300 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 73
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.162,1.162,1.162,1.162,1.162,1.162,1.162,1.162,%
Batch: 60 | Loss: nan | Acc: 1.127,1.127,1.127,1.127,1.127,1.127,1.127,1.127,%
Batch: 80 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 100 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 120 | Loss: nan | Acc: 1.059,1.059,1.059,1.059,1.059,1.059,1.059,1.059,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 180 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 200 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 220 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.978,0.978,0.978,0.978,0.978,0.978,0.978,0.978,%
Batch: 340 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 74
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.744,0.744,0.744,0.744,0.744,0.744,0.744,0.744,%
Batch: 40 | Loss: nan | Acc: 0.896,0.896,0.896,0.896,0.896,0.896,0.896,0.896,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 100 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 120 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 140 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 160 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 180 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 240 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 260 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 340 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 360 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 75
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 1.076,1.076,1.076,1.076,1.076,1.076,1.076,1.076,%
Batch: 80 | Loss: nan | Acc: 1.138,1.138,1.138,1.138,1.138,1.138,1.138,1.138,%
Batch: 100 | Loss: nan | Acc: 1.052,1.052,1.052,1.052,1.052,1.052,1.052,1.052,%
Batch: 120 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 140 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 160 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 220 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 240 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 260 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 280 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 300 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 320 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 340 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 360 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 76
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.178,1.178,1.178,1.178,1.178,1.178,1.178,1.178,%
Batch: 80 | Loss: nan | Acc: 1.157,1.157,1.157,1.157,1.157,1.157,1.157,1.157,%
Batch: 100 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 120 | Loss: nan | Acc: 1.091,1.091,1.091,1.091,1.091,1.091,1.091,1.091,%
Batch: 140 | Loss: nan | Acc: 1.064,1.064,1.064,1.064,1.064,1.064,1.064,1.064,%
Batch: 160 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 180 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 200 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 220 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 77
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 80 | Loss: nan | Acc: 0.916,0.916,0.916,0.916,0.916,0.916,0.916,0.916,%
Batch: 100 | Loss: nan | Acc: 0.928,0.928,0.928,0.928,0.928,0.928,0.928,0.928,%
Batch: 120 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 140 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 160 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 180 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.057,1.057,1.057,1.057,1.057,1.057,1.057,1.057,%
Batch: 240 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 260 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 320 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 340 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 78
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.856,0.856,0.856,0.856,0.856,0.856,0.856,0.856,%
Batch: 40 | Loss: nan | Acc: 0.934,0.934,0.934,0.934,0.934,0.934,0.934,0.934,%
Batch: 60 | Loss: nan | Acc: 0.935,0.935,0.935,0.935,0.935,0.935,0.935,0.935,%
Batch: 80 | Loss: nan | Acc: 0.887,0.887,0.887,0.887,0.887,0.887,0.887,0.887,%
Batch: 100 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 120 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 140 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 160 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 280 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 360 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 380 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 79
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 80 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 100 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.072,1.072,1.072,1.072,1.072,1.072,1.072,1.072,%
Batch: 180 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 200 | Loss: nan | Acc: 1.038,1.038,1.038,1.038,1.038,1.038,1.038,1.038,%
Batch: 220 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 320 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 340 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 80
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 100 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 160 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 180 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 240 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 260 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 320 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 81
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.116,1.116,1.116,1.116,1.116,1.116,1.116,1.116,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 80 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 100 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 120 | Loss: nan | Acc: 1.072,1.072,1.072,1.072,1.072,1.072,1.072,1.072,%
Batch: 140 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 160 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 180 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 200 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 220 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 280 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 300 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 320 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 340 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 82
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.670,0.670,0.670,0.670,0.670,0.670,0.670,0.670,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 100 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 120 | Loss: nan | Acc: 0.962,0.962,0.962,0.962,0.962,0.962,0.962,0.962,%
Batch: 140 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 160 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 180 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 200 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 220 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 240 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 260 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 280 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 300 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 83
Batch: 0 | Loss: nan | Acc: 3.125,3.125,3.125,3.125,3.125,3.125,3.125,3.125,%
Batch: 20 | Loss: nan | Acc: 1.116,1.116,1.116,1.116,1.116,1.116,1.116,1.116,%
Batch: 40 | Loss: nan | Acc: 1.143,1.143,1.143,1.143,1.143,1.143,1.143,1.143,%
Batch: 60 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 80 | Loss: nan | Acc: 1.109,1.109,1.109,1.109,1.109,1.109,1.109,1.109,%
Batch: 100 | Loss: nan | Acc: 1.098,1.098,1.098,1.098,1.098,1.098,1.098,1.098,%
Batch: 120 | Loss: nan | Acc: 1.111,1.111,1.111,1.111,1.111,1.111,1.111,1.111,%
Batch: 140 | Loss: nan | Acc: 1.125,1.125,1.125,1.125,1.125,1.125,1.125,1.125,%
Batch: 160 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 180 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 200 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 220 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 240 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 320 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 340 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 84
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.165,1.165,1.165,1.165,1.165,1.165,1.165,1.165,%
Batch: 80 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 100 | Loss: nan | Acc: 0.936,0.936,0.936,0.936,0.936,0.936,0.936,0.936,%
Batch: 120 | Loss: nan | Acc: 0.923,0.923,0.923,0.923,0.923,0.923,0.923,0.923,%
Batch: 140 | Loss: nan | Acc: 0.925,0.925,0.925,0.925,0.925,0.925,0.925,0.925,%
Batch: 160 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 180 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 200 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 220 | Loss: nan | Acc: 0.940,0.940,0.940,0.940,0.940,0.940,0.940,0.940,%
Batch: 240 | Loss: nan | Acc: 0.947,0.947,0.947,0.947,0.947,0.947,0.947,0.947,%
Batch: 260 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 280 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 85
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.228,1.228,1.228,1.228,1.228,1.228,1.228,1.228,%
Batch: 40 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 60 | Loss: nan | Acc: 1.101,1.101,1.101,1.101,1.101,1.101,1.101,1.101,%
Batch: 80 | Loss: nan | Acc: 1.100,1.100,1.100,1.100,1.100,1.100,1.100,1.100,%
Batch: 100 | Loss: nan | Acc: 1.106,1.106,1.106,1.106,1.106,1.106,1.106,1.106,%
Batch: 120 | Loss: nan | Acc: 1.091,1.091,1.091,1.091,1.091,1.091,1.091,1.091,%
Batch: 140 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 160 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 180 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 200 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 220 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 240 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 260 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 320 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 340 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 86
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.228,1.228,1.228,1.228,1.228,1.228,1.228,1.228,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 80 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 100 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 120 | Loss: nan | Acc: 0.917,0.917,0.917,0.917,0.917,0.917,0.917,0.917,%
Batch: 140 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 160 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 220 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 240 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 260 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 320 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 380 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 87
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 80 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 100 | Loss: nan | Acc: 1.106,1.106,1.106,1.106,1.106,1.106,1.106,1.106,%
Batch: 120 | Loss: nan | Acc: 1.078,1.078,1.078,1.078,1.078,1.078,1.078,1.078,%
Batch: 140 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 160 | Loss: nan | Acc: 1.038,1.038,1.038,1.038,1.038,1.038,1.038,1.038,%
Batch: 180 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 280 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 88
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.265,1.265,1.265,1.265,1.265,1.265,1.265,1.265,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 100 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 120 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 140 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 160 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 180 | Loss: nan | Acc: 1.062,1.062,1.062,1.062,1.062,1.062,1.062,1.062,%
Batch: 200 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 220 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 240 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 300 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 320 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 89
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.877,0.877,0.877,0.877,0.877,0.877,0.877,0.877,%
Batch: 60 | Loss: nan | Acc: 0.884,0.884,0.884,0.884,0.884,0.884,0.884,0.884,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 140 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 160 | Loss: nan | Acc: 0.932,0.932,0.932,0.932,0.932,0.932,0.932,0.932,%
Batch: 180 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 200 | Loss: nan | Acc: 0.952,0.952,0.952,0.952,0.952,0.952,0.952,0.952,%
Batch: 220 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 240 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 260 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 280 | Loss: nan | Acc: 0.954,0.954,0.954,0.954,0.954,0.954,0.954,0.954,%
Batch: 300 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 320 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 340 | Loss: nan | Acc: 0.978,0.978,0.978,0.978,0.978,0.978,0.978,0.978,%
Batch: 360 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 90
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.265,1.265,1.265,1.265,1.265,1.265,1.265,1.265,%
Batch: 40 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 60 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 80 | Loss: nan | Acc: 0.878,0.878,0.878,0.878,0.878,0.878,0.878,0.878,%
Batch: 100 | Loss: nan | Acc: 0.944,0.944,0.944,0.944,0.944,0.944,0.944,0.944,%
Batch: 120 | Loss: nan | Acc: 0.923,0.923,0.923,0.923,0.923,0.923,0.923,0.923,%
Batch: 140 | Loss: nan | Acc: 0.920,0.920,0.920,0.920,0.920,0.920,0.920,0.920,%
Batch: 160 | Loss: nan | Acc: 0.907,0.907,0.907,0.907,0.907,0.907,0.907,0.907,%
Batch: 180 | Loss: nan | Acc: 0.932,0.932,0.932,0.932,0.932,0.932,0.932,0.932,%
Batch: 200 | Loss: nan | Acc: 0.933,0.933,0.933,0.933,0.933,0.933,0.933,0.933,%
Batch: 220 | Loss: nan | Acc: 0.954,0.954,0.954,0.954,0.954,0.954,0.954,0.954,%
Batch: 240 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 260 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 280 | Loss: nan | Acc: 0.954,0.954,0.954,0.954,0.954,0.954,0.954,0.954,%
Batch: 300 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 320 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 340 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 91
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.800,0.800,0.800,0.800,0.800,0.800,0.800,0.800,%
Batch: 60 | Loss: nan | Acc: 0.884,0.884,0.884,0.884,0.884,0.884,0.884,0.884,%
Batch: 80 | Loss: nan | Acc: 0.907,0.907,0.907,0.907,0.907,0.907,0.907,0.907,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 140 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 160 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 180 | Loss: nan | Acc: 0.971,0.971,0.971,0.971,0.971,0.971,0.971,0.971,%
Batch: 200 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 220 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 240 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 260 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 320 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 92
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 80 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 100 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 120 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 140 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 160 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 180 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 200 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 220 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 280 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 300 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 320 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 93
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.818,0.818,0.818,0.818,0.818,0.818,0.818,0.818,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,%
Batch: 80 | Loss: nan | Acc: 0.907,0.907,0.907,0.907,0.907,0.907,0.907,0.907,%
Batch: 100 | Loss: nan | Acc: 0.835,0.835,0.835,0.835,0.835,0.835,0.835,0.835,%
Batch: 120 | Loss: nan | Acc: 0.904,0.904,0.904,0.904,0.904,0.904,0.904,0.904,%
Batch: 140 | Loss: nan | Acc: 0.903,0.903,0.903,0.903,0.903,0.903,0.903,0.903,%
Batch: 160 | Loss: nan | Acc: 0.873,0.873,0.873,0.873,0.873,0.873,0.873,0.873,%
Batch: 180 | Loss: nan | Acc: 0.855,0.855,0.855,0.855,0.855,0.855,0.855,0.855,%
Batch: 200 | Loss: nan | Acc: 0.867,0.867,0.867,0.867,0.867,0.867,0.867,0.867,%
Batch: 220 | Loss: nan | Acc: 0.901,0.901,0.901,0.901,0.901,0.901,0.901,0.901,%
Batch: 240 | Loss: nan | Acc: 0.924,0.924,0.924,0.924,0.924,0.924,0.924,0.924,%
Batch: 260 | Loss: nan | Acc: 0.925,0.925,0.925,0.925,0.925,0.925,0.925,0.925,%
Batch: 280 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 300 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 320 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 94
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.856,0.856,0.856,0.856,0.856,0.856,0.856,0.856,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 80 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 100 | Loss: nan | Acc: 0.843,0.843,0.843,0.843,0.843,0.843,0.843,0.843,%
Batch: 120 | Loss: nan | Acc: 0.872,0.872,0.872,0.872,0.872,0.872,0.872,0.872,%
Batch: 140 | Loss: nan | Acc: 0.881,0.881,0.881,0.881,0.881,0.881,0.881,0.881,%
Batch: 160 | Loss: nan | Acc: 0.917,0.917,0.917,0.917,0.917,0.917,0.917,0.917,%
Batch: 180 | Loss: nan | Acc: 0.963,0.963,0.963,0.963,0.963,0.963,0.963,0.963,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 240 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 260 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 280 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 300 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 320 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 340 | Loss: nan | Acc: 0.978,0.978,0.978,0.978,0.978,0.978,0.978,0.978,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 95
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 40 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 60 | Loss: nan | Acc: 1.114,1.114,1.114,1.114,1.114,1.114,1.114,1.114,%
Batch: 80 | Loss: nan | Acc: 1.080,1.080,1.080,1.080,1.080,1.080,1.080,1.080,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 140 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 160 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 240 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 260 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 280 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 300 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 320 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 340 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 360 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 96
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.832,0.832,0.832,0.832,0.832,0.832,0.832,0.832,%
Batch: 80 | Loss: nan | Acc: 0.887,0.887,0.887,0.887,0.887,0.887,0.887,0.887,%
Batch: 100 | Loss: nan | Acc: 0.928,0.928,0.928,0.928,0.928,0.928,0.928,0.928,%
Batch: 120 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 140 | Loss: nan | Acc: 0.909,0.909,0.909,0.909,0.909,0.909,0.909,0.909,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 200 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 220 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 280 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 300 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 320 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 380 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 97
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 0.922,0.922,0.922,0.922,0.922,0.922,0.922,0.922,%
Batch: 80 | Loss: nan | Acc: 0.916,0.916,0.916,0.916,0.916,0.916,0.916,0.916,%
Batch: 100 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 120 | Loss: nan | Acc: 0.949,0.949,0.949,0.949,0.949,0.949,0.949,0.949,%
Batch: 140 | Loss: nan | Acc: 0.936,0.936,0.936,0.936,0.936,0.936,0.936,0.936,%
Batch: 160 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 180 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 200 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 220 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 240 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 260 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 280 | Loss: nan | Acc: 1.034,1.034,1.034,1.034,1.034,1.034,1.034,1.034,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 98
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 60 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 80 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 100 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 120 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 140 | Loss: nan | Acc: 0.925,0.925,0.925,0.925,0.925,0.925,0.925,0.925,%
Batch: 160 | Loss: nan | Acc: 0.941,0.941,0.941,0.941,0.941,0.941,0.941,0.941,%
Batch: 180 | Loss: nan | Acc: 0.932,0.932,0.932,0.932,0.932,0.932,0.932,0.932,%
Batch: 200 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 220 | Loss: nan | Acc: 0.958,0.958,0.958,0.958,0.958,0.958,0.958,0.958,%
Batch: 240 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 260 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 280 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 300 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 320 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 340 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 99
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 100 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 120 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 140 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 160 | Loss: nan | Acc: 0.941,0.941,0.941,0.941,0.941,0.941,0.941,0.941,%
Batch: 180 | Loss: nan | Acc: 0.924,0.924,0.924,0.924,0.924,0.924,0.924,0.924,%
Batch: 200 | Loss: nan | Acc: 0.948,0.948,0.948,0.948,0.948,0.948,0.948,0.948,%
Batch: 220 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 240 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 300 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 320 | Loss: nan | Acc: 0.978,0.978,0.978,0.978,0.978,0.978,0.978,0.978,%
Batch: 340 | Loss: nan | Acc: 0.969,0.969,0.969,0.969,0.969,0.969,0.969,0.969,%
Batch: 360 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 380 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
