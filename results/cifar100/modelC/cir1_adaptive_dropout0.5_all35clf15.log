==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=128, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=128, bias=True)
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Train all parameters

Epoch: 0
Batch: 0 | Loss: 14.302 | Acc: 1.562,1.562,2.344,%
Batch: 20 | Loss: 13.628 | Acc: 1.451,3.385,3.943,%
Batch: 40 | Loss: 13.320 | Acc: 2.534,4.402,5.050,%
Batch: 60 | Loss: 13.134 | Acc: 3.253,4.982,5.686,%
Batch: 80 | Loss: 13.007 | Acc: 3.704,5.507,6.289,%
Batch: 100 | Loss: 12.882 | Acc: 4.192,5.917,6.830,%
Batch: 120 | Loss: 12.769 | Acc: 4.487,6.205,7.193,%
Batch: 140 | Loss: 12.651 | Acc: 4.798,6.754,7.774,%
Batch: 160 | Loss: 12.574 | Acc: 5.051,7.211,8.254,%
Batch: 180 | Loss: 12.517 | Acc: 5.275,7.497,8.555,%
Batch: 200 | Loss: 12.444 | Acc: 5.508,7.750,9.068,%
Batch: 220 | Loss: 12.372 | Acc: 5.787,8.113,9.513,%
Batch: 240 | Loss: 12.312 | Acc: 6.068,8.331,9.816,%
Batch: 260 | Loss: 12.256 | Acc: 6.316,8.636,10.147,%
Batch: 280 | Loss: 12.182 | Acc: 6.584,9.036,10.621,%
Batch: 300 | Loss: 12.130 | Acc: 6.738,9.282,10.979,%
Batch: 320 | Loss: 12.079 | Acc: 6.997,9.609,11.303,%
Batch: 340 | Loss: 12.039 | Acc: 7.150,9.787,11.565,%
Batch: 360 | Loss: 11.988 | Acc: 7.302,9.998,11.831,%
Batch: 380 | Loss: 11.949 | Acc: 7.445,10.154,12.080,%
Batch: 0 | Loss: 11.405 | Acc: 10.156,17.969,17.969,%
Batch: 20 | Loss: 11.282 | Acc: 9.821,14.472,16.295,%
Batch: 40 | Loss: 11.273 | Acc: 9.375,13.948,15.701,%
Batch: 60 | Loss: 11.271 | Acc: 9.426,14.037,16.048,%
Train all parameters

Epoch: 1
Batch: 0 | Loss: 10.885 | Acc: 12.500,10.938,18.750,%
Batch: 20 | Loss: 11.006 | Acc: 11.198,15.699,19.382,%
Batch: 40 | Loss: 10.966 | Acc: 11.033,16.082,20.179,%
Batch: 60 | Loss: 10.924 | Acc: 11.027,16.227,19.736,%
Batch: 80 | Loss: 10.903 | Acc: 11.208,15.856,19.907,%
Batch: 100 | Loss: 10.873 | Acc: 11.301,16.089,20.235,%
Batch: 120 | Loss: 10.823 | Acc: 11.383,16.342,20.467,%
Batch: 140 | Loss: 10.792 | Acc: 11.447,16.401,20.634,%
Batch: 160 | Loss: 10.775 | Acc: 11.457,16.586,20.754,%
Batch: 180 | Loss: 10.739 | Acc: 11.443,16.678,20.908,%
Batch: 200 | Loss: 10.705 | Acc: 11.435,16.725,21.152,%
Batch: 220 | Loss: 10.692 | Acc: 11.454,16.788,21.242,%
Batch: 240 | Loss: 10.661 | Acc: 11.485,16.954,21.454,%
Batch: 260 | Loss: 10.638 | Acc: 11.581,17.101,21.648,%
Batch: 280 | Loss: 10.600 | Acc: 11.633,17.343,21.875,%
Batch: 300 | Loss: 10.583 | Acc: 11.680,17.447,22.036,%
Batch: 320 | Loss: 10.555 | Acc: 11.792,17.674,22.347,%
Batch: 340 | Loss: 10.531 | Acc: 11.836,17.838,22.510,%
Batch: 360 | Loss: 10.514 | Acc: 11.872,17.921,22.656,%
Batch: 380 | Loss: 10.490 | Acc: 11.965,18.086,22.831,%
Batch: 0 | Loss: 10.124 | Acc: 14.844,18.750,25.781,%
Batch: 20 | Loss: 10.149 | Acc: 13.504,20.499,26.823,%
Batch: 40 | Loss: 10.136 | Acc: 12.729,20.084,26.353,%
Batch: 60 | Loss: 10.142 | Acc: 12.897,20.453,26.281,%
Train all parameters

Epoch: 2
Batch: 0 | Loss: 10.383 | Acc: 13.281,18.750,19.531,%
Batch: 20 | Loss: 10.017 | Acc: 13.579,21.652,26.972,%
Batch: 40 | Loss: 9.944 | Acc: 13.434,21.627,27.058,%
Batch: 60 | Loss: 9.963 | Acc: 13.256,21.414,27.152,%
Batch: 80 | Loss: 9.919 | Acc: 13.349,21.576,27.508,%
Batch: 100 | Loss: 9.885 | Acc: 13.413,21.597,27.885,%
Batch: 120 | Loss: 9.864 | Acc: 13.397,21.752,28.002,%
Batch: 140 | Loss: 9.860 | Acc: 13.447,21.770,28.009,%
Batch: 160 | Loss: 9.854 | Acc: 13.587,21.870,28.062,%
Batch: 180 | Loss: 9.832 | Acc: 13.635,22.177,28.354,%
Batch: 200 | Loss: 9.799 | Acc: 13.724,22.442,28.599,%
Batch: 220 | Loss: 9.780 | Acc: 13.865,22.540,28.790,%
Batch: 240 | Loss: 9.753 | Acc: 13.926,22.692,29.101,%
Batch: 260 | Loss: 9.721 | Acc: 14.083,22.788,29.307,%
Batch: 280 | Loss: 9.706 | Acc: 14.124,22.795,29.312,%
Batch: 300 | Loss: 9.676 | Acc: 14.268,23.014,29.480,%
Batch: 320 | Loss: 9.665 | Acc: 14.325,23.060,29.566,%
Batch: 340 | Loss: 9.649 | Acc: 14.390,23.163,29.722,%
Batch: 360 | Loss: 9.622 | Acc: 14.543,23.316,29.977,%
Batch: 380 | Loss: 9.600 | Acc: 14.667,23.421,30.151,%
Batch: 0 | Loss: 9.578 | Acc: 19.531,22.656,27.344,%
Batch: 20 | Loss: 9.557 | Acc: 14.658,24.516,31.845,%
Batch: 40 | Loss: 9.539 | Acc: 14.444,24.123,31.212,%
Batch: 60 | Loss: 9.546 | Acc: 14.728,24.065,31.032,%
Train all parameters

Epoch: 3
Batch: 0 | Loss: 8.604 | Acc: 21.875,28.906,38.281,%
Batch: 20 | Loss: 9.130 | Acc: 16.704,27.195,34.301,%
Batch: 40 | Loss: 9.227 | Acc: 15.987,26.353,33.479,%
Batch: 60 | Loss: 9.203 | Acc: 15.804,26.358,33.299,%
Batch: 80 | Loss: 9.140 | Acc: 16.348,26.476,34.066,%
Batch: 100 | Loss: 9.137 | Acc: 16.437,26.454,34.205,%
Batch: 120 | Loss: 9.123 | Acc: 16.503,26.601,34.149,%
Batch: 140 | Loss: 9.096 | Acc: 16.755,26.734,34.325,%
Batch: 160 | Loss: 9.093 | Acc: 16.804,26.829,34.453,%
Batch: 180 | Loss: 9.102 | Acc: 16.855,26.761,34.397,%
Batch: 200 | Loss: 9.093 | Acc: 16.904,26.873,34.655,%
Batch: 220 | Loss: 9.076 | Acc: 17.043,26.958,34.718,%
Batch: 240 | Loss: 9.067 | Acc: 17.035,26.864,34.761,%
Batch: 260 | Loss: 9.057 | Acc: 17.041,26.886,34.839,%
Batch: 280 | Loss: 9.038 | Acc: 17.107,26.966,34.906,%
Batch: 300 | Loss: 9.020 | Acc: 17.128,26.988,34.959,%
Batch: 320 | Loss: 9.004 | Acc: 17.329,27.005,35.069,%
Batch: 340 | Loss: 8.992 | Acc: 17.440,27.099,35.177,%
Batch: 360 | Loss: 8.976 | Acc: 17.562,27.255,35.377,%
Batch: 380 | Loss: 8.962 | Acc: 17.626,27.303,35.548,%
Batch: 0 | Loss: 8.984 | Acc: 20.312,27.344,33.594,%
Batch: 20 | Loss: 9.214 | Acc: 16.406,26.004,34.487,%
Batch: 40 | Loss: 9.186 | Acc: 16.025,26.048,34.756,%
Batch: 60 | Loss: 9.195 | Acc: 15.971,26.089,34.490,%
Train all parameters

Epoch: 4
Batch: 0 | Loss: 8.342 | Acc: 21.094,30.469,39.844,%
Batch: 20 | Loss: 8.582 | Acc: 19.196,30.878,40.179,%
Batch: 40 | Loss: 8.581 | Acc: 19.493,30.621,39.024,%
Batch: 60 | Loss: 8.597 | Acc: 19.365,30.328,39.037,%
Batch: 80 | Loss: 8.626 | Acc: 19.145,29.784,39.072,%
Batch: 100 | Loss: 8.625 | Acc: 19.052,29.672,39.171,%
Batch: 120 | Loss: 8.599 | Acc: 19.344,30.068,39.340,%
Batch: 140 | Loss: 8.598 | Acc: 19.321,29.970,39.229,%
Batch: 160 | Loss: 8.592 | Acc: 19.405,30.158,39.412,%
Batch: 180 | Loss: 8.588 | Acc: 19.328,30.201,39.386,%
Batch: 200 | Loss: 8.563 | Acc: 19.578,30.263,39.478,%
Batch: 220 | Loss: 8.549 | Acc: 19.605,30.497,39.660,%
Batch: 240 | Loss: 8.532 | Acc: 19.723,30.469,39.691,%
Batch: 260 | Loss: 8.521 | Acc: 19.750,30.505,39.640,%
Batch: 280 | Loss: 8.504 | Acc: 19.826,30.558,39.771,%
Batch: 300 | Loss: 8.490 | Acc: 19.905,30.648,39.852,%
Batch: 320 | Loss: 8.479 | Acc: 19.957,30.654,39.961,%
Batch: 340 | Loss: 8.478 | Acc: 19.902,30.597,40.027,%
Batch: 360 | Loss: 8.468 | Acc: 19.929,30.588,40.121,%
Batch: 380 | Loss: 8.460 | Acc: 19.976,30.659,40.145,%
Batch: 0 | Loss: 8.322 | Acc: 25.781,39.844,44.531,%
Batch: 20 | Loss: 8.566 | Acc: 19.085,30.357,41.257,%
Batch: 40 | Loss: 8.529 | Acc: 18.750,31.040,41.787,%
Batch: 60 | Loss: 8.543 | Acc: 18.648,30.968,41.560,%
Train all parameters

Epoch: 5
Batch: 0 | Loss: 7.560 | Acc: 29.688,38.281,47.656,%
Batch: 20 | Loss: 8.245 | Acc: 21.019,32.999,43.936,%
Batch: 40 | Loss: 8.192 | Acc: 21.303,32.546,44.284,%
Batch: 60 | Loss: 8.115 | Acc: 21.811,33.094,44.621,%
Batch: 80 | Loss: 8.116 | Acc: 21.663,32.957,44.155,%
Batch: 100 | Loss: 8.112 | Acc: 21.481,32.929,44.230,%
Batch: 120 | Loss: 8.126 | Acc: 21.365,32.845,44.028,%
Batch: 140 | Loss: 8.139 | Acc: 21.266,32.873,43.711,%
Batch: 160 | Loss: 8.125 | Acc: 21.375,32.948,43.556,%
Batch: 180 | Loss: 8.121 | Acc: 21.413,33.020,43.504,%
Batch: 200 | Loss: 8.115 | Acc: 21.498,32.956,43.525,%
Batch: 220 | Loss: 8.116 | Acc: 21.550,32.929,43.301,%
Batch: 240 | Loss: 8.112 | Acc: 21.674,32.949,43.309,%
Batch: 260 | Loss: 8.095 | Acc: 21.770,33.100,43.493,%
Batch: 280 | Loss: 8.087 | Acc: 21.719,33.174,43.642,%
Batch: 300 | Loss: 8.088 | Acc: 21.670,33.280,43.657,%
Batch: 320 | Loss: 8.076 | Acc: 21.719,33.389,43.755,%
Batch: 340 | Loss: 8.060 | Acc: 21.831,33.546,43.908,%
Batch: 360 | Loss: 8.045 | Acc: 21.905,33.628,43.934,%
Batch: 380 | Loss: 8.025 | Acc: 21.982,33.735,44.031,%
Batch: 0 | Loss: 8.055 | Acc: 21.875,33.594,45.312,%
Batch: 20 | Loss: 8.207 | Acc: 20.052,33.222,44.457,%
Batch: 40 | Loss: 8.227 | Acc: 19.893,32.774,44.074,%
Batch: 60 | Loss: 8.223 | Acc: 19.903,32.518,43.981,%
Train all parameters

Epoch: 6
Batch: 0 | Loss: 7.999 | Acc: 17.969,27.344,40.625,%
Batch: 20 | Loss: 7.667 | Acc: 22.135,36.235,47.842,%
Batch: 40 | Loss: 7.678 | Acc: 22.732,35.614,47.752,%
Batch: 60 | Loss: 7.669 | Acc: 23.527,35.758,47.695,%
Batch: 80 | Loss: 7.655 | Acc: 23.669,35.909,47.840,%
Batch: 100 | Loss: 7.650 | Acc: 23.824,36.262,47.602,%
Batch: 120 | Loss: 7.687 | Acc: 23.399,35.905,47.321,%
Batch: 140 | Loss: 7.686 | Acc: 23.576,36.226,47.479,%
Batch: 160 | Loss: 7.679 | Acc: 23.636,36.277,47.409,%
Batch: 180 | Loss: 7.686 | Acc: 23.481,36.140,47.371,%
Batch: 200 | Loss: 7.679 | Acc: 23.457,36.147,47.524,%
Batch: 220 | Loss: 7.678 | Acc: 23.430,36.139,47.430,%
Batch: 240 | Loss: 7.667 | Acc: 23.428,36.268,47.601,%
Batch: 260 | Loss: 7.656 | Acc: 23.491,36.315,47.629,%
Batch: 280 | Loss: 7.671 | Acc: 23.421,36.252,47.523,%
Batch: 300 | Loss: 7.670 | Acc: 23.487,36.265,47.524,%
Batch: 320 | Loss: 7.659 | Acc: 23.491,36.324,47.569,%
Batch: 340 | Loss: 7.656 | Acc: 23.511,36.332,47.638,%
Batch: 360 | Loss: 7.654 | Acc: 23.600,36.431,47.667,%
Batch: 380 | Loss: 7.640 | Acc: 23.684,36.495,47.775,%
Batch: 0 | Loss: 7.563 | Acc: 25.781,38.281,50.000,%
Batch: 20 | Loss: 7.953 | Acc: 21.838,34.115,46.540,%
Batch: 40 | Loss: 7.953 | Acc: 21.494,34.489,45.960,%
Batch: 60 | Loss: 7.962 | Acc: 21.363,35.054,45.889,%
Train all parameters

Epoch: 7
Batch: 0 | Loss: 7.403 | Acc: 23.438,29.688,52.344,%
Batch: 20 | Loss: 7.448 | Acc: 24.591,39.472,51.637,%
Batch: 40 | Loss: 7.431 | Acc: 25.095,38.662,51.048,%
Batch: 60 | Loss: 7.382 | Acc: 25.115,38.973,51.447,%
Batch: 80 | Loss: 7.395 | Acc: 24.643,38.995,51.032,%
Batch: 100 | Loss: 7.402 | Acc: 24.714,38.908,50.944,%
Batch: 120 | Loss: 7.402 | Acc: 24.877,38.959,50.329,%
Batch: 140 | Loss: 7.390 | Acc: 24.861,38.907,50.332,%
Batch: 160 | Loss: 7.391 | Acc: 24.888,38.965,50.286,%
Batch: 180 | Loss: 7.393 | Acc: 24.763,38.860,50.302,%
Batch: 200 | Loss: 7.374 | Acc: 24.965,38.934,50.245,%
Batch: 220 | Loss: 7.373 | Acc: 24.951,38.956,50.233,%
Batch: 240 | Loss: 7.370 | Acc: 25.156,39.027,50.152,%
Batch: 260 | Loss: 7.354 | Acc: 25.156,39.057,50.162,%
Batch: 280 | Loss: 7.350 | Acc: 25.181,39.032,50.108,%
Batch: 300 | Loss: 7.343 | Acc: 25.236,39.078,50.241,%
Batch: 320 | Loss: 7.340 | Acc: 25.170,39.109,50.329,%
Batch: 340 | Loss: 7.335 | Acc: 25.231,39.200,50.362,%
Batch: 360 | Loss: 7.325 | Acc: 25.271,39.182,50.325,%
Batch: 380 | Loss: 7.322 | Acc: 25.291,39.073,50.262,%
Batch: 0 | Loss: 7.301 | Acc: 31.250,42.969,53.906,%
Batch: 20 | Loss: 7.641 | Acc: 22.433,36.421,48.214,%
Batch: 40 | Loss: 7.629 | Acc: 22.275,36.547,48.609,%
Batch: 60 | Loss: 7.641 | Acc: 22.029,36.514,48.271,%
Train all parameters

Epoch: 8
Batch: 0 | Loss: 7.256 | Acc: 17.969,28.125,52.344,%
Batch: 20 | Loss: 6.986 | Acc: 27.307,41.183,53.534,%
Batch: 40 | Loss: 7.058 | Acc: 26.715,40.206,52.611,%
Batch: 60 | Loss: 7.050 | Acc: 26.844,40.433,52.907,%
Batch: 80 | Loss: 7.088 | Acc: 26.225,40.538,52.894,%
Batch: 100 | Loss: 7.077 | Acc: 26.416,40.625,52.862,%
Batch: 120 | Loss: 7.077 | Acc: 26.466,40.799,52.989,%
Batch: 140 | Loss: 7.062 | Acc: 26.524,40.758,53.258,%
Batch: 160 | Loss: 7.068 | Acc: 26.485,40.853,53.183,%
Batch: 180 | Loss: 7.076 | Acc: 26.463,40.828,53.198,%
Batch: 200 | Loss: 7.069 | Acc: 26.504,41.041,53.218,%
Batch: 220 | Loss: 7.070 | Acc: 26.495,41.000,53.231,%
Batch: 240 | Loss: 7.071 | Acc: 26.520,41.089,53.206,%
Batch: 260 | Loss: 7.060 | Acc: 26.545,41.206,53.293,%
Batch: 280 | Loss: 7.066 | Acc: 26.524,41.125,53.272,%
Batch: 300 | Loss: 7.063 | Acc: 26.607,41.251,53.208,%
Batch: 320 | Loss: 7.055 | Acc: 26.740,41.280,53.215,%
Batch: 340 | Loss: 7.054 | Acc: 26.819,41.276,53.182,%
Batch: 360 | Loss: 7.054 | Acc: 26.775,41.389,53.242,%
Batch: 380 | Loss: 7.055 | Acc: 26.720,41.349,53.121,%
Batch: 0 | Loss: 7.078 | Acc: 28.125,42.969,56.250,%
Batch: 20 | Loss: 7.352 | Acc: 24.368,39.769,52.344,%
Batch: 40 | Loss: 7.362 | Acc: 23.952,39.005,51.029,%
Batch: 60 | Loss: 7.381 | Acc: 24.488,38.922,50.845,%
Train all parameters

Epoch: 9
Batch: 0 | Loss: 7.465 | Acc: 14.062,40.625,51.562,%
Batch: 20 | Loss: 6.984 | Acc: 25.446,42.336,54.613,%
Batch: 40 | Loss: 6.903 | Acc: 26.524,42.473,55.145,%
Batch: 60 | Loss: 6.919 | Acc: 26.460,42.162,55.110,%
Batch: 80 | Loss: 6.895 | Acc: 26.447,42.670,54.900,%
Batch: 100 | Loss: 6.903 | Acc: 26.485,42.683,54.943,%
Batch: 120 | Loss: 6.916 | Acc: 26.588,42.446,54.797,%
Batch: 140 | Loss: 6.898 | Acc: 26.762,42.880,54.942,%
Batch: 160 | Loss: 6.903 | Acc: 26.795,42.843,54.935,%
Batch: 180 | Loss: 6.895 | Acc: 26.990,42.930,54.826,%
Batch: 200 | Loss: 6.875 | Acc: 27.169,42.903,54.963,%
Batch: 220 | Loss: 6.875 | Acc: 27.220,42.902,54.963,%
Batch: 240 | Loss: 6.887 | Acc: 27.175,42.794,54.820,%
Batch: 260 | Loss: 6.877 | Acc: 27.143,42.795,54.816,%
Batch: 280 | Loss: 6.868 | Acc: 27.174,42.816,54.924,%
Batch: 300 | Loss: 6.863 | Acc: 27.248,42.842,54.921,%
Batch: 320 | Loss: 6.863 | Acc: 27.280,42.794,54.868,%
Batch: 340 | Loss: 6.860 | Acc: 27.378,42.797,54.880,%
Batch: 360 | Loss: 6.851 | Acc: 27.465,42.962,54.960,%
Batch: 380 | Loss: 6.841 | Acc: 27.510,43.079,55.055,%
Batch: 0 | Loss: 7.042 | Acc: 31.250,42.969,54.688,%
Batch: 20 | Loss: 7.204 | Acc: 25.409,41.890,53.757,%
Batch: 40 | Loss: 7.237 | Acc: 25.076,40.816,51.963,%
Batch: 60 | Loss: 7.254 | Acc: 24.680,40.574,51.678,%
Train all parameters

Epoch: 10
Batch: 0 | Loss: 6.461 | Acc: 25.781,49.219,56.250,%
Batch: 20 | Loss: 6.839 | Acc: 27.232,44.234,55.469,%
Batch: 40 | Loss: 6.737 | Acc: 27.954,44.550,56.212,%
Batch: 60 | Loss: 6.727 | Acc: 27.459,44.762,56.826,%
Batch: 80 | Loss: 6.714 | Acc: 27.537,44.502,56.694,%
Batch: 100 | Loss: 6.683 | Acc: 27.847,44.462,56.931,%
Batch: 120 | Loss: 6.666 | Acc: 27.912,44.473,56.786,%
Batch: 140 | Loss: 6.638 | Acc: 28.197,44.587,56.893,%
Batch: 160 | Loss: 6.627 | Acc: 28.198,44.488,56.769,%
Batch: 180 | Loss: 6.605 | Acc: 28.371,44.639,56.820,%
Batch: 200 | Loss: 6.633 | Acc: 28.331,44.419,56.681,%
Batch: 220 | Loss: 6.614 | Acc: 28.471,44.694,56.840,%
Batch: 240 | Loss: 6.614 | Acc: 28.433,44.765,56.765,%
Batch: 260 | Loss: 6.619 | Acc: 28.487,44.648,56.645,%
Batch: 280 | Loss: 6.611 | Acc: 28.539,44.720,56.731,%
Batch: 300 | Loss: 6.620 | Acc: 28.556,44.741,56.691,%
Batch: 320 | Loss: 6.606 | Acc: 28.568,44.772,56.783,%
Batch: 340 | Loss: 6.606 | Acc: 28.650,44.879,56.765,%
Batch: 360 | Loss: 6.602 | Acc: 28.662,44.942,56.817,%
Batch: 380 | Loss: 6.596 | Acc: 28.644,45.003,56.890,%
Batch: 0 | Loss: 6.498 | Acc: 32.812,50.781,60.156,%
Batch: 20 | Loss: 7.050 | Acc: 24.814,43.229,54.390,%
Batch: 40 | Loss: 7.024 | Acc: 24.848,42.854,54.135,%
Batch: 60 | Loss: 7.031 | Acc: 24.808,42.661,54.098,%
Train all parameters

Epoch: 11
Batch: 0 | Loss: 6.683 | Acc: 33.594,46.094,57.031,%
Batch: 20 | Loss: 6.360 | Acc: 30.506,46.615,57.254,%
Batch: 40 | Loss: 6.324 | Acc: 30.126,47.675,58.994,%
Batch: 60 | Loss: 6.368 | Acc: 29.892,47.170,59.413,%
Batch: 80 | Loss: 6.400 | Acc: 29.716,46.354,58.951,%
Batch: 100 | Loss: 6.383 | Acc: 29.595,46.357,59.151,%
Batch: 120 | Loss: 6.377 | Acc: 29.649,46.526,59.065,%
Batch: 140 | Loss: 6.365 | Acc: 29.715,46.504,59.220,%
Batch: 160 | Loss: 6.359 | Acc: 29.785,46.598,59.263,%
Batch: 180 | Loss: 6.373 | Acc: 29.644,46.487,59.189,%
Batch: 200 | Loss: 6.386 | Acc: 29.493,46.370,59.130,%
Batch: 220 | Loss: 6.368 | Acc: 29.624,46.461,59.230,%
Batch: 240 | Loss: 6.382 | Acc: 29.516,46.528,59.171,%
Batch: 260 | Loss: 6.382 | Acc: 29.601,46.507,59.079,%
Batch: 280 | Loss: 6.390 | Acc: 29.560,46.511,59.125,%
Batch: 300 | Loss: 6.392 | Acc: 29.591,46.504,59.123,%
Batch: 320 | Loss: 6.381 | Acc: 29.683,46.593,59.229,%
Batch: 340 | Loss: 6.387 | Acc: 29.671,46.566,59.112,%
Batch: 360 | Loss: 6.389 | Acc: 29.651,46.576,59.053,%
Batch: 380 | Loss: 6.391 | Acc: 29.679,46.572,59.016,%
Batch: 0 | Loss: 6.706 | Acc: 29.688,45.312,54.688,%
Batch: 20 | Loss: 6.865 | Acc: 26.525,44.829,55.878,%
Batch: 40 | Loss: 6.873 | Acc: 26.258,44.512,55.030,%
Batch: 60 | Loss: 6.876 | Acc: 26.537,44.595,55.392,%
Train all parameters

Epoch: 12
Batch: 0 | Loss: 6.307 | Acc: 24.219,48.438,58.594,%
Batch: 20 | Loss: 6.205 | Acc: 29.501,48.735,62.240,%
Batch: 40 | Loss: 6.239 | Acc: 28.525,47.790,61.890,%
Batch: 60 | Loss: 6.204 | Acc: 29.559,48.066,61.360,%
Batch: 80 | Loss: 6.197 | Acc: 29.986,48.139,61.285,%
Batch: 100 | Loss: 6.192 | Acc: 30.051,48.430,61.448,%
Batch: 120 | Loss: 6.178 | Acc: 30.320,48.463,61.635,%
Batch: 140 | Loss: 6.177 | Acc: 30.424,48.282,61.564,%
Batch: 160 | Loss: 6.192 | Acc: 30.406,48.331,61.457,%
Batch: 180 | Loss: 6.183 | Acc: 30.594,48.446,61.473,%
Batch: 200 | Loss: 6.186 | Acc: 30.605,48.469,61.416,%
Batch: 220 | Loss: 6.196 | Acc: 30.585,48.452,61.390,%
Batch: 240 | Loss: 6.203 | Acc: 30.563,48.418,61.242,%
Batch: 260 | Loss: 6.213 | Acc: 30.454,48.234,61.054,%
Batch: 280 | Loss: 6.218 | Acc: 30.430,48.196,60.896,%
Batch: 300 | Loss: 6.212 | Acc: 30.505,48.206,60.865,%
Batch: 320 | Loss: 6.199 | Acc: 30.637,48.355,60.952,%
Batch: 340 | Loss: 6.206 | Acc: 30.592,48.353,60.919,%
Batch: 360 | Loss: 6.214 | Acc: 30.583,48.305,60.862,%
Batch: 380 | Loss: 6.210 | Acc: 30.664,48.360,60.872,%
Batch: 0 | Loss: 6.539 | Acc: 34.375,49.219,58.594,%
Batch: 20 | Loss: 6.739 | Acc: 26.749,45.052,55.878,%
Batch: 40 | Loss: 6.733 | Acc: 27.382,45.084,55.926,%
Batch: 60 | Loss: 6.729 | Acc: 27.395,45.530,56.212,%
Train all parameters

Epoch: 13
Batch: 0 | Loss: 6.279 | Acc: 32.812,48.438,62.500,%
Batch: 20 | Loss: 6.120 | Acc: 31.510,47.731,62.649,%
Batch: 40 | Loss: 6.094 | Acc: 30.507,47.809,62.919,%
Batch: 60 | Loss: 6.029 | Acc: 31.148,48.796,63.204,%
Batch: 80 | Loss: 5.988 | Acc: 31.481,48.997,63.378,%
Batch: 100 | Loss: 6.042 | Acc: 31.095,48.600,62.786,%
Batch: 120 | Loss: 6.040 | Acc: 31.315,48.702,62.649,%
Batch: 140 | Loss: 6.043 | Acc: 31.377,49.030,62.633,%
Batch: 160 | Loss: 6.044 | Acc: 31.405,49.185,62.481,%
Batch: 180 | Loss: 6.024 | Acc: 31.556,49.452,62.526,%
Batch: 200 | Loss: 6.027 | Acc: 31.514,49.366,62.500,%
Batch: 220 | Loss: 6.045 | Acc: 31.395,49.374,62.373,%
Batch: 240 | Loss: 6.051 | Acc: 31.286,49.407,62.338,%
Batch: 260 | Loss: 6.047 | Acc: 31.415,49.461,62.386,%
Batch: 280 | Loss: 6.049 | Acc: 31.464,49.494,62.383,%
Batch: 300 | Loss: 6.053 | Acc: 31.523,49.507,62.386,%
Batch: 320 | Loss: 6.048 | Acc: 31.557,49.550,62.352,%
Batch: 340 | Loss: 6.053 | Acc: 31.589,49.519,62.257,%
Batch: 360 | Loss: 6.054 | Acc: 31.572,49.530,62.271,%
Batch: 380 | Loss: 6.055 | Acc: 31.574,49.524,62.229,%
Batch: 0 | Loss: 6.334 | Acc: 29.688,51.562,55.469,%
Batch: 20 | Loss: 6.585 | Acc: 27.716,46.391,57.552,%
Batch: 40 | Loss: 6.590 | Acc: 27.572,46.113,57.489,%
Batch: 60 | Loss: 6.603 | Acc: 27.613,46.183,57.300,%
Train all parameters

Epoch: 14
Batch: 0 | Loss: 5.873 | Acc: 25.781,52.344,65.625,%
Batch: 20 | Loss: 5.834 | Acc: 32.403,51.637,64.621,%
Batch: 40 | Loss: 5.851 | Acc: 32.184,51.315,63.948,%
Batch: 60 | Loss: 5.790 | Acc: 32.800,51.639,64.383,%
Batch: 80 | Loss: 5.852 | Acc: 32.571,51.389,64.246,%
Batch: 100 | Loss: 5.871 | Acc: 32.325,51.168,64.310,%
Batch: 120 | Loss: 5.878 | Acc: 32.212,51.130,64.250,%
Batch: 140 | Loss: 5.864 | Acc: 32.364,51.369,64.301,%
Batch: 160 | Loss: 5.889 | Acc: 32.216,51.237,64.150,%
Batch: 180 | Loss: 5.899 | Acc: 32.213,51.092,64.218,%
Batch: 200 | Loss: 5.907 | Acc: 32.253,51.073,64.164,%
Batch: 220 | Loss: 5.911 | Acc: 32.222,50.848,63.992,%
Batch: 240 | Loss: 5.922 | Acc: 32.096,50.840,63.823,%
Batch: 260 | Loss: 5.929 | Acc: 32.112,50.952,63.937,%
Batch: 280 | Loss: 5.925 | Acc: 32.204,51.076,63.935,%
Batch: 300 | Loss: 5.927 | Acc: 32.161,51.030,63.868,%
Batch: 320 | Loss: 5.921 | Acc: 32.255,51.068,63.819,%
Batch: 340 | Loss: 5.920 | Acc: 32.281,51.029,63.790,%
Batch: 360 | Loss: 5.922 | Acc: 32.217,50.972,63.679,%
Batch: 380 | Loss: 5.926 | Acc: 32.218,50.984,63.675,%
Batch: 0 | Loss: 6.423 | Acc: 30.469,52.344,61.719,%
Batch: 20 | Loss: 6.654 | Acc: 26.674,46.949,57.031,%
Batch: 40 | Loss: 6.627 | Acc: 27.058,46.627,56.784,%
Batch: 60 | Loss: 6.632 | Acc: 27.203,46.529,56.519,%
Train all parameters

Epoch: 15
Batch: 0 | Loss: 5.398 | Acc: 30.469,53.125,70.312,%
Batch: 20 | Loss: 5.618 | Acc: 33.185,53.237,67.969,%
Batch: 40 | Loss: 5.708 | Acc: 33.346,52.591,67.302,%
Batch: 60 | Loss: 5.686 | Acc: 33.312,52.613,67.175,%
Batch: 80 | Loss: 5.709 | Acc: 33.169,52.469,66.908,%
Batch: 100 | Loss: 5.735 | Acc: 32.588,52.220,66.685,%
Batch: 120 | Loss: 5.738 | Acc: 32.599,51.995,66.445,%
Batch: 140 | Loss: 5.752 | Acc: 32.569,52.072,66.240,%
Batch: 160 | Loss: 5.744 | Acc: 32.648,52.203,66.363,%
Batch: 180 | Loss: 5.742 | Acc: 32.730,52.141,66.285,%
Batch: 200 | Loss: 5.741 | Acc: 32.739,52.223,66.294,%
Batch: 220 | Loss: 5.740 | Acc: 32.816,52.270,66.198,%
Batch: 240 | Loss: 5.743 | Acc: 32.894,52.292,66.053,%
Batch: 260 | Loss: 5.751 | Acc: 32.774,52.263,65.915,%
Batch: 280 | Loss: 5.752 | Acc: 32.696,52.238,65.834,%
Batch: 300 | Loss: 5.756 | Acc: 32.727,52.276,65.669,%
Batch: 320 | Loss: 5.751 | Acc: 32.761,52.232,65.696,%
Batch: 340 | Loss: 5.760 | Acc: 32.755,52.087,65.554,%
Batch: 360 | Loss: 5.769 | Acc: 32.724,52.030,65.419,%
Batch: 380 | Loss: 5.777 | Acc: 32.724,52.022,65.342,%
Batch: 0 | Loss: 5.986 | Acc: 35.938,55.469,66.406,%
Batch: 20 | Loss: 6.404 | Acc: 28.683,49.702,59.115,%
Batch: 40 | Loss: 6.406 | Acc: 28.963,49.085,58.594,%
Batch: 60 | Loss: 6.411 | Acc: 28.663,49.052,58.888,%
Train all parameters

Epoch: 16
Batch: 0 | Loss: 5.456 | Acc: 28.125,46.094,66.406,%
Batch: 20 | Loss: 5.446 | Acc: 35.491,51.190,68.452,%
Batch: 40 | Loss: 5.517 | Acc: 35.080,53.030,69.131,%
Batch: 60 | Loss: 5.515 | Acc: 34.465,53.919,68.981,%
Batch: 80 | Loss: 5.578 | Acc: 34.288,53.472,68.181,%
Batch: 100 | Loss: 5.591 | Acc: 33.934,53.287,68.031,%
Batch: 120 | Loss: 5.600 | Acc: 33.936,53.222,67.943,%
Batch: 140 | Loss: 5.598 | Acc: 33.932,53.197,67.886,%
Batch: 160 | Loss: 5.611 | Acc: 33.720,53.237,67.949,%
Batch: 180 | Loss: 5.614 | Acc: 33.702,53.324,67.632,%
Batch: 200 | Loss: 5.635 | Acc: 33.586,53.183,67.467,%
Batch: 220 | Loss: 5.647 | Acc: 33.442,52.994,67.248,%
Batch: 240 | Loss: 5.649 | Acc: 33.441,52.986,67.243,%
Batch: 260 | Loss: 5.657 | Acc: 33.405,52.877,67.217,%
Batch: 280 | Loss: 5.652 | Acc: 33.491,52.978,67.213,%
Batch: 300 | Loss: 5.655 | Acc: 33.480,52.915,67.011,%
Batch: 320 | Loss: 5.661 | Acc: 33.443,52.865,66.822,%
Batch: 340 | Loss: 5.660 | Acc: 33.520,52.894,66.754,%
Batch: 360 | Loss: 5.668 | Acc: 33.466,52.889,66.595,%
Batch: 380 | Loss: 5.675 | Acc: 33.436,52.867,66.429,%
Batch: 0 | Loss: 6.040 | Acc: 32.812,57.812,59.375,%
Batch: 20 | Loss: 6.425 | Acc: 28.757,48.735,58.668,%
Batch: 40 | Loss: 6.419 | Acc: 28.925,47.866,58.365,%
Batch: 60 | Loss: 6.438 | Acc: 28.765,47.707,58.350,%
Train all parameters

Epoch: 17
Batch: 0 | Loss: 5.331 | Acc: 34.375,49.219,69.531,%
Batch: 20 | Loss: 5.573 | Acc: 33.705,54.501,69.196,%
Batch: 40 | Loss: 5.479 | Acc: 34.261,54.287,69.588,%
Batch: 60 | Loss: 5.463 | Acc: 34.362,53.842,69.390,%
Batch: 80 | Loss: 5.500 | Acc: 33.652,53.694,69.107,%
Batch: 100 | Loss: 5.526 | Acc: 33.470,53.481,68.781,%
Batch: 120 | Loss: 5.501 | Acc: 33.600,53.706,68.653,%
Batch: 140 | Loss: 5.491 | Acc: 33.760,53.613,68.395,%
Batch: 160 | Loss: 5.523 | Acc: 33.565,53.513,68.076,%
Batch: 180 | Loss: 5.529 | Acc: 33.620,53.522,67.904,%
Batch: 200 | Loss: 5.530 | Acc: 33.637,53.751,67.934,%
Batch: 220 | Loss: 5.533 | Acc: 33.700,53.740,67.852,%
Batch: 240 | Loss: 5.540 | Acc: 33.717,53.783,67.752,%
Batch: 260 | Loss: 5.534 | Acc: 33.821,53.807,67.753,%
Batch: 280 | Loss: 5.539 | Acc: 33.808,53.714,67.713,%
Batch: 300 | Loss: 5.539 | Acc: 33.817,53.675,67.665,%
Batch: 320 | Loss: 5.552 | Acc: 33.793,53.670,67.684,%
Batch: 340 | Loss: 5.549 | Acc: 33.802,53.769,67.701,%
Batch: 360 | Loss: 5.550 | Acc: 33.806,53.807,67.620,%
Batch: 380 | Loss: 5.554 | Acc: 33.901,53.765,67.509,%
Batch: 0 | Loss: 6.105 | Acc: 36.719,53.125,61.719,%
Batch: 20 | Loss: 6.312 | Acc: 30.246,48.847,58.891,%
Batch: 40 | Loss: 6.280 | Acc: 30.926,49.257,59.089,%
Batch: 60 | Loss: 6.293 | Acc: 30.661,49.103,58.940,%
Train all parameters

Epoch: 18
Batch: 0 | Loss: 5.573 | Acc: 27.344,50.781,62.500,%
Batch: 20 | Loss: 5.478 | Acc: 32.664,54.799,70.610,%
Batch: 40 | Loss: 5.452 | Acc: 33.384,55.316,69.627,%
Batch: 60 | Loss: 5.378 | Acc: 34.209,56.045,69.877,%
Batch: 80 | Loss: 5.403 | Acc: 34.597,55.671,69.763,%
Batch: 100 | Loss: 5.423 | Acc: 34.406,55.306,69.678,%
Batch: 120 | Loss: 5.391 | Acc: 34.575,55.378,69.970,%
Batch: 140 | Loss: 5.393 | Acc: 34.480,55.042,69.936,%
Batch: 160 | Loss: 5.405 | Acc: 34.385,54.930,69.827,%
Batch: 180 | Loss: 5.409 | Acc: 34.379,54.852,69.695,%
Batch: 200 | Loss: 5.420 | Acc: 34.422,54.707,69.411,%
Batch: 220 | Loss: 5.412 | Acc: 34.513,54.794,69.443,%
Batch: 240 | Loss: 5.415 | Acc: 34.440,54.671,69.353,%
Batch: 260 | Loss: 5.427 | Acc: 34.498,54.664,69.271,%
Batch: 280 | Loss: 5.425 | Acc: 34.614,54.785,69.256,%
Batch: 300 | Loss: 5.413 | Acc: 34.692,54.893,69.248,%
Batch: 320 | Loss: 5.415 | Acc: 34.687,54.821,69.164,%
Batch: 340 | Loss: 5.429 | Acc: 34.600,54.701,68.958,%
Batch: 360 | Loss: 5.432 | Acc: 34.546,54.642,68.869,%
Batch: 380 | Loss: 5.440 | Acc: 34.500,54.677,68.791,%
Batch: 0 | Loss: 6.082 | Acc: 35.156,49.219,67.188,%
Batch: 20 | Loss: 6.352 | Acc: 30.580,49.219,60.231,%
Batch: 40 | Loss: 6.362 | Acc: 30.507,48.666,58.975,%
Batch: 60 | Loss: 6.343 | Acc: 30.738,48.783,58.901,%
Train all parameters

Epoch: 19
Batch: 0 | Loss: 4.764 | Acc: 38.281,61.719,78.125,%
Batch: 20 | Loss: 5.282 | Acc: 35.491,56.994,71.652,%
Batch: 40 | Loss: 5.242 | Acc: 35.042,57.184,71.532,%
Batch: 60 | Loss: 5.302 | Acc: 34.273,56.173,71.350,%
Batch: 80 | Loss: 5.264 | Acc: 34.433,56.009,71.769,%
Batch: 100 | Loss: 5.296 | Acc: 34.282,55.801,71.651,%
Batch: 120 | Loss: 5.278 | Acc: 34.588,55.843,71.636,%
Batch: 140 | Loss: 5.277 | Acc: 34.430,55.929,71.243,%
Batch: 160 | Loss: 5.276 | Acc: 34.419,55.852,71.118,%
Batch: 180 | Loss: 5.285 | Acc: 34.586,55.870,71.146,%
Batch: 200 | Loss: 5.293 | Acc: 34.558,55.881,71.164,%
Batch: 220 | Loss: 5.299 | Acc: 34.562,55.953,70.991,%
Batch: 240 | Loss: 5.312 | Acc: 34.450,55.978,70.880,%
Batch: 260 | Loss: 5.313 | Acc: 34.516,55.966,70.863,%
Batch: 280 | Loss: 5.325 | Acc: 34.445,55.969,70.696,%
Batch: 300 | Loss: 5.331 | Acc: 34.357,55.985,70.606,%
Batch: 320 | Loss: 5.334 | Acc: 34.382,55.861,70.439,%
Batch: 340 | Loss: 5.346 | Acc: 34.338,55.831,70.377,%
Batch: 360 | Loss: 5.337 | Acc: 34.531,55.854,70.325,%
Batch: 380 | Loss: 5.343 | Acc: 34.609,55.823,70.259,%
Batch: 0 | Loss: 5.863 | Acc: 35.938,56.250,64.062,%
Batch: 20 | Loss: 6.092 | Acc: 31.585,50.149,61.421,%
Batch: 40 | Loss: 6.110 | Acc: 31.364,50.229,61.147,%
Batch: 60 | Loss: 6.132 | Acc: 30.802,50.243,61.219,%
Train all parameters

Epoch: 20
Batch: 0 | Loss: 5.159 | Acc: 36.719,56.250,73.438,%
Batch: 20 | Loss: 5.207 | Acc: 35.826,55.283,71.801,%
Batch: 40 | Loss: 5.238 | Acc: 34.985,54.573,71.970,%
Batch: 60 | Loss: 5.203 | Acc: 35.207,55.686,72.144,%
Batch: 80 | Loss: 5.209 | Acc: 34.905,55.565,71.904,%
Batch: 100 | Loss: 5.198 | Acc: 35.110,56.026,71.906,%
Batch: 120 | Loss: 5.189 | Acc: 35.169,56.250,71.836,%
Batch: 140 | Loss: 5.200 | Acc: 35.062,56.167,71.809,%
Batch: 160 | Loss: 5.211 | Acc: 35.020,56.134,71.642,%
Batch: 180 | Loss: 5.214 | Acc: 35.186,56.306,71.681,%
Batch: 200 | Loss: 5.210 | Acc: 35.386,56.483,71.770,%
Batch: 220 | Loss: 5.208 | Acc: 35.404,56.554,71.772,%
Batch: 240 | Loss: 5.213 | Acc: 35.283,56.542,71.655,%
Batch: 260 | Loss: 5.234 | Acc: 35.147,56.358,71.483,%
Batch: 280 | Loss: 5.226 | Acc: 35.340,56.397,71.458,%
Batch: 300 | Loss: 5.233 | Acc: 35.328,56.403,71.405,%
Batch: 320 | Loss: 5.235 | Acc: 35.358,56.304,71.276,%
Batch: 340 | Loss: 5.244 | Acc: 35.307,56.252,71.229,%
Batch: 360 | Loss: 5.237 | Acc: 35.409,56.347,71.252,%
Batch: 380 | Loss: 5.234 | Acc: 35.441,56.289,71.176,%
Batch: 0 | Loss: 5.520 | Acc: 36.719,58.594,67.969,%
Batch: 20 | Loss: 6.054 | Acc: 31.362,51.562,61.793,%
Batch: 40 | Loss: 6.025 | Acc: 31.612,51.639,61.738,%
Batch: 60 | Loss: 6.027 | Acc: 31.583,51.729,61.834,%
Train all parameters

Epoch: 21
Batch: 0 | Loss: 5.147 | Acc: 34.375,53.906,71.875,%
Batch: 20 | Loss: 5.182 | Acc: 34.412,56.882,74.033,%
Batch: 40 | Loss: 5.107 | Acc: 35.804,57.431,73.571,%
Batch: 60 | Loss: 5.114 | Acc: 35.848,57.313,73.322,%
Batch: 80 | Loss: 5.159 | Acc: 35.484,57.436,73.322,%
Batch: 100 | Loss: 5.180 | Acc: 35.149,57.263,73.004,%
Batch: 120 | Loss: 5.170 | Acc: 35.440,57.264,72.843,%
Batch: 140 | Loss: 5.171 | Acc: 35.533,57.164,72.662,%
Batch: 160 | Loss: 5.173 | Acc: 35.656,56.963,72.559,%
Batch: 180 | Loss: 5.166 | Acc: 35.743,57.044,72.518,%
Batch: 200 | Loss: 5.171 | Acc: 35.724,57.086,72.481,%
Batch: 220 | Loss: 5.163 | Acc: 35.920,57.109,72.515,%
Batch: 240 | Loss: 5.158 | Acc: 35.905,57.180,72.413,%
Batch: 260 | Loss: 5.164 | Acc: 35.767,57.046,72.384,%
Batch: 280 | Loss: 5.172 | Acc: 35.754,57.020,72.220,%
Batch: 300 | Loss: 5.176 | Acc: 35.691,56.974,72.140,%
Batch: 320 | Loss: 5.177 | Acc: 35.680,57.036,72.014,%
Batch: 340 | Loss: 5.184 | Acc: 35.713,56.979,71.953,%
Batch: 360 | Loss: 5.189 | Acc: 35.708,56.994,71.938,%
Batch: 380 | Loss: 5.196 | Acc: 35.634,56.931,71.854,%
Batch: 0 | Loss: 6.064 | Acc: 36.719,53.906,62.500,%
Batch: 20 | Loss: 6.232 | Acc: 30.134,51.302,61.198,%
Batch: 40 | Loss: 6.238 | Acc: 30.926,50.514,59.870,%
Batch: 60 | Loss: 6.235 | Acc: 30.430,50.525,60.272,%
Train all parameters

Epoch: 22
Batch: 0 | Loss: 5.234 | Acc: 42.969,57.031,78.125,%
Batch: 20 | Loss: 5.098 | Acc: 36.384,57.626,74.070,%
Batch: 40 | Loss: 5.009 | Acc: 36.300,58.365,75.781,%
Batch: 60 | Loss: 4.963 | Acc: 37.141,58.210,75.051,%
Batch: 80 | Loss: 4.965 | Acc: 37.018,58.362,74.537,%
Batch: 100 | Loss: 4.999 | Acc: 36.564,58.176,74.157,%
Batch: 120 | Loss: 5.019 | Acc: 36.661,57.780,73.689,%
Batch: 140 | Loss: 5.026 | Acc: 36.713,57.729,73.476,%
Batch: 160 | Loss: 5.032 | Acc: 36.641,57.691,73.462,%
Batch: 180 | Loss: 5.064 | Acc: 36.481,57.402,73.269,%
Batch: 200 | Loss: 5.064 | Acc: 36.583,57.595,73.305,%
Batch: 220 | Loss: 5.069 | Acc: 36.599,57.675,73.187,%
Batch: 240 | Loss: 5.086 | Acc: 36.349,57.621,72.961,%
Batch: 260 | Loss: 5.091 | Acc: 36.285,57.642,72.830,%
Batch: 280 | Loss: 5.095 | Acc: 36.304,57.726,72.806,%
Batch: 300 | Loss: 5.090 | Acc: 36.350,57.672,72.739,%
Batch: 320 | Loss: 5.093 | Acc: 36.237,57.657,72.707,%
Batch: 340 | Loss: 5.100 | Acc: 36.201,57.670,72.636,%
Batch: 360 | Loss: 5.099 | Acc: 36.113,57.648,72.576,%
Batch: 380 | Loss: 5.106 | Acc: 36.089,57.571,72.447,%
Batch: 0 | Loss: 5.600 | Acc: 35.156,53.906,70.312,%
Batch: 20 | Loss: 5.956 | Acc: 31.213,52.716,63.839,%
Batch: 40 | Loss: 5.954 | Acc: 31.707,52.496,62.862,%
Batch: 60 | Loss: 5.957 | Acc: 31.506,52.549,62.577,%
Train all parameters

Epoch: 23
Batch: 0 | Loss: 4.916 | Acc: 35.938,56.250,82.812,%
Batch: 20 | Loss: 4.910 | Acc: 35.379,57.552,76.488,%
Batch: 40 | Loss: 4.944 | Acc: 36.071,57.489,75.991,%
Batch: 60 | Loss: 4.959 | Acc: 36.219,58.491,75.935,%
Batch: 80 | Loss: 4.933 | Acc: 36.680,58.661,75.829,%
Batch: 100 | Loss: 4.924 | Acc: 36.425,58.818,75.727,%
Batch: 120 | Loss: 4.944 | Acc: 36.318,58.710,75.633,%
Batch: 140 | Loss: 4.947 | Acc: 36.458,58.893,75.643,%
Batch: 160 | Loss: 4.968 | Acc: 36.350,58.788,75.427,%
Batch: 180 | Loss: 4.979 | Acc: 36.425,58.611,75.177,%
Batch: 200 | Loss: 4.985 | Acc: 36.353,58.574,75.058,%
Batch: 220 | Loss: 4.989 | Acc: 36.312,58.523,74.894,%
Batch: 240 | Loss: 4.990 | Acc: 36.258,58.513,74.760,%
Batch: 260 | Loss: 4.994 | Acc: 36.270,58.552,74.581,%
Batch: 280 | Loss: 5.008 | Acc: 36.288,58.502,74.477,%
Batch: 300 | Loss: 5.009 | Acc: 36.290,58.508,74.395,%
Batch: 320 | Loss: 5.017 | Acc: 36.359,58.450,74.216,%
Batch: 340 | Loss: 5.020 | Acc: 36.290,58.424,74.106,%
Batch: 360 | Loss: 5.028 | Acc: 36.299,58.338,73.989,%
Batch: 380 | Loss: 5.032 | Acc: 36.237,58.329,73.948,%
Batch: 0 | Loss: 5.726 | Acc: 36.719,56.250,65.625,%
Batch: 20 | Loss: 5.951 | Acc: 33.668,52.679,62.054,%
Batch: 40 | Loss: 5.920 | Acc: 33.613,52.611,62.062,%
Batch: 60 | Loss: 5.921 | Acc: 32.915,52.574,62.116,%
Train all parameters

Epoch: 24
Batch: 0 | Loss: 5.047 | Acc: 30.469,57.812,78.906,%
Batch: 20 | Loss: 4.820 | Acc: 37.054,60.454,75.744,%
Batch: 40 | Loss: 4.869 | Acc: 36.547,60.290,75.553,%
Batch: 60 | Loss: 4.862 | Acc: 36.783,60.259,75.461,%
Batch: 80 | Loss: 4.832 | Acc: 36.777,60.446,75.878,%
Batch: 100 | Loss: 4.824 | Acc: 37.059,60.412,75.673,%
Batch: 120 | Loss: 4.835 | Acc: 37.061,60.150,75.536,%
Batch: 140 | Loss: 4.861 | Acc: 36.846,59.924,75.344,%
Batch: 160 | Loss: 4.871 | Acc: 36.855,59.865,75.393,%
Batch: 180 | Loss: 4.884 | Acc: 36.909,59.548,75.229,%
Batch: 200 | Loss: 4.886 | Acc: 36.936,59.499,75.361,%
Batch: 220 | Loss: 4.884 | Acc: 36.956,59.428,75.304,%
Batch: 240 | Loss: 4.887 | Acc: 36.981,59.372,75.107,%
Batch: 260 | Loss: 4.903 | Acc: 36.961,59.198,74.988,%
Batch: 280 | Loss: 4.910 | Acc: 36.961,59.058,74.939,%
Batch: 300 | Loss: 4.917 | Acc: 36.926,59.012,74.782,%
Batch: 320 | Loss: 4.921 | Acc: 36.994,58.934,74.686,%
Batch: 340 | Loss: 4.937 | Acc: 36.950,58.743,74.544,%
Batch: 360 | Loss: 4.947 | Acc: 36.918,58.570,74.375,%
Batch: 380 | Loss: 4.946 | Acc: 36.981,58.542,74.321,%
Batch: 0 | Loss: 5.519 | Acc: 32.812,58.594,65.625,%
Batch: 20 | Loss: 5.950 | Acc: 32.068,53.609,61.719,%
Batch: 40 | Loss: 5.948 | Acc: 32.774,52.649,61.547,%
Batch: 60 | Loss: 5.964 | Acc: 32.031,52.382,61.591,%
Train all parameters

Epoch: 25
Batch: 0 | Loss: 5.107 | Acc: 39.844,59.375,75.781,%
Batch: 20 | Loss: 4.923 | Acc: 36.570,60.119,76.376,%
Batch: 40 | Loss: 4.878 | Acc: 36.509,59.737,77.248,%
Batch: 60 | Loss: 4.815 | Acc: 37.295,59.939,77.536,%
Batch: 80 | Loss: 4.804 | Acc: 37.346,60.012,77.701,%
Batch: 100 | Loss: 4.790 | Acc: 37.376,60.179,77.483,%
Batch: 120 | Loss: 4.803 | Acc: 37.590,60.098,77.240,%
Batch: 140 | Loss: 4.807 | Acc: 37.361,59.885,77.039,%
Batch: 160 | Loss: 4.814 | Acc: 37.369,59.744,76.849,%
Batch: 180 | Loss: 4.814 | Acc: 37.526,59.643,76.722,%
Batch: 200 | Loss: 4.832 | Acc: 37.484,59.476,76.454,%
Batch: 220 | Loss: 4.835 | Acc: 37.648,59.371,76.213,%
Batch: 240 | Loss: 4.837 | Acc: 37.669,59.284,76.186,%
Batch: 260 | Loss: 4.845 | Acc: 37.674,59.216,76.116,%
Batch: 280 | Loss: 4.844 | Acc: 37.728,59.180,76.034,%
Batch: 300 | Loss: 4.848 | Acc: 37.632,59.196,75.877,%
Batch: 320 | Loss: 4.861 | Acc: 37.571,59.236,75.764,%
Batch: 340 | Loss: 4.868 | Acc: 37.550,59.079,75.687,%
Batch: 360 | Loss: 4.871 | Acc: 37.513,59.182,75.645,%
Batch: 380 | Loss: 4.873 | Acc: 37.453,59.193,75.562,%
Batch: 0 | Loss: 5.312 | Acc: 39.844,59.375,68.750,%
Batch: 20 | Loss: 5.792 | Acc: 33.222,54.315,63.542,%
Batch: 40 | Loss: 5.818 | Acc: 33.213,53.392,63.300,%
Batch: 60 | Loss: 5.826 | Acc: 32.915,53.381,63.076,%
Train all parameters

Epoch: 26
Batch: 0 | Loss: 4.859 | Acc: 35.156,67.188,80.469,%
Batch: 20 | Loss: 4.626 | Acc: 38.765,60.751,79.129,%
Batch: 40 | Loss: 4.699 | Acc: 37.633,59.870,78.982,%
Batch: 60 | Loss: 4.729 | Acc: 37.346,59.657,78.176,%
Batch: 80 | Loss: 4.710 | Acc: 37.809,59.645,77.961,%
Batch: 100 | Loss: 4.729 | Acc: 37.763,60.025,78.117,%
Batch: 120 | Loss: 4.723 | Acc: 38.100,60.059,78.022,%
Batch: 140 | Loss: 4.730 | Acc: 38.259,60.068,77.903,%
Batch: 160 | Loss: 4.741 | Acc: 38.223,60.016,77.698,%
Batch: 180 | Loss: 4.749 | Acc: 38.044,59.940,77.538,%
Batch: 200 | Loss: 4.757 | Acc: 37.974,59.954,77.278,%
Batch: 220 | Loss: 4.753 | Acc: 38.041,59.983,77.185,%
Batch: 240 | Loss: 4.756 | Acc: 37.989,59.997,77.029,%
Batch: 260 | Loss: 4.766 | Acc: 37.829,60.001,77.011,%
Batch: 280 | Loss: 4.784 | Acc: 37.714,59.842,76.729,%
Batch: 300 | Loss: 4.799 | Acc: 37.552,59.759,76.581,%
Batch: 320 | Loss: 4.800 | Acc: 37.619,59.764,76.465,%
Batch: 340 | Loss: 4.818 | Acc: 37.555,59.707,76.340,%
Batch: 360 | Loss: 4.817 | Acc: 37.567,59.749,76.273,%
Batch: 380 | Loss: 4.823 | Acc: 37.525,59.640,76.107,%
Batch: 0 | Loss: 5.348 | Acc: 37.500,61.719,67.969,%
Batch: 20 | Loss: 5.734 | Acc: 34.375,55.432,63.244,%
Batch: 40 | Loss: 5.768 | Acc: 34.394,54.002,62.633,%
Batch: 60 | Loss: 5.763 | Acc: 34.029,54.175,62.590,%
Train all parameters

Epoch: 27
Batch: 0 | Loss: 5.158 | Acc: 34.375,59.375,79.688,%
Batch: 20 | Loss: 4.808 | Acc: 37.686,59.896,78.088,%
Batch: 40 | Loss: 4.721 | Acc: 37.900,60.595,78.601,%
Batch: 60 | Loss: 4.730 | Acc: 37.295,60.912,78.829,%
Batch: 80 | Loss: 4.700 | Acc: 37.741,61.323,78.964,%
Batch: 100 | Loss: 4.673 | Acc: 38.034,61.386,79.177,%
Batch: 120 | Loss: 4.668 | Acc: 37.984,61.396,79.074,%
Batch: 140 | Loss: 4.684 | Acc: 38.032,61.303,78.995,%
Batch: 160 | Loss: 4.694 | Acc: 37.951,61.224,78.843,%
Batch: 180 | Loss: 4.697 | Acc: 38.208,61.175,78.565,%
Batch: 200 | Loss: 4.716 | Acc: 38.009,60.930,78.455,%
Batch: 220 | Loss: 4.727 | Acc: 37.907,60.831,78.334,%
Batch: 240 | Loss: 4.744 | Acc: 37.805,60.698,78.148,%
Batch: 260 | Loss: 4.750 | Acc: 37.907,60.647,77.975,%
Batch: 280 | Loss: 4.756 | Acc: 37.884,60.671,77.922,%
Batch: 300 | Loss: 4.751 | Acc: 37.941,60.771,77.827,%
Batch: 320 | Loss: 4.747 | Acc: 38.016,60.809,77.770,%
Batch: 340 | Loss: 4.752 | Acc: 37.974,60.770,77.655,%
Batch: 360 | Loss: 4.758 | Acc: 37.870,60.730,77.519,%
Batch: 380 | Loss: 4.762 | Acc: 37.922,60.708,77.379,%
Batch: 0 | Loss: 5.695 | Acc: 38.281,57.812,65.625,%
Batch: 20 | Loss: 5.917 | Acc: 34.598,53.720,62.463,%
Batch: 40 | Loss: 5.925 | Acc: 34.223,52.877,61.890,%
Batch: 60 | Loss: 5.918 | Acc: 33.760,52.626,61.924,%
Train all parameters

Epoch: 28
Batch: 0 | Loss: 5.135 | Acc: 31.250,60.156,82.031,%
Batch: 20 | Loss: 4.698 | Acc: 38.393,59.673,79.688,%
Batch: 40 | Loss: 4.695 | Acc: 38.281,59.337,78.754,%
Batch: 60 | Loss: 4.695 | Acc: 38.358,60.207,79.137,%
Batch: 80 | Loss: 4.708 | Acc: 38.349,60.330,79.215,%
Batch: 100 | Loss: 4.682 | Acc: 38.637,60.721,79.192,%
Batch: 120 | Loss: 4.699 | Acc: 38.475,60.789,79.087,%
Batch: 140 | Loss: 4.677 | Acc: 38.691,61.198,79.172,%
Batch: 160 | Loss: 4.683 | Acc: 38.572,61.374,79.144,%
Batch: 180 | Loss: 4.678 | Acc: 38.640,61.529,78.945,%
Batch: 200 | Loss: 4.676 | Acc: 38.623,61.318,78.895,%
Batch: 220 | Loss: 4.677 | Acc: 38.465,61.206,78.765,%
Batch: 240 | Loss: 4.672 | Acc: 38.434,61.226,78.725,%
Batch: 260 | Loss: 4.686 | Acc: 38.281,61.045,78.436,%
Batch: 280 | Loss: 4.692 | Acc: 38.273,61.015,78.306,%
Batch: 300 | Loss: 4.698 | Acc: 38.227,60.976,78.195,%
Batch: 320 | Loss: 4.698 | Acc: 38.250,60.969,78.071,%
Batch: 340 | Loss: 4.704 | Acc: 38.215,60.866,77.994,%
Batch: 360 | Loss: 4.701 | Acc: 38.288,60.860,77.822,%
Batch: 380 | Loss: 4.702 | Acc: 38.289,60.903,77.795,%
Batch: 0 | Loss: 5.431 | Acc: 36.719,58.594,66.406,%
Batch: 20 | Loss: 5.958 | Acc: 32.812,52.604,61.682,%
Batch: 40 | Loss: 5.960 | Acc: 32.774,51.734,61.452,%
Batch: 60 | Loss: 5.951 | Acc: 32.595,52.126,61.360,%
Train all parameters

Epoch: 29
Batch: 0 | Loss: 5.552 | Acc: 27.344,56.250,82.812,%
Batch: 20 | Loss: 4.682 | Acc: 37.314,61.793,79.650,%
Batch: 40 | Loss: 4.656 | Acc: 37.386,61.604,80.069,%
Batch: 60 | Loss: 4.609 | Acc: 37.897,62.295,80.315,%
Batch: 80 | Loss: 4.584 | Acc: 38.194,62.240,80.170,%
Batch: 100 | Loss: 4.603 | Acc: 37.918,62.260,80.105,%
Batch: 120 | Loss: 4.604 | Acc: 38.152,61.971,80.062,%
Batch: 140 | Loss: 4.585 | Acc: 38.287,62.123,80.037,%
Batch: 160 | Loss: 4.589 | Acc: 38.378,61.923,79.906,%
Batch: 180 | Loss: 4.595 | Acc: 38.458,61.878,79.739,%
Batch: 200 | Loss: 4.594 | Acc: 38.343,61.905,79.633,%
Batch: 220 | Loss: 4.599 | Acc: 38.391,61.927,79.606,%
Batch: 240 | Loss: 4.603 | Acc: 38.417,61.829,79.568,%
Batch: 260 | Loss: 4.610 | Acc: 38.500,61.695,79.430,%
Batch: 280 | Loss: 4.620 | Acc: 38.573,61.602,79.245,%
Batch: 300 | Loss: 4.639 | Acc: 38.468,61.412,78.940,%
Batch: 320 | Loss: 4.639 | Acc: 38.456,61.368,78.741,%
Batch: 340 | Loss: 4.651 | Acc: 38.481,61.311,78.636,%
Batch: 360 | Loss: 4.655 | Acc: 38.461,61.279,78.556,%
Batch: 380 | Loss: 4.666 | Acc: 38.412,61.274,78.453,%
Batch: 0 | Loss: 5.476 | Acc: 39.844,57.812,67.188,%
Batch: 20 | Loss: 5.786 | Acc: 34.226,54.836,63.690,%
Batch: 40 | Loss: 5.759 | Acc: 34.489,54.611,62.710,%
Batch: 60 | Loss: 5.751 | Acc: 34.068,54.841,63.179,%
Train all parameters

Epoch: 30
Batch: 0 | Loss: 4.676 | Acc: 34.375,67.969,82.031,%
Batch: 20 | Loss: 4.421 | Acc: 38.281,62.946,80.097,%
Batch: 40 | Loss: 4.478 | Acc: 38.548,62.005,80.469,%
Batch: 60 | Loss: 4.466 | Acc: 38.794,62.346,80.507,%
Batch: 80 | Loss: 4.527 | Acc: 38.686,61.777,80.353,%
Batch: 100 | Loss: 4.514 | Acc: 38.939,61.796,80.384,%
Batch: 120 | Loss: 4.538 | Acc: 38.540,61.680,80.152,%
Batch: 140 | Loss: 4.546 | Acc: 38.719,61.658,80.098,%
Batch: 160 | Loss: 4.537 | Acc: 39.024,61.835,80.051,%
Batch: 180 | Loss: 4.548 | Acc: 38.946,61.654,79.951,%
Batch: 200 | Loss: 4.566 | Acc: 38.763,61.478,79.730,%
Batch: 220 | Loss: 4.579 | Acc: 38.652,61.524,79.627,%
Batch: 240 | Loss: 4.582 | Acc: 38.579,61.469,79.574,%
Batch: 260 | Loss: 4.583 | Acc: 38.637,61.437,79.493,%
Batch: 280 | Loss: 4.585 | Acc: 38.629,61.380,79.448,%
Batch: 300 | Loss: 4.601 | Acc: 38.585,61.293,79.225,%
Batch: 320 | Loss: 4.598 | Acc: 38.627,61.373,79.145,%
Batch: 340 | Loss: 4.601 | Acc: 38.600,61.336,79.044,%
Batch: 360 | Loss: 4.605 | Acc: 38.628,61.392,78.943,%
Batch: 380 | Loss: 4.611 | Acc: 38.548,61.403,78.869,%
Batch: 0 | Loss: 5.647 | Acc: 35.156,62.500,63.281,%
Batch: 20 | Loss: 6.025 | Acc: 31.027,53.274,61.905,%
Batch: 40 | Loss: 6.029 | Acc: 31.460,52.268,61.814,%
Batch: 60 | Loss: 6.029 | Acc: 31.109,52.113,61.744,%
Train all parameters

Epoch: 31
Batch: 0 | Loss: 4.984 | Acc: 35.938,60.156,75.000,%
Batch: 20 | Loss: 4.509 | Acc: 38.988,62.500,80.469,%
Batch: 40 | Loss: 4.461 | Acc: 38.891,62.881,80.469,%
Batch: 60 | Loss: 4.491 | Acc: 38.448,62.244,80.802,%
Batch: 80 | Loss: 4.475 | Acc: 38.792,62.616,81.076,%
Batch: 100 | Loss: 4.478 | Acc: 38.923,62.570,80.825,%
Batch: 120 | Loss: 4.478 | Acc: 38.972,62.829,80.714,%
Batch: 140 | Loss: 4.491 | Acc: 39.212,62.594,80.585,%
Batch: 160 | Loss: 4.493 | Acc: 39.101,62.694,80.760,%
Batch: 180 | Loss: 4.514 | Acc: 38.933,62.569,80.516,%
Batch: 200 | Loss: 4.517 | Acc: 38.864,62.426,80.294,%
Batch: 220 | Loss: 4.530 | Acc: 38.790,62.253,80.069,%
Batch: 240 | Loss: 4.521 | Acc: 39.069,62.208,79.892,%
Batch: 260 | Loss: 4.524 | Acc: 38.991,62.159,79.840,%
Batch: 280 | Loss: 4.535 | Acc: 38.990,62.083,79.768,%
Batch: 300 | Loss: 4.544 | Acc: 38.943,61.958,79.643,%
Batch: 320 | Loss: 4.545 | Acc: 38.977,62.011,79.612,%
Batch: 340 | Loss: 4.552 | Acc: 38.870,61.939,79.523,%
Batch: 360 | Loss: 4.562 | Acc: 38.809,61.833,79.419,%
Batch: 380 | Loss: 4.571 | Acc: 38.835,61.715,79.216,%
Batch: 0 | Loss: 5.518 | Acc: 36.719,58.594,68.750,%
Batch: 20 | Loss: 5.796 | Acc: 34.263,54.985,63.058,%
Batch: 40 | Loss: 5.781 | Acc: 34.394,54.478,63.148,%
Batch: 60 | Loss: 5.793 | Acc: 34.209,54.393,62.999,%
Train all parameters

Epoch: 32
Batch: 0 | Loss: 4.721 | Acc: 39.844,57.031,79.688,%
Batch: 20 | Loss: 4.467 | Acc: 40.365,63.095,80.134,%
Batch: 40 | Loss: 4.422 | Acc: 40.091,62.748,81.155,%
Batch: 60 | Loss: 4.344 | Acc: 40.484,63.128,81.519,%
Batch: 80 | Loss: 4.358 | Acc: 40.557,63.243,81.404,%
Batch: 100 | Loss: 4.388 | Acc: 40.130,63.405,81.327,%
Batch: 120 | Loss: 4.386 | Acc: 40.309,63.449,81.192,%
Batch: 140 | Loss: 4.409 | Acc: 39.982,63.193,80.834,%
Batch: 160 | Loss: 4.423 | Acc: 39.887,63.107,80.663,%
Batch: 180 | Loss: 4.443 | Acc: 39.848,62.901,80.546,%
Batch: 200 | Loss: 4.455 | Acc: 39.719,62.811,80.465,%
Batch: 220 | Loss: 4.460 | Acc: 39.674,62.868,80.433,%
Batch: 240 | Loss: 4.467 | Acc: 39.665,62.763,80.300,%
Batch: 260 | Loss: 4.477 | Acc: 39.476,62.757,80.235,%
Batch: 280 | Loss: 4.489 | Acc: 39.435,62.519,80.068,%
Batch: 300 | Loss: 4.494 | Acc: 39.405,62.490,79.973,%
Batch: 320 | Loss: 4.502 | Acc: 39.340,62.468,79.875,%
Batch: 340 | Loss: 4.515 | Acc: 39.294,62.401,79.802,%
Batch: 360 | Loss: 4.522 | Acc: 39.255,62.338,79.683,%
Batch: 380 | Loss: 4.527 | Acc: 39.157,62.281,79.636,%
Batch: 0 | Loss: 5.170 | Acc: 42.188,60.156,68.750,%
Batch: 20 | Loss: 5.544 | Acc: 35.900,56.696,66.295,%
Batch: 40 | Loss: 5.527 | Acc: 35.347,56.574,65.511,%
Batch: 60 | Loss: 5.561 | Acc: 34.849,55.789,64.767,%
Train all parameters

Epoch: 33
Batch: 0 | Loss: 4.337 | Acc: 40.625,70.312,84.375,%
Batch: 20 | Loss: 4.399 | Acc: 40.402,63.839,82.850,%
Batch: 40 | Loss: 4.381 | Acc: 40.434,63.148,82.565,%
Batch: 60 | Loss: 4.342 | Acc: 40.804,63.858,82.454,%
Batch: 80 | Loss: 4.362 | Acc: 40.201,63.677,82.282,%
Batch: 100 | Loss: 4.370 | Acc: 39.968,63.614,82.441,%
Batch: 120 | Loss: 4.378 | Acc: 39.844,63.565,82.257,%
Batch: 140 | Loss: 4.400 | Acc: 39.617,63.497,82.170,%
Batch: 160 | Loss: 4.418 | Acc: 39.587,63.281,82.007,%
Batch: 180 | Loss: 4.414 | Acc: 39.650,63.221,81.854,%
Batch: 200 | Loss: 4.421 | Acc: 39.564,63.184,81.643,%
Batch: 220 | Loss: 4.419 | Acc: 39.635,63.097,81.487,%
Batch: 240 | Loss: 4.421 | Acc: 39.685,62.973,81.480,%
Batch: 260 | Loss: 4.427 | Acc: 39.736,63.054,81.325,%
Batch: 280 | Loss: 4.442 | Acc: 39.649,62.889,81.186,%
Batch: 300 | Loss: 4.441 | Acc: 39.724,62.882,81.112,%
Batch: 320 | Loss: 4.438 | Acc: 39.800,62.831,81.007,%
Batch: 340 | Loss: 4.441 | Acc: 39.780,62.851,80.874,%
Batch: 360 | Loss: 4.445 | Acc: 39.768,62.758,80.754,%
Batch: 380 | Loss: 4.449 | Acc: 39.799,62.728,80.637,%
Batch: 0 | Loss: 5.294 | Acc: 43.750,64.062,69.531,%
Batch: 20 | Loss: 5.631 | Acc: 35.528,55.655,63.876,%
Batch: 40 | Loss: 5.605 | Acc: 35.823,55.716,63.758,%
Batch: 60 | Loss: 5.612 | Acc: 35.617,55.251,63.627,%
Train all parameters

Epoch: 34
Batch: 0 | Loss: 4.888 | Acc: 37.500,56.250,80.469,%
Batch: 20 | Loss: 4.303 | Acc: 40.774,64.137,81.882,%
Batch: 40 | Loss: 4.250 | Acc: 40.663,64.215,83.041,%
Batch: 60 | Loss: 4.325 | Acc: 40.190,63.294,82.223,%
Batch: 80 | Loss: 4.382 | Acc: 40.017,63.185,82.012,%
Batch: 100 | Loss: 4.382 | Acc: 39.766,63.382,82.008,%
Batch: 120 | Loss: 4.394 | Acc: 39.631,63.184,81.883,%
Batch: 140 | Loss: 4.371 | Acc: 39.883,63.309,81.638,%
Batch: 160 | Loss: 4.382 | Acc: 39.965,63.403,81.716,%
Batch: 180 | Loss: 4.372 | Acc: 40.262,63.419,81.613,%
Batch: 200 | Loss: 4.382 | Acc: 40.170,63.211,81.522,%
Batch: 220 | Loss: 4.387 | Acc: 40.165,63.147,81.363,%
Batch: 240 | Loss: 4.395 | Acc: 40.058,63.064,81.276,%
Batch: 260 | Loss: 4.397 | Acc: 39.996,63.057,81.256,%
Batch: 280 | Loss: 4.402 | Acc: 39.955,62.995,81.167,%
Batch: 300 | Loss: 4.415 | Acc: 39.789,62.915,81.086,%
Batch: 320 | Loss: 4.412 | Acc: 39.793,62.863,80.970,%
Batch: 340 | Loss: 4.424 | Acc: 39.702,62.777,80.870,%
Batch: 360 | Loss: 4.429 | Acc: 39.710,62.781,80.772,%
Batch: 380 | Loss: 4.436 | Acc: 39.710,62.779,80.707,%
Batch: 0 | Loss: 5.538 | Acc: 37.500,65.625,65.625,%
Batch: 20 | Loss: 5.728 | Acc: 34.226,56.399,63.728,%
Batch: 40 | Loss: 5.709 | Acc: 34.375,55.716,64.082,%
Batch: 60 | Loss: 5.729 | Acc: 33.773,55.443,64.075,%
Train classifier parameters

Epoch: 35
Batch: 0 | Loss: 4.438 | Acc: 39.062,63.281,82.031,%
Batch: 20 | Loss: 4.772 | Acc: 36.310,58.333,76.711,%
Batch: 40 | Loss: 5.062 | Acc: 35.118,56.726,74.181,%
Batch: 60 | Loss: 5.102 | Acc: 34.618,56.890,74.039,%
Batch: 80 | Loss: 5.088 | Acc: 34.819,57.928,74.286,%
Batch: 100 | Loss: 5.092 | Acc: 34.808,57.836,74.273,%
Batch: 120 | Loss: 5.080 | Acc: 35.305,58.161,74.348,%
Batch: 140 | Loss: 5.039 | Acc: 35.383,58.533,74.590,%
Batch: 160 | Loss: 5.036 | Acc: 35.525,58.657,74.617,%
Batch: 180 | Loss: 5.023 | Acc: 35.808,58.624,74.564,%
Batch: 200 | Loss: 5.012 | Acc: 36.081,58.703,74.681,%
Batch: 220 | Loss: 4.998 | Acc: 36.185,58.806,74.671,%
Batch: 240 | Loss: 4.999 | Acc: 36.161,58.860,74.695,%
Batch: 260 | Loss: 5.000 | Acc: 36.168,58.959,74.832,%
Batch: 280 | Loss: 5.000 | Acc: 36.079,58.858,74.933,%
Batch: 300 | Loss: 4.998 | Acc: 36.021,58.887,75.036,%
Batch: 320 | Loss: 4.977 | Acc: 36.210,59.017,75.105,%
Batch: 340 | Loss: 4.977 | Acc: 36.277,59.041,75.172,%
Batch: 360 | Loss: 4.981 | Acc: 36.273,59.003,75.208,%
Batch: 380 | Loss: 4.971 | Acc: 36.305,59.039,75.306,%
Batch: 0 | Loss: 5.469 | Acc: 38.281,62.500,67.188,%
Batch: 20 | Loss: 5.887 | Acc: 32.887,55.320,63.839,%
Batch: 40 | Loss: 5.899 | Acc: 33.060,54.116,63.377,%
Batch: 60 | Loss: 5.906 | Acc: 32.838,54.239,63.320,%
Train classifier parameters

Epoch: 36
Batch: 0 | Loss: 5.222 | Acc: 36.719,60.938,78.125,%
Batch: 20 | Loss: 4.689 | Acc: 38.653,62.872,78.757,%
Batch: 40 | Loss: 4.812 | Acc: 37.824,61.852,77.934,%
Batch: 60 | Loss: 4.839 | Acc: 38.064,61.642,77.626,%
Batch: 80 | Loss: 4.837 | Acc: 37.992,61.439,77.488,%
Batch: 100 | Loss: 4.869 | Acc: 37.392,61.231,77.212,%
Batch: 120 | Loss: 4.823 | Acc: 37.616,61.415,77.499,%
Batch: 140 | Loss: 4.816 | Acc: 37.517,61.436,77.604,%
Batch: 160 | Loss: 4.805 | Acc: 37.553,61.219,77.567,%
Batch: 180 | Loss: 4.828 | Acc: 37.345,60.881,77.460,%
Batch: 200 | Loss: 4.832 | Acc: 37.259,60.864,77.425,%
Batch: 220 | Loss: 4.826 | Acc: 37.253,60.828,77.475,%
Batch: 240 | Loss: 4.830 | Acc: 37.137,60.724,77.454,%
Batch: 260 | Loss: 4.828 | Acc: 37.075,60.644,77.392,%
Batch: 280 | Loss: 4.827 | Acc: 37.186,60.476,77.352,%
Batch: 300 | Loss: 4.822 | Acc: 37.243,60.455,77.424,%
Batch: 320 | Loss: 4.827 | Acc: 37.252,60.431,77.439,%
Batch: 340 | Loss: 4.831 | Acc: 37.220,60.383,77.406,%
Batch: 360 | Loss: 4.829 | Acc: 37.262,60.379,77.435,%
Batch: 380 | Loss: 4.829 | Acc: 37.258,60.404,77.448,%
Batch: 0 | Loss: 5.479 | Acc: 39.062,60.938,69.531,%
Batch: 20 | Loss: 5.825 | Acc: 34.152,55.097,63.579,%
Batch: 40 | Loss: 5.820 | Acc: 33.746,54.840,63.510,%
Batch: 60 | Loss: 5.830 | Acc: 33.453,54.777,63.294,%
Train classifier parameters

Epoch: 37
Batch: 0 | Loss: 3.357 | Acc: 45.312,72.656,87.500,%
Batch: 20 | Loss: 4.826 | Acc: 37.984,60.379,78.348,%
Batch: 40 | Loss: 4.720 | Acc: 38.796,60.728,78.449,%
Batch: 60 | Loss: 4.700 | Acc: 39.088,60.502,78.330,%
Batch: 80 | Loss: 4.682 | Acc: 38.966,60.841,78.231,%
Batch: 100 | Loss: 4.693 | Acc: 38.823,60.667,78.226,%
Batch: 120 | Loss: 4.697 | Acc: 38.946,60.563,77.983,%
Batch: 140 | Loss: 4.686 | Acc: 39.018,60.583,78.114,%
Batch: 160 | Loss: 4.694 | Acc: 38.931,60.622,78.091,%
Batch: 180 | Loss: 4.696 | Acc: 38.855,60.571,78.082,%
Batch: 200 | Loss: 4.693 | Acc: 38.818,60.560,78.168,%
Batch: 220 | Loss: 4.696 | Acc: 38.850,60.679,78.061,%
Batch: 240 | Loss: 4.691 | Acc: 38.978,60.649,78.041,%
Batch: 260 | Loss: 4.707 | Acc: 38.751,60.569,78.023,%
Batch: 280 | Loss: 4.704 | Acc: 38.673,60.662,78.092,%
Batch: 300 | Loss: 4.717 | Acc: 38.499,60.634,78.083,%
Batch: 320 | Loss: 4.713 | Acc: 38.456,60.765,78.152,%
Batch: 340 | Loss: 4.705 | Acc: 38.515,60.782,78.157,%
Batch: 360 | Loss: 4.706 | Acc: 38.515,60.803,78.175,%
Batch: 380 | Loss: 4.710 | Acc: 38.394,60.761,78.180,%
Batch: 0 | Loss: 5.430 | Acc: 39.062,59.375,66.406,%
Batch: 20 | Loss: 5.783 | Acc: 34.189,55.990,64.211,%
Batch: 40 | Loss: 5.787 | Acc: 34.280,55.259,63.891,%
Batch: 60 | Loss: 5.795 | Acc: 33.927,55.110,63.640,%
Train classifier parameters

Epoch: 38
Batch: 0 | Loss: 4.959 | Acc: 38.281,63.281,76.562,%
Batch: 20 | Loss: 4.587 | Acc: 39.174,60.565,80.171,%
Batch: 40 | Loss: 4.608 | Acc: 38.967,61.585,79.611,%
Batch: 60 | Loss: 4.617 | Acc: 38.473,61.424,79.803,%
Batch: 80 | Loss: 4.686 | Acc: 38.137,61.246,79.292,%
Batch: 100 | Loss: 4.687 | Acc: 38.026,61.518,79.208,%
Batch: 120 | Loss: 4.678 | Acc: 38.230,61.583,79.093,%
Batch: 140 | Loss: 4.690 | Acc: 38.314,61.469,79.161,%
Batch: 160 | Loss: 4.680 | Acc: 38.272,61.432,79.052,%
Batch: 180 | Loss: 4.689 | Acc: 38.255,61.464,79.088,%
Batch: 200 | Loss: 4.690 | Acc: 38.246,61.548,79.097,%
Batch: 220 | Loss: 4.696 | Acc: 38.196,61.676,79.069,%
Batch: 240 | Loss: 4.690 | Acc: 38.207,61.709,79.062,%
Batch: 260 | Loss: 4.691 | Acc: 38.120,61.665,79.056,%
Batch: 280 | Loss: 4.694 | Acc: 38.034,61.480,78.981,%
Batch: 300 | Loss: 4.685 | Acc: 38.115,61.498,78.904,%
Batch: 320 | Loss: 4.698 | Acc: 38.113,61.337,78.819,%
Batch: 340 | Loss: 4.696 | Acc: 38.151,61.228,78.808,%
Batch: 360 | Loss: 4.696 | Acc: 38.158,61.143,78.748,%
Batch: 380 | Loss: 4.695 | Acc: 38.162,61.194,78.720,%
Batch: 0 | Loss: 5.408 | Acc: 38.281,60.156,65.625,%
Batch: 20 | Loss: 5.797 | Acc: 34.115,55.097,63.021,%
Batch: 40 | Loss: 5.793 | Acc: 33.803,54.421,63.377,%
Batch: 60 | Loss: 5.795 | Acc: 33.607,54.764,63.422,%
Train classifier parameters

Epoch: 39
Batch: 0 | Loss: 4.355 | Acc: 44.531,60.156,78.906,%
Batch: 20 | Loss: 4.550 | Acc: 39.546,63.393,80.432,%
Batch: 40 | Loss: 4.596 | Acc: 39.005,62.252,79.954,%
Batch: 60 | Loss: 4.523 | Acc: 39.472,63.038,80.085,%
Batch: 80 | Loss: 4.557 | Acc: 39.217,62.587,79.823,%
Batch: 100 | Loss: 4.587 | Acc: 39.202,62.314,79.510,%
Batch: 120 | Loss: 4.582 | Acc: 39.140,62.384,79.662,%
Batch: 140 | Loss: 4.554 | Acc: 39.362,62.555,79.671,%
Batch: 160 | Loss: 4.576 | Acc: 38.970,62.398,79.581,%
Batch: 180 | Loss: 4.598 | Acc: 38.700,62.297,79.519,%
Batch: 200 | Loss: 4.606 | Acc: 38.588,62.185,79.536,%
Batch: 220 | Loss: 4.601 | Acc: 38.589,62.157,79.535,%
Batch: 240 | Loss: 4.611 | Acc: 38.447,62.033,79.483,%
Batch: 260 | Loss: 4.617 | Acc: 38.419,61.862,79.424,%
Batch: 280 | Loss: 4.631 | Acc: 38.412,61.799,79.354,%
Batch: 300 | Loss: 4.633 | Acc: 38.473,61.729,79.238,%
Batch: 320 | Loss: 4.640 | Acc: 38.439,61.799,79.262,%
Batch: 340 | Loss: 4.636 | Acc: 38.538,61.824,79.241,%
Batch: 360 | Loss: 4.637 | Acc: 38.541,61.792,79.255,%
Batch: 380 | Loss: 4.643 | Acc: 38.431,61.795,79.224,%
Batch: 0 | Loss: 5.471 | Acc: 41.406,59.375,67.188,%
Batch: 20 | Loss: 5.729 | Acc: 34.896,57.180,64.286,%
Batch: 40 | Loss: 5.729 | Acc: 34.566,55.812,64.082,%
Batch: 60 | Loss: 5.732 | Acc: 34.234,55.776,64.178,%
Train classifier parameters

Epoch: 40
Batch: 0 | Loss: 4.678 | Acc: 35.938,60.156,82.812,%
Batch: 20 | Loss: 4.732 | Acc: 37.277,61.496,80.134,%
Batch: 40 | Loss: 4.716 | Acc: 38.014,61.547,79.345,%
Batch: 60 | Loss: 4.728 | Acc: 38.268,61.514,79.355,%
Batch: 80 | Loss: 4.688 | Acc: 38.677,62.085,79.601,%
Batch: 100 | Loss: 4.664 | Acc: 38.807,61.827,79.463,%
Batch: 120 | Loss: 4.635 | Acc: 38.817,62.029,79.513,%
Batch: 140 | Loss: 4.608 | Acc: 38.852,62.223,79.510,%
Batch: 160 | Loss: 4.630 | Acc: 38.699,62.058,79.372,%
Batch: 180 | Loss: 4.635 | Acc: 38.557,61.943,79.416,%
Batch: 200 | Loss: 4.633 | Acc: 38.635,61.835,79.287,%
Batch: 220 | Loss: 4.644 | Acc: 38.578,61.878,79.246,%
Batch: 240 | Loss: 4.644 | Acc: 38.579,61.829,79.286,%
Batch: 260 | Loss: 4.628 | Acc: 38.793,61.865,79.292,%
Batch: 280 | Loss: 4.623 | Acc: 38.840,61.869,79.207,%
Batch: 300 | Loss: 4.632 | Acc: 38.751,61.789,79.231,%
Batch: 320 | Loss: 4.625 | Acc: 38.766,61.836,79.318,%
Batch: 340 | Loss: 4.627 | Acc: 38.776,61.875,79.332,%
Batch: 360 | Loss: 4.619 | Acc: 38.866,61.929,79.289,%
Batch: 380 | Loss: 4.627 | Acc: 38.765,61.942,79.263,%
Batch: 0 | Loss: 5.371 | Acc: 36.719,60.938,67.969,%
Batch: 20 | Loss: 5.700 | Acc: 34.821,55.506,64.844,%
Batch: 40 | Loss: 5.702 | Acc: 34.318,55.069,64.558,%
Batch: 60 | Loss: 5.710 | Acc: 34.298,55.353,64.306,%
Train classifier parameters

Epoch: 41
Batch: 0 | Loss: 4.106 | Acc: 41.406,66.406,85.938,%
Batch: 20 | Loss: 4.547 | Acc: 37.165,64.844,81.659,%
Batch: 40 | Loss: 4.566 | Acc: 37.805,64.234,80.431,%
Batch: 60 | Loss: 4.555 | Acc: 37.999,63.909,80.379,%
Batch: 80 | Loss: 4.571 | Acc: 38.137,63.503,80.276,%
Batch: 100 | Loss: 4.566 | Acc: 38.219,63.165,80.260,%
Batch: 120 | Loss: 4.566 | Acc: 38.494,62.849,80.010,%
Batch: 140 | Loss: 4.557 | Acc: 38.719,62.755,80.031,%
Batch: 160 | Loss: 4.565 | Acc: 38.606,62.670,79.984,%
Batch: 180 | Loss: 4.567 | Acc: 38.717,62.617,79.964,%
Batch: 200 | Loss: 4.558 | Acc: 38.829,62.745,79.952,%
Batch: 220 | Loss: 4.569 | Acc: 38.769,62.518,79.818,%
Batch: 240 | Loss: 4.566 | Acc: 38.871,62.510,79.726,%
Batch: 260 | Loss: 4.568 | Acc: 38.913,62.443,79.711,%
Batch: 280 | Loss: 4.572 | Acc: 38.943,62.414,79.701,%
Batch: 300 | Loss: 4.586 | Acc: 38.813,62.368,79.688,%
Batch: 320 | Loss: 4.586 | Acc: 38.836,62.354,79.668,%
Batch: 340 | Loss: 4.591 | Acc: 38.778,62.298,79.690,%
Batch: 360 | Loss: 4.601 | Acc: 38.785,62.121,79.599,%
Batch: 380 | Loss: 4.610 | Acc: 38.747,62.078,79.573,%
Batch: 0 | Loss: 5.329 | Acc: 41.406,61.719,63.281,%
Batch: 20 | Loss: 5.697 | Acc: 35.379,56.324,64.286,%
Batch: 40 | Loss: 5.692 | Acc: 34.909,55.564,64.310,%
Batch: 60 | Loss: 5.702 | Acc: 34.631,55.584,64.191,%
Train classifier parameters

Epoch: 42
Batch: 0 | Loss: 4.718 | Acc: 32.812,57.812,81.250,%
Batch: 20 | Loss: 4.515 | Acc: 39.695,63.356,81.027,%
Batch: 40 | Loss: 4.468 | Acc: 40.511,63.377,81.174,%
Batch: 60 | Loss: 4.508 | Acc: 39.511,63.217,80.584,%
Batch: 80 | Loss: 4.496 | Acc: 39.632,62.751,80.179,%
Batch: 100 | Loss: 4.516 | Acc: 39.813,62.500,79.533,%
Batch: 120 | Loss: 4.546 | Acc: 39.508,62.500,79.539,%
Batch: 140 | Loss: 4.549 | Acc: 39.395,62.550,79.654,%
Batch: 160 | Loss: 4.548 | Acc: 39.320,62.471,79.775,%
Batch: 180 | Loss: 4.565 | Acc: 39.145,62.371,79.752,%
Batch: 200 | Loss: 4.550 | Acc: 39.171,62.442,79.765,%
Batch: 220 | Loss: 4.535 | Acc: 39.306,62.433,79.801,%
Batch: 240 | Loss: 4.535 | Acc: 39.293,62.490,79.814,%
Batch: 260 | Loss: 4.539 | Acc: 39.422,62.317,79.756,%
Batch: 280 | Loss: 4.542 | Acc: 39.354,62.161,79.757,%
Batch: 300 | Loss: 4.546 | Acc: 39.294,62.199,79.768,%
Batch: 320 | Loss: 4.542 | Acc: 39.384,62.237,79.751,%
Batch: 340 | Loss: 4.543 | Acc: 39.349,62.145,79.646,%
Batch: 360 | Loss: 4.548 | Acc: 39.290,62.208,79.651,%
Batch: 380 | Loss: 4.557 | Acc: 39.319,62.186,79.638,%
Batch: 0 | Loss: 5.332 | Acc: 40.625,61.719,67.969,%
Batch: 20 | Loss: 5.678 | Acc: 34.859,56.734,65.030,%
Batch: 40 | Loss: 5.672 | Acc: 34.889,55.755,64.672,%
Batch: 60 | Loss: 5.690 | Acc: 34.657,55.840,64.447,%
Train classifier parameters

Epoch: 43
Batch: 0 | Loss: 4.594 | Acc: 42.969,62.500,76.562,%
Batch: 20 | Loss: 4.641 | Acc: 38.579,62.723,80.990,%
Batch: 40 | Loss: 4.549 | Acc: 40.168,63.319,80.602,%
Batch: 60 | Loss: 4.570 | Acc: 39.844,62.833,80.161,%
Batch: 80 | Loss: 4.547 | Acc: 40.181,63.079,80.189,%
Batch: 100 | Loss: 4.518 | Acc: 40.292,63.165,80.500,%
Batch: 120 | Loss: 4.542 | Acc: 40.102,62.887,80.165,%
Batch: 140 | Loss: 4.558 | Acc: 39.866,62.661,80.125,%
Batch: 160 | Loss: 4.565 | Acc: 39.790,62.597,80.081,%
Batch: 180 | Loss: 4.544 | Acc: 39.792,62.720,80.197,%
Batch: 200 | Loss: 4.548 | Acc: 39.591,62.714,80.236,%
Batch: 220 | Loss: 4.560 | Acc: 39.522,62.482,80.122,%
Batch: 240 | Loss: 4.573 | Acc: 39.338,62.429,80.102,%
Batch: 260 | Loss: 4.572 | Acc: 39.233,62.347,80.098,%
Batch: 280 | Loss: 4.568 | Acc: 39.174,62.378,80.102,%
Batch: 300 | Loss: 4.559 | Acc: 39.291,62.445,80.194,%
Batch: 320 | Loss: 4.561 | Acc: 39.194,62.405,80.162,%
Batch: 340 | Loss: 4.561 | Acc: 39.166,62.365,80.173,%
Batch: 360 | Loss: 4.559 | Acc: 39.147,62.312,80.157,%
Batch: 380 | Loss: 4.563 | Acc: 39.034,62.313,80.042,%
Batch: 0 | Loss: 5.298 | Acc: 41.406,64.844,65.625,%
Batch: 20 | Loss: 5.654 | Acc: 35.082,57.031,64.360,%
Batch: 40 | Loss: 5.660 | Acc: 35.156,55.888,64.348,%
Batch: 60 | Loss: 5.675 | Acc: 34.785,55.955,64.114,%
Train classifier parameters

Epoch: 44
Batch: 0 | Loss: 4.120 | Acc: 42.969,64.844,77.344,%
Batch: 20 | Loss: 4.368 | Acc: 39.807,64.472,80.357,%
Batch: 40 | Loss: 4.383 | Acc: 39.596,64.062,81.002,%
Batch: 60 | Loss: 4.496 | Acc: 39.255,63.166,80.661,%
Batch: 80 | Loss: 4.522 | Acc: 38.889,63.117,80.729,%
Batch: 100 | Loss: 4.522 | Acc: 38.869,63.405,80.693,%
Batch: 120 | Loss: 4.540 | Acc: 38.688,63.217,80.688,%
Batch: 140 | Loss: 4.535 | Acc: 38.492,63.309,80.602,%
Batch: 160 | Loss: 4.542 | Acc: 38.403,63.194,80.629,%
Batch: 180 | Loss: 4.539 | Acc: 38.562,63.113,80.702,%
Batch: 200 | Loss: 4.542 | Acc: 38.584,63.110,80.562,%
Batch: 220 | Loss: 4.546 | Acc: 38.561,63.013,80.550,%
Batch: 240 | Loss: 4.544 | Acc: 38.641,62.892,80.488,%
Batch: 260 | Loss: 4.548 | Acc: 38.823,62.952,80.454,%
Batch: 280 | Loss: 4.543 | Acc: 38.910,62.936,80.438,%
Batch: 300 | Loss: 4.557 | Acc: 38.889,62.736,80.287,%
Batch: 320 | Loss: 4.559 | Acc: 38.931,62.697,80.206,%
Batch: 340 | Loss: 4.555 | Acc: 38.893,62.647,80.194,%
Batch: 360 | Loss: 4.553 | Acc: 38.872,62.677,80.216,%
Batch: 380 | Loss: 4.548 | Acc: 38.857,62.660,80.221,%
Batch: 0 | Loss: 5.381 | Acc: 39.844,60.938,65.625,%
Batch: 20 | Loss: 5.649 | Acc: 35.751,57.217,64.881,%
Batch: 40 | Loss: 5.643 | Acc: 35.232,56.326,64.958,%
Batch: 60 | Loss: 5.651 | Acc: 34.939,56.596,64.728,%
Train classifier parameters

Epoch: 45
Batch: 0 | Loss: 5.245 | Acc: 35.156,59.375,71.875,%
Batch: 20 | Loss: 4.407 | Acc: 40.513,63.318,81.287,%
Batch: 40 | Loss: 4.420 | Acc: 40.053,63.777,81.421,%
Batch: 60 | Loss: 4.449 | Acc: 40.113,63.614,81.276,%
Batch: 80 | Loss: 4.453 | Acc: 39.959,63.397,81.076,%
Batch: 100 | Loss: 4.472 | Acc: 39.929,63.134,80.809,%
Batch: 120 | Loss: 4.474 | Acc: 39.941,63.139,80.940,%
Batch: 140 | Loss: 4.498 | Acc: 39.711,63.248,80.906,%
Batch: 160 | Loss: 4.517 | Acc: 39.460,62.980,80.624,%
Batch: 180 | Loss: 4.539 | Acc: 39.149,62.828,80.525,%
Batch: 200 | Loss: 4.540 | Acc: 39.195,62.819,80.477,%
Batch: 220 | Loss: 4.535 | Acc: 39.172,62.917,80.557,%
Batch: 240 | Loss: 4.535 | Acc: 39.312,62.766,80.560,%
Batch: 260 | Loss: 4.532 | Acc: 39.302,62.760,80.541,%
Batch: 280 | Loss: 4.536 | Acc: 39.213,62.664,80.383,%
Batch: 300 | Loss: 4.545 | Acc: 39.182,62.619,80.303,%
Batch: 320 | Loss: 4.537 | Acc: 39.323,62.663,80.286,%
Batch: 340 | Loss: 4.547 | Acc: 39.255,62.589,80.178,%
Batch: 360 | Loss: 4.545 | Acc: 39.368,62.522,80.216,%
Batch: 380 | Loss: 4.542 | Acc: 39.413,62.543,80.249,%
Batch: 0 | Loss: 5.257 | Acc: 41.406,63.281,67.969,%
Batch: 20 | Loss: 5.651 | Acc: 35.305,56.399,64.658,%
Batch: 40 | Loss: 5.633 | Acc: 35.118,55.812,64.558,%
Batch: 60 | Loss: 5.643 | Acc: 34.913,56.096,64.434,%
Train classifier parameters

Epoch: 46
Batch: 0 | Loss: 4.242 | Acc: 39.062,64.062,78.125,%
Batch: 20 | Loss: 4.336 | Acc: 40.402,63.765,81.324,%
Batch: 40 | Loss: 4.424 | Acc: 39.996,63.148,81.383,%
Batch: 60 | Loss: 4.425 | Acc: 40.100,62.910,80.520,%
Batch: 80 | Loss: 4.444 | Acc: 39.882,63.223,80.237,%
Batch: 100 | Loss: 4.456 | Acc: 39.681,63.134,80.237,%
Batch: 120 | Loss: 4.455 | Acc: 39.728,63.301,80.436,%
Batch: 140 | Loss: 4.440 | Acc: 39.960,63.364,80.557,%
Batch: 160 | Loss: 4.442 | Acc: 39.834,63.441,80.537,%
Batch: 180 | Loss: 4.463 | Acc: 39.632,63.178,80.447,%
Batch: 200 | Loss: 4.459 | Acc: 39.661,63.126,80.445,%
Batch: 220 | Loss: 4.462 | Acc: 39.780,63.133,80.391,%
Batch: 240 | Loss: 4.454 | Acc: 39.928,63.054,80.423,%
Batch: 260 | Loss: 4.469 | Acc: 39.778,62.919,80.295,%
Batch: 280 | Loss: 4.474 | Acc: 39.688,62.767,80.146,%
Batch: 300 | Loss: 4.477 | Acc: 39.704,62.734,80.129,%
Batch: 320 | Loss: 4.474 | Acc: 39.727,62.712,80.162,%
Batch: 340 | Loss: 4.476 | Acc: 39.786,62.681,80.141,%
Batch: 360 | Loss: 4.480 | Acc: 39.820,62.621,80.133,%
Batch: 380 | Loss: 4.488 | Acc: 39.766,62.613,80.104,%
Batch: 0 | Loss: 5.331 | Acc: 39.844,60.156,67.188,%
Batch: 20 | Loss: 5.640 | Acc: 35.603,57.106,65.253,%
Batch: 40 | Loss: 5.636 | Acc: 35.194,56.174,64.787,%
Batch: 60 | Loss: 5.650 | Acc: 35.041,56.301,64.677,%
Train classifier parameters

Epoch: 47
Batch: 0 | Loss: 3.981 | Acc: 46.094,65.625,82.031,%
Batch: 20 | Loss: 4.304 | Acc: 40.737,63.839,81.659,%
Batch: 40 | Loss: 4.373 | Acc: 40.949,63.967,81.250,%
Batch: 60 | Loss: 4.348 | Acc: 41.227,64.229,81.493,%
Batch: 80 | Loss: 4.340 | Acc: 41.503,64.361,81.481,%
Batch: 100 | Loss: 4.378 | Acc: 41.128,64.140,81.320,%
Batch: 120 | Loss: 4.377 | Acc: 40.877,63.882,81.308,%
Batch: 140 | Loss: 4.399 | Acc: 40.730,63.697,81.123,%
Batch: 160 | Loss: 4.407 | Acc: 40.494,63.660,81.104,%
Batch: 180 | Loss: 4.423 | Acc: 40.211,63.432,81.064,%
Batch: 200 | Loss: 4.429 | Acc: 40.217,63.375,81.017,%
Batch: 220 | Loss: 4.436 | Acc: 40.190,63.373,80.974,%
Batch: 240 | Loss: 4.451 | Acc: 39.935,63.336,80.994,%
Batch: 260 | Loss: 4.442 | Acc: 39.957,63.329,80.996,%
Batch: 280 | Loss: 4.444 | Acc: 39.986,63.276,80.877,%
Batch: 300 | Loss: 4.444 | Acc: 39.966,63.237,80.853,%
Batch: 320 | Loss: 4.441 | Acc: 39.965,63.237,80.824,%
Batch: 340 | Loss: 4.454 | Acc: 39.869,63.160,80.728,%
Batch: 360 | Loss: 4.460 | Acc: 39.762,63.154,80.640,%
Batch: 380 | Loss: 4.463 | Acc: 39.776,63.097,80.610,%
Batch: 0 | Loss: 5.278 | Acc: 39.062,60.938,64.844,%
Batch: 20 | Loss: 5.654 | Acc: 35.156,56.734,65.439,%
Batch: 40 | Loss: 5.643 | Acc: 34.661,55.945,64.863,%
Batch: 60 | Loss: 5.652 | Acc: 34.529,56.391,64.703,%
Train classifier parameters

Epoch: 48
Batch: 0 | Loss: 4.605 | Acc: 36.719,67.969,83.594,%
Batch: 20 | Loss: 4.481 | Acc: 40.402,62.054,80.171,%
Batch: 40 | Loss: 4.503 | Acc: 40.015,61.852,80.545,%
Batch: 60 | Loss: 4.447 | Acc: 39.946,62.090,80.699,%
Batch: 80 | Loss: 4.478 | Acc: 39.718,62.230,80.874,%
Batch: 100 | Loss: 4.487 | Acc: 39.705,62.693,80.809,%
Batch: 120 | Loss: 4.505 | Acc: 39.276,62.384,80.669,%
Batch: 140 | Loss: 4.507 | Acc: 39.484,62.262,80.613,%
Batch: 160 | Loss: 4.502 | Acc: 39.562,62.471,80.488,%
Batch: 180 | Loss: 4.515 | Acc: 39.533,62.496,80.516,%
Batch: 200 | Loss: 4.490 | Acc: 39.630,62.523,80.570,%
Batch: 220 | Loss: 4.493 | Acc: 39.653,62.546,80.543,%
Batch: 240 | Loss: 4.497 | Acc: 39.640,62.532,80.443,%
Batch: 260 | Loss: 4.490 | Acc: 39.604,62.653,80.433,%
Batch: 280 | Loss: 4.494 | Acc: 39.552,62.747,80.516,%
Batch: 300 | Loss: 4.500 | Acc: 39.543,62.617,80.451,%
Batch: 320 | Loss: 4.505 | Acc: 39.564,62.653,80.447,%
Batch: 340 | Loss: 4.501 | Acc: 39.686,62.770,80.411,%
Batch: 360 | Loss: 4.503 | Acc: 39.627,62.790,80.393,%
Batch: 380 | Loss: 4.506 | Acc: 39.536,62.857,80.374,%
Batch: 0 | Loss: 5.235 | Acc: 39.062,64.062,69.531,%
Batch: 20 | Loss: 5.626 | Acc: 35.379,57.031,64.509,%
Batch: 40 | Loss: 5.612 | Acc: 35.271,56.212,64.634,%
Batch: 60 | Loss: 5.624 | Acc: 34.939,56.442,64.267,%
Train classifier parameters

Epoch: 49
Batch: 0 | Loss: 4.096 | Acc: 38.281,63.281,83.594,%
Batch: 20 | Loss: 4.389 | Acc: 40.253,63.579,81.957,%
Batch: 40 | Loss: 4.353 | Acc: 39.558,63.377,81.574,%
Batch: 60 | Loss: 4.361 | Acc: 39.869,63.332,81.288,%
Batch: 80 | Loss: 4.397 | Acc: 39.419,63.831,81.530,%
Batch: 100 | Loss: 4.425 | Acc: 39.209,63.490,81.521,%
Batch: 120 | Loss: 4.422 | Acc: 39.301,63.294,81.198,%
Batch: 140 | Loss: 4.428 | Acc: 39.395,63.126,81.012,%
Batch: 160 | Loss: 4.429 | Acc: 39.596,63.242,80.949,%
Batch: 180 | Loss: 4.430 | Acc: 39.688,63.368,80.961,%
Batch: 200 | Loss: 4.452 | Acc: 39.661,63.134,80.955,%
Batch: 220 | Loss: 4.457 | Acc: 39.465,63.030,80.851,%
Batch: 240 | Loss: 4.462 | Acc: 39.484,63.054,80.806,%
Batch: 260 | Loss: 4.463 | Acc: 39.538,63.090,80.882,%
Batch: 280 | Loss: 4.465 | Acc: 39.493,62.995,80.794,%
Batch: 300 | Loss: 4.461 | Acc: 39.582,63.185,80.853,%
Batch: 320 | Loss: 4.462 | Acc: 39.561,63.272,80.848,%
Batch: 340 | Loss: 4.459 | Acc: 39.633,63.176,80.783,%
Batch: 360 | Loss: 4.460 | Acc: 39.681,63.167,80.763,%
Batch: 380 | Loss: 4.462 | Acc: 39.676,63.160,80.700,%
Batch: 0 | Loss: 5.227 | Acc: 37.500,60.156,67.969,%
Batch: 20 | Loss: 5.641 | Acc: 35.193,55.804,64.955,%
Batch: 40 | Loss: 5.640 | Acc: 34.832,55.736,65.015,%
Batch: 60 | Loss: 5.648 | Acc: 34.823,55.955,64.793,%
Train all parameters

Epoch: 50
Batch: 0 | Loss: 4.241 | Acc: 46.094,63.281,82.812,%
Batch: 20 | Loss: 5.003 | Acc: 37.388,59.338,73.400,%
Batch: 40 | Loss: 5.881 | Acc: 33.784,52.534,61.185,%
Batch: 60 | Loss: 6.183 | Acc: 32.492,50.397,56.775,%
Batch: 80 | Loss: 6.226 | Acc: 32.562,49.904,55.758,%
Batch: 100 | Loss: 6.181 | Acc: 33.037,50.464,55.917,%
Batch: 120 | Loss: 6.168 | Acc: 33.335,50.594,55.772,%
Batch: 140 | Loss: 6.145 | Acc: 33.333,50.853,55.995,%
Batch: 160 | Loss: 6.111 | Acc: 33.497,51.014,56.415,%
Batch: 180 | Loss: 6.072 | Acc: 33.835,51.204,56.837,%
Batch: 200 | Loss: 6.041 | Acc: 34.002,51.407,57.121,%
Batch: 220 | Loss: 6.001 | Acc: 34.297,51.601,57.452,%
Batch: 240 | Loss: 5.972 | Acc: 34.297,51.815,57.783,%
Batch: 260 | Loss: 5.939 | Acc: 34.420,52.059,58.172,%
Batch: 280 | Loss: 5.914 | Acc: 34.506,52.302,58.513,%
Batch: 300 | Loss: 5.883 | Acc: 34.585,52.453,58.786,%
Batch: 320 | Loss: 5.857 | Acc: 34.723,52.650,59.073,%
Batch: 340 | Loss: 5.831 | Acc: 34.874,52.889,59.419,%
Batch: 360 | Loss: 5.809 | Acc: 34.981,53.069,59.661,%
Batch: 380 | Loss: 5.790 | Acc: 35.175,53.221,59.922,%
Batch: 0 | Loss: 5.657 | Acc: 35.938,60.156,64.062,%
Batch: 20 | Loss: 6.176 | Acc: 31.920,51.637,58.519,%
Batch: 40 | Loss: 6.213 | Acc: 32.012,50.648,57.203,%
Batch: 60 | Loss: 6.206 | Acc: 31.890,50.525,57.223,%
Train all parameters

Epoch: 51
Batch: 0 | Loss: 5.090 | Acc: 36.719,55.469,67.969,%
Batch: 20 | Loss: 5.069 | Acc: 37.984,59.821,69.531,%
Batch: 40 | Loss: 5.063 | Acc: 37.938,59.661,69.436,%
Batch: 60 | Loss: 5.100 | Acc: 38.230,59.144,68.648,%
Batch: 80 | Loss: 5.128 | Acc: 37.886,58.497,68.239,%
Batch: 100 | Loss: 5.169 | Acc: 37.268,58.029,68.093,%
Batch: 120 | Loss: 5.154 | Acc: 37.106,58.219,68.182,%
Batch: 140 | Loss: 5.151 | Acc: 37.295,57.990,68.323,%
Batch: 160 | Loss: 5.159 | Acc: 37.369,57.963,68.299,%
Batch: 180 | Loss: 5.164 | Acc: 37.383,58.119,68.271,%
Batch: 200 | Loss: 5.148 | Acc: 37.504,58.053,68.272,%
Batch: 220 | Loss: 5.142 | Acc: 37.504,58.078,68.393,%
Batch: 240 | Loss: 5.139 | Acc: 37.555,58.036,68.442,%
Batch: 260 | Loss: 5.130 | Acc: 37.787,58.088,68.412,%
Batch: 280 | Loss: 5.132 | Acc: 37.900,58.152,68.447,%
Batch: 300 | Loss: 5.127 | Acc: 37.954,58.267,68.493,%
Batch: 320 | Loss: 5.133 | Acc: 37.953,58.236,68.443,%
Batch: 340 | Loss: 5.124 | Acc: 37.995,58.294,68.496,%
Batch: 360 | Loss: 5.118 | Acc: 38.065,58.375,68.579,%
Batch: 380 | Loss: 5.120 | Acc: 37.998,58.378,68.606,%
Batch: 0 | Loss: 5.495 | Acc: 34.375,60.938,67.969,%
Batch: 20 | Loss: 6.022 | Acc: 31.808,52.530,60.603,%
Batch: 40 | Loss: 6.019 | Acc: 31.974,51.886,60.480,%
Batch: 60 | Loss: 6.029 | Acc: 31.365,51.524,60.207,%
Train all parameters

Epoch: 52
Batch: 0 | Loss: 5.255 | Acc: 32.031,56.250,69.531,%
Batch: 20 | Loss: 4.867 | Acc: 38.504,61.161,73.438,%
Batch: 40 | Loss: 4.804 | Acc: 38.548,60.957,73.838,%
Batch: 60 | Loss: 4.818 | Acc: 38.332,60.835,73.655,%
Batch: 80 | Loss: 4.825 | Acc: 38.609,60.889,73.283,%
Batch: 100 | Loss: 4.836 | Acc: 38.908,60.574,72.927,%
Batch: 120 | Loss: 4.849 | Acc: 38.824,60.440,72.785,%
Batch: 140 | Loss: 4.852 | Acc: 38.863,60.356,72.889,%
Batch: 160 | Loss: 4.862 | Acc: 38.766,60.316,72.608,%
Batch: 180 | Loss: 4.861 | Acc: 38.816,60.432,72.600,%
Batch: 200 | Loss: 4.874 | Acc: 38.705,60.389,72.489,%
Batch: 220 | Loss: 4.877 | Acc: 38.730,60.351,72.487,%
Batch: 240 | Loss: 4.880 | Acc: 38.826,60.247,72.481,%
Batch: 260 | Loss: 4.871 | Acc: 38.961,60.231,72.537,%
Batch: 280 | Loss: 4.867 | Acc: 38.943,60.251,72.456,%
Batch: 300 | Loss: 4.867 | Acc: 38.917,60.172,72.301,%
Batch: 320 | Loss: 4.872 | Acc: 38.890,60.171,72.277,%
Batch: 340 | Loss: 4.874 | Acc: 38.815,60.193,72.203,%
Batch: 360 | Loss: 4.875 | Acc: 38.809,60.256,72.156,%
Batch: 380 | Loss: 4.872 | Acc: 38.872,60.361,72.146,%
Batch: 0 | Loss: 5.428 | Acc: 38.281,55.469,65.625,%
Batch: 20 | Loss: 5.968 | Acc: 32.180,53.051,62.128,%
Batch: 40 | Loss: 5.959 | Acc: 32.031,52.858,62.138,%
Batch: 60 | Loss: 5.953 | Acc: 31.942,52.779,62.001,%
Train all parameters

Epoch: 53
Batch: 0 | Loss: 4.428 | Acc: 43.750,71.094,75.000,%
Batch: 20 | Loss: 4.571 | Acc: 40.476,62.426,76.823,%
Batch: 40 | Loss: 4.603 | Acc: 40.053,61.947,77.001,%
Batch: 60 | Loss: 4.601 | Acc: 39.818,61.847,76.473,%
Batch: 80 | Loss: 4.643 | Acc: 39.689,62.018,76.186,%
Batch: 100 | Loss: 4.681 | Acc: 39.295,61.734,75.712,%
Batch: 120 | Loss: 4.645 | Acc: 39.495,62.054,75.891,%
Batch: 140 | Loss: 4.650 | Acc: 39.412,62.029,75.870,%
Batch: 160 | Loss: 4.636 | Acc: 39.650,62.146,75.922,%
Batch: 180 | Loss: 4.636 | Acc: 39.913,62.129,75.855,%
Batch: 200 | Loss: 4.630 | Acc: 39.972,62.166,75.890,%
Batch: 220 | Loss: 4.645 | Acc: 39.886,61.973,75.675,%
Batch: 240 | Loss: 4.656 | Acc: 39.724,61.929,75.538,%
Batch: 260 | Loss: 4.681 | Acc: 39.556,61.731,75.287,%
Batch: 280 | Loss: 4.684 | Acc: 39.524,61.858,75.281,%
Batch: 300 | Loss: 4.689 | Acc: 39.473,61.887,75.138,%
Batch: 320 | Loss: 4.706 | Acc: 39.355,61.682,74.949,%
Batch: 340 | Loss: 4.704 | Acc: 39.413,61.616,74.828,%
Batch: 360 | Loss: 4.715 | Acc: 39.320,61.652,74.688,%
Batch: 380 | Loss: 4.721 | Acc: 39.446,61.641,74.555,%
Batch: 0 | Loss: 5.251 | Acc: 39.062,59.375,71.094,%
Batch: 20 | Loss: 6.116 | Acc: 32.143,51.562,61.719,%
Batch: 40 | Loss: 6.087 | Acc: 32.355,51.410,60.861,%
Batch: 60 | Loss: 6.075 | Acc: 31.980,51.358,60.886,%
Train all parameters

Epoch: 54
Batch: 0 | Loss: 3.904 | Acc: 45.312,71.875,75.781,%
Batch: 20 | Loss: 4.649 | Acc: 38.356,62.686,77.567,%
Batch: 40 | Loss: 4.572 | Acc: 39.386,62.500,78.525,%
Batch: 60 | Loss: 4.536 | Acc: 39.703,63.140,78.471,%
Batch: 80 | Loss: 4.558 | Acc: 40.027,63.002,78.029,%
Batch: 100 | Loss: 4.554 | Acc: 40.014,62.910,77.700,%
Batch: 120 | Loss: 4.548 | Acc: 40.270,62.823,77.667,%
Batch: 140 | Loss: 4.548 | Acc: 40.226,62.783,77.543,%
Batch: 160 | Loss: 4.525 | Acc: 40.523,63.039,77.489,%
Batch: 180 | Loss: 4.522 | Acc: 40.444,63.178,77.396,%
Batch: 200 | Loss: 4.525 | Acc: 40.427,63.060,77.192,%
Batch: 220 | Loss: 4.533 | Acc: 40.512,62.885,77.142,%
Batch: 240 | Loss: 4.542 | Acc: 40.619,62.805,76.977,%
Batch: 260 | Loss: 4.553 | Acc: 40.517,62.704,76.826,%
Batch: 280 | Loss: 4.556 | Acc: 40.547,62.553,76.610,%
Batch: 300 | Loss: 4.554 | Acc: 40.612,62.710,76.573,%
Batch: 320 | Loss: 4.564 | Acc: 40.496,62.631,76.419,%
Batch: 340 | Loss: 4.576 | Acc: 40.510,62.489,76.269,%
Batch: 360 | Loss: 4.583 | Acc: 40.538,62.483,76.223,%
Batch: 380 | Loss: 4.587 | Acc: 40.471,62.389,76.202,%
Batch: 0 | Loss: 5.157 | Acc: 41.406,60.938,69.531,%
Batch: 20 | Loss: 5.657 | Acc: 34.747,56.027,64.286,%
Batch: 40 | Loss: 5.668 | Acc: 35.175,55.964,63.796,%
Batch: 60 | Loss: 5.680 | Acc: 34.452,55.686,63.704,%
Train all parameters

Epoch: 55
Batch: 0 | Loss: 4.590 | Acc: 40.625,63.281,79.688,%
Batch: 20 | Loss: 4.268 | Acc: 42.708,65.104,79.688,%
Batch: 40 | Loss: 4.401 | Acc: 41.521,63.796,79.478,%
Batch: 60 | Loss: 4.428 | Acc: 41.048,63.358,79.726,%
Batch: 80 | Loss: 4.413 | Acc: 41.223,63.445,79.456,%
Batch: 100 | Loss: 4.425 | Acc: 40.958,63.459,79.316,%
Batch: 120 | Loss: 4.427 | Acc: 40.799,63.488,79.126,%
Batch: 140 | Loss: 4.414 | Acc: 40.863,63.697,79.095,%
Batch: 160 | Loss: 4.424 | Acc: 40.805,63.776,78.897,%
Batch: 180 | Loss: 4.441 | Acc: 40.724,63.605,78.669,%
Batch: 200 | Loss: 4.446 | Acc: 40.602,63.717,78.483,%
Batch: 220 | Loss: 4.452 | Acc: 40.583,63.663,78.337,%
Batch: 240 | Loss: 4.461 | Acc: 40.521,63.573,78.232,%
Batch: 260 | Loss: 4.467 | Acc: 40.553,63.509,78.206,%
Batch: 280 | Loss: 4.474 | Acc: 40.503,63.518,78.183,%
Batch: 300 | Loss: 4.488 | Acc: 40.399,63.447,77.990,%
Batch: 320 | Loss: 4.491 | Acc: 40.489,63.452,77.843,%
Batch: 340 | Loss: 4.493 | Acc: 40.529,63.412,77.786,%
Batch: 360 | Loss: 4.500 | Acc: 40.465,63.327,77.740,%
Batch: 380 | Loss: 4.512 | Acc: 40.438,63.214,77.551,%
Batch: 0 | Loss: 5.396 | Acc: 35.938,59.375,66.406,%
Batch: 20 | Loss: 5.656 | Acc: 35.156,56.548,63.653,%
Batch: 40 | Loss: 5.642 | Acc: 35.385,55.983,63.529,%
Batch: 60 | Loss: 5.661 | Acc: 35.092,55.571,63.217,%
Train all parameters

Epoch: 56
Batch: 0 | Loss: 4.185 | Acc: 44.531,58.594,78.906,%
Batch: 20 | Loss: 4.355 | Acc: 40.811,63.467,81.399,%
Batch: 40 | Loss: 4.384 | Acc: 40.263,64.139,80.888,%
Batch: 60 | Loss: 4.341 | Acc: 40.574,64.498,80.943,%
Batch: 80 | Loss: 4.327 | Acc: 40.799,64.487,80.990,%
Batch: 100 | Loss: 4.317 | Acc: 41.019,64.658,80.871,%
Batch: 120 | Loss: 4.315 | Acc: 41.154,64.405,80.727,%
Batch: 140 | Loss: 4.335 | Acc: 40.913,64.085,80.713,%
Batch: 160 | Loss: 4.334 | Acc: 40.906,64.121,80.488,%
Batch: 180 | Loss: 4.337 | Acc: 41.139,64.123,80.275,%
Batch: 200 | Loss: 4.351 | Acc: 41.088,64.160,79.983,%
Batch: 220 | Loss: 4.368 | Acc: 41.077,64.002,79.645,%
Batch: 240 | Loss: 4.380 | Acc: 41.037,63.952,79.470,%
Batch: 260 | Loss: 4.401 | Acc: 40.739,63.781,79.376,%
Batch: 280 | Loss: 4.414 | Acc: 40.675,63.754,79.181,%
Batch: 300 | Loss: 4.417 | Acc: 40.794,63.665,79.065,%
Batch: 320 | Loss: 4.423 | Acc: 40.781,63.588,78.909,%
Batch: 340 | Loss: 4.429 | Acc: 40.751,63.549,78.760,%
Batch: 360 | Loss: 4.431 | Acc: 40.727,63.560,78.733,%
Batch: 380 | Loss: 4.433 | Acc: 40.695,63.558,78.625,%
Batch: 0 | Loss: 5.442 | Acc: 41.406,55.469,63.281,%
Batch: 20 | Loss: 5.675 | Acc: 35.268,55.655,62.537,%
Batch: 40 | Loss: 5.672 | Acc: 35.690,55.069,62.214,%
Batch: 60 | Loss: 5.689 | Acc: 35.348,54.675,62.039,%
Train all parameters

Epoch: 57
Batch: 0 | Loss: 4.374 | Acc: 41.406,57.031,76.562,%
Batch: 20 | Loss: 4.322 | Acc: 40.811,62.240,80.394,%
Batch: 40 | Loss: 4.324 | Acc: 39.939,63.224,80.716,%
Batch: 60 | Loss: 4.291 | Acc: 40.497,63.345,80.994,%
Batch: 80 | Loss: 4.277 | Acc: 40.837,63.628,81.096,%
Batch: 100 | Loss: 4.269 | Acc: 40.826,64.055,80.979,%
Batch: 120 | Loss: 4.299 | Acc: 40.961,64.088,80.843,%
Batch: 140 | Loss: 4.291 | Acc: 40.969,64.112,80.873,%
Batch: 160 | Loss: 4.293 | Acc: 41.076,64.276,80.804,%
Batch: 180 | Loss: 4.300 | Acc: 41.186,64.127,80.698,%
Batch: 200 | Loss: 4.309 | Acc: 41.111,64.269,80.574,%
Batch: 220 | Loss: 4.303 | Acc: 41.247,64.313,80.448,%
Batch: 240 | Loss: 4.306 | Acc: 41.397,64.309,80.329,%
Batch: 260 | Loss: 4.317 | Acc: 41.343,64.311,80.301,%
Batch: 280 | Loss: 4.313 | Acc: 41.392,64.343,80.185,%
Batch: 300 | Loss: 4.317 | Acc: 41.437,64.392,80.204,%
Batch: 320 | Loss: 4.320 | Acc: 41.409,64.318,80.057,%
Batch: 340 | Loss: 4.327 | Acc: 41.459,64.246,79.953,%
Batch: 360 | Loss: 4.329 | Acc: 41.469,64.201,79.850,%
Batch: 380 | Loss: 4.334 | Acc: 41.552,64.198,79.798,%
Batch: 0 | Loss: 5.413 | Acc: 36.719,58.594,66.406,%
Batch: 20 | Loss: 5.843 | Acc: 32.775,54.315,62.277,%
Batch: 40 | Loss: 5.858 | Acc: 32.222,54.078,62.138,%
Batch: 60 | Loss: 5.849 | Acc: 32.134,54.060,62.116,%
Train all parameters

Epoch: 58
Batch: 0 | Loss: 3.860 | Acc: 34.375,71.094,87.500,%
Batch: 20 | Loss: 4.171 | Acc: 40.737,65.216,82.292,%
Batch: 40 | Loss: 4.162 | Acc: 41.387,65.111,82.812,%
Batch: 60 | Loss: 4.214 | Acc: 41.112,64.857,82.697,%
Batch: 80 | Loss: 4.217 | Acc: 41.059,65.162,82.542,%
Batch: 100 | Loss: 4.201 | Acc: 41.584,65.385,82.062,%
Batch: 120 | Loss: 4.243 | Acc: 41.284,65.108,81.579,%
Batch: 140 | Loss: 4.260 | Acc: 41.146,65.093,81.538,%
Batch: 160 | Loss: 4.241 | Acc: 41.377,64.941,81.488,%
Batch: 180 | Loss: 4.248 | Acc: 41.389,65.068,81.574,%
Batch: 200 | Loss: 4.253 | Acc: 41.282,64.960,81.468,%
Batch: 220 | Loss: 4.252 | Acc: 41.318,64.911,81.204,%
Batch: 240 | Loss: 4.268 | Acc: 41.348,64.789,80.965,%
Batch: 260 | Loss: 4.280 | Acc: 41.233,64.598,80.816,%
Batch: 280 | Loss: 4.283 | Acc: 41.245,64.571,80.708,%
Batch: 300 | Loss: 4.290 | Acc: 41.261,64.610,80.552,%
Batch: 320 | Loss: 4.301 | Acc: 41.219,64.564,80.449,%
Batch: 340 | Loss: 4.302 | Acc: 41.244,64.512,80.366,%
Batch: 360 | Loss: 4.317 | Acc: 41.144,64.433,80.339,%
Batch: 380 | Loss: 4.313 | Acc: 41.177,64.487,80.350,%
Batch: 0 | Loss: 5.188 | Acc: 39.062,63.281,67.969,%
Batch: 20 | Loss: 5.553 | Acc: 36.235,57.106,65.402,%
Batch: 40 | Loss: 5.547 | Acc: 36.719,56.726,64.920,%
Batch: 60 | Loss: 5.567 | Acc: 35.886,56.109,64.600,%
Train all parameters

Epoch: 59
Batch: 0 | Loss: 4.315 | Acc: 41.406,64.062,79.688,%
Batch: 20 | Loss: 4.169 | Acc: 41.815,66.555,82.478,%
Batch: 40 | Loss: 4.150 | Acc: 42.035,66.521,82.965,%
Batch: 60 | Loss: 4.099 | Acc: 42.021,66.227,83.350,%
Batch: 80 | Loss: 4.129 | Acc: 41.956,66.194,83.410,%
Batch: 100 | Loss: 4.096 | Acc: 42.273,66.700,83.563,%
Batch: 120 | Loss: 4.112 | Acc: 42.097,66.768,83.452,%
Batch: 140 | Loss: 4.143 | Acc: 41.994,66.467,83.295,%
Batch: 160 | Loss: 4.148 | Acc: 42.139,66.241,83.050,%
Batch: 180 | Loss: 4.172 | Acc: 42.067,66.013,82.808,%
Batch: 200 | Loss: 4.181 | Acc: 41.927,65.944,82.700,%
Batch: 220 | Loss: 4.193 | Acc: 41.739,65.784,82.487,%
Batch: 240 | Loss: 4.204 | Acc: 41.672,65.580,82.151,%
Batch: 260 | Loss: 4.215 | Acc: 41.634,65.490,81.915,%
Batch: 280 | Loss: 4.220 | Acc: 41.668,65.447,81.756,%
Batch: 300 | Loss: 4.228 | Acc: 41.783,65.384,81.616,%
Batch: 320 | Loss: 4.238 | Acc: 41.698,65.238,81.479,%
Batch: 340 | Loss: 4.247 | Acc: 41.665,65.158,81.314,%
Batch: 360 | Loss: 4.245 | Acc: 41.817,65.192,81.254,%
Batch: 380 | Loss: 4.254 | Acc: 41.859,65.176,81.186,%
Batch: 0 | Loss: 5.336 | Acc: 44.531,56.250,65.625,%
Batch: 20 | Loss: 5.512 | Acc: 36.496,57.478,64.918,%
Batch: 40 | Loss: 5.497 | Acc: 36.719,57.584,65.091,%
Batch: 60 | Loss: 5.511 | Acc: 36.194,57.082,64.780,%
Train all parameters

Epoch: 60
Batch: 0 | Loss: 3.733 | Acc: 46.875,76.562,85.938,%
Batch: 20 | Loss: 4.125 | Acc: 41.964,67.597,83.185,%
Batch: 40 | Loss: 4.140 | Acc: 42.111,66.273,83.784,%
Batch: 60 | Loss: 4.070 | Acc: 42.725,66.919,83.952,%
Batch: 80 | Loss: 4.085 | Acc: 42.294,66.667,83.758,%
Batch: 100 | Loss: 4.100 | Acc: 41.948,66.569,83.555,%
Batch: 120 | Loss: 4.096 | Acc: 42.123,66.729,83.439,%
Batch: 140 | Loss: 4.128 | Acc: 41.850,66.406,83.383,%
Batch: 160 | Loss: 4.129 | Acc: 41.746,66.363,83.332,%
Batch: 180 | Loss: 4.132 | Acc: 41.825,66.281,83.318,%
Batch: 200 | Loss: 4.151 | Acc: 41.752,66.154,83.123,%
Batch: 220 | Loss: 4.164 | Acc: 41.799,65.904,82.972,%
Batch: 240 | Loss: 4.173 | Acc: 41.688,65.868,82.968,%
Batch: 260 | Loss: 4.168 | Acc: 41.712,65.912,82.815,%
Batch: 280 | Loss: 4.176 | Acc: 41.762,65.764,82.618,%
Batch: 300 | Loss: 4.192 | Acc: 41.663,65.568,82.428,%
Batch: 320 | Loss: 4.199 | Acc: 41.715,65.606,82.258,%
Batch: 340 | Loss: 4.201 | Acc: 41.718,65.650,82.164,%
Batch: 360 | Loss: 4.199 | Acc: 41.822,65.627,82.079,%
Batch: 380 | Loss: 4.198 | Acc: 41.909,65.553,81.998,%
Batch: 0 | Loss: 5.019 | Acc: 37.500,58.594,70.312,%
Batch: 20 | Loss: 5.600 | Acc: 34.821,56.585,63.914,%
Batch: 40 | Loss: 5.576 | Acc: 35.575,56.860,64.253,%
Batch: 60 | Loss: 5.566 | Acc: 35.656,56.814,64.293,%
Train all parameters

Epoch: 61
Batch: 0 | Loss: 3.652 | Acc: 39.844,68.750,88.281,%
Batch: 20 | Loss: 4.023 | Acc: 41.555,66.220,84.561,%
Batch: 40 | Loss: 4.037 | Acc: 42.302,66.787,84.318,%
Batch: 60 | Loss: 4.075 | Acc: 42.034,66.957,84.080,%
Batch: 80 | Loss: 4.114 | Acc: 42.120,66.599,83.536,%
Batch: 100 | Loss: 4.097 | Acc: 42.365,66.530,83.501,%
Batch: 120 | Loss: 4.092 | Acc: 42.194,66.451,83.361,%
Batch: 140 | Loss: 4.102 | Acc: 42.309,66.290,83.289,%
Batch: 160 | Loss: 4.124 | Acc: 42.090,65.994,83.235,%
Batch: 180 | Loss: 4.130 | Acc: 42.175,65.919,83.128,%
Batch: 200 | Loss: 4.126 | Acc: 42.250,66.006,83.034,%
Batch: 220 | Loss: 4.142 | Acc: 42.004,65.943,82.922,%
Batch: 240 | Loss: 4.158 | Acc: 41.792,65.680,82.829,%
Batch: 260 | Loss: 4.161 | Acc: 41.798,65.730,82.726,%
Batch: 280 | Loss: 4.172 | Acc: 41.740,65.647,82.562,%
Batch: 300 | Loss: 4.171 | Acc: 41.840,65.617,82.485,%
Batch: 320 | Loss: 4.171 | Acc: 41.910,65.696,82.374,%
Batch: 340 | Loss: 4.184 | Acc: 41.814,65.650,82.295,%
Batch: 360 | Loss: 4.188 | Acc: 41.917,65.595,82.189,%
Batch: 380 | Loss: 4.189 | Acc: 41.927,65.498,82.060,%
Batch: 0 | Loss: 4.925 | Acc: 41.406,63.281,71.094,%
Batch: 20 | Loss: 5.325 | Acc: 37.240,58.854,66.220,%
Batch: 40 | Loss: 5.316 | Acc: 38.396,58.136,65.530,%
Batch: 60 | Loss: 5.367 | Acc: 37.884,57.544,65.190,%
Train all parameters

Epoch: 62
Batch: 0 | Loss: 3.509 | Acc: 46.094,64.844,85.156,%
Batch: 20 | Loss: 4.091 | Acc: 42.336,66.220,83.891,%
Batch: 40 | Loss: 4.027 | Acc: 42.893,67.454,84.737,%
Batch: 60 | Loss: 4.039 | Acc: 42.623,67.328,85.041,%
Batch: 80 | Loss: 4.048 | Acc: 42.564,67.159,84.915,%
Batch: 100 | Loss: 4.079 | Acc: 42.234,66.600,84.762,%
Batch: 120 | Loss: 4.094 | Acc: 42.323,66.535,84.491,%
Batch: 140 | Loss: 4.079 | Acc: 42.348,66.534,84.253,%
Batch: 160 | Loss: 4.085 | Acc: 42.289,66.644,84.142,%
Batch: 180 | Loss: 4.091 | Acc: 42.291,66.639,84.012,%
Batch: 200 | Loss: 4.080 | Acc: 42.475,66.659,83.765,%
Batch: 220 | Loss: 4.081 | Acc: 42.615,66.509,83.622,%
Batch: 240 | Loss: 4.088 | Acc: 42.560,66.422,83.480,%
Batch: 260 | Loss: 4.096 | Acc: 42.532,66.409,83.372,%
Batch: 280 | Loss: 4.095 | Acc: 42.652,66.334,83.288,%
Batch: 300 | Loss: 4.098 | Acc: 42.686,66.310,83.111,%
Batch: 320 | Loss: 4.108 | Acc: 42.689,66.250,82.983,%
Batch: 340 | Loss: 4.119 | Acc: 42.655,66.145,82.884,%
Batch: 360 | Loss: 4.132 | Acc: 42.579,66.073,82.715,%
Batch: 380 | Loss: 4.142 | Acc: 42.454,66.066,82.675,%
Batch: 0 | Loss: 5.072 | Acc: 42.969,64.844,67.969,%
Batch: 20 | Loss: 5.365 | Acc: 37.723,58.482,66.071,%
Batch: 40 | Loss: 5.407 | Acc: 37.443,57.470,65.320,%
Batch: 60 | Loss: 5.427 | Acc: 37.013,57.185,64.895,%
Train all parameters

Epoch: 63
Batch: 0 | Loss: 3.884 | Acc: 48.438,67.188,83.594,%
Batch: 20 | Loss: 4.004 | Acc: 42.262,68.155,85.751,%
Batch: 40 | Loss: 4.012 | Acc: 42.492,67.759,85.080,%
Batch: 60 | Loss: 4.017 | Acc: 42.239,67.546,84.708,%
Batch: 80 | Loss: 4.022 | Acc: 42.323,67.303,84.423,%
Batch: 100 | Loss: 4.027 | Acc: 42.133,67.226,84.491,%
Batch: 120 | Loss: 4.000 | Acc: 42.439,67.355,84.588,%
Batch: 140 | Loss: 4.003 | Acc: 42.492,67.409,84.508,%
Batch: 160 | Loss: 4.001 | Acc: 42.576,67.328,84.428,%
Batch: 180 | Loss: 4.036 | Acc: 42.390,66.972,84.202,%
Batch: 200 | Loss: 4.043 | Acc: 42.588,66.869,83.982,%
Batch: 220 | Loss: 4.059 | Acc: 42.523,66.731,83.933,%
Batch: 240 | Loss: 4.055 | Acc: 42.683,66.782,83.804,%
Batch: 260 | Loss: 4.049 | Acc: 42.828,66.864,83.764,%
Batch: 280 | Loss: 4.067 | Acc: 42.616,66.693,83.666,%
Batch: 300 | Loss: 4.071 | Acc: 42.522,66.570,83.589,%
Batch: 320 | Loss: 4.075 | Acc: 42.487,66.421,83.509,%
Batch: 340 | Loss: 4.082 | Acc: 42.481,66.344,83.436,%
Batch: 360 | Loss: 4.092 | Acc: 42.497,66.328,83.334,%
Batch: 380 | Loss: 4.101 | Acc: 42.411,66.240,83.212,%
Batch: 0 | Loss: 5.022 | Acc: 42.969,63.281,67.969,%
Batch: 20 | Loss: 5.404 | Acc: 36.607,58.073,65.439,%
Batch: 40 | Loss: 5.412 | Acc: 36.662,57.603,64.996,%
Batch: 60 | Loss: 5.433 | Acc: 36.040,57.198,64.716,%
Train all parameters

Epoch: 64
Batch: 0 | Loss: 3.888 | Acc: 48.438,67.969,87.500,%
Batch: 20 | Loss: 3.878 | Acc: 45.126,67.783,86.533,%
Batch: 40 | Loss: 3.858 | Acc: 44.798,68.083,86.776,%
Batch: 60 | Loss: 3.959 | Acc: 43.558,67.392,86.424,%
Batch: 80 | Loss: 4.003 | Acc: 42.728,66.956,85.976,%
Batch: 100 | Loss: 4.004 | Acc: 42.590,67.211,86.131,%
Batch: 120 | Loss: 4.023 | Acc: 42.342,67.226,85.815,%
Batch: 140 | Loss: 4.034 | Acc: 42.337,67.049,85.561,%
Batch: 160 | Loss: 4.031 | Acc: 42.401,67.042,85.384,%
Batch: 180 | Loss: 4.024 | Acc: 42.416,67.071,85.221,%
Batch: 200 | Loss: 4.026 | Acc: 42.444,67.032,85.012,%
Batch: 220 | Loss: 4.031 | Acc: 42.481,66.891,84.852,%
Batch: 240 | Loss: 4.046 | Acc: 42.424,66.653,84.670,%
Batch: 260 | Loss: 4.050 | Acc: 42.499,66.535,84.501,%
Batch: 280 | Loss: 4.064 | Acc: 42.407,66.440,84.356,%
Batch: 300 | Loss: 4.075 | Acc: 42.341,66.422,84.149,%
Batch: 320 | Loss: 4.085 | Acc: 42.319,66.328,84.037,%
Batch: 340 | Loss: 4.089 | Acc: 42.375,66.260,83.882,%
Batch: 360 | Loss: 4.089 | Acc: 42.406,66.268,83.810,%
Batch: 380 | Loss: 4.094 | Acc: 42.298,66.230,83.702,%
Batch: 0 | Loss: 5.693 | Acc: 41.406,58.594,63.281,%
Batch: 20 | Loss: 5.764 | Acc: 35.045,55.506,62.016,%
Batch: 40 | Loss: 5.710 | Acc: 35.614,55.259,62.557,%
Batch: 60 | Loss: 5.704 | Acc: 35.028,54.662,62.961,%
Train all parameters

Epoch: 65
Batch: 0 | Loss: 4.257 | Acc: 47.656,64.844,85.156,%
Batch: 20 | Loss: 3.975 | Acc: 43.080,66.927,85.528,%
Batch: 40 | Loss: 3.887 | Acc: 43.769,67.359,86.280,%
Batch: 60 | Loss: 3.900 | Acc: 43.443,67.252,86.258,%
Batch: 80 | Loss: 3.923 | Acc: 43.191,67.438,86.188,%
Batch: 100 | Loss: 3.964 | Acc: 42.830,67.203,86.177,%
Batch: 120 | Loss: 3.945 | Acc: 43.079,67.078,86.041,%
Batch: 140 | Loss: 3.949 | Acc: 43.068,67.038,85.871,%
Batch: 160 | Loss: 3.943 | Acc: 43.216,67.047,85.700,%
Batch: 180 | Loss: 3.958 | Acc: 43.051,67.006,85.346,%
Batch: 200 | Loss: 3.973 | Acc: 42.907,66.931,85.110,%
Batch: 220 | Loss: 3.991 | Acc: 42.721,66.731,84.820,%
Batch: 240 | Loss: 3.986 | Acc: 42.745,66.727,84.767,%
Batch: 260 | Loss: 3.999 | Acc: 42.846,66.577,84.614,%
Batch: 280 | Loss: 4.003 | Acc: 42.830,66.556,84.514,%
Batch: 300 | Loss: 4.018 | Acc: 42.751,66.500,84.346,%
Batch: 320 | Loss: 4.017 | Acc: 42.825,66.494,84.175,%
Batch: 340 | Loss: 4.018 | Acc: 42.886,66.535,84.034,%
Batch: 360 | Loss: 4.031 | Acc: 42.815,66.501,83.884,%
Batch: 380 | Loss: 4.032 | Acc: 42.838,66.527,83.750,%
Batch: 0 | Loss: 5.467 | Acc: 42.969,62.500,64.844,%
Batch: 20 | Loss: 5.499 | Acc: 37.091,57.850,64.435,%
Batch: 40 | Loss: 5.472 | Acc: 37.157,57.622,64.863,%
Batch: 60 | Loss: 5.498 | Acc: 36.501,57.057,64.716,%
Train all parameters

Epoch: 66
Batch: 0 | Loss: 4.004 | Acc: 48.438,67.969,81.250,%
Batch: 20 | Loss: 3.944 | Acc: 42.746,66.481,85.342,%
Batch: 40 | Loss: 3.950 | Acc: 42.721,66.959,85.690,%
Batch: 60 | Loss: 3.955 | Acc: 42.956,66.855,85.886,%
Batch: 80 | Loss: 3.983 | Acc: 42.380,67.110,85.812,%
Batch: 100 | Loss: 3.992 | Acc: 42.311,67.157,85.736,%
Batch: 120 | Loss: 3.996 | Acc: 42.349,67.071,85.576,%
Batch: 140 | Loss: 4.006 | Acc: 42.265,66.960,85.417,%
Batch: 160 | Loss: 4.028 | Acc: 42.188,66.828,85.214,%
Batch: 180 | Loss: 4.018 | Acc: 42.369,66.877,85.087,%
Batch: 200 | Loss: 4.020 | Acc: 42.382,66.981,85.063,%
Batch: 220 | Loss: 4.026 | Acc: 42.294,67.053,85.064,%
Batch: 240 | Loss: 4.005 | Acc: 42.499,67.200,85.053,%
Batch: 260 | Loss: 4.010 | Acc: 42.328,67.173,84.932,%
Batch: 280 | Loss: 4.014 | Acc: 42.254,67.090,84.798,%
Batch: 300 | Loss: 4.018 | Acc: 42.359,67.029,84.616,%
Batch: 320 | Loss: 4.018 | Acc: 42.436,66.978,84.533,%
Batch: 340 | Loss: 4.022 | Acc: 42.504,66.970,84.464,%
Batch: 360 | Loss: 4.023 | Acc: 42.566,66.973,84.330,%
Batch: 380 | Loss: 4.027 | Acc: 42.569,66.980,84.352,%
Batch: 0 | Loss: 5.533 | Acc: 42.188,59.375,66.406,%
Batch: 20 | Loss: 5.517 | Acc: 37.500,56.250,65.848,%
Batch: 40 | Loss: 5.460 | Acc: 37.614,56.669,65.968,%
Batch: 60 | Loss: 5.452 | Acc: 36.719,56.212,65.945,%
Train all parameters

Epoch: 67
Batch: 0 | Loss: 4.133 | Acc: 48.438,70.312,89.844,%
Batch: 20 | Loss: 3.938 | Acc: 42.560,68.899,88.542,%
Batch: 40 | Loss: 3.904 | Acc: 42.111,68.693,87.729,%
Batch: 60 | Loss: 3.916 | Acc: 42.597,68.315,87.538,%
Batch: 80 | Loss: 3.913 | Acc: 43.181,68.297,87.182,%
Batch: 100 | Loss: 3.938 | Acc: 42.907,67.946,86.649,%
Batch: 120 | Loss: 3.954 | Acc: 43.014,67.891,86.473,%
Batch: 140 | Loss: 3.954 | Acc: 42.891,67.819,86.431,%
Batch: 160 | Loss: 3.954 | Acc: 42.765,67.755,86.316,%
Batch: 180 | Loss: 3.947 | Acc: 42.835,67.878,86.291,%
Batch: 200 | Loss: 3.949 | Acc: 42.802,67.751,86.217,%
Batch: 220 | Loss: 3.956 | Acc: 42.909,67.781,86.097,%
Batch: 240 | Loss: 3.963 | Acc: 42.774,67.648,86.044,%
Batch: 260 | Loss: 3.978 | Acc: 42.726,67.385,85.854,%
Batch: 280 | Loss: 3.981 | Acc: 42.666,67.251,85.646,%
Batch: 300 | Loss: 3.987 | Acc: 42.637,67.169,85.533,%
Batch: 320 | Loss: 3.990 | Acc: 42.679,67.071,85.400,%
Batch: 340 | Loss: 4.005 | Acc: 42.533,67.007,85.216,%
Batch: 360 | Loss: 4.020 | Acc: 42.484,66.895,85.035,%
Batch: 380 | Loss: 4.033 | Acc: 42.364,66.825,84.898,%
Batch: 0 | Loss: 5.033 | Acc: 42.188,63.281,71.875,%
Batch: 20 | Loss: 5.406 | Acc: 37.165,58.333,65.067,%
Batch: 40 | Loss: 5.428 | Acc: 37.081,57.736,65.053,%
Batch: 60 | Loss: 5.432 | Acc: 36.885,57.697,64.613,%
Train all parameters

Epoch: 68
Batch: 0 | Loss: 4.127 | Acc: 41.406,68.750,86.719,%
Batch: 20 | Loss: 3.942 | Acc: 42.374,68.713,86.421,%
Batch: 40 | Loss: 3.808 | Acc: 43.312,68.521,86.814,%
Batch: 60 | Loss: 3.866 | Acc: 42.392,68.391,86.488,%
Batch: 80 | Loss: 3.839 | Acc: 42.834,68.625,86.372,%
Batch: 100 | Loss: 3.869 | Acc: 42.752,68.263,86.185,%
Batch: 120 | Loss: 3.877 | Acc: 42.723,68.233,86.183,%
Batch: 140 | Loss: 3.882 | Acc: 42.847,68.091,86.420,%
Batch: 160 | Loss: 3.886 | Acc: 42.678,68.168,86.437,%
Batch: 180 | Loss: 3.881 | Acc: 42.882,68.193,86.468,%
Batch: 200 | Loss: 3.895 | Acc: 42.774,68.136,86.167,%
Batch: 220 | Loss: 3.902 | Acc: 42.983,67.965,86.015,%
Batch: 240 | Loss: 3.910 | Acc: 42.998,67.813,85.989,%
Batch: 260 | Loss: 3.926 | Acc: 42.897,67.687,85.911,%
Batch: 280 | Loss: 3.938 | Acc: 42.908,67.466,85.710,%
Batch: 300 | Loss: 3.960 | Acc: 42.701,67.268,85.556,%
Batch: 320 | Loss: 3.969 | Acc: 42.665,67.197,85.453,%
Batch: 340 | Loss: 3.983 | Acc: 42.650,67.096,85.310,%
Batch: 360 | Loss: 3.989 | Acc: 42.748,67.112,85.251,%
Batch: 380 | Loss: 3.992 | Acc: 42.827,67.058,85.169,%
Batch: 0 | Loss: 4.991 | Acc: 41.406,59.375,68.750,%
Batch: 20 | Loss: 5.393 | Acc: 37.909,58.147,65.513,%
Batch: 40 | Loss: 5.366 | Acc: 37.767,57.946,65.701,%
Batch: 60 | Loss: 5.375 | Acc: 37.346,57.953,65.471,%
Train all parameters

Epoch: 69
Batch: 0 | Loss: 3.634 | Acc: 39.844,65.625,86.719,%
Batch: 20 | Loss: 3.723 | Acc: 44.308,68.862,87.574,%
Batch: 40 | Loss: 3.757 | Acc: 43.178,68.826,87.824,%
Batch: 60 | Loss: 3.802 | Acc: 43.379,68.507,87.410,%
Batch: 80 | Loss: 3.798 | Acc: 43.596,68.335,87.191,%
Batch: 100 | Loss: 3.813 | Acc: 43.472,68.209,87.067,%
Batch: 120 | Loss: 3.835 | Acc: 43.324,68.208,86.971,%
Batch: 140 | Loss: 3.821 | Acc: 43.573,68.279,86.863,%
Batch: 160 | Loss: 3.831 | Acc: 43.570,68.386,86.758,%
Batch: 180 | Loss: 3.844 | Acc: 43.418,68.103,86.546,%
Batch: 200 | Loss: 3.860 | Acc: 43.404,67.961,86.357,%
Batch: 220 | Loss: 3.883 | Acc: 43.262,67.781,86.192,%
Batch: 240 | Loss: 3.890 | Acc: 43.406,67.713,86.019,%
Batch: 260 | Loss: 3.905 | Acc: 43.349,67.720,85.845,%
Batch: 280 | Loss: 3.908 | Acc: 43.475,67.746,85.665,%
Batch: 300 | Loss: 3.914 | Acc: 43.488,67.605,85.597,%
Batch: 320 | Loss: 3.933 | Acc: 43.370,67.414,85.514,%
Batch: 340 | Loss: 3.943 | Acc: 43.207,67.412,85.436,%
Batch: 360 | Loss: 3.950 | Acc: 43.174,67.335,85.358,%
Batch: 380 | Loss: 3.959 | Acc: 43.207,67.218,85.279,%
Batch: 0 | Loss: 5.049 | Acc: 44.531,62.500,70.312,%
Batch: 20 | Loss: 5.417 | Acc: 37.463,57.775,66.815,%
Batch: 40 | Loss: 5.471 | Acc: 37.214,56.955,65.625,%
Batch: 60 | Loss: 5.472 | Acc: 37.065,56.890,65.459,%
Train all parameters

Epoch: 70
Batch: 0 | Loss: 3.802 | Acc: 43.750,75.000,91.406,%
Batch: 20 | Loss: 3.899 | Acc: 42.299,68.787,86.235,%
Batch: 40 | Loss: 3.903 | Acc: 41.540,69.303,87.519,%
Batch: 60 | Loss: 3.898 | Acc: 42.418,68.865,87.449,%
Batch: 80 | Loss: 3.903 | Acc: 42.380,68.673,87.413,%
Batch: 100 | Loss: 3.868 | Acc: 43.108,69.059,87.430,%
Batch: 120 | Loss: 3.856 | Acc: 42.975,68.776,87.636,%
Batch: 140 | Loss: 3.848 | Acc: 43.124,68.611,87.550,%
Batch: 160 | Loss: 3.855 | Acc: 42.978,68.308,87.393,%
Batch: 180 | Loss: 3.877 | Acc: 42.969,68.206,87.034,%
Batch: 200 | Loss: 3.862 | Acc: 43.113,68.319,86.983,%
Batch: 220 | Loss: 3.870 | Acc: 43.089,68.163,86.857,%
Batch: 240 | Loss: 3.879 | Acc: 43.209,68.004,86.686,%
Batch: 260 | Loss: 3.877 | Acc: 43.271,67.969,86.542,%
Batch: 280 | Loss: 3.882 | Acc: 43.347,67.935,86.444,%
Batch: 300 | Loss: 3.892 | Acc: 43.389,67.873,86.233,%
Batch: 320 | Loss: 3.901 | Acc: 43.210,67.801,86.120,%
Batch: 340 | Loss: 3.906 | Acc: 43.248,67.701,85.992,%
Batch: 360 | Loss: 3.917 | Acc: 43.237,67.625,85.825,%
Batch: 380 | Loss: 3.923 | Acc: 43.198,67.555,85.691,%
Batch: 0 | Loss: 5.140 | Acc: 44.531,60.156,67.188,%
Batch: 20 | Loss: 5.331 | Acc: 37.351,59.263,66.741,%
Batch: 40 | Loss: 5.334 | Acc: 37.748,59.127,66.197,%
Batch: 60 | Loss: 5.353 | Acc: 37.231,58.478,65.996,%
Train all parameters

Epoch: 71
Batch: 0 | Loss: 4.373 | Acc: 35.938,65.625,88.281,%
Batch: 20 | Loss: 3.939 | Acc: 42.783,67.746,88.132,%
Batch: 40 | Loss: 3.910 | Acc: 42.340,67.283,87.233,%
Batch: 60 | Loss: 3.893 | Acc: 42.777,67.495,87.564,%
Batch: 80 | Loss: 3.841 | Acc: 43.586,67.737,87.693,%
Batch: 100 | Loss: 3.828 | Acc: 43.510,68.038,87.987,%
Batch: 120 | Loss: 3.840 | Acc: 43.221,67.807,87.784,%
Batch: 140 | Loss: 3.841 | Acc: 43.401,67.786,87.622,%
Batch: 160 | Loss: 3.832 | Acc: 43.663,67.760,87.495,%
Batch: 180 | Loss: 3.829 | Acc: 43.832,67.835,87.375,%
Batch: 200 | Loss: 3.824 | Acc: 43.975,67.910,87.310,%
Batch: 220 | Loss: 3.830 | Acc: 43.934,67.877,87.277,%
Batch: 240 | Loss: 3.830 | Acc: 43.967,67.852,87.182,%
Batch: 260 | Loss: 3.847 | Acc: 43.936,67.870,86.925,%
Batch: 280 | Loss: 3.856 | Acc: 43.861,67.760,86.727,%
Batch: 300 | Loss: 3.864 | Acc: 43.753,67.774,86.488,%
Batch: 320 | Loss: 3.878 | Acc: 43.728,67.708,86.307,%
Batch: 340 | Loss: 3.879 | Acc: 43.718,67.600,86.238,%
Batch: 360 | Loss: 3.888 | Acc: 43.681,67.490,86.087,%
Batch: 380 | Loss: 3.896 | Acc: 43.666,67.419,85.921,%
Batch: 0 | Loss: 5.023 | Acc: 46.875,64.844,64.062,%
Batch: 20 | Loss: 5.404 | Acc: 38.504,58.891,65.811,%
Batch: 40 | Loss: 5.357 | Acc: 38.720,58.651,65.854,%
Batch: 60 | Loss: 5.392 | Acc: 37.833,58.235,65.651,%
Train all parameters

Epoch: 72
Batch: 0 | Loss: 3.102 | Acc: 52.344,75.000,91.406,%
Batch: 20 | Loss: 3.775 | Acc: 44.606,68.899,88.579,%
Batch: 40 | Loss: 3.785 | Acc: 44.379,68.750,87.748,%
Batch: 60 | Loss: 3.800 | Acc: 43.660,68.660,88.025,%
Batch: 80 | Loss: 3.829 | Acc: 43.287,68.220,88.088,%
Batch: 100 | Loss: 3.805 | Acc: 43.564,68.154,87.941,%
Batch: 120 | Loss: 3.812 | Acc: 43.453,67.956,87.713,%
Batch: 140 | Loss: 3.832 | Acc: 43.312,67.886,87.483,%
Batch: 160 | Loss: 3.828 | Acc: 43.313,67.823,87.437,%
Batch: 180 | Loss: 3.815 | Acc: 43.599,67.934,87.345,%
Batch: 200 | Loss: 3.822 | Acc: 43.521,67.864,87.170,%
Batch: 220 | Loss: 3.836 | Acc: 43.478,67.711,87.055,%
Batch: 240 | Loss: 3.849 | Acc: 43.358,67.645,86.991,%
Batch: 260 | Loss: 3.860 | Acc: 43.175,67.616,86.931,%
Batch: 280 | Loss: 3.868 | Acc: 43.111,67.596,86.872,%
Batch: 300 | Loss: 3.866 | Acc: 43.174,67.626,86.784,%
Batch: 320 | Loss: 3.875 | Acc: 43.180,67.584,86.626,%
Batch: 340 | Loss: 3.877 | Acc: 43.273,67.621,86.597,%
Batch: 360 | Loss: 3.892 | Acc: 43.209,67.439,86.375,%
Batch: 380 | Loss: 3.901 | Acc: 43.149,67.380,86.229,%
Batch: 0 | Loss: 5.304 | Acc: 42.188,61.719,68.750,%
Batch: 20 | Loss: 5.561 | Acc: 37.277,57.552,65.253,%
Batch: 40 | Loss: 5.499 | Acc: 37.595,57.755,64.806,%
Batch: 60 | Loss: 5.522 | Acc: 37.001,57.287,64.062,%
Train all parameters

Epoch: 73
Batch: 0 | Loss: 4.113 | Acc: 39.062,54.688,89.844,%
Batch: 20 | Loss: 3.826 | Acc: 45.089,68.378,86.793,%
Batch: 40 | Loss: 3.848 | Acc: 43.598,68.274,87.538,%
Batch: 60 | Loss: 3.871 | Acc: 43.046,68.046,87.615,%
Batch: 80 | Loss: 3.852 | Acc: 43.306,68.374,87.500,%
Batch: 100 | Loss: 3.863 | Acc: 43.379,68.154,87.461,%
Batch: 120 | Loss: 3.851 | Acc: 43.853,68.182,87.080,%
Batch: 140 | Loss: 3.855 | Acc: 44.021,68.080,86.763,%
Batch: 160 | Loss: 3.868 | Acc: 43.711,68.085,86.675,%
Batch: 180 | Loss: 3.871 | Acc: 43.767,68.124,86.676,%
Batch: 200 | Loss: 3.873 | Acc: 43.727,68.097,86.692,%
Batch: 220 | Loss: 3.869 | Acc: 43.644,68.117,86.680,%
Batch: 240 | Loss: 3.876 | Acc: 43.659,67.988,86.647,%
Batch: 260 | Loss: 3.865 | Acc: 43.828,68.068,86.524,%
Batch: 280 | Loss: 3.870 | Acc: 43.817,68.019,86.435,%
Batch: 300 | Loss: 3.878 | Acc: 43.742,67.919,86.361,%
Batch: 320 | Loss: 3.884 | Acc: 43.735,67.966,86.232,%
Batch: 340 | Loss: 3.898 | Acc: 43.613,67.753,86.100,%
Batch: 360 | Loss: 3.907 | Acc: 43.594,67.648,85.970,%
Batch: 380 | Loss: 3.916 | Acc: 43.555,67.630,85.909,%
Batch: 0 | Loss: 5.091 | Acc: 44.531,65.625,67.969,%
Batch: 20 | Loss: 5.503 | Acc: 36.979,56.324,65.737,%
Batch: 40 | Loss: 5.503 | Acc: 36.947,55.888,65.072,%
Batch: 60 | Loss: 5.518 | Acc: 36.552,56.032,64.664,%
Train all parameters

Epoch: 74
Batch: 0 | Loss: 3.452 | Acc: 50.000,69.531,89.062,%
Batch: 20 | Loss: 3.862 | Acc: 43.862,67.560,87.128,%
Batch: 40 | Loss: 3.760 | Acc: 44.684,68.674,87.938,%
Batch: 60 | Loss: 3.771 | Acc: 44.442,68.801,88.230,%
Batch: 80 | Loss: 3.789 | Acc: 44.473,68.885,88.223,%
Batch: 100 | Loss: 3.786 | Acc: 44.500,68.943,87.995,%
Batch: 120 | Loss: 3.793 | Acc: 44.183,68.847,87.849,%
Batch: 140 | Loss: 3.803 | Acc: 44.149,68.916,87.655,%
Batch: 160 | Loss: 3.826 | Acc: 43.760,68.735,87.539,%
Batch: 180 | Loss: 3.837 | Acc: 43.551,68.526,87.280,%
Batch: 200 | Loss: 3.850 | Acc: 43.420,68.470,87.197,%
Batch: 220 | Loss: 3.849 | Acc: 43.485,68.439,87.185,%
Batch: 240 | Loss: 3.855 | Acc: 43.478,68.400,87.043,%
Batch: 260 | Loss: 3.852 | Acc: 43.567,68.211,86.991,%
Batch: 280 | Loss: 3.849 | Acc: 43.642,68.291,86.947,%
Batch: 300 | Loss: 3.855 | Acc: 43.657,68.187,86.854,%
Batch: 320 | Loss: 3.857 | Acc: 43.665,68.139,86.804,%
Batch: 340 | Loss: 3.859 | Acc: 43.640,68.056,86.792,%
Batch: 360 | Loss: 3.868 | Acc: 43.570,67.943,86.693,%
Batch: 380 | Loss: 3.861 | Acc: 43.658,68.008,86.680,%
Batch: 0 | Loss: 5.014 | Acc: 46.094,65.625,67.969,%
Batch: 20 | Loss: 5.399 | Acc: 37.314,59.301,65.662,%
Batch: 40 | Loss: 5.407 | Acc: 37.576,58.175,65.434,%
Batch: 60 | Loss: 5.421 | Acc: 36.988,57.812,65.151,%
Train all parameters

Epoch: 75
Batch: 0 | Loss: 3.196 | Acc: 48.438,67.188,89.844,%
Batch: 20 | Loss: 3.835 | Acc: 45.238,68.006,87.500,%
Batch: 40 | Loss: 3.739 | Acc: 45.465,68.979,87.633,%
Batch: 60 | Loss: 3.763 | Acc: 44.711,68.840,87.666,%
Batch: 80 | Loss: 3.720 | Acc: 44.994,69.338,87.982,%
Batch: 100 | Loss: 3.741 | Acc: 44.562,68.928,87.887,%
Batch: 120 | Loss: 3.757 | Acc: 44.383,68.892,87.874,%
Batch: 140 | Loss: 3.752 | Acc: 44.581,68.855,87.860,%
Batch: 160 | Loss: 3.760 | Acc: 44.536,68.765,87.815,%
Batch: 180 | Loss: 3.770 | Acc: 44.467,68.836,87.716,%
Batch: 200 | Loss: 3.792 | Acc: 44.306,68.606,87.551,%
Batch: 220 | Loss: 3.783 | Acc: 44.464,68.679,87.496,%
Batch: 240 | Loss: 3.793 | Acc: 44.457,68.614,87.377,%
Batch: 260 | Loss: 3.800 | Acc: 44.397,68.499,87.252,%
Batch: 280 | Loss: 3.819 | Acc: 44.223,68.333,87.047,%
Batch: 300 | Loss: 3.825 | Acc: 44.243,68.381,86.919,%
Batch: 320 | Loss: 3.835 | Acc: 44.210,68.246,86.794,%
Batch: 340 | Loss: 3.841 | Acc: 44.240,68.235,86.661,%
Batch: 360 | Loss: 3.847 | Acc: 44.159,68.155,86.522,%
Batch: 380 | Loss: 3.856 | Acc: 44.049,68.069,86.327,%
Batch: 0 | Loss: 5.083 | Acc: 42.969,60.156,67.969,%
Batch: 20 | Loss: 5.335 | Acc: 39.211,60.231,66.443,%
Batch: 40 | Loss: 5.325 | Acc: 39.996,59.604,66.063,%
Batch: 60 | Loss: 5.331 | Acc: 39.293,59.388,65.932,%
Train all parameters

Epoch: 76
Batch: 0 | Loss: 4.303 | Acc: 43.750,65.625,90.625,%
Batch: 20 | Loss: 3.636 | Acc: 44.978,70.164,89.472,%
Batch: 40 | Loss: 3.678 | Acc: 44.569,69.093,89.177,%
Batch: 60 | Loss: 3.687 | Acc: 44.506,69.032,88.678,%
Batch: 80 | Loss: 3.693 | Acc: 44.589,69.724,88.667,%
Batch: 100 | Loss: 3.692 | Acc: 44.779,69.725,88.691,%
Batch: 120 | Loss: 3.702 | Acc: 44.615,69.486,88.520,%
Batch: 140 | Loss: 3.735 | Acc: 44.282,69.121,88.314,%
Batch: 160 | Loss: 3.740 | Acc: 44.420,68.973,88.170,%
Batch: 180 | Loss: 3.753 | Acc: 44.337,68.884,88.117,%
Batch: 200 | Loss: 3.762 | Acc: 44.294,68.633,88.032,%
Batch: 220 | Loss: 3.777 | Acc: 44.132,68.457,87.945,%
Batch: 240 | Loss: 3.785 | Acc: 44.032,68.474,87.834,%
Batch: 260 | Loss: 3.808 | Acc: 43.831,68.316,87.662,%
Batch: 280 | Loss: 3.810 | Acc: 43.881,68.355,87.572,%
Batch: 300 | Loss: 3.811 | Acc: 43.885,68.376,87.516,%
Batch: 320 | Loss: 3.821 | Acc: 43.894,68.368,87.381,%
Batch: 340 | Loss: 3.827 | Acc: 43.904,68.367,87.292,%
Batch: 360 | Loss: 3.829 | Acc: 43.856,68.319,87.100,%
Batch: 380 | Loss: 3.838 | Acc: 43.746,68.254,87.010,%
Batch: 0 | Loss: 5.318 | Acc: 40.625,63.281,68.750,%
Batch: 20 | Loss: 5.486 | Acc: 37.537,57.180,65.141,%
Batch: 40 | Loss: 5.468 | Acc: 37.862,56.688,64.977,%
Batch: 60 | Loss: 5.492 | Acc: 37.462,56.980,64.639,%
Train all parameters

Epoch: 77
Batch: 0 | Loss: 3.523 | Acc: 46.875,75.781,91.406,%
Batch: 20 | Loss: 3.866 | Acc: 43.341,67.969,88.504,%
Batch: 40 | Loss: 3.818 | Acc: 43.045,68.350,88.815,%
Batch: 60 | Loss: 3.777 | Acc: 43.327,69.224,89.011,%
Batch: 80 | Loss: 3.743 | Acc: 43.924,69.319,88.870,%
Batch: 100 | Loss: 3.738 | Acc: 44.090,69.539,88.761,%
Batch: 120 | Loss: 3.735 | Acc: 44.183,69.473,88.798,%
Batch: 140 | Loss: 3.748 | Acc: 44.116,69.204,88.603,%
Batch: 160 | Loss: 3.747 | Acc: 44.070,69.216,88.514,%
Batch: 180 | Loss: 3.756 | Acc: 43.914,68.841,88.342,%
Batch: 200 | Loss: 3.771 | Acc: 43.991,68.606,88.126,%
Batch: 220 | Loss: 3.772 | Acc: 44.036,68.605,87.984,%
Batch: 240 | Loss: 3.790 | Acc: 43.928,68.497,87.918,%
Batch: 260 | Loss: 3.799 | Acc: 43.804,68.469,87.901,%
Batch: 280 | Loss: 3.803 | Acc: 43.825,68.436,87.820,%
Batch: 300 | Loss: 3.807 | Acc: 43.802,68.415,87.664,%
Batch: 320 | Loss: 3.822 | Acc: 43.670,68.309,87.446,%
Batch: 340 | Loss: 3.831 | Acc: 43.647,68.173,87.342,%
Batch: 360 | Loss: 3.837 | Acc: 43.562,68.211,87.206,%
Batch: 380 | Loss: 3.837 | Acc: 43.652,68.291,87.178,%
Batch: 0 | Loss: 5.403 | Acc: 37.500,60.156,66.406,%
Batch: 20 | Loss: 5.450 | Acc: 35.491,58.594,66.257,%
Batch: 40 | Loss: 5.471 | Acc: 36.014,57.622,65.263,%
Batch: 60 | Loss: 5.497 | Acc: 35.822,57.057,64.716,%
Train all parameters

Epoch: 78
Batch: 0 | Loss: 4.270 | Acc: 36.719,62.500,90.625,%
Batch: 20 | Loss: 3.724 | Acc: 45.461,69.271,89.546,%
Batch: 40 | Loss: 3.684 | Acc: 44.970,70.332,89.748,%
Batch: 60 | Loss: 3.706 | Acc: 44.647,69.954,89.344,%
Batch: 80 | Loss: 3.729 | Acc: 44.473,70.004,89.313,%
Batch: 100 | Loss: 3.713 | Acc: 44.230,70.158,89.055,%
Batch: 120 | Loss: 3.746 | Acc: 44.099,69.777,88.746,%
Batch: 140 | Loss: 3.736 | Acc: 44.116,69.659,88.664,%
Batch: 160 | Loss: 3.721 | Acc: 44.560,69.623,88.679,%
Batch: 180 | Loss: 3.720 | Acc: 44.242,69.592,88.553,%
Batch: 200 | Loss: 3.732 | Acc: 44.306,69.310,88.394,%
Batch: 220 | Loss: 3.741 | Acc: 44.323,69.192,88.193,%
Batch: 240 | Loss: 3.767 | Acc: 44.103,69.116,88.002,%
Batch: 260 | Loss: 3.772 | Acc: 44.067,69.145,87.832,%
Batch: 280 | Loss: 3.774 | Acc: 44.081,69.070,87.670,%
Batch: 300 | Loss: 3.773 | Acc: 44.261,69.064,87.565,%
Batch: 320 | Loss: 3.781 | Acc: 44.322,68.986,87.369,%
Batch: 340 | Loss: 3.802 | Acc: 44.231,68.755,87.159,%
Batch: 360 | Loss: 3.803 | Acc: 44.326,68.605,86.974,%
Batch: 380 | Loss: 3.809 | Acc: 44.341,68.549,86.862,%
Batch: 0 | Loss: 4.855 | Acc: 44.531,69.531,67.969,%
Batch: 20 | Loss: 5.325 | Acc: 37.686,59.747,65.923,%
Batch: 40 | Loss: 5.292 | Acc: 37.748,59.947,66.311,%
Batch: 60 | Loss: 5.338 | Acc: 37.321,59.106,65.702,%
Train all parameters

Epoch: 79
Batch: 0 | Loss: 3.770 | Acc: 39.844,71.875,91.406,%
Batch: 20 | Loss: 3.691 | Acc: 44.903,70.126,89.435,%
Batch: 40 | Loss: 3.639 | Acc: 45.046,69.607,89.291,%
Batch: 60 | Loss: 3.642 | Acc: 44.595,69.877,89.357,%
Batch: 80 | Loss: 3.694 | Acc: 44.444,69.551,89.313,%
Batch: 100 | Loss: 3.684 | Acc: 44.485,69.415,89.155,%
Batch: 120 | Loss: 3.694 | Acc: 44.409,69.480,89.075,%
Batch: 140 | Loss: 3.699 | Acc: 44.492,69.454,88.869,%
Batch: 160 | Loss: 3.726 | Acc: 44.337,69.323,88.655,%
Batch: 180 | Loss: 3.744 | Acc: 44.259,69.108,88.329,%
Batch: 200 | Loss: 3.737 | Acc: 44.356,68.956,88.169,%
Batch: 220 | Loss: 3.731 | Acc: 44.351,69.082,88.090,%
Batch: 240 | Loss: 3.751 | Acc: 44.249,68.964,87.921,%
Batch: 260 | Loss: 3.767 | Acc: 44.253,68.998,87.713,%
Batch: 280 | Loss: 3.773 | Acc: 44.253,68.922,87.617,%
Batch: 300 | Loss: 3.774 | Acc: 44.318,68.916,87.513,%
Batch: 320 | Loss: 3.775 | Acc: 44.337,68.806,87.349,%
Batch: 340 | Loss: 3.789 | Acc: 44.275,68.702,87.198,%
Batch: 360 | Loss: 3.796 | Acc: 44.235,68.692,87.113,%
Batch: 380 | Loss: 3.797 | Acc: 44.273,68.680,87.028,%
Batch: 0 | Loss: 5.099 | Acc: 47.656,67.969,70.312,%
Batch: 20 | Loss: 5.388 | Acc: 37.686,59.487,66.332,%
Batch: 40 | Loss: 5.366 | Acc: 37.519,58.765,66.387,%
Batch: 60 | Loss: 5.416 | Acc: 36.757,57.774,65.651,%
Train all parameters

Epoch: 80
Batch: 0 | Loss: 3.872 | Acc: 42.969,64.062,89.844,%
Batch: 20 | Loss: 3.775 | Acc: 44.717,69.308,88.430,%
Batch: 40 | Loss: 3.780 | Acc: 44.436,70.141,89.139,%
Batch: 60 | Loss: 3.718 | Acc: 44.941,69.813,89.024,%
Batch: 80 | Loss: 3.728 | Acc: 44.676,69.666,88.918,%
Batch: 100 | Loss: 3.738 | Acc: 44.245,69.516,88.939,%
Batch: 120 | Loss: 3.714 | Acc: 44.421,69.693,88.914,%
Batch: 140 | Loss: 3.709 | Acc: 44.670,69.443,88.708,%
Batch: 160 | Loss: 3.719 | Acc: 44.507,69.492,88.538,%
Batch: 180 | Loss: 3.737 | Acc: 44.251,69.419,88.497,%
Batch: 200 | Loss: 3.736 | Acc: 44.263,69.333,88.456,%
Batch: 220 | Loss: 3.737 | Acc: 44.284,69.369,88.394,%
Batch: 240 | Loss: 3.743 | Acc: 44.321,69.252,88.291,%
Batch: 260 | Loss: 3.766 | Acc: 44.139,68.972,88.111,%
Batch: 280 | Loss: 3.776 | Acc: 44.078,68.836,87.945,%
Batch: 300 | Loss: 3.788 | Acc: 43.945,68.685,87.788,%
Batch: 320 | Loss: 3.797 | Acc: 44.032,68.640,87.683,%
Batch: 340 | Loss: 3.797 | Acc: 44.130,68.697,87.644,%
Batch: 360 | Loss: 3.807 | Acc: 44.116,68.715,87.600,%
Batch: 380 | Loss: 3.807 | Acc: 44.146,68.721,87.551,%
Batch: 0 | Loss: 4.982 | Acc: 40.625,57.812,67.188,%
Batch: 20 | Loss: 5.493 | Acc: 37.165,56.957,64.360,%
Batch: 40 | Loss: 5.525 | Acc: 37.481,56.288,64.024,%
Batch: 60 | Loss: 5.531 | Acc: 36.911,56.378,63.640,%
Train all parameters

Epoch: 81
Batch: 0 | Loss: 4.072 | Acc: 40.625,69.531,82.812,%
Batch: 20 | Loss: 3.800 | Acc: 44.196,68.490,87.649,%
Batch: 40 | Loss: 3.739 | Acc: 44.360,68.845,88.300,%
Batch: 60 | Loss: 3.697 | Acc: 44.800,69.570,88.819,%
Batch: 80 | Loss: 3.686 | Acc: 44.608,69.946,88.985,%
Batch: 100 | Loss: 3.705 | Acc: 44.794,69.539,88.815,%
Batch: 120 | Loss: 3.705 | Acc: 44.822,69.434,88.908,%
Batch: 140 | Loss: 3.714 | Acc: 44.819,69.332,88.907,%
Batch: 160 | Loss: 3.721 | Acc: 44.813,69.221,88.766,%
Batch: 180 | Loss: 3.738 | Acc: 44.704,69.259,88.622,%
Batch: 200 | Loss: 3.744 | Acc: 44.761,69.189,88.542,%
Batch: 220 | Loss: 3.730 | Acc: 44.835,69.139,88.440,%
Batch: 240 | Loss: 3.725 | Acc: 44.901,69.071,88.304,%
Batch: 260 | Loss: 3.735 | Acc: 44.843,68.927,88.168,%
Batch: 280 | Loss: 3.744 | Acc: 44.823,68.920,88.073,%
Batch: 300 | Loss: 3.756 | Acc: 44.710,68.898,87.962,%
Batch: 320 | Loss: 3.762 | Acc: 44.706,68.860,87.887,%
Batch: 340 | Loss: 3.763 | Acc: 44.733,68.812,87.809,%
Batch: 360 | Loss: 3.759 | Acc: 44.763,68.780,87.688,%
Batch: 380 | Loss: 3.769 | Acc: 44.695,68.764,87.555,%
Batch: 0 | Loss: 5.061 | Acc: 43.750,58.594,70.312,%
Batch: 20 | Loss: 5.325 | Acc: 38.504,58.371,65.737,%
Batch: 40 | Loss: 5.375 | Acc: 38.167,58.136,64.405,%
Batch: 60 | Loss: 5.368 | Acc: 37.987,58.376,64.600,%
Train all parameters

Epoch: 82
Batch: 0 | Loss: 3.561 | Acc: 39.844,78.906,88.281,%
Batch: 20 | Loss: 3.586 | Acc: 46.354,71.875,88.095,%
Batch: 40 | Loss: 3.663 | Acc: 45.255,70.827,88.643,%
Batch: 60 | Loss: 3.642 | Acc: 45.287,70.953,88.947,%
Batch: 80 | Loss: 3.661 | Acc: 45.187,70.602,88.966,%
Batch: 100 | Loss: 3.680 | Acc: 45.019,70.119,88.645,%
Batch: 120 | Loss: 3.702 | Acc: 44.628,70.087,88.546,%
Batch: 140 | Loss: 3.726 | Acc: 44.470,69.709,88.314,%
Batch: 160 | Loss: 3.724 | Acc: 44.526,69.687,88.121,%
Batch: 180 | Loss: 3.719 | Acc: 44.570,69.635,88.113,%
Batch: 200 | Loss: 3.734 | Acc: 44.411,69.496,88.005,%
Batch: 220 | Loss: 3.752 | Acc: 44.206,69.369,87.963,%
Batch: 240 | Loss: 3.760 | Acc: 44.158,69.275,87.857,%
Batch: 260 | Loss: 3.766 | Acc: 44.160,69.244,87.802,%
Batch: 280 | Loss: 3.770 | Acc: 44.092,69.114,87.728,%
Batch: 300 | Loss: 3.775 | Acc: 44.134,69.106,87.710,%
Batch: 320 | Loss: 3.777 | Acc: 44.120,69.083,87.646,%
Batch: 340 | Loss: 3.779 | Acc: 44.146,69.112,87.603,%
Batch: 360 | Loss: 3.779 | Acc: 44.252,69.118,87.496,%
Batch: 380 | Loss: 3.783 | Acc: 44.250,69.113,87.420,%
Batch: 0 | Loss: 4.940 | Acc: 42.969,61.719,70.312,%
Batch: 20 | Loss: 5.453 | Acc: 36.384,57.515,65.737,%
Batch: 40 | Loss: 5.447 | Acc: 37.024,57.641,65.225,%
Batch: 60 | Loss: 5.438 | Acc: 36.706,57.646,65.407,%
Train all parameters

Epoch: 83
Batch: 0 | Loss: 3.674 | Acc: 43.750,67.969,85.938,%
Batch: 20 | Loss: 3.732 | Acc: 44.196,68.973,89.546,%
Batch: 40 | Loss: 3.646 | Acc: 44.874,69.703,89.539,%
Batch: 60 | Loss: 3.638 | Acc: 44.992,70.018,89.985,%
Batch: 80 | Loss: 3.649 | Acc: 45.110,70.042,89.834,%
Batch: 100 | Loss: 3.670 | Acc: 44.995,69.709,89.472,%
Batch: 120 | Loss: 3.672 | Acc: 44.835,69.886,89.353,%
Batch: 140 | Loss: 3.664 | Acc: 44.686,69.786,89.306,%
Batch: 160 | Loss: 3.682 | Acc: 44.560,69.439,89.271,%
Batch: 180 | Loss: 3.686 | Acc: 44.669,69.531,89.140,%
Batch: 200 | Loss: 3.695 | Acc: 44.551,69.317,88.965,%
Batch: 220 | Loss: 3.696 | Acc: 44.694,69.185,88.978,%
Batch: 240 | Loss: 3.722 | Acc: 44.356,68.938,88.871,%
Batch: 260 | Loss: 3.736 | Acc: 44.331,68.864,88.799,%
Batch: 280 | Loss: 3.731 | Acc: 44.426,68.900,88.587,%
Batch: 300 | Loss: 3.742 | Acc: 44.365,68.867,88.440,%
Batch: 320 | Loss: 3.750 | Acc: 44.288,68.874,88.320,%
Batch: 340 | Loss: 3.756 | Acc: 44.314,68.904,88.153,%
Batch: 360 | Loss: 3.760 | Acc: 44.334,68.871,88.067,%
Batch: 380 | Loss: 3.764 | Acc: 44.271,68.828,87.972,%
Batch: 0 | Loss: 5.027 | Acc: 49.219,64.844,66.406,%
Batch: 20 | Loss: 5.332 | Acc: 39.583,59.115,65.774,%
Batch: 40 | Loss: 5.323 | Acc: 40.091,58.594,65.149,%
Batch: 60 | Loss: 5.340 | Acc: 39.780,58.338,65.292,%
Train all parameters

Epoch: 84
Batch: 0 | Loss: 4.070 | Acc: 42.188,71.094,94.531,%
Batch: 20 | Loss: 3.634 | Acc: 46.131,69.568,89.583,%
Batch: 40 | Loss: 3.638 | Acc: 45.751,69.703,89.634,%
Batch: 60 | Loss: 3.631 | Acc: 45.645,69.813,89.306,%
Batch: 80 | Loss: 3.651 | Acc: 45.544,69.551,89.468,%
Batch: 100 | Loss: 3.644 | Acc: 45.575,69.763,89.558,%
Batch: 120 | Loss: 3.658 | Acc: 45.280,69.576,89.469,%
Batch: 140 | Loss: 3.670 | Acc: 45.312,69.515,89.273,%
Batch: 160 | Loss: 3.674 | Acc: 45.201,69.347,89.038,%
Batch: 180 | Loss: 3.681 | Acc: 45.179,69.397,88.968,%
Batch: 200 | Loss: 3.695 | Acc: 45.021,69.442,88.930,%
Batch: 220 | Loss: 3.703 | Acc: 44.899,69.397,88.861,%
Batch: 240 | Loss: 3.700 | Acc: 45.043,69.366,88.761,%
Batch: 260 | Loss: 3.699 | Acc: 45.106,69.382,88.658,%
Batch: 280 | Loss: 3.706 | Acc: 45.073,69.326,88.548,%
Batch: 300 | Loss: 3.717 | Acc: 44.996,69.215,88.460,%
Batch: 320 | Loss: 3.722 | Acc: 44.979,69.069,88.276,%
Batch: 340 | Loss: 3.735 | Acc: 44.875,69.030,88.025,%
Batch: 360 | Loss: 3.743 | Acc: 44.847,68.873,87.861,%
Batch: 380 | Loss: 3.755 | Acc: 44.697,68.807,87.689,%
Batch: 0 | Loss: 5.338 | Acc: 46.094,57.812,63.281,%
Batch: 20 | Loss: 5.546 | Acc: 38.356,57.143,64.137,%
Batch: 40 | Loss: 5.535 | Acc: 37.805,57.050,63.319,%
Batch: 60 | Loss: 5.518 | Acc: 37.526,56.903,63.320,%
Train classifier parameters

Epoch: 85
Batch: 0 | Loss: 3.416 | Acc: 51.562,71.094,84.375,%
Batch: 20 | Loss: 3.869 | Acc: 44.234,69.271,84.077,%
Batch: 40 | Loss: 4.184 | Acc: 41.101,67.207,82.412,%
Batch: 60 | Loss: 4.230 | Acc: 40.420,66.547,82.082,%
Batch: 80 | Loss: 4.222 | Acc: 40.394,66.773,82.195,%
Batch: 100 | Loss: 4.229 | Acc: 40.408,66.460,82.519,%
Batch: 120 | Loss: 4.262 | Acc: 40.328,66.083,82.109,%
Batch: 140 | Loss: 4.266 | Acc: 40.320,65.952,82.048,%
Batch: 160 | Loss: 4.279 | Acc: 40.169,65.882,82.172,%
Batch: 180 | Loss: 4.268 | Acc: 40.331,65.884,82.096,%
Batch: 200 | Loss: 4.264 | Acc: 40.446,65.808,82.261,%
Batch: 220 | Loss: 4.259 | Acc: 40.480,65.834,82.413,%
Batch: 240 | Loss: 4.244 | Acc: 40.612,65.949,82.657,%
Batch: 260 | Loss: 4.251 | Acc: 40.499,65.996,82.690,%
Batch: 280 | Loss: 4.240 | Acc: 40.472,65.984,82.729,%
Batch: 300 | Loss: 4.225 | Acc: 40.609,66.027,82.818,%
Batch: 320 | Loss: 4.230 | Acc: 40.523,66.044,82.856,%
Batch: 340 | Loss: 4.224 | Acc: 40.641,66.074,82.916,%
Batch: 360 | Loss: 4.220 | Acc: 40.718,66.138,83.012,%
Batch: 380 | Loss: 4.217 | Acc: 40.795,66.170,83.087,%
Batch: 0 | Loss: 5.182 | Acc: 40.625,63.281,68.750,%
Batch: 20 | Loss: 5.461 | Acc: 36.198,59.524,65.699,%
Batch: 40 | Loss: 5.455 | Acc: 36.300,59.108,65.225,%
Batch: 60 | Loss: 5.462 | Acc: 36.245,58.747,65.318,%
Train classifier parameters

Epoch: 86
Batch: 0 | Loss: 4.119 | Acc: 38.281,68.750,87.500,%
Batch: 20 | Loss: 4.095 | Acc: 41.369,67.113,84.635,%
Batch: 40 | Loss: 4.144 | Acc: 40.949,67.702,84.661,%
Batch: 60 | Loss: 4.070 | Acc: 41.983,67.789,84.874,%
Batch: 80 | Loss: 4.060 | Acc: 42.081,67.660,84.867,%
Batch: 100 | Loss: 4.044 | Acc: 42.071,67.528,85.110,%
Batch: 120 | Loss: 4.067 | Acc: 41.794,67.200,84.969,%
Batch: 140 | Loss: 4.090 | Acc: 41.617,67.154,84.990,%
Batch: 160 | Loss: 4.083 | Acc: 41.722,67.304,85.103,%
Batch: 180 | Loss: 4.081 | Acc: 41.803,67.343,85.117,%
Batch: 200 | Loss: 4.076 | Acc: 41.970,67.316,85.016,%
Batch: 220 | Loss: 4.080 | Acc: 41.841,67.393,85.011,%
Batch: 240 | Loss: 4.076 | Acc: 41.899,67.372,85.007,%
Batch: 260 | Loss: 4.079 | Acc: 41.828,67.316,85.093,%
Batch: 280 | Loss: 4.081 | Acc: 41.729,67.307,85.042,%
Batch: 300 | Loss: 4.081 | Acc: 41.759,67.372,85.034,%
Batch: 320 | Loss: 4.077 | Acc: 41.737,67.387,85.013,%
Batch: 340 | Loss: 4.073 | Acc: 41.683,67.449,85.044,%
Batch: 360 | Loss: 4.074 | Acc: 41.685,67.497,85.065,%
Batch: 380 | Loss: 4.074 | Acc: 41.667,67.585,85.064,%
Batch: 0 | Loss: 5.199 | Acc: 41.406,64.062,67.969,%
Batch: 20 | Loss: 5.435 | Acc: 37.202,60.082,65.439,%
Batch: 40 | Loss: 5.418 | Acc: 37.005,59.261,65.282,%
Batch: 60 | Loss: 5.414 | Acc: 36.988,59.055,65.471,%
Train classifier parameters

Epoch: 87
Batch: 0 | Loss: 3.920 | Acc: 39.844,68.750,89.844,%
Batch: 20 | Loss: 3.955 | Acc: 43.564,69.420,86.644,%
Batch: 40 | Loss: 3.995 | Acc: 42.302,68.102,86.338,%
Batch: 60 | Loss: 4.018 | Acc: 41.842,67.661,86.014,%
Batch: 80 | Loss: 4.032 | Acc: 41.898,67.429,86.053,%
Batch: 100 | Loss: 4.027 | Acc: 41.994,67.690,86.077,%
Batch: 120 | Loss: 4.033 | Acc: 42.136,67.762,86.021,%
Batch: 140 | Loss: 4.020 | Acc: 42.127,67.725,86.043,%
Batch: 160 | Loss: 4.024 | Acc: 42.032,67.682,85.957,%
Batch: 180 | Loss: 4.026 | Acc: 42.093,67.727,85.955,%
Batch: 200 | Loss: 4.022 | Acc: 42.246,67.895,85.949,%
Batch: 220 | Loss: 4.031 | Acc: 42.290,67.891,85.828,%
Batch: 240 | Loss: 4.015 | Acc: 42.495,67.982,85.814,%
Batch: 260 | Loss: 4.008 | Acc: 42.490,68.035,85.857,%
Batch: 280 | Loss: 4.007 | Acc: 42.541,67.999,85.843,%
Batch: 300 | Loss: 4.019 | Acc: 42.442,67.917,85.761,%
Batch: 320 | Loss: 4.023 | Acc: 42.426,67.891,85.755,%
Batch: 340 | Loss: 4.023 | Acc: 42.433,67.882,85.734,%
Batch: 360 | Loss: 4.022 | Acc: 42.423,67.835,85.710,%
Batch: 380 | Loss: 4.024 | Acc: 42.442,67.831,85.722,%
Batch: 0 | Loss: 5.108 | Acc: 43.750,64.062,71.094,%
Batch: 20 | Loss: 5.369 | Acc: 36.979,59.412,66.853,%
Batch: 40 | Loss: 5.355 | Acc: 37.500,59.204,66.292,%
Batch: 60 | Loss: 5.359 | Acc: 37.308,58.824,66.099,%
Train classifier parameters

Epoch: 88
Batch: 0 | Loss: 4.106 | Acc: 41.406,74.219,89.062,%
Batch: 20 | Loss: 3.986 | Acc: 42.411,69.010,86.570,%
Batch: 40 | Loss: 3.924 | Acc: 42.683,68.941,86.623,%
Batch: 60 | Loss: 3.852 | Acc: 43.404,69.352,86.847,%
Batch: 80 | Loss: 3.875 | Acc: 43.499,69.348,86.680,%
Batch: 100 | Loss: 3.891 | Acc: 43.441,68.936,86.595,%
Batch: 120 | Loss: 3.899 | Acc: 43.382,69.028,86.648,%
Batch: 140 | Loss: 3.907 | Acc: 43.268,68.999,86.619,%
Batch: 160 | Loss: 3.932 | Acc: 43.066,68.750,86.500,%
Batch: 180 | Loss: 3.931 | Acc: 42.964,68.603,86.386,%
Batch: 200 | Loss: 3.913 | Acc: 43.070,68.734,86.462,%
Batch: 220 | Loss: 3.919 | Acc: 42.993,68.633,86.457,%
Batch: 240 | Loss: 3.923 | Acc: 42.998,68.510,86.378,%
Batch: 260 | Loss: 3.933 | Acc: 42.876,68.499,86.389,%
Batch: 280 | Loss: 3.948 | Acc: 42.835,68.494,86.355,%
Batch: 300 | Loss: 3.954 | Acc: 42.792,68.550,86.379,%
Batch: 320 | Loss: 3.958 | Acc: 42.852,68.529,86.346,%
Batch: 340 | Loss: 3.971 | Acc: 42.675,68.459,86.224,%
Batch: 360 | Loss: 3.976 | Acc: 42.601,68.460,86.247,%
Batch: 380 | Loss: 3.976 | Acc: 42.616,68.467,86.253,%
Batch: 0 | Loss: 5.121 | Acc: 42.969,64.062,69.531,%
Batch: 20 | Loss: 5.336 | Acc: 36.868,59.933,66.629,%
Batch: 40 | Loss: 5.324 | Acc: 37.233,59.718,66.406,%
Batch: 60 | Loss: 5.327 | Acc: 37.308,59.170,66.253,%
Train classifier parameters

Epoch: 89
Batch: 0 | Loss: 3.196 | Acc: 49.219,76.562,86.719,%
Batch: 20 | Loss: 3.711 | Acc: 44.940,69.085,87.277,%
Batch: 40 | Loss: 3.811 | Acc: 44.207,69.379,87.005,%
Batch: 60 | Loss: 3.895 | Acc: 43.276,68.712,86.911,%
Batch: 80 | Loss: 3.883 | Acc: 43.239,69.117,87.124,%
Batch: 100 | Loss: 3.925 | Acc: 42.690,68.866,87.198,%
Batch: 120 | Loss: 3.913 | Acc: 42.846,68.924,87.235,%
Batch: 140 | Loss: 3.920 | Acc: 42.980,68.767,87.112,%
Batch: 160 | Loss: 3.909 | Acc: 43.143,68.750,86.893,%
Batch: 180 | Loss: 3.893 | Acc: 43.297,68.810,86.913,%
Batch: 200 | Loss: 3.886 | Acc: 43.241,68.902,86.890,%
Batch: 220 | Loss: 3.892 | Acc: 43.252,68.810,86.952,%
Batch: 240 | Loss: 3.896 | Acc: 43.303,68.701,86.858,%
Batch: 260 | Loss: 3.895 | Acc: 43.292,68.681,86.862,%
Batch: 280 | Loss: 3.899 | Acc: 43.291,68.733,86.785,%
Batch: 300 | Loss: 3.907 | Acc: 43.195,68.646,86.786,%
Batch: 320 | Loss: 3.912 | Acc: 43.159,68.606,86.821,%
Batch: 340 | Loss: 3.916 | Acc: 43.136,68.624,86.785,%
Batch: 360 | Loss: 3.919 | Acc: 43.042,68.579,86.775,%
Batch: 380 | Loss: 3.923 | Acc: 42.934,68.592,86.694,%
Batch: 0 | Loss: 5.188 | Acc: 42.969,64.844,66.406,%
Batch: 20 | Loss: 5.329 | Acc: 37.984,60.826,66.109,%
Batch: 40 | Loss: 5.315 | Acc: 38.053,60.023,66.006,%
Batch: 60 | Loss: 5.311 | Acc: 38.025,59.554,66.214,%
Train classifier parameters

Epoch: 90
Batch: 0 | Loss: 3.189 | Acc: 52.344,75.781,88.281,%
Batch: 20 | Loss: 3.872 | Acc: 43.192,70.350,88.132,%
Batch: 40 | Loss: 3.829 | Acc: 43.293,70.541,87.691,%
Batch: 60 | Loss: 3.877 | Acc: 42.982,70.031,87.180,%
Batch: 80 | Loss: 3.880 | Acc: 43.229,69.743,87.191,%
Batch: 100 | Loss: 3.915 | Acc: 43.162,69.175,86.989,%
Batch: 120 | Loss: 3.914 | Acc: 43.124,69.228,87.009,%
Batch: 140 | Loss: 3.913 | Acc: 42.941,69.188,87.029,%
Batch: 160 | Loss: 3.903 | Acc: 42.949,69.027,86.966,%
Batch: 180 | Loss: 3.900 | Acc: 42.964,68.949,86.926,%
Batch: 200 | Loss: 3.914 | Acc: 42.736,68.991,86.878,%
Batch: 220 | Loss: 3.916 | Acc: 42.788,69.043,86.977,%
Batch: 240 | Loss: 3.910 | Acc: 42.803,69.013,86.962,%
Batch: 260 | Loss: 3.912 | Acc: 42.816,68.960,86.922,%
Batch: 280 | Loss: 3.921 | Acc: 42.732,68.925,86.874,%
Batch: 300 | Loss: 3.923 | Acc: 42.899,68.792,86.836,%
Batch: 320 | Loss: 3.921 | Acc: 42.940,68.833,86.828,%
Batch: 340 | Loss: 3.918 | Acc: 42.996,68.750,86.769,%
Batch: 360 | Loss: 3.924 | Acc: 42.975,68.722,86.721,%
Batch: 380 | Loss: 3.926 | Acc: 42.917,68.738,86.698,%
Batch: 0 | Loss: 5.139 | Acc: 41.406,65.625,67.188,%
Batch: 20 | Loss: 5.344 | Acc: 37.463,60.268,66.592,%
Batch: 40 | Loss: 5.334 | Acc: 37.957,59.566,66.139,%
Batch: 60 | Loss: 5.333 | Acc: 37.782,59.209,66.278,%
Train classifier parameters

Epoch: 91
Batch: 0 | Loss: 3.525 | Acc: 49.219,69.531,85.938,%
Batch: 20 | Loss: 4.076 | Acc: 41.406,67.969,87.016,%
Batch: 40 | Loss: 3.960 | Acc: 42.245,68.674,87.100,%
Batch: 60 | Loss: 3.920 | Acc: 42.213,68.891,87.001,%
Batch: 80 | Loss: 3.922 | Acc: 41.975,68.962,86.941,%
Batch: 100 | Loss: 3.933 | Acc: 42.079,69.346,86.982,%
Batch: 120 | Loss: 3.933 | Acc: 42.446,69.241,87.003,%
Batch: 140 | Loss: 3.922 | Acc: 42.520,69.215,87.140,%
Batch: 160 | Loss: 3.932 | Acc: 42.435,69.279,87.223,%
Batch: 180 | Loss: 3.938 | Acc: 42.408,69.255,87.051,%
Batch: 200 | Loss: 3.928 | Acc: 42.654,69.407,87.107,%
Batch: 220 | Loss: 3.913 | Acc: 42.810,69.422,87.072,%
Batch: 240 | Loss: 3.912 | Acc: 42.842,69.392,87.049,%
Batch: 260 | Loss: 3.906 | Acc: 42.897,69.349,87.000,%
Batch: 280 | Loss: 3.891 | Acc: 43.013,69.476,87.036,%
Batch: 300 | Loss: 3.903 | Acc: 42.948,69.326,86.958,%
Batch: 320 | Loss: 3.908 | Acc: 42.905,69.266,86.911,%
Batch: 340 | Loss: 3.919 | Acc: 42.918,69.158,86.838,%
Batch: 360 | Loss: 3.928 | Acc: 42.791,69.027,86.829,%
Batch: 380 | Loss: 3.928 | Acc: 42.770,68.910,86.797,%
Batch: 0 | Loss: 5.116 | Acc: 40.625,64.062,68.750,%
Batch: 20 | Loss: 5.335 | Acc: 37.500,59.635,66.927,%
Batch: 40 | Loss: 5.315 | Acc: 37.557,59.280,66.349,%
Batch: 60 | Loss: 5.308 | Acc: 37.551,59.016,66.317,%
Train classifier parameters

Epoch: 92
Batch: 0 | Loss: 4.077 | Acc: 42.188,71.875,91.406,%
Batch: 20 | Loss: 3.974 | Acc: 43.378,70.275,87.686,%
Batch: 40 | Loss: 3.889 | Acc: 44.074,69.836,87.405,%
Batch: 60 | Loss: 3.845 | Acc: 44.262,70.146,87.628,%
Batch: 80 | Loss: 3.844 | Acc: 43.769,70.168,87.741,%
Batch: 100 | Loss: 3.847 | Acc: 43.572,69.794,87.639,%
Batch: 120 | Loss: 3.856 | Acc: 43.485,69.247,87.584,%
Batch: 140 | Loss: 3.860 | Acc: 43.290,69.199,87.434,%
Batch: 160 | Loss: 3.876 | Acc: 43.202,69.240,87.437,%
Batch: 180 | Loss: 3.882 | Acc: 43.064,69.354,87.448,%
Batch: 200 | Loss: 3.883 | Acc: 42.926,69.349,87.298,%
Batch: 220 | Loss: 3.888 | Acc: 42.838,69.231,87.214,%
Batch: 240 | Loss: 3.887 | Acc: 42.904,69.136,87.263,%
Batch: 260 | Loss: 3.884 | Acc: 43.050,69.226,87.350,%
Batch: 280 | Loss: 3.883 | Acc: 42.941,69.228,87.336,%
Batch: 300 | Loss: 3.880 | Acc: 43.008,69.155,87.311,%
Batch: 320 | Loss: 3.883 | Acc: 42.998,69.052,87.254,%
Batch: 340 | Loss: 3.879 | Acc: 43.035,69.094,87.230,%
Batch: 360 | Loss: 3.874 | Acc: 43.166,69.135,87.216,%
Batch: 380 | Loss: 3.872 | Acc: 43.248,69.078,87.184,%
Batch: 0 | Loss: 5.120 | Acc: 42.969,64.844,68.750,%
Batch: 20 | Loss: 5.316 | Acc: 37.984,59.040,66.592,%
Batch: 40 | Loss: 5.290 | Acc: 37.957,59.280,66.521,%
Batch: 60 | Loss: 5.287 | Acc: 37.884,59.106,66.509,%
Train classifier parameters

Epoch: 93
Batch: 0 | Loss: 3.486 | Acc: 49.219,75.781,85.156,%
Batch: 20 | Loss: 3.832 | Acc: 42.746,71.168,87.649,%
Batch: 40 | Loss: 3.873 | Acc: 42.607,69.912,87.767,%
Batch: 60 | Loss: 3.874 | Acc: 42.636,69.647,87.487,%
Batch: 80 | Loss: 3.846 | Acc: 42.795,69.792,87.741,%
Batch: 100 | Loss: 3.853 | Acc: 42.837,69.392,87.833,%
Batch: 120 | Loss: 3.869 | Acc: 42.749,69.267,87.765,%
Batch: 140 | Loss: 3.848 | Acc: 42.858,69.481,87.755,%
Batch: 160 | Loss: 3.850 | Acc: 42.780,69.245,87.655,%
Batch: 180 | Loss: 3.878 | Acc: 42.563,69.173,87.535,%
Batch: 200 | Loss: 3.881 | Acc: 42.600,69.228,87.655,%
Batch: 220 | Loss: 3.882 | Acc: 42.640,69.397,87.578,%
Batch: 240 | Loss: 3.878 | Acc: 42.790,69.392,87.539,%
Batch: 260 | Loss: 3.890 | Acc: 42.687,69.313,87.533,%
Batch: 280 | Loss: 3.894 | Acc: 42.713,69.234,87.506,%
Batch: 300 | Loss: 3.890 | Acc: 42.800,69.225,87.453,%
Batch: 320 | Loss: 3.895 | Acc: 42.898,69.113,87.400,%
Batch: 340 | Loss: 3.902 | Acc: 42.834,68.988,87.335,%
Batch: 360 | Loss: 3.895 | Acc: 42.923,69.021,87.361,%
Batch: 380 | Loss: 3.893 | Acc: 42.942,69.019,87.326,%
Batch: 0 | Loss: 5.059 | Acc: 42.969,64.062,70.312,%
Batch: 20 | Loss: 5.285 | Acc: 38.690,60.826,66.964,%
Batch: 40 | Loss: 5.269 | Acc: 38.491,60.061,66.387,%
Batch: 60 | Loss: 5.267 | Acc: 38.294,59.695,66.445,%
Train classifier parameters

Epoch: 94
Batch: 0 | Loss: 3.915 | Acc: 39.844,71.875,86.719,%
Batch: 20 | Loss: 3.793 | Acc: 43.266,69.159,88.579,%
Batch: 40 | Loss: 3.729 | Acc: 44.112,68.883,88.491,%
Batch: 60 | Loss: 3.780 | Acc: 44.057,68.904,88.051,%
Batch: 80 | Loss: 3.816 | Acc: 43.682,69.068,88.137,%
Batch: 100 | Loss: 3.829 | Acc: 43.750,69.175,87.655,%
Batch: 120 | Loss: 3.837 | Acc: 43.744,68.873,87.590,%
Batch: 140 | Loss: 3.852 | Acc: 43.573,68.944,87.506,%
Batch: 160 | Loss: 3.830 | Acc: 43.799,69.075,87.549,%
Batch: 180 | Loss: 3.828 | Acc: 43.659,69.346,87.435,%
Batch: 200 | Loss: 3.844 | Acc: 43.509,69.240,87.442,%
Batch: 220 | Loss: 3.840 | Acc: 43.538,69.344,87.532,%
Batch: 240 | Loss: 3.856 | Acc: 43.429,69.282,87.468,%
Batch: 260 | Loss: 3.855 | Acc: 43.385,69.343,87.461,%
Batch: 280 | Loss: 3.854 | Acc: 43.397,69.437,87.453,%
Batch: 300 | Loss: 3.861 | Acc: 43.314,69.370,87.373,%
Batch: 320 | Loss: 3.859 | Acc: 43.385,69.480,87.398,%
Batch: 340 | Loss: 3.861 | Acc: 43.436,69.330,87.376,%
Batch: 360 | Loss: 3.869 | Acc: 43.222,69.274,87.390,%
Batch: 380 | Loss: 3.867 | Acc: 43.274,69.133,87.363,%
Batch: 0 | Loss: 5.023 | Acc: 42.188,65.625,70.312,%
Batch: 20 | Loss: 5.286 | Acc: 37.835,60.491,67.336,%
Batch: 40 | Loss: 5.286 | Acc: 38.224,59.451,67.054,%
Batch: 60 | Loss: 5.276 | Acc: 38.115,59.234,67.175,%
Train classifier parameters

Epoch: 95
Batch: 0 | Loss: 4.129 | Acc: 45.312,67.969,89.844,%
Batch: 20 | Loss: 3.799 | Acc: 45.908,67.076,88.542,%
Batch: 40 | Loss: 3.779 | Acc: 44.588,67.797,88.396,%
Batch: 60 | Loss: 3.755 | Acc: 44.890,68.686,88.601,%
Batch: 80 | Loss: 3.777 | Acc: 44.811,68.895,88.349,%
Batch: 100 | Loss: 3.758 | Acc: 44.647,69.106,88.142,%
Batch: 120 | Loss: 3.776 | Acc: 44.396,69.099,87.797,%
Batch: 140 | Loss: 3.779 | Acc: 44.564,69.127,87.910,%
Batch: 160 | Loss: 3.799 | Acc: 44.327,69.036,87.801,%
Batch: 180 | Loss: 3.819 | Acc: 44.199,68.996,87.647,%
Batch: 200 | Loss: 3.813 | Acc: 44.185,69.150,87.683,%
Batch: 220 | Loss: 3.814 | Acc: 44.096,69.114,87.698,%
Batch: 240 | Loss: 3.819 | Acc: 44.019,69.152,87.698,%
Batch: 260 | Loss: 3.843 | Acc: 43.870,69.124,87.629,%
Batch: 280 | Loss: 3.838 | Acc: 43.856,69.084,87.642,%
Batch: 300 | Loss: 3.837 | Acc: 43.802,69.103,87.617,%
Batch: 320 | Loss: 3.846 | Acc: 43.731,69.027,87.600,%
Batch: 340 | Loss: 3.850 | Acc: 43.635,69.043,87.521,%
Batch: 360 | Loss: 3.854 | Acc: 43.514,69.034,87.556,%
Batch: 380 | Loss: 3.855 | Acc: 43.442,69.062,87.549,%
Batch: 0 | Loss: 5.070 | Acc: 39.062,64.062,67.188,%
Batch: 20 | Loss: 5.263 | Acc: 38.430,60.417,66.927,%
Batch: 40 | Loss: 5.246 | Acc: 38.205,60.042,66.864,%
Batch: 60 | Loss: 5.255 | Acc: 37.923,59.682,66.714,%
Train classifier parameters

Epoch: 96
Batch: 0 | Loss: 3.432 | Acc: 43.750,72.656,89.062,%
Batch: 20 | Loss: 3.707 | Acc: 44.382,68.490,88.690,%
Batch: 40 | Loss: 3.761 | Acc: 44.150,68.883,88.681,%
Batch: 60 | Loss: 3.762 | Acc: 43.724,69.045,88.384,%
Batch: 80 | Loss: 3.747 | Acc: 44.107,69.406,88.329,%
Batch: 100 | Loss: 3.776 | Acc: 43.758,69.346,88.281,%
Batch: 120 | Loss: 3.759 | Acc: 44.092,69.170,88.139,%
Batch: 140 | Loss: 3.764 | Acc: 44.088,69.481,88.049,%
Batch: 160 | Loss: 3.784 | Acc: 43.832,69.264,87.883,%
Batch: 180 | Loss: 3.762 | Acc: 44.031,69.432,87.975,%
Batch: 200 | Loss: 3.778 | Acc: 43.933,69.329,87.974,%
Batch: 220 | Loss: 3.778 | Acc: 44.093,69.461,87.921,%
Batch: 240 | Loss: 3.775 | Acc: 44.201,69.421,87.837,%
Batch: 260 | Loss: 3.772 | Acc: 44.199,69.516,87.841,%
Batch: 280 | Loss: 3.780 | Acc: 44.153,69.503,87.811,%
Batch: 300 | Loss: 3.779 | Acc: 44.072,69.555,87.840,%
Batch: 320 | Loss: 3.773 | Acc: 44.098,69.616,87.870,%
Batch: 340 | Loss: 3.774 | Acc: 44.096,69.561,87.800,%
Batch: 360 | Loss: 3.790 | Acc: 43.977,69.414,87.716,%
Batch: 380 | Loss: 3.794 | Acc: 43.828,69.455,87.707,%
Batch: 0 | Loss: 5.008 | Acc: 44.531,65.625,68.750,%
Batch: 20 | Loss: 5.279 | Acc: 38.430,60.119,66.481,%
Batch: 40 | Loss: 5.261 | Acc: 38.491,59.985,66.311,%
Batch: 60 | Loss: 5.257 | Acc: 38.473,59.862,66.381,%
Train classifier parameters

Epoch: 97
Batch: 0 | Loss: 3.541 | Acc: 42.969,64.062,87.500,%
Batch: 20 | Loss: 3.681 | Acc: 45.238,71.838,88.616,%
Batch: 40 | Loss: 3.755 | Acc: 44.760,70.503,87.862,%
Batch: 60 | Loss: 3.708 | Acc: 45.184,70.069,87.859,%
Batch: 80 | Loss: 3.761 | Acc: 44.551,69.975,87.963,%
Batch: 100 | Loss: 3.758 | Acc: 44.261,70.181,88.235,%
Batch: 120 | Loss: 3.757 | Acc: 44.208,70.158,88.055,%
Batch: 140 | Loss: 3.766 | Acc: 44.071,70.213,88.137,%
Batch: 160 | Loss: 3.775 | Acc: 43.866,70.283,88.150,%
Batch: 180 | Loss: 3.786 | Acc: 43.733,70.209,88.091,%
Batch: 200 | Loss: 3.800 | Acc: 43.560,70.064,87.943,%
Batch: 220 | Loss: 3.819 | Acc: 43.396,69.959,87.797,%
Batch: 240 | Loss: 3.829 | Acc: 43.228,69.891,87.795,%
Batch: 260 | Loss: 3.830 | Acc: 43.322,69.866,87.724,%
Batch: 280 | Loss: 3.840 | Acc: 43.213,69.690,87.603,%
Batch: 300 | Loss: 3.838 | Acc: 43.223,69.638,87.627,%
Batch: 320 | Loss: 3.838 | Acc: 43.297,69.646,87.680,%
Batch: 340 | Loss: 3.839 | Acc: 43.315,69.760,87.679,%
Batch: 360 | Loss: 3.838 | Acc: 43.265,69.739,87.654,%
Batch: 380 | Loss: 3.832 | Acc: 43.397,69.712,87.676,%
Batch: 0 | Loss: 5.042 | Acc: 45.312,64.062,71.094,%
Batch: 20 | Loss: 5.259 | Acc: 38.207,60.379,67.225,%
Batch: 40 | Loss: 5.236 | Acc: 38.472,60.213,66.978,%
Batch: 60 | Loss: 5.238 | Acc: 38.268,60.003,66.867,%
Train classifier parameters

Epoch: 98
Batch: 0 | Loss: 3.841 | Acc: 38.281,64.844,85.938,%
Batch: 20 | Loss: 3.880 | Acc: 41.853,70.052,88.356,%
Batch: 40 | Loss: 3.874 | Acc: 42.511,69.379,88.281,%
Batch: 60 | Loss: 3.950 | Acc: 41.650,69.121,88.064,%
Batch: 80 | Loss: 3.862 | Acc: 42.814,69.570,87.992,%
Batch: 100 | Loss: 3.864 | Acc: 43.031,69.361,87.871,%
Batch: 120 | Loss: 3.859 | Acc: 43.079,69.583,87.907,%
Batch: 140 | Loss: 3.857 | Acc: 43.251,69.709,88.049,%
Batch: 160 | Loss: 3.841 | Acc: 43.410,69.720,88.039,%
Batch: 180 | Loss: 3.834 | Acc: 43.465,69.812,88.001,%
Batch: 200 | Loss: 3.835 | Acc: 43.424,69.893,88.009,%
Batch: 220 | Loss: 3.843 | Acc: 43.478,69.917,87.956,%
Batch: 240 | Loss: 3.836 | Acc: 43.439,69.923,87.902,%
Batch: 260 | Loss: 3.828 | Acc: 43.463,70.016,87.874,%
Batch: 280 | Loss: 3.822 | Acc: 43.569,70.023,87.842,%
Batch: 300 | Loss: 3.825 | Acc: 43.524,70.017,87.762,%
Batch: 320 | Loss: 3.829 | Acc: 43.434,69.896,87.726,%
Batch: 340 | Loss: 3.826 | Acc: 43.422,69.841,87.743,%
Batch: 360 | Loss: 3.820 | Acc: 43.451,69.806,87.721,%
Batch: 380 | Loss: 3.812 | Acc: 43.485,69.812,87.758,%
Batch: 0 | Loss: 5.121 | Acc: 43.750,64.844,70.312,%
Batch: 20 | Loss: 5.279 | Acc: 38.504,60.342,66.443,%
Batch: 40 | Loss: 5.266 | Acc: 38.586,60.194,66.444,%
Batch: 60 | Loss: 5.258 | Acc: 38.320,60.041,66.381,%
Train classifier parameters

Epoch: 99
Batch: 0 | Loss: 3.763 | Acc: 42.188,72.656,85.156,%
Batch: 20 | Loss: 3.780 | Acc: 43.266,69.978,87.165,%
Batch: 40 | Loss: 3.851 | Acc: 42.835,69.607,87.233,%
Batch: 60 | Loss: 3.730 | Acc: 43.904,69.967,87.718,%
Batch: 80 | Loss: 3.734 | Acc: 43.789,70.476,88.011,%
Batch: 100 | Loss: 3.762 | Acc: 43.557,70.026,88.049,%
Batch: 120 | Loss: 3.778 | Acc: 43.543,69.706,88.049,%
Batch: 140 | Loss: 3.791 | Acc: 43.484,69.531,87.877,%
Batch: 160 | Loss: 3.811 | Acc: 43.435,69.415,87.859,%
Batch: 180 | Loss: 3.802 | Acc: 43.465,69.393,87.681,%
Batch: 200 | Loss: 3.802 | Acc: 43.408,69.504,87.729,%
Batch: 220 | Loss: 3.804 | Acc: 43.556,69.584,87.737,%
Batch: 240 | Loss: 3.800 | Acc: 43.685,69.444,87.779,%
Batch: 260 | Loss: 3.801 | Acc: 43.624,69.471,87.772,%
Batch: 280 | Loss: 3.786 | Acc: 43.875,69.598,87.828,%
Batch: 300 | Loss: 3.784 | Acc: 43.914,69.674,87.840,%
Batch: 320 | Loss: 3.785 | Acc: 43.908,69.731,87.836,%
Batch: 340 | Loss: 3.787 | Acc: 43.789,69.724,87.876,%
Batch: 360 | Loss: 3.794 | Acc: 43.720,69.698,87.853,%
Batch: 380 | Loss: 3.801 | Acc: 43.639,69.652,87.797,%
Batch: 0 | Loss: 5.057 | Acc: 44.531,64.844,66.406,%
Batch: 20 | Loss: 5.272 | Acc: 38.653,59.710,66.629,%
Batch: 40 | Loss: 5.261 | Acc: 38.758,59.737,66.730,%
Batch: 60 | Loss: 5.256 | Acc: 38.397,59.695,66.547,%
Train all parameters

Epoch: 100
Batch: 0 | Loss: 2.917 | Acc: 59.375,75.000,89.844,%
Batch: 20 | Loss: 4.494 | Acc: 40.327,62.760,78.385,%
Batch: 40 | Loss: 5.396 | Acc: 36.871,57.260,65.892,%
Batch: 60 | Loss: 5.788 | Acc: 35.745,54.495,59.606,%
Batch: 80 | Loss: 5.950 | Acc: 35.060,53.607,57.677,%
Batch: 100 | Loss: 5.957 | Acc: 35.342,53.690,57.232,%
Batch: 120 | Loss: 5.960 | Acc: 35.182,53.603,57.160,%
Batch: 140 | Loss: 5.917 | Acc: 35.444,53.917,57.469,%
Batch: 160 | Loss: 5.874 | Acc: 35.680,54.207,57.900,%
Batch: 180 | Loss: 5.804 | Acc: 36.140,54.735,58.525,%
Batch: 200 | Loss: 5.798 | Acc: 36.112,54.750,58.741,%
Batch: 220 | Loss: 5.776 | Acc: 36.312,54.931,59.060,%
Batch: 240 | Loss: 5.729 | Acc: 36.547,55.258,59.582,%
Batch: 260 | Loss: 5.694 | Acc: 36.659,55.403,60.034,%
Batch: 280 | Loss: 5.664 | Acc: 36.724,55.611,60.482,%
Batch: 300 | Loss: 5.630 | Acc: 36.929,55.876,60.860,%
Batch: 320 | Loss: 5.601 | Acc: 37.035,56.014,61.152,%
Batch: 340 | Loss: 5.569 | Acc: 37.177,56.190,61.446,%
Batch: 360 | Loss: 5.535 | Acc: 37.364,56.401,61.784,%
Batch: 380 | Loss: 5.513 | Acc: 37.346,56.512,62.104,%
Batch: 0 | Loss: 5.970 | Acc: 33.594,53.906,60.156,%
Batch: 20 | Loss: 6.213 | Acc: 33.296,51.897,57.478,%
Batch: 40 | Loss: 6.243 | Acc: 32.679,51.601,57.412,%
Batch: 60 | Loss: 6.261 | Acc: 32.287,51.486,56.980,%
Train all parameters

Epoch: 101
Batch: 0 | Loss: 5.405 | Acc: 33.594,61.719,70.312,%
Batch: 20 | Loss: 4.779 | Acc: 41.109,60.491,72.731,%
Batch: 40 | Loss: 4.752 | Acc: 40.606,60.785,72.180,%
Batch: 60 | Loss: 4.693 | Acc: 41.432,61.424,72.451,%
Batch: 80 | Loss: 4.712 | Acc: 41.551,61.391,72.328,%
Batch: 100 | Loss: 4.718 | Acc: 41.545,61.564,72.548,%
Batch: 120 | Loss: 4.727 | Acc: 41.180,61.538,72.566,%
Batch: 140 | Loss: 4.715 | Acc: 41.146,61.619,72.612,%
Batch: 160 | Loss: 4.736 | Acc: 40.698,61.510,72.506,%
Batch: 180 | Loss: 4.732 | Acc: 40.586,61.555,72.475,%
Batch: 200 | Loss: 4.721 | Acc: 40.749,61.742,72.477,%
Batch: 220 | Loss: 4.740 | Acc: 40.625,61.510,72.320,%
Batch: 240 | Loss: 4.735 | Acc: 40.615,61.605,72.264,%
Batch: 260 | Loss: 4.734 | Acc: 40.526,61.611,72.201,%
Batch: 280 | Loss: 4.730 | Acc: 40.572,61.705,72.286,%
Batch: 300 | Loss: 4.730 | Acc: 40.550,61.807,72.254,%
Batch: 320 | Loss: 4.727 | Acc: 40.681,61.836,72.281,%
Batch: 340 | Loss: 4.716 | Acc: 40.813,61.932,72.388,%
Batch: 360 | Loss: 4.710 | Acc: 40.896,62.041,72.429,%
Batch: 380 | Loss: 4.707 | Acc: 40.873,62.078,72.412,%
Batch: 0 | Loss: 5.234 | Acc: 42.188,56.250,67.188,%
Batch: 20 | Loss: 5.860 | Acc: 33.966,54.427,60.454,%
Batch: 40 | Loss: 5.824 | Acc: 34.985,54.364,60.537,%
Batch: 60 | Loss: 5.822 | Acc: 34.772,54.201,60.310,%
Train all parameters

Epoch: 102
Batch: 0 | Loss: 4.826 | Acc: 38.281,60.938,81.250,%
Batch: 20 | Loss: 4.256 | Acc: 43.452,65.216,77.195,%
Batch: 40 | Loss: 4.290 | Acc: 42.912,66.216,78.525,%
Batch: 60 | Loss: 4.297 | Acc: 42.700,65.996,78.458,%
Batch: 80 | Loss: 4.291 | Acc: 42.554,65.625,78.607,%
Batch: 100 | Loss: 4.307 | Acc: 42.226,65.486,78.574,%
Batch: 120 | Loss: 4.300 | Acc: 42.317,65.483,78.512,%
Batch: 140 | Loss: 4.351 | Acc: 42.016,64.866,78.164,%
Batch: 160 | Loss: 4.352 | Acc: 42.090,64.781,78.033,%
Batch: 180 | Loss: 4.346 | Acc: 42.088,64.857,77.814,%
Batch: 200 | Loss: 4.367 | Acc: 42.009,64.638,77.554,%
Batch: 220 | Loss: 4.384 | Acc: 41.894,64.473,77.333,%
Batch: 240 | Loss: 4.394 | Acc: 41.912,64.409,77.318,%
Batch: 260 | Loss: 4.396 | Acc: 41.870,64.440,77.299,%
Batch: 280 | Loss: 4.402 | Acc: 41.879,64.410,77.149,%
Batch: 300 | Loss: 4.412 | Acc: 41.892,64.223,76.991,%
Batch: 320 | Loss: 4.420 | Acc: 41.852,64.189,76.862,%
Batch: 340 | Loss: 4.417 | Acc: 41.897,64.243,76.803,%
Batch: 360 | Loss: 4.418 | Acc: 41.921,64.207,76.783,%
Batch: 380 | Loss: 4.423 | Acc: 41.892,64.181,76.788,%
Batch: 0 | Loss: 5.265 | Acc: 43.750,63.281,64.062,%
Batch: 20 | Loss: 5.622 | Acc: 36.719,57.478,63.802,%
Batch: 40 | Loss: 5.588 | Acc: 36.757,56.707,63.148,%
Batch: 60 | Loss: 5.595 | Acc: 36.322,56.749,63.128,%
Train all parameters

Epoch: 103
Batch: 0 | Loss: 4.112 | Acc: 44.531,67.969,82.812,%
Batch: 20 | Loss: 4.181 | Acc: 42.188,65.365,80.804,%
Batch: 40 | Loss: 4.166 | Acc: 42.702,66.235,81.193,%
Batch: 60 | Loss: 4.162 | Acc: 42.380,66.701,81.762,%
Batch: 80 | Loss: 4.159 | Acc: 42.506,66.831,81.723,%
Batch: 100 | Loss: 4.155 | Acc: 42.636,66.762,81.614,%
Batch: 120 | Loss: 4.148 | Acc: 42.775,66.658,81.411,%
Batch: 140 | Loss: 4.170 | Acc: 42.537,66.539,81.078,%
Batch: 160 | Loss: 4.167 | Acc: 42.624,66.464,80.818,%
Batch: 180 | Loss: 4.152 | Acc: 42.904,66.497,80.693,%
Batch: 200 | Loss: 4.163 | Acc: 42.980,66.406,80.585,%
Batch: 220 | Loss: 4.176 | Acc: 42.955,66.318,80.352,%
Batch: 240 | Loss: 4.182 | Acc: 42.894,66.338,80.297,%
Batch: 260 | Loss: 4.189 | Acc: 42.921,66.206,80.166,%
Batch: 280 | Loss: 4.192 | Acc: 42.908,66.053,80.082,%
Batch: 300 | Loss: 4.195 | Acc: 42.919,65.981,80.043,%
Batch: 320 | Loss: 4.214 | Acc: 42.893,65.958,79.890,%
Batch: 340 | Loss: 4.215 | Acc: 42.923,65.916,79.857,%
Batch: 360 | Loss: 4.209 | Acc: 42.995,65.945,79.813,%
Batch: 380 | Loss: 4.215 | Acc: 42.967,65.918,79.673,%
Batch: 0 | Loss: 5.538 | Acc: 37.500,59.375,66.406,%
Batch: 20 | Loss: 5.830 | Acc: 34.710,54.539,62.649,%
Batch: 40 | Loss: 5.771 | Acc: 35.690,54.745,62.443,%
Batch: 60 | Loss: 5.779 | Acc: 35.464,54.688,62.167,%
Train all parameters

Epoch: 104
Batch: 0 | Loss: 4.394 | Acc: 35.156,68.750,85.938,%
Batch: 20 | Loss: 4.099 | Acc: 41.592,68.155,83.259,%
Batch: 40 | Loss: 4.075 | Acc: 42.283,68.388,83.003,%
Batch: 60 | Loss: 4.098 | Acc: 42.252,68.263,82.736,%
Batch: 80 | Loss: 4.138 | Acc: 42.207,67.593,82.137,%
Batch: 100 | Loss: 4.103 | Acc: 43.000,67.744,81.799,%
Batch: 120 | Loss: 4.123 | Acc: 43.143,67.304,81.405,%
Batch: 140 | Loss: 4.131 | Acc: 43.224,67.082,81.178,%
Batch: 160 | Loss: 4.133 | Acc: 43.197,67.056,81.124,%
Batch: 180 | Loss: 4.152 | Acc: 42.917,67.049,81.103,%
Batch: 200 | Loss: 4.148 | Acc: 42.938,67.083,81.098,%
Batch: 220 | Loss: 4.149 | Acc: 43.032,66.986,81.063,%
Batch: 240 | Loss: 4.156 | Acc: 43.108,66.912,81.010,%
Batch: 260 | Loss: 4.161 | Acc: 43.127,66.882,80.987,%
Batch: 280 | Loss: 4.154 | Acc: 43.277,66.896,81.019,%
Batch: 300 | Loss: 4.144 | Acc: 43.327,66.884,81.037,%
Batch: 320 | Loss: 4.146 | Acc: 43.339,66.803,80.951,%
Batch: 340 | Loss: 4.154 | Acc: 43.269,66.704,80.822,%
Batch: 360 | Loss: 4.162 | Acc: 43.235,66.575,80.752,%
Batch: 380 | Loss: 4.165 | Acc: 43.223,66.468,80.649,%
Batch: 0 | Loss: 5.064 | Acc: 37.500,64.844,67.969,%
Batch: 20 | Loss: 5.455 | Acc: 37.016,59.487,64.918,%
Batch: 40 | Loss: 5.472 | Acc: 37.176,59.165,64.386,%
Batch: 60 | Loss: 5.494 | Acc: 36.770,59.132,64.165,%
Train all parameters

Epoch: 105
Batch: 0 | Loss: 4.040 | Acc: 42.969,62.500,82.031,%
Batch: 20 | Loss: 3.891 | Acc: 43.676,68.936,84.635,%
Batch: 40 | Loss: 3.876 | Acc: 44.665,69.207,85.099,%
Batch: 60 | Loss: 3.870 | Acc: 44.582,68.865,84.618,%
Batch: 80 | Loss: 3.885 | Acc: 44.589,68.634,84.144,%
Batch: 100 | Loss: 3.934 | Acc: 44.121,68.201,83.919,%
Batch: 120 | Loss: 3.979 | Acc: 43.744,67.949,83.678,%
Batch: 140 | Loss: 4.000 | Acc: 43.828,67.869,83.383,%
Batch: 160 | Loss: 4.012 | Acc: 43.799,67.692,83.113,%
Batch: 180 | Loss: 4.012 | Acc: 43.901,67.641,82.890,%
Batch: 200 | Loss: 4.035 | Acc: 43.657,67.506,82.626,%
Batch: 220 | Loss: 4.047 | Acc: 43.729,67.364,82.434,%
Batch: 240 | Loss: 4.041 | Acc: 43.915,67.376,82.411,%
Batch: 260 | Loss: 4.034 | Acc: 44.010,67.409,82.328,%
Batch: 280 | Loss: 4.043 | Acc: 43.931,67.276,82.209,%
Batch: 300 | Loss: 4.050 | Acc: 43.911,67.216,82.073,%
Batch: 320 | Loss: 4.043 | Acc: 44.040,67.202,82.041,%
Batch: 340 | Loss: 4.053 | Acc: 43.995,67.153,81.979,%
Batch: 360 | Loss: 4.061 | Acc: 43.932,67.075,81.841,%
Batch: 380 | Loss: 4.067 | Acc: 43.853,67.058,81.742,%
Batch: 0 | Loss: 5.308 | Acc: 42.188,60.156,67.969,%
Batch: 20 | Loss: 5.509 | Acc: 35.789,57.068,64.174,%
Batch: 40 | Loss: 5.499 | Acc: 36.738,57.031,63.624,%
Batch: 60 | Loss: 5.489 | Acc: 36.834,56.814,63.627,%
Train all parameters

Epoch: 106
Batch: 0 | Loss: 3.178 | Acc: 47.656,73.438,86.719,%
Batch: 20 | Loss: 3.680 | Acc: 45.126,68.527,84.747,%
Batch: 40 | Loss: 3.795 | Acc: 44.836,67.988,84.108,%
Batch: 60 | Loss: 3.842 | Acc: 44.237,68.174,83.619,%
Batch: 80 | Loss: 3.864 | Acc: 44.686,67.988,83.729,%
Batch: 100 | Loss: 3.869 | Acc: 44.663,68.301,83.725,%
Batch: 120 | Loss: 3.868 | Acc: 44.764,68.169,83.775,%
Batch: 140 | Loss: 3.890 | Acc: 44.470,68.118,83.771,%
Batch: 160 | Loss: 3.894 | Acc: 44.556,68.168,83.686,%
Batch: 180 | Loss: 3.890 | Acc: 44.570,68.236,83.719,%
Batch: 200 | Loss: 3.896 | Acc: 44.356,68.183,83.718,%
Batch: 220 | Loss: 3.908 | Acc: 44.231,68.025,83.590,%
Batch: 240 | Loss: 3.925 | Acc: 44.058,67.985,83.458,%
Batch: 260 | Loss: 3.938 | Acc: 44.064,67.912,83.339,%
Batch: 280 | Loss: 3.939 | Acc: 44.086,67.955,83.316,%
Batch: 300 | Loss: 3.945 | Acc: 44.010,67.943,83.249,%
Batch: 320 | Loss: 3.942 | Acc: 44.105,67.949,83.248,%
Batch: 340 | Loss: 3.942 | Acc: 44.107,67.893,83.204,%
Batch: 360 | Loss: 3.948 | Acc: 44.072,67.813,83.122,%
Batch: 380 | Loss: 3.959 | Acc: 43.937,67.721,83.061,%
Batch: 0 | Loss: 5.100 | Acc: 45.312,62.500,66.406,%
Batch: 20 | Loss: 5.397 | Acc: 38.430,58.557,65.513,%
Batch: 40 | Loss: 5.377 | Acc: 39.196,58.556,64.958,%
Batch: 60 | Loss: 5.370 | Acc: 38.448,58.427,65.330,%
Train all parameters

Epoch: 107
Batch: 0 | Loss: 3.304 | Acc: 38.281,66.406,86.719,%
Batch: 20 | Loss: 3.804 | Acc: 44.345,68.080,85.975,%
Batch: 40 | Loss: 3.776 | Acc: 44.512,68.064,86.033,%
Batch: 60 | Loss: 3.754 | Acc: 44.749,69.237,86.168,%
Batch: 80 | Loss: 3.774 | Acc: 44.850,68.933,86.217,%
Batch: 100 | Loss: 3.783 | Acc: 44.864,68.881,86.301,%
Batch: 120 | Loss: 3.782 | Acc: 44.835,68.821,86.286,%
Batch: 140 | Loss: 3.775 | Acc: 44.880,69.049,86.453,%
Batch: 160 | Loss: 3.786 | Acc: 44.779,68.896,86.301,%
Batch: 180 | Loss: 3.806 | Acc: 44.458,68.690,86.205,%
Batch: 200 | Loss: 3.816 | Acc: 44.391,68.789,86.035,%
Batch: 220 | Loss: 3.831 | Acc: 44.298,68.598,85.747,%
Batch: 240 | Loss: 3.851 | Acc: 44.145,68.465,85.584,%
Batch: 260 | Loss: 3.851 | Acc: 44.118,68.442,85.387,%
Batch: 280 | Loss: 3.873 | Acc: 43.975,68.369,85.173,%
Batch: 300 | Loss: 3.879 | Acc: 44.054,68.275,85.032,%
Batch: 320 | Loss: 3.891 | Acc: 44.052,68.263,84.918,%
Batch: 340 | Loss: 3.888 | Acc: 44.123,68.251,84.886,%
Batch: 360 | Loss: 3.899 | Acc: 43.997,68.259,84.812,%
Batch: 380 | Loss: 3.914 | Acc: 43.924,68.147,84.640,%
Batch: 0 | Loss: 5.111 | Acc: 39.062,66.406,69.531,%
Batch: 20 | Loss: 5.433 | Acc: 37.426,58.594,64.881,%
Batch: 40 | Loss: 5.425 | Acc: 38.014,57.889,64.463,%
Batch: 60 | Loss: 5.437 | Acc: 37.564,57.851,64.460,%
Train all parameters

Epoch: 108
Batch: 0 | Loss: 4.334 | Acc: 38.281,68.750,83.594,%
Batch: 20 | Loss: 3.824 | Acc: 44.754,70.275,85.975,%
Batch: 40 | Loss: 3.844 | Acc: 44.360,69.455,86.319,%
Batch: 60 | Loss: 3.845 | Acc: 44.019,69.006,86.168,%
Batch: 80 | Loss: 3.821 | Acc: 44.396,69.435,86.034,%
Batch: 100 | Loss: 3.823 | Acc: 44.562,69.222,85.806,%
Batch: 120 | Loss: 3.837 | Acc: 44.267,69.196,85.640,%
Batch: 140 | Loss: 3.833 | Acc: 44.470,69.177,85.649,%
Batch: 160 | Loss: 3.830 | Acc: 44.502,69.138,85.612,%
Batch: 180 | Loss: 3.843 | Acc: 44.492,69.130,85.510,%
Batch: 200 | Loss: 3.861 | Acc: 44.520,68.983,85.277,%
Batch: 220 | Loss: 3.857 | Acc: 44.641,68.888,85.284,%
Batch: 240 | Loss: 3.858 | Acc: 44.612,68.766,85.192,%
Batch: 260 | Loss: 3.865 | Acc: 44.531,68.747,85.084,%
Batch: 280 | Loss: 3.872 | Acc: 44.476,68.764,85.020,%
Batch: 300 | Loss: 3.874 | Acc: 44.578,68.779,84.969,%
Batch: 320 | Loss: 3.871 | Acc: 44.621,68.767,84.927,%
Batch: 340 | Loss: 3.879 | Acc: 44.563,68.702,84.895,%
Batch: 360 | Loss: 3.887 | Acc: 44.410,68.666,84.799,%
Batch: 380 | Loss: 3.903 | Acc: 44.328,68.502,84.599,%
Batch: 0 | Loss: 5.321 | Acc: 42.188,63.281,67.188,%
Batch: 20 | Loss: 5.701 | Acc: 36.012,56.771,62.388,%
Batch: 40 | Loss: 5.638 | Acc: 36.909,56.002,62.348,%
Batch: 60 | Loss: 5.637 | Acc: 36.463,56.135,62.820,%
Train all parameters

Epoch: 109
Batch: 0 | Loss: 4.484 | Acc: 36.719,67.969,84.375,%
Batch: 20 | Loss: 3.669 | Acc: 44.457,72.321,87.202,%
Batch: 40 | Loss: 3.640 | Acc: 45.465,71.399,87.500,%
Batch: 60 | Loss: 3.704 | Acc: 44.954,70.633,87.385,%
Batch: 80 | Loss: 3.704 | Acc: 44.734,70.341,87.240,%
Batch: 100 | Loss: 3.703 | Acc: 45.088,70.150,87.106,%
Batch: 120 | Loss: 3.731 | Acc: 44.861,70.015,86.990,%
Batch: 140 | Loss: 3.749 | Acc: 44.670,69.470,86.641,%
Batch: 160 | Loss: 3.773 | Acc: 44.565,69.240,86.583,%
Batch: 180 | Loss: 3.772 | Acc: 44.643,69.126,86.568,%
Batch: 200 | Loss: 3.773 | Acc: 44.718,69.146,86.505,%
Batch: 220 | Loss: 3.776 | Acc: 44.697,69.114,86.337,%
Batch: 240 | Loss: 3.794 | Acc: 44.483,68.964,86.297,%
Batch: 260 | Loss: 3.796 | Acc: 44.618,68.903,86.258,%
Batch: 280 | Loss: 3.808 | Acc: 44.465,68.850,86.204,%
Batch: 300 | Loss: 3.810 | Acc: 44.446,68.937,86.137,%
Batch: 320 | Loss: 3.820 | Acc: 44.470,68.852,86.054,%
Batch: 340 | Loss: 3.821 | Acc: 44.566,68.832,85.917,%
Batch: 360 | Loss: 3.831 | Acc: 44.609,68.830,85.728,%
Batch: 380 | Loss: 3.838 | Acc: 44.644,68.779,85.620,%
Batch: 0 | Loss: 5.213 | Acc: 47.656,61.719,64.844,%
Batch: 20 | Loss: 5.319 | Acc: 38.690,59.189,66.071,%
Batch: 40 | Loss: 5.282 | Acc: 39.310,59.089,65.625,%
Batch: 60 | Loss: 5.285 | Acc: 39.280,58.811,65.459,%
Train all parameters

Epoch: 110
Batch: 0 | Loss: 4.030 | Acc: 48.438,68.750,85.156,%
Batch: 20 | Loss: 3.743 | Acc: 45.015,69.606,87.463,%
Batch: 40 | Loss: 3.666 | Acc: 45.713,70.484,88.205,%
Batch: 60 | Loss: 3.742 | Acc: 44.851,70.197,88.089,%
Batch: 80 | Loss: 3.760 | Acc: 44.203,69.994,87.973,%
Batch: 100 | Loss: 3.734 | Acc: 44.183,70.073,88.111,%
Batch: 120 | Loss: 3.756 | Acc: 44.092,69.628,87.991,%
Batch: 140 | Loss: 3.758 | Acc: 44.293,69.531,87.771,%
Batch: 160 | Loss: 3.758 | Acc: 44.235,69.483,87.490,%
Batch: 180 | Loss: 3.763 | Acc: 44.290,69.458,87.491,%
Batch: 200 | Loss: 3.779 | Acc: 44.263,69.286,87.189,%
Batch: 220 | Loss: 3.793 | Acc: 44.188,69.178,86.931,%
Batch: 240 | Loss: 3.789 | Acc: 44.269,69.184,86.904,%
Batch: 260 | Loss: 3.800 | Acc: 44.298,69.016,86.755,%
Batch: 280 | Loss: 3.797 | Acc: 44.378,68.989,86.674,%
Batch: 300 | Loss: 3.806 | Acc: 44.376,68.973,86.506,%
Batch: 320 | Loss: 3.814 | Acc: 44.427,68.928,86.320,%
Batch: 340 | Loss: 3.822 | Acc: 44.417,68.812,86.187,%
Batch: 360 | Loss: 3.820 | Acc: 44.466,68.839,86.119,%
Batch: 380 | Loss: 3.831 | Acc: 44.353,68.721,85.909,%
Batch: 0 | Loss: 5.142 | Acc: 46.094,62.500,70.312,%
Batch: 20 | Loss: 5.556 | Acc: 36.942,56.994,65.104,%
Batch: 40 | Loss: 5.548 | Acc: 36.909,57.374,64.882,%
Batch: 60 | Loss: 5.553 | Acc: 36.821,57.031,64.434,%
Train all parameters

Epoch: 111
Batch: 0 | Loss: 3.638 | Acc: 42.188,67.969,86.719,%
Batch: 20 | Loss: 3.726 | Acc: 45.052,71.875,87.128,%
Batch: 40 | Loss: 3.698 | Acc: 45.427,71.284,87.252,%
Batch: 60 | Loss: 3.713 | Acc: 44.839,70.671,87.807,%
Batch: 80 | Loss: 3.713 | Acc: 44.676,70.341,87.722,%
Batch: 100 | Loss: 3.710 | Acc: 44.887,70.073,87.856,%
Batch: 120 | Loss: 3.701 | Acc: 44.848,70.099,87.700,%
Batch: 140 | Loss: 3.718 | Acc: 44.709,70.235,87.439,%
Batch: 160 | Loss: 3.728 | Acc: 44.759,70.114,87.253,%
Batch: 180 | Loss: 3.734 | Acc: 44.838,70.058,87.064,%
Batch: 200 | Loss: 3.744 | Acc: 44.768,69.881,86.894,%
Batch: 220 | Loss: 3.754 | Acc: 44.687,69.839,86.768,%
Batch: 240 | Loss: 3.761 | Acc: 44.787,69.794,86.647,%
Batch: 260 | Loss: 3.767 | Acc: 44.606,69.735,86.500,%
Batch: 280 | Loss: 3.774 | Acc: 44.526,69.667,86.516,%
Batch: 300 | Loss: 3.778 | Acc: 44.630,69.747,86.498,%
Batch: 320 | Loss: 3.788 | Acc: 44.534,69.563,86.385,%
Batch: 340 | Loss: 3.791 | Acc: 44.669,69.451,86.279,%
Batch: 360 | Loss: 3.796 | Acc: 44.648,69.425,86.184,%
Batch: 380 | Loss: 3.807 | Acc: 44.593,69.328,86.046,%
Batch: 0 | Loss: 5.289 | Acc: 35.938,60.938,67.188,%
Batch: 20 | Loss: 5.594 | Acc: 35.454,57.180,63.244,%
Batch: 40 | Loss: 5.660 | Acc: 35.556,56.059,62.271,%
Batch: 60 | Loss: 5.656 | Acc: 35.131,55.725,62.500,%
Train all parameters

Epoch: 112
Batch: 0 | Loss: 3.380 | Acc: 43.750,70.312,89.844,%
Batch: 20 | Loss: 3.652 | Acc: 44.568,70.982,87.798,%
Batch: 40 | Loss: 3.645 | Acc: 44.226,71.246,88.434,%
Batch: 60 | Loss: 3.608 | Acc: 45.082,71.094,88.473,%
Batch: 80 | Loss: 3.612 | Acc: 45.361,70.795,88.349,%
Batch: 100 | Loss: 3.627 | Acc: 45.173,70.637,88.188,%
Batch: 120 | Loss: 3.630 | Acc: 45.222,70.461,88.126,%
Batch: 140 | Loss: 3.648 | Acc: 45.146,70.157,88.126,%
Batch: 160 | Loss: 3.646 | Acc: 45.196,70.007,87.898,%
Batch: 180 | Loss: 3.661 | Acc: 45.127,69.993,87.884,%
Batch: 200 | Loss: 3.685 | Acc: 45.048,69.780,87.776,%
Batch: 220 | Loss: 3.706 | Acc: 44.892,69.644,87.613,%
Batch: 240 | Loss: 3.712 | Acc: 44.881,69.554,87.374,%
Batch: 260 | Loss: 3.725 | Acc: 44.765,69.412,87.159,%
Batch: 280 | Loss: 3.728 | Acc: 44.865,69.420,87.091,%
Batch: 300 | Loss: 3.732 | Acc: 44.941,69.485,87.033,%
Batch: 320 | Loss: 3.735 | Acc: 44.950,69.414,86.901,%
Batch: 340 | Loss: 3.740 | Acc: 44.923,69.337,86.771,%
Batch: 360 | Loss: 3.747 | Acc: 44.936,69.269,86.634,%
Batch: 380 | Loss: 3.761 | Acc: 44.865,69.187,86.551,%
Batch: 0 | Loss: 5.191 | Acc: 42.969,61.719,67.969,%
Batch: 20 | Loss: 5.423 | Acc: 37.351,57.812,65.476,%
Batch: 40 | Loss: 5.386 | Acc: 38.300,57.584,65.187,%
Batch: 60 | Loss: 5.407 | Acc: 38.115,57.992,64.613,%
Train all parameters

Epoch: 113
Batch: 0 | Loss: 3.208 | Acc: 48.438,71.875,90.625,%
Batch: 20 | Loss: 3.562 | Acc: 46.763,70.871,88.951,%
Batch: 40 | Loss: 3.717 | Acc: 44.912,69.684,88.510,%
Batch: 60 | Loss: 3.703 | Acc: 45.069,69.723,88.166,%
Batch: 80 | Loss: 3.692 | Acc: 45.149,70.091,88.532,%
Batch: 100 | Loss: 3.703 | Acc: 45.235,70.080,88.188,%
Batch: 120 | Loss: 3.672 | Acc: 45.629,70.229,88.210,%
Batch: 140 | Loss: 3.683 | Acc: 45.529,69.875,87.899,%
Batch: 160 | Loss: 3.676 | Acc: 45.550,70.128,87.859,%
Batch: 180 | Loss: 3.681 | Acc: 45.451,70.127,87.677,%
Batch: 200 | Loss: 3.689 | Acc: 45.538,70.056,87.496,%
Batch: 220 | Loss: 3.681 | Acc: 45.677,70.139,87.479,%
Batch: 240 | Loss: 3.691 | Acc: 45.559,70.024,87.429,%
Batch: 260 | Loss: 3.692 | Acc: 45.504,70.022,87.425,%
Batch: 280 | Loss: 3.697 | Acc: 45.404,69.976,87.419,%
Batch: 300 | Loss: 3.701 | Acc: 45.403,69.840,87.383,%
Batch: 320 | Loss: 3.700 | Acc: 45.427,69.865,87.359,%
Batch: 340 | Loss: 3.710 | Acc: 45.395,69.820,87.253,%
Batch: 360 | Loss: 3.725 | Acc: 45.328,69.678,87.089,%
Batch: 380 | Loss: 3.736 | Acc: 45.308,69.544,86.934,%
Batch: 0 | Loss: 4.838 | Acc: 44.531,64.844,71.094,%
Batch: 20 | Loss: 5.235 | Acc: 40.513,59.710,66.778,%
Batch: 40 | Loss: 5.225 | Acc: 40.739,59.146,65.644,%
Batch: 60 | Loss: 5.245 | Acc: 40.202,59.093,65.548,%
Train all parameters

Epoch: 114
Batch: 0 | Loss: 4.076 | Acc: 44.531,59.375,82.031,%
Batch: 20 | Loss: 3.651 | Acc: 46.280,70.238,88.690,%
Batch: 40 | Loss: 3.613 | Acc: 45.675,70.751,88.720,%
Batch: 60 | Loss: 3.555 | Acc: 46.414,70.812,88.627,%
Batch: 80 | Loss: 3.575 | Acc: 46.373,70.775,88.638,%
Batch: 100 | Loss: 3.612 | Acc: 46.063,70.490,88.274,%
Batch: 120 | Loss: 3.624 | Acc: 45.932,70.364,88.133,%
Batch: 140 | Loss: 3.619 | Acc: 46.227,70.423,88.198,%
Batch: 160 | Loss: 3.638 | Acc: 45.914,70.075,88.097,%
Batch: 180 | Loss: 3.654 | Acc: 45.554,70.079,88.078,%
Batch: 200 | Loss: 3.678 | Acc: 45.309,70.106,87.986,%
Batch: 220 | Loss: 3.685 | Acc: 45.277,70.079,87.846,%
Batch: 240 | Loss: 3.698 | Acc: 45.296,69.982,87.691,%
Batch: 260 | Loss: 3.697 | Acc: 45.274,69.917,87.650,%
Batch: 280 | Loss: 3.697 | Acc: 45.318,69.826,87.620,%
Batch: 300 | Loss: 3.713 | Acc: 45.276,69.638,87.471,%
Batch: 320 | Loss: 3.719 | Acc: 45.239,69.616,87.395,%
Batch: 340 | Loss: 3.722 | Acc: 45.219,69.547,87.308,%
Batch: 360 | Loss: 3.723 | Acc: 45.293,69.488,87.175,%
Batch: 380 | Loss: 3.730 | Acc: 45.233,69.498,87.135,%
Batch: 0 | Loss: 5.033 | Acc: 42.969,61.719,68.750,%
Batch: 20 | Loss: 5.306 | Acc: 39.062,59.226,66.443,%
Batch: 40 | Loss: 5.275 | Acc: 39.977,59.032,66.444,%
Batch: 60 | Loss: 5.284 | Acc: 39.844,58.760,66.329,%
Train all parameters

Epoch: 115
Batch: 0 | Loss: 3.834 | Acc: 36.719,73.438,89.062,%
Batch: 20 | Loss: 3.588 | Acc: 44.903,72.359,89.249,%
Batch: 40 | Loss: 3.593 | Acc: 45.027,71.799,89.329,%
Batch: 60 | Loss: 3.591 | Acc: 45.261,71.760,89.447,%
Batch: 80 | Loss: 3.599 | Acc: 45.042,71.412,89.593,%
Batch: 100 | Loss: 3.587 | Acc: 45.328,70.985,89.403,%
Batch: 120 | Loss: 3.602 | Acc: 45.293,70.913,89.405,%
Batch: 140 | Loss: 3.609 | Acc: 45.545,70.955,89.317,%
Batch: 160 | Loss: 3.610 | Acc: 45.395,70.880,89.213,%
Batch: 180 | Loss: 3.619 | Acc: 45.537,70.895,89.218,%
Batch: 200 | Loss: 3.627 | Acc: 45.600,70.849,89.132,%
Batch: 220 | Loss: 3.628 | Acc: 45.662,70.804,88.995,%
Batch: 240 | Loss: 3.634 | Acc: 45.766,70.559,88.926,%
Batch: 260 | Loss: 3.651 | Acc: 45.687,70.411,88.700,%
Batch: 280 | Loss: 3.664 | Acc: 45.593,70.307,88.451,%
Batch: 300 | Loss: 3.669 | Acc: 45.611,70.245,88.302,%
Batch: 320 | Loss: 3.672 | Acc: 45.573,70.193,88.155,%
Batch: 340 | Loss: 3.670 | Acc: 45.642,70.182,88.077,%
Batch: 360 | Loss: 3.677 | Acc: 45.635,70.111,87.976,%
Batch: 380 | Loss: 3.684 | Acc: 45.577,70.120,87.791,%
Batch: 0 | Loss: 4.856 | Acc: 42.969,63.281,71.094,%
Batch: 20 | Loss: 5.278 | Acc: 38.430,59.375,65.997,%
Batch: 40 | Loss: 5.260 | Acc: 39.234,59.413,65.377,%
Batch: 60 | Loss: 5.263 | Acc: 38.934,59.144,65.817,%
Train all parameters

Epoch: 116
Batch: 0 | Loss: 3.245 | Acc: 52.344,71.094,89.844,%
Batch: 20 | Loss: 3.558 | Acc: 45.089,70.052,88.579,%
Batch: 40 | Loss: 3.585 | Acc: 46.113,70.370,88.243,%
Batch: 60 | Loss: 3.565 | Acc: 45.722,70.466,88.691,%
Batch: 80 | Loss: 3.587 | Acc: 45.409,70.727,88.744,%
Batch: 100 | Loss: 3.557 | Acc: 45.575,70.962,88.923,%
Batch: 120 | Loss: 3.566 | Acc: 45.416,71.010,88.882,%
Batch: 140 | Loss: 3.590 | Acc: 45.213,70.761,88.808,%
Batch: 160 | Loss: 3.608 | Acc: 45.181,70.346,88.699,%
Batch: 180 | Loss: 3.626 | Acc: 45.127,70.256,88.583,%
Batch: 200 | Loss: 3.626 | Acc: 45.223,70.126,88.522,%
Batch: 220 | Loss: 3.627 | Acc: 45.373,69.952,88.444,%
Batch: 240 | Loss: 3.642 | Acc: 45.212,69.820,88.288,%
Batch: 260 | Loss: 3.648 | Acc: 45.393,69.869,88.153,%
Batch: 280 | Loss: 3.654 | Acc: 45.410,69.820,88.059,%
Batch: 300 | Loss: 3.654 | Acc: 45.479,69.835,88.035,%
Batch: 320 | Loss: 3.652 | Acc: 45.578,69.860,88.031,%
Batch: 340 | Loss: 3.666 | Acc: 45.498,69.813,87.901,%
Batch: 360 | Loss: 3.672 | Acc: 45.397,69.787,87.796,%
Batch: 380 | Loss: 3.675 | Acc: 45.364,69.816,87.713,%
Batch: 0 | Loss: 4.828 | Acc: 40.625,67.188,75.000,%
Batch: 20 | Loss: 5.381 | Acc: 37.872,58.594,64.918,%
Batch: 40 | Loss: 5.328 | Acc: 38.243,58.784,65.263,%
Batch: 60 | Loss: 5.301 | Acc: 38.281,58.555,65.804,%
Train all parameters

Epoch: 117
Batch: 0 | Loss: 3.227 | Acc: 47.656,72.656,92.188,%
Batch: 20 | Loss: 3.547 | Acc: 45.610,71.801,89.583,%
Batch: 40 | Loss: 3.539 | Acc: 45.522,72.428,89.920,%
Batch: 60 | Loss: 3.487 | Acc: 46.094,72.067,89.818,%
Batch: 80 | Loss: 3.513 | Acc: 46.152,71.885,89.516,%
Batch: 100 | Loss: 3.535 | Acc: 46.125,71.573,89.333,%
Batch: 120 | Loss: 3.565 | Acc: 46.145,71.210,89.037,%
Batch: 140 | Loss: 3.554 | Acc: 46.304,71.049,88.974,%
Batch: 160 | Loss: 3.587 | Acc: 46.089,70.992,88.791,%
Batch: 180 | Loss: 3.592 | Acc: 46.025,70.951,88.670,%
Batch: 200 | Loss: 3.590 | Acc: 46.121,71.117,88.775,%
Batch: 220 | Loss: 3.601 | Acc: 45.892,71.065,88.716,%
Batch: 240 | Loss: 3.606 | Acc: 45.864,70.932,88.696,%
Batch: 260 | Loss: 3.617 | Acc: 45.690,70.791,88.703,%
Batch: 280 | Loss: 3.622 | Acc: 45.632,70.721,88.598,%
Batch: 300 | Loss: 3.630 | Acc: 45.653,70.595,88.517,%
Batch: 320 | Loss: 3.631 | Acc: 45.724,70.558,88.396,%
Batch: 340 | Loss: 3.639 | Acc: 45.629,70.475,88.332,%
Batch: 360 | Loss: 3.637 | Acc: 45.674,70.351,88.216,%
Batch: 380 | Loss: 3.649 | Acc: 45.604,70.249,88.113,%
Batch: 0 | Loss: 4.601 | Acc: 44.531,68.750,76.562,%
Batch: 20 | Loss: 5.116 | Acc: 40.030,60.640,67.448,%
Batch: 40 | Loss: 5.097 | Acc: 40.587,60.595,66.730,%
Batch: 60 | Loss: 5.097 | Acc: 40.305,60.848,66.547,%
Train all parameters

Epoch: 118
Batch: 0 | Loss: 3.783 | Acc: 39.844,70.312,87.500,%
Batch: 20 | Loss: 3.426 | Acc: 46.094,72.656,90.216,%
Batch: 40 | Loss: 3.569 | Acc: 45.198,71.570,89.901,%
Batch: 60 | Loss: 3.643 | Acc: 44.749,71.145,89.267,%
Batch: 80 | Loss: 3.634 | Acc: 44.705,70.920,89.236,%
Batch: 100 | Loss: 3.637 | Acc: 44.701,70.893,89.248,%
Batch: 120 | Loss: 3.627 | Acc: 44.796,70.810,89.276,%
Batch: 140 | Loss: 3.622 | Acc: 44.803,70.767,89.190,%
Batch: 160 | Loss: 3.623 | Acc: 44.944,70.497,89.072,%
Batch: 180 | Loss: 3.639 | Acc: 44.980,70.312,88.795,%
Batch: 200 | Loss: 3.632 | Acc: 45.180,70.200,88.584,%
Batch: 220 | Loss: 3.643 | Acc: 45.217,70.164,88.490,%
Batch: 240 | Loss: 3.641 | Acc: 45.241,70.231,88.460,%
Batch: 260 | Loss: 3.630 | Acc: 45.471,70.298,88.401,%
Batch: 280 | Loss: 3.636 | Acc: 45.438,70.204,88.315,%
Batch: 300 | Loss: 3.647 | Acc: 45.432,70.100,88.141,%
Batch: 320 | Loss: 3.661 | Acc: 45.351,70.098,88.006,%
Batch: 340 | Loss: 3.664 | Acc: 45.418,70.145,87.947,%
Batch: 360 | Loss: 3.674 | Acc: 45.425,69.999,87.820,%
Batch: 380 | Loss: 3.681 | Acc: 45.362,69.980,87.736,%
Batch: 0 | Loss: 5.084 | Acc: 41.406,61.719,67.969,%
Batch: 20 | Loss: 5.630 | Acc: 35.528,56.362,63.430,%
Batch: 40 | Loss: 5.642 | Acc: 36.509,55.393,63.281,%
Batch: 60 | Loss: 5.647 | Acc: 36.309,55.418,63.064,%
Train all parameters

Epoch: 119
Batch: 0 | Loss: 3.476 | Acc: 43.750,75.000,92.188,%
Batch: 20 | Loss: 3.653 | Acc: 43.973,71.466,88.616,%
Batch: 40 | Loss: 3.584 | Acc: 45.389,71.075,89.082,%
Batch: 60 | Loss: 3.556 | Acc: 45.697,71.030,89.331,%
Batch: 80 | Loss: 3.592 | Acc: 45.640,70.264,89.429,%
Batch: 100 | Loss: 3.599 | Acc: 45.498,70.328,89.542,%
Batch: 120 | Loss: 3.628 | Acc: 45.274,70.222,89.392,%
Batch: 140 | Loss: 3.635 | Acc: 45.506,70.152,89.384,%
Batch: 160 | Loss: 3.620 | Acc: 45.638,70.283,89.422,%
Batch: 180 | Loss: 3.604 | Acc: 45.779,70.464,89.481,%
Batch: 200 | Loss: 3.611 | Acc: 45.767,70.402,89.331,%
Batch: 220 | Loss: 3.610 | Acc: 45.641,70.457,89.285,%
Batch: 240 | Loss: 3.628 | Acc: 45.504,70.300,89.160,%
Batch: 260 | Loss: 3.626 | Acc: 45.537,70.360,89.092,%
Batch: 280 | Loss: 3.628 | Acc: 45.529,70.257,88.979,%
Batch: 300 | Loss: 3.646 | Acc: 45.468,70.141,88.782,%
Batch: 320 | Loss: 3.652 | Acc: 45.405,70.147,88.751,%
Batch: 340 | Loss: 3.656 | Acc: 45.482,70.214,88.723,%
Batch: 360 | Loss: 3.664 | Acc: 45.406,70.235,88.625,%
Batch: 380 | Loss: 3.663 | Acc: 45.433,70.189,88.585,%
Batch: 0 | Loss: 5.001 | Acc: 42.188,66.406,64.844,%
Batch: 20 | Loss: 5.284 | Acc: 39.323,59.449,66.406,%
Batch: 40 | Loss: 5.248 | Acc: 40.034,59.223,65.816,%
Batch: 60 | Loss: 5.251 | Acc: 39.882,59.349,65.907,%
Train all parameters

Epoch: 120
Batch: 0 | Loss: 3.130 | Acc: 57.031,75.781,94.531,%
Batch: 20 | Loss: 3.776 | Acc: 43.676,70.238,89.174,%
Batch: 40 | Loss: 3.680 | Acc: 44.112,70.808,89.253,%
Batch: 60 | Loss: 3.646 | Acc: 44.416,70.850,89.664,%
Batch: 80 | Loss: 3.627 | Acc: 44.676,70.949,89.728,%
Batch: 100 | Loss: 3.630 | Acc: 45.127,71.194,89.604,%
Batch: 120 | Loss: 3.600 | Acc: 45.455,71.429,89.553,%
Batch: 140 | Loss: 3.586 | Acc: 45.473,71.415,89.545,%
Batch: 160 | Loss: 3.587 | Acc: 45.502,71.210,89.688,%
Batch: 180 | Loss: 3.585 | Acc: 45.589,71.085,89.602,%
Batch: 200 | Loss: 3.592 | Acc: 45.437,70.892,89.525,%
Batch: 220 | Loss: 3.600 | Acc: 45.401,70.793,89.367,%
Batch: 240 | Loss: 3.611 | Acc: 45.449,70.708,89.221,%
Batch: 260 | Loss: 3.620 | Acc: 45.432,70.492,89.065,%
Batch: 280 | Loss: 3.612 | Acc: 45.616,70.507,89.001,%
Batch: 300 | Loss: 3.622 | Acc: 45.572,70.463,88.868,%
Batch: 320 | Loss: 3.631 | Acc: 45.561,70.320,88.746,%
Batch: 340 | Loss: 3.631 | Acc: 45.636,70.290,88.682,%
Batch: 360 | Loss: 3.630 | Acc: 45.702,70.282,88.658,%
Batch: 380 | Loss: 3.636 | Acc: 45.714,70.159,88.536,%
Batch: 0 | Loss: 4.786 | Acc: 46.875,67.188,70.312,%
Batch: 20 | Loss: 5.412 | Acc: 37.984,58.259,65.365,%
Batch: 40 | Loss: 5.350 | Acc: 38.948,58.479,65.396,%
Batch: 60 | Loss: 5.371 | Acc: 38.768,58.376,64.946,%
Train all parameters

Epoch: 121
Batch: 0 | Loss: 2.987 | Acc: 50.781,75.781,89.062,%
Batch: 20 | Loss: 3.566 | Acc: 45.052,70.982,89.323,%
Batch: 40 | Loss: 3.499 | Acc: 46.780,70.922,89.405,%
Batch: 60 | Loss: 3.493 | Acc: 47.195,71.209,89.921,%
Batch: 80 | Loss: 3.485 | Acc: 47.000,71.277,90.220,%
Batch: 100 | Loss: 3.496 | Acc: 46.945,71.040,90.331,%
Batch: 120 | Loss: 3.520 | Acc: 46.655,70.810,90.102,%
Batch: 140 | Loss: 3.528 | Acc: 46.642,70.855,89.982,%
Batch: 160 | Loss: 3.523 | Acc: 46.797,70.735,89.781,%
Batch: 180 | Loss: 3.544 | Acc: 46.741,70.761,89.628,%
Batch: 200 | Loss: 3.558 | Acc: 46.650,70.713,89.463,%
Batch: 220 | Loss: 3.564 | Acc: 46.444,70.599,89.388,%
Batch: 240 | Loss: 3.568 | Acc: 46.288,70.520,89.367,%
Batch: 260 | Loss: 3.581 | Acc: 46.216,70.492,89.212,%
Batch: 280 | Loss: 3.580 | Acc: 46.166,70.540,89.202,%
Batch: 300 | Loss: 3.578 | Acc: 46.252,70.502,89.114,%
Batch: 320 | Loss: 3.590 | Acc: 46.174,70.507,88.953,%
Batch: 340 | Loss: 3.593 | Acc: 46.227,70.448,88.859,%
Batch: 360 | Loss: 3.597 | Acc: 46.187,70.440,88.753,%
Batch: 380 | Loss: 3.610 | Acc: 46.112,70.390,88.689,%
Batch: 0 | Loss: 5.112 | Acc: 43.750,59.375,68.750,%
Batch: 20 | Loss: 5.403 | Acc: 38.132,58.557,65.402,%
Batch: 40 | Loss: 5.378 | Acc: 38.929,58.136,64.234,%
Batch: 60 | Loss: 5.390 | Acc: 38.768,58.261,63.870,%
Train all parameters

Epoch: 122
Batch: 0 | Loss: 3.397 | Acc: 46.875,68.750,92.188,%
Batch: 20 | Loss: 3.478 | Acc: 47.396,71.763,89.807,%
Batch: 40 | Loss: 3.476 | Acc: 47.694,71.894,89.539,%
Batch: 60 | Loss: 3.492 | Acc: 47.477,71.837,89.408,%
Batch: 80 | Loss: 3.508 | Acc: 47.145,71.615,89.419,%
Batch: 100 | Loss: 3.520 | Acc: 46.782,71.860,89.465,%
Batch: 120 | Loss: 3.523 | Acc: 46.559,71.572,89.540,%
Batch: 140 | Loss: 3.529 | Acc: 46.515,71.382,89.522,%
Batch: 160 | Loss: 3.535 | Acc: 46.361,71.336,89.446,%
Batch: 180 | Loss: 3.551 | Acc: 46.241,71.029,89.326,%
Batch: 200 | Loss: 3.559 | Acc: 46.168,70.794,89.156,%
Batch: 220 | Loss: 3.570 | Acc: 45.977,70.684,89.073,%
Batch: 240 | Loss: 3.581 | Acc: 46.016,70.770,88.956,%
Batch: 260 | Loss: 3.591 | Acc: 46.079,70.756,88.832,%
Batch: 280 | Loss: 3.598 | Acc: 46.016,70.693,88.718,%
Batch: 300 | Loss: 3.603 | Acc: 45.922,70.642,88.543,%
Batch: 320 | Loss: 3.610 | Acc: 45.899,70.507,88.478,%
Batch: 340 | Loss: 3.615 | Acc: 45.821,70.450,88.439,%
Batch: 360 | Loss: 3.616 | Acc: 45.942,70.436,88.370,%
Batch: 380 | Loss: 3.627 | Acc: 45.889,70.425,88.236,%
Batch: 0 | Loss: 4.910 | Acc: 47.656,63.281,72.656,%
Batch: 20 | Loss: 5.292 | Acc: 39.769,59.970,65.625,%
Batch: 40 | Loss: 5.254 | Acc: 40.206,60.309,65.987,%
Batch: 60 | Loss: 5.281 | Acc: 39.767,60.015,65.484,%
Train all parameters

Epoch: 123
Batch: 0 | Loss: 4.005 | Acc: 45.312,65.625,91.406,%
Batch: 20 | Loss: 3.551 | Acc: 46.577,71.577,90.848,%
Batch: 40 | Loss: 3.504 | Acc: 46.341,71.951,90.796,%
Batch: 60 | Loss: 3.553 | Acc: 45.684,71.299,90.740,%
Batch: 80 | Loss: 3.576 | Acc: 45.775,71.238,90.413,%
Batch: 100 | Loss: 3.554 | Acc: 45.931,71.573,90.339,%
Batch: 120 | Loss: 3.552 | Acc: 45.835,71.513,90.218,%
Batch: 140 | Loss: 3.551 | Acc: 45.800,71.332,90.176,%
Batch: 160 | Loss: 3.561 | Acc: 45.749,71.336,90.033,%
Batch: 180 | Loss: 3.573 | Acc: 45.731,71.098,89.900,%
Batch: 200 | Loss: 3.579 | Acc: 45.794,71.214,89.665,%
Batch: 220 | Loss: 3.577 | Acc: 45.832,71.073,89.420,%
Batch: 240 | Loss: 3.586 | Acc: 45.873,71.068,89.348,%
Batch: 260 | Loss: 3.605 | Acc: 45.735,70.833,89.212,%
Batch: 280 | Loss: 3.611 | Acc: 45.755,70.788,88.979,%
Batch: 300 | Loss: 3.612 | Acc: 45.795,70.590,88.837,%
Batch: 320 | Loss: 3.616 | Acc: 45.807,70.575,88.729,%
Batch: 340 | Loss: 3.638 | Acc: 45.617,70.402,88.602,%
Batch: 360 | Loss: 3.635 | Acc: 45.622,70.464,88.589,%
Batch: 380 | Loss: 3.635 | Acc: 45.645,70.466,88.517,%
Batch: 0 | Loss: 4.833 | Acc: 43.750,67.188,71.094,%
Batch: 20 | Loss: 5.172 | Acc: 40.885,60.119,66.741,%
Batch: 40 | Loss: 5.146 | Acc: 41.845,59.947,66.139,%
Batch: 60 | Loss: 5.163 | Acc: 41.227,60.156,66.201,%
Train all parameters

Epoch: 124
Batch: 0 | Loss: 2.558 | Acc: 51.562,75.781,95.312,%
Batch: 20 | Loss: 3.497 | Acc: 45.387,72.321,90.253,%
Batch: 40 | Loss: 3.564 | Acc: 45.370,72.027,90.816,%
Batch: 60 | Loss: 3.527 | Acc: 45.556,72.362,91.189,%
Batch: 80 | Loss: 3.496 | Acc: 46.036,72.733,91.175,%
Batch: 100 | Loss: 3.478 | Acc: 46.272,72.416,91.097,%
Batch: 120 | Loss: 3.468 | Acc: 46.597,72.501,90.935,%
Batch: 140 | Loss: 3.487 | Acc: 46.504,72.224,90.581,%
Batch: 160 | Loss: 3.485 | Acc: 46.424,71.972,90.426,%
Batch: 180 | Loss: 3.495 | Acc: 46.305,71.784,90.314,%
Batch: 200 | Loss: 3.499 | Acc: 46.187,71.739,90.248,%
Batch: 220 | Loss: 3.524 | Acc: 45.906,71.666,90.013,%
Batch: 240 | Loss: 3.519 | Acc: 46.074,71.651,89.889,%
Batch: 260 | Loss: 3.524 | Acc: 46.064,71.543,89.742,%
Batch: 280 | Loss: 3.533 | Acc: 46.088,71.425,89.646,%
Batch: 300 | Loss: 3.547 | Acc: 46.008,71.252,89.465,%
Batch: 320 | Loss: 3.565 | Acc: 45.836,71.057,89.306,%
Batch: 340 | Loss: 3.567 | Acc: 45.940,71.023,89.161,%
Batch: 360 | Loss: 3.579 | Acc: 45.890,70.929,88.993,%
Batch: 380 | Loss: 3.588 | Acc: 45.880,70.891,88.888,%
Batch: 0 | Loss: 5.339 | Acc: 42.969,63.281,64.844,%
Batch: 20 | Loss: 5.263 | Acc: 38.207,60.603,66.890,%
Batch: 40 | Loss: 5.273 | Acc: 39.043,59.928,66.120,%
Batch: 60 | Loss: 5.300 | Acc: 38.653,59.516,65.561,%
Train all parameters

Epoch: 125
Batch: 0 | Loss: 3.290 | Acc: 47.656,76.562,92.969,%
Batch: 20 | Loss: 3.573 | Acc: 46.466,72.098,89.807,%
Batch: 40 | Loss: 3.498 | Acc: 46.780,71.341,90.415,%
Batch: 60 | Loss: 3.456 | Acc: 46.913,71.849,90.497,%
Batch: 80 | Loss: 3.465 | Acc: 47.116,71.711,90.133,%
Batch: 100 | Loss: 3.472 | Acc: 47.037,71.798,90.138,%
Batch: 120 | Loss: 3.465 | Acc: 47.166,71.552,90.096,%
Batch: 140 | Loss: 3.481 | Acc: 46.903,71.570,90.076,%
Batch: 160 | Loss: 3.484 | Acc: 46.846,71.506,89.931,%
Batch: 180 | Loss: 3.499 | Acc: 46.840,71.284,89.723,%
Batch: 200 | Loss: 3.510 | Acc: 46.774,71.230,89.599,%
Batch: 220 | Loss: 3.526 | Acc: 46.737,71.196,89.589,%
Batch: 240 | Loss: 3.541 | Acc: 46.629,71.178,89.481,%
Batch: 260 | Loss: 3.542 | Acc: 46.630,71.103,89.401,%
Batch: 280 | Loss: 3.558 | Acc: 46.536,70.946,89.149,%
Batch: 300 | Loss: 3.564 | Acc: 46.496,70.909,89.042,%
Batch: 320 | Loss: 3.569 | Acc: 46.573,70.772,88.909,%
Batch: 340 | Loss: 3.575 | Acc: 46.598,70.681,88.788,%
Batch: 360 | Loss: 3.589 | Acc: 46.516,70.559,88.645,%
Batch: 380 | Loss: 3.598 | Acc: 46.350,70.499,88.544,%
Batch: 0 | Loss: 4.730 | Acc: 48.438,67.969,73.438,%
Batch: 20 | Loss: 5.161 | Acc: 40.030,62.016,67.225,%
Batch: 40 | Loss: 5.120 | Acc: 41.101,61.833,67.016,%
Batch: 60 | Loss: 5.151 | Acc: 40.830,61.373,66.573,%
Train all parameters

Epoch: 126
Batch: 0 | Loss: 3.397 | Acc: 40.625,78.906,92.188,%
Batch: 20 | Loss: 3.389 | Acc: 47.842,73.177,90.774,%
Batch: 40 | Loss: 3.413 | Acc: 47.046,72.161,90.835,%
Batch: 60 | Loss: 3.451 | Acc: 46.619,72.605,90.817,%
Batch: 80 | Loss: 3.427 | Acc: 46.750,72.454,90.770,%
Batch: 100 | Loss: 3.463 | Acc: 46.117,72.393,90.733,%
Batch: 120 | Loss: 3.481 | Acc: 46.397,71.940,90.715,%
Batch: 140 | Loss: 3.478 | Acc: 46.504,71.781,90.741,%
Batch: 160 | Loss: 3.481 | Acc: 46.414,71.700,90.780,%
Batch: 180 | Loss: 3.495 | Acc: 46.297,71.361,90.634,%
Batch: 200 | Loss: 3.510 | Acc: 46.156,71.401,90.528,%
Batch: 220 | Loss: 3.516 | Acc: 46.083,71.359,90.406,%
Batch: 240 | Loss: 3.517 | Acc: 46.220,71.340,90.268,%
Batch: 260 | Loss: 3.524 | Acc: 46.327,71.151,90.122,%
Batch: 280 | Loss: 3.535 | Acc: 46.291,71.080,89.947,%
Batch: 300 | Loss: 3.557 | Acc: 46.089,70.928,89.800,%
Batch: 320 | Loss: 3.557 | Acc: 46.167,70.999,89.693,%
Batch: 340 | Loss: 3.562 | Acc: 46.179,70.977,89.626,%
Batch: 360 | Loss: 3.572 | Acc: 46.083,70.888,89.532,%
Batch: 380 | Loss: 3.578 | Acc: 46.083,70.862,89.493,%
Batch: 0 | Loss: 4.749 | Acc: 44.531,61.719,70.312,%
Batch: 20 | Loss: 5.253 | Acc: 39.621,58.854,65.960,%
Batch: 40 | Loss: 5.229 | Acc: 40.244,59.051,65.358,%
Batch: 60 | Loss: 5.241 | Acc: 40.266,59.170,65.318,%
Train all parameters

Epoch: 127
Batch: 0 | Loss: 3.844 | Acc: 42.969,67.188,89.062,%
Batch: 20 | Loss: 3.582 | Acc: 45.945,71.429,91.443,%
Batch: 40 | Loss: 3.569 | Acc: 46.151,71.646,91.311,%
Batch: 60 | Loss: 3.539 | Acc: 46.709,71.491,91.009,%
Batch: 80 | Loss: 3.533 | Acc: 46.470,71.451,90.953,%
Batch: 100 | Loss: 3.515 | Acc: 46.651,71.511,90.834,%
Batch: 120 | Loss: 3.492 | Acc: 46.894,71.610,90.735,%
Batch: 140 | Loss: 3.497 | Acc: 46.792,71.454,90.509,%
Batch: 160 | Loss: 3.522 | Acc: 46.385,70.997,90.280,%
Batch: 180 | Loss: 3.511 | Acc: 46.564,71.094,90.129,%
Batch: 200 | Loss: 3.503 | Acc: 46.673,71.148,90.291,%
Batch: 220 | Loss: 3.512 | Acc: 46.589,70.984,90.279,%
Batch: 240 | Loss: 3.519 | Acc: 46.473,70.951,90.285,%
Batch: 260 | Loss: 3.524 | Acc: 46.375,70.905,90.239,%
Batch: 280 | Loss: 3.534 | Acc: 46.419,70.802,90.116,%
Batch: 300 | Loss: 3.548 | Acc: 46.257,70.723,89.979,%
Batch: 320 | Loss: 3.558 | Acc: 46.157,70.682,89.841,%
Batch: 340 | Loss: 3.556 | Acc: 46.183,70.720,89.796,%
Batch: 360 | Loss: 3.563 | Acc: 46.146,70.609,89.692,%
Batch: 380 | Loss: 3.577 | Acc: 46.040,70.507,89.497,%
Batch: 0 | Loss: 4.787 | Acc: 46.094,64.062,67.969,%
Batch: 20 | Loss: 5.546 | Acc: 37.277,56.027,63.802,%
Batch: 40 | Loss: 5.498 | Acc: 38.300,57.050,63.700,%
Batch: 60 | Loss: 5.494 | Acc: 37.833,56.929,63.819,%
Train all parameters

Epoch: 128
Batch: 0 | Loss: 2.586 | Acc: 57.031,80.469,90.625,%
Batch: 20 | Loss: 3.572 | Acc: 44.643,71.652,89.323,%
Batch: 40 | Loss: 3.506 | Acc: 45.484,72.313,89.748,%
Batch: 60 | Loss: 3.499 | Acc: 45.607,71.696,89.728,%
Batch: 80 | Loss: 3.495 | Acc: 45.457,71.489,89.844,%
Batch: 100 | Loss: 3.524 | Acc: 45.475,71.457,89.797,%
Batch: 120 | Loss: 3.521 | Acc: 45.745,71.378,89.818,%
Batch: 140 | Loss: 3.520 | Acc: 45.750,71.520,89.716,%
Batch: 160 | Loss: 3.545 | Acc: 45.570,71.327,89.655,%
Batch: 180 | Loss: 3.529 | Acc: 45.930,71.396,89.788,%
Batch: 200 | Loss: 3.539 | Acc: 45.849,71.343,89.743,%
Batch: 220 | Loss: 3.533 | Acc: 45.906,71.225,89.699,%
Batch: 240 | Loss: 3.535 | Acc: 45.818,71.188,89.740,%
Batch: 260 | Loss: 3.535 | Acc: 45.812,71.118,89.799,%
Batch: 280 | Loss: 3.533 | Acc: 45.971,71.135,89.710,%
Batch: 300 | Loss: 3.547 | Acc: 45.938,71.120,89.675,%
Batch: 320 | Loss: 3.546 | Acc: 46.013,71.052,89.617,%
Batch: 340 | Loss: 3.553 | Acc: 45.988,70.991,89.569,%
Batch: 360 | Loss: 3.557 | Acc: 45.947,70.940,89.502,%
Batch: 380 | Loss: 3.564 | Acc: 45.956,70.917,89.415,%
Batch: 0 | Loss: 4.765 | Acc: 50.000,67.969,68.750,%
Batch: 20 | Loss: 5.388 | Acc: 37.872,58.594,65.662,%
Batch: 40 | Loss: 5.375 | Acc: 38.700,58.041,64.748,%
Batch: 60 | Loss: 5.379 | Acc: 38.371,58.043,65.010,%
Train all parameters

Epoch: 129
Batch: 0 | Loss: 3.407 | Acc: 45.312,74.219,89.844,%
Batch: 20 | Loss: 3.563 | Acc: 45.536,70.089,90.662,%
Batch: 40 | Loss: 3.507 | Acc: 46.227,70.179,90.835,%
Batch: 60 | Loss: 3.522 | Acc: 46.183,70.774,90.868,%
Batch: 80 | Loss: 3.487 | Acc: 46.672,70.631,90.924,%
Batch: 100 | Loss: 3.509 | Acc: 46.535,70.645,90.625,%
Batch: 120 | Loss: 3.508 | Acc: 46.584,70.739,90.741,%
Batch: 140 | Loss: 3.510 | Acc: 46.676,70.839,90.730,%
Batch: 160 | Loss: 3.519 | Acc: 46.628,70.783,90.470,%
Batch: 180 | Loss: 3.514 | Acc: 46.620,70.925,90.314,%
Batch: 200 | Loss: 3.511 | Acc: 46.758,70.954,90.155,%
Batch: 220 | Loss: 3.530 | Acc: 46.645,70.733,89.975,%
Batch: 240 | Loss: 3.540 | Acc: 46.586,70.682,89.802,%
Batch: 260 | Loss: 3.551 | Acc: 46.495,70.594,89.616,%
Batch: 280 | Loss: 3.560 | Acc: 46.377,70.652,89.538,%
Batch: 300 | Loss: 3.564 | Acc: 46.423,70.606,89.384,%
Batch: 320 | Loss: 3.572 | Acc: 46.461,70.617,89.230,%
Batch: 340 | Loss: 3.573 | Acc: 46.534,70.624,89.138,%
Batch: 360 | Loss: 3.586 | Acc: 46.369,70.470,89.062,%
Batch: 380 | Loss: 3.584 | Acc: 46.346,70.487,88.983,%
Batch: 0 | Loss: 5.160 | Acc: 42.969,61.719,64.062,%
Batch: 20 | Loss: 5.305 | Acc: 38.244,59.710,65.439,%
Batch: 40 | Loss: 5.236 | Acc: 39.482,59.851,65.377,%
Batch: 60 | Loss: 5.270 | Acc: 39.178,59.388,65.215,%
Train all parameters

Epoch: 130
Batch: 0 | Loss: 3.020 | Acc: 53.906,73.438,93.750,%
Batch: 20 | Loss: 3.386 | Acc: 47.433,74.479,91.443,%
Batch: 40 | Loss: 3.421 | Acc: 46.456,73.571,91.292,%
Batch: 60 | Loss: 3.443 | Acc: 45.774,73.322,91.470,%
Batch: 80 | Loss: 3.454 | Acc: 45.920,73.110,91.358,%
Batch: 100 | Loss: 3.481 | Acc: 46.132,72.463,91.027,%
Batch: 120 | Loss: 3.477 | Acc: 46.281,72.230,90.890,%
Batch: 140 | Loss: 3.478 | Acc: 46.382,71.980,90.691,%
Batch: 160 | Loss: 3.497 | Acc: 46.239,71.793,90.625,%
Batch: 180 | Loss: 3.492 | Acc: 46.297,71.780,90.573,%
Batch: 200 | Loss: 3.486 | Acc: 46.385,71.653,90.376,%
Batch: 220 | Loss: 3.495 | Acc: 46.444,71.539,90.056,%
Batch: 240 | Loss: 3.499 | Acc: 46.476,71.424,89.886,%
Batch: 260 | Loss: 3.504 | Acc: 46.456,71.390,89.748,%
Batch: 280 | Loss: 3.509 | Acc: 46.394,71.347,89.641,%
Batch: 300 | Loss: 3.519 | Acc: 46.346,71.224,89.486,%
Batch: 320 | Loss: 3.529 | Acc: 46.376,71.101,89.367,%
Batch: 340 | Loss: 3.539 | Acc: 46.284,71.011,89.374,%
Batch: 360 | Loss: 3.551 | Acc: 46.180,70.938,89.307,%
Batch: 380 | Loss: 3.553 | Acc: 46.192,70.870,89.222,%
Batch: 0 | Loss: 4.808 | Acc: 50.000,64.844,68.750,%
Batch: 20 | Loss: 5.113 | Acc: 40.625,61.756,67.485,%
Batch: 40 | Loss: 5.114 | Acc: 40.968,61.147,66.997,%
Batch: 60 | Loss: 5.131 | Acc: 40.907,60.912,66.726,%
Train all parameters

Epoch: 131
Batch: 0 | Loss: 3.060 | Acc: 47.656,71.094,94.531,%
Batch: 20 | Loss: 3.338 | Acc: 47.507,73.289,91.964,%
Batch: 40 | Loss: 3.425 | Acc: 46.341,72.618,91.387,%
Batch: 60 | Loss: 3.446 | Acc: 46.145,72.259,90.958,%
Batch: 80 | Loss: 3.481 | Acc: 46.190,72.068,90.789,%
Batch: 100 | Loss: 3.488 | Acc: 45.978,71.906,90.989,%
Batch: 120 | Loss: 3.493 | Acc: 46.081,71.714,90.999,%
Batch: 140 | Loss: 3.485 | Acc: 46.193,71.637,90.891,%
Batch: 160 | Loss: 3.487 | Acc: 46.074,71.623,90.800,%
Batch: 180 | Loss: 3.487 | Acc: 46.197,71.474,90.707,%
Batch: 200 | Loss: 3.485 | Acc: 46.276,71.533,90.668,%
Batch: 220 | Loss: 3.494 | Acc: 46.341,71.440,90.469,%
Batch: 240 | Loss: 3.500 | Acc: 46.253,71.398,90.372,%
Batch: 260 | Loss: 3.510 | Acc: 46.139,71.309,90.182,%
Batch: 280 | Loss: 3.522 | Acc: 46.099,71.149,89.974,%
Batch: 300 | Loss: 3.522 | Acc: 46.208,71.239,89.901,%
Batch: 320 | Loss: 3.533 | Acc: 46.196,71.079,89.788,%
Batch: 340 | Loss: 3.537 | Acc: 46.114,71.000,89.752,%
Batch: 360 | Loss: 3.533 | Acc: 46.128,71.037,89.762,%
Batch: 380 | Loss: 3.532 | Acc: 46.202,71.049,89.682,%
Batch: 0 | Loss: 4.533 | Acc: 46.875,67.188,70.312,%
Batch: 20 | Loss: 5.279 | Acc: 39.844,60.305,66.555,%
Batch: 40 | Loss: 5.241 | Acc: 40.282,59.356,66.273,%
Batch: 60 | Loss: 5.270 | Acc: 39.575,59.132,65.894,%
Train all parameters

Epoch: 132
Batch: 0 | Loss: 3.839 | Acc: 41.406,73.438,86.719,%
Batch: 20 | Loss: 3.490 | Acc: 46.615,70.759,90.253,%
Batch: 40 | Loss: 3.484 | Acc: 45.713,71.437,90.835,%
Batch: 60 | Loss: 3.495 | Acc: 45.914,71.619,91.137,%
Batch: 80 | Loss: 3.499 | Acc: 46.229,71.296,91.040,%
Batch: 100 | Loss: 3.490 | Acc: 46.341,71.210,90.973,%
Batch: 120 | Loss: 3.470 | Acc: 46.333,71.371,91.032,%
Batch: 140 | Loss: 3.467 | Acc: 46.398,71.493,91.041,%
Batch: 160 | Loss: 3.466 | Acc: 46.487,71.632,90.848,%
Batch: 180 | Loss: 3.469 | Acc: 46.361,71.551,90.703,%
Batch: 200 | Loss: 3.474 | Acc: 46.358,71.424,90.497,%
Batch: 220 | Loss: 3.481 | Acc: 46.338,71.313,90.335,%
Batch: 240 | Loss: 3.487 | Acc: 46.480,71.337,90.210,%
Batch: 260 | Loss: 3.504 | Acc: 46.315,71.210,90.053,%
Batch: 280 | Loss: 3.515 | Acc: 46.191,71.152,89.888,%
Batch: 300 | Loss: 3.525 | Acc: 46.203,71.127,89.763,%
Batch: 320 | Loss: 3.532 | Acc: 46.220,71.065,89.661,%
Batch: 340 | Loss: 3.540 | Acc: 46.185,71.057,89.546,%
Batch: 360 | Loss: 3.540 | Acc: 46.163,71.012,89.480,%
Batch: 380 | Loss: 3.551 | Acc: 46.133,70.919,89.386,%
Batch: 0 | Loss: 4.709 | Acc: 46.094,67.188,75.000,%
Batch: 20 | Loss: 5.213 | Acc: 39.844,59.449,67.001,%
Batch: 40 | Loss: 5.202 | Acc: 40.511,59.451,66.101,%
Batch: 60 | Loss: 5.212 | Acc: 40.241,59.503,66.214,%
Train all parameters

Epoch: 133
Batch: 0 | Loss: 3.532 | Acc: 48.438,75.781,91.406,%
Batch: 20 | Loss: 3.294 | Acc: 47.284,72.247,91.406,%
Batch: 40 | Loss: 3.388 | Acc: 47.027,71.799,90.930,%
Batch: 60 | Loss: 3.394 | Acc: 47.067,71.811,91.086,%
Batch: 80 | Loss: 3.423 | Acc: 46.557,71.865,90.914,%
Batch: 100 | Loss: 3.448 | Acc: 46.318,71.759,90.903,%
Batch: 120 | Loss: 3.447 | Acc: 46.358,71.914,90.806,%
Batch: 140 | Loss: 3.458 | Acc: 46.072,71.953,90.691,%
Batch: 160 | Loss: 3.472 | Acc: 46.040,72.045,90.727,%
Batch: 180 | Loss: 3.487 | Acc: 46.007,71.897,90.638,%
Batch: 200 | Loss: 3.491 | Acc: 46.074,71.929,90.578,%
Batch: 220 | Loss: 3.492 | Acc: 46.316,71.790,90.466,%
Batch: 240 | Loss: 3.506 | Acc: 46.204,71.671,90.314,%
Batch: 260 | Loss: 3.510 | Acc: 46.202,71.645,90.251,%
Batch: 280 | Loss: 3.510 | Acc: 46.258,71.653,90.175,%
Batch: 300 | Loss: 3.518 | Acc: 46.356,71.512,90.051,%
Batch: 320 | Loss: 3.519 | Acc: 46.379,71.398,89.946,%
Batch: 340 | Loss: 3.523 | Acc: 46.332,71.318,89.899,%
Batch: 360 | Loss: 3.525 | Acc: 46.423,71.245,89.757,%
Batch: 380 | Loss: 3.534 | Acc: 46.496,71.233,89.653,%
Batch: 0 | Loss: 4.844 | Acc: 46.094,69.531,69.531,%
Batch: 20 | Loss: 5.229 | Acc: 39.807,60.565,65.290,%
Batch: 40 | Loss: 5.207 | Acc: 40.473,60.213,65.244,%
Batch: 60 | Loss: 5.221 | Acc: 40.061,60.028,65.087,%
Train all parameters

Epoch: 134
Batch: 0 | Loss: 3.546 | Acc: 43.750,62.500,88.281,%
Batch: 20 | Loss: 3.398 | Acc: 46.577,71.763,91.183,%
Batch: 40 | Loss: 3.350 | Acc: 46.551,72.294,91.902,%
Batch: 60 | Loss: 3.402 | Acc: 46.555,72.041,91.752,%
Batch: 80 | Loss: 3.406 | Acc: 46.508,72.309,91.696,%
Batch: 100 | Loss: 3.420 | Acc: 46.457,72.300,91.491,%
Batch: 120 | Loss: 3.419 | Acc: 46.494,72.372,91.193,%
Batch: 140 | Loss: 3.444 | Acc: 46.426,72.185,91.018,%
Batch: 160 | Loss: 3.445 | Acc: 46.467,72.059,90.916,%
Batch: 180 | Loss: 3.463 | Acc: 46.314,71.879,90.660,%
Batch: 200 | Loss: 3.470 | Acc: 46.436,71.875,90.532,%
Batch: 220 | Loss: 3.472 | Acc: 46.419,71.907,90.469,%
Batch: 240 | Loss: 3.486 | Acc: 46.272,71.726,90.291,%
Batch: 260 | Loss: 3.497 | Acc: 46.237,71.651,90.116,%
Batch: 280 | Loss: 3.510 | Acc: 46.333,71.569,89.913,%
Batch: 300 | Loss: 3.512 | Acc: 46.442,71.478,89.792,%
Batch: 320 | Loss: 3.525 | Acc: 46.327,71.308,89.576,%
Batch: 340 | Loss: 3.526 | Acc: 46.334,71.213,89.402,%
Batch: 360 | Loss: 3.540 | Acc: 46.254,71.122,89.255,%
Batch: 380 | Loss: 3.550 | Acc: 46.284,70.997,89.126,%
Batch: 0 | Loss: 5.323 | Acc: 46.094,62.500,69.531,%
Batch: 20 | Loss: 5.290 | Acc: 40.216,60.045,65.290,%
Batch: 40 | Loss: 5.244 | Acc: 40.644,59.813,65.034,%
Batch: 60 | Loss: 5.268 | Acc: 40.074,59.631,64.716,%
Train classifier parameters

Epoch: 135
Batch: 0 | Loss: 3.597 | Acc: 46.094,74.219,89.844,%
Batch: 20 | Loss: 3.907 | Acc: 42.820,68.415,85.156,%
Batch: 40 | Loss: 4.122 | Acc: 42.283,67.054,83.289,%
Batch: 60 | Loss: 4.169 | Acc: 42.380,67.290,82.556,%
Batch: 80 | Loss: 4.165 | Acc: 42.245,67.014,82.321,%
Batch: 100 | Loss: 4.185 | Acc: 42.334,67.249,82.441,%
Batch: 120 | Loss: 4.172 | Acc: 42.233,67.342,82.348,%
Batch: 140 | Loss: 4.148 | Acc: 42.387,67.276,82.602,%
Batch: 160 | Loss: 4.148 | Acc: 42.445,67.246,82.686,%
Batch: 180 | Loss: 4.124 | Acc: 42.895,67.326,82.718,%
Batch: 200 | Loss: 4.113 | Acc: 42.922,67.436,82.793,%
Batch: 220 | Loss: 4.089 | Acc: 43.153,67.530,82.975,%
Batch: 240 | Loss: 4.092 | Acc: 43.108,67.424,83.059,%
Batch: 260 | Loss: 4.092 | Acc: 43.220,67.499,83.190,%
Batch: 280 | Loss: 4.094 | Acc: 43.266,67.491,83.238,%
Batch: 300 | Loss: 4.081 | Acc: 43.327,67.559,83.308,%
Batch: 320 | Loss: 4.077 | Acc: 43.271,67.647,83.377,%
Batch: 340 | Loss: 4.071 | Acc: 43.338,67.692,83.385,%
Batch: 360 | Loss: 4.067 | Acc: 43.371,67.755,83.455,%
Batch: 380 | Loss: 4.061 | Acc: 43.385,67.684,83.477,%
Batch: 0 | Loss: 5.171 | Acc: 40.625,62.500,64.844,%
Batch: 20 | Loss: 5.322 | Acc: 38.839,60.975,64.955,%
Batch: 40 | Loss: 5.315 | Acc: 39.577,60.156,64.482,%
Batch: 60 | Loss: 5.345 | Acc: 39.395,59.939,64.357,%
Train classifier parameters

Epoch: 136
Batch: 0 | Loss: 3.803 | Acc: 44.531,76.562,83.594,%
Batch: 20 | Loss: 3.760 | Acc: 45.201,70.908,86.533,%
Batch: 40 | Loss: 3.807 | Acc: 44.703,70.293,86.681,%
Batch: 60 | Loss: 3.805 | Acc: 44.506,69.915,86.258,%
Batch: 80 | Loss: 3.819 | Acc: 44.290,69.425,86.314,%
Batch: 100 | Loss: 3.847 | Acc: 44.206,69.253,85.999,%
Batch: 120 | Loss: 3.867 | Acc: 44.124,69.183,85.983,%
Batch: 140 | Loss: 3.876 | Acc: 44.387,69.010,85.865,%
Batch: 160 | Loss: 3.885 | Acc: 44.308,69.095,85.705,%
Batch: 180 | Loss: 3.910 | Acc: 44.177,68.823,85.597,%
Batch: 200 | Loss: 3.905 | Acc: 44.244,68.746,85.549,%
Batch: 220 | Loss: 3.912 | Acc: 44.284,68.686,85.573,%
Batch: 240 | Loss: 3.898 | Acc: 44.418,68.756,85.578,%
Batch: 260 | Loss: 3.886 | Acc: 44.579,68.774,85.671,%
Batch: 280 | Loss: 3.883 | Acc: 44.509,68.806,85.612,%
Batch: 300 | Loss: 3.894 | Acc: 44.453,68.737,85.595,%
Batch: 320 | Loss: 3.901 | Acc: 44.483,68.755,85.543,%
Batch: 340 | Loss: 3.904 | Acc: 44.469,68.670,85.537,%
Batch: 360 | Loss: 3.897 | Acc: 44.533,68.692,85.593,%
Batch: 380 | Loss: 3.907 | Acc: 44.484,68.717,85.554,%
Batch: 0 | Loss: 5.070 | Acc: 38.281,64.844,67.188,%
Batch: 20 | Loss: 5.272 | Acc: 39.286,60.528,65.737,%
Batch: 40 | Loss: 5.268 | Acc: 40.091,60.537,65.149,%
Batch: 60 | Loss: 5.287 | Acc: 39.703,60.156,64.959,%
Train classifier parameters

Epoch: 137
Batch: 0 | Loss: 3.675 | Acc: 48.438,68.750,85.938,%
Batch: 20 | Loss: 3.815 | Acc: 45.312,69.308,86.198,%
Batch: 40 | Loss: 3.855 | Acc: 45.332,69.245,86.433,%
Batch: 60 | Loss: 3.868 | Acc: 45.415,69.416,85.848,%
Batch: 80 | Loss: 3.880 | Acc: 45.303,69.348,85.716,%
Batch: 100 | Loss: 3.862 | Acc: 45.173,69.438,86.108,%
Batch: 120 | Loss: 3.839 | Acc: 45.422,69.441,86.073,%
Batch: 140 | Loss: 3.856 | Acc: 45.279,69.143,85.949,%
Batch: 160 | Loss: 3.852 | Acc: 45.201,69.138,85.865,%
Batch: 180 | Loss: 3.861 | Acc: 45.066,69.126,85.873,%
Batch: 200 | Loss: 3.865 | Acc: 45.068,69.076,85.813,%
Batch: 220 | Loss: 3.851 | Acc: 45.249,69.157,85.771,%
Batch: 240 | Loss: 3.863 | Acc: 45.001,69.217,85.740,%
Batch: 260 | Loss: 3.869 | Acc: 44.914,69.151,85.752,%
Batch: 280 | Loss: 3.854 | Acc: 44.884,69.314,85.832,%
Batch: 300 | Loss: 3.843 | Acc: 44.905,69.363,85.886,%
Batch: 320 | Loss: 3.852 | Acc: 44.826,69.305,85.818,%
Batch: 340 | Loss: 3.851 | Acc: 44.827,69.240,85.809,%
Batch: 360 | Loss: 3.858 | Acc: 44.752,69.276,85.873,%
Batch: 380 | Loss: 3.856 | Acc: 44.745,69.240,85.886,%
Batch: 0 | Loss: 5.049 | Acc: 42.969,64.062,67.969,%
Batch: 20 | Loss: 5.235 | Acc: 40.848,60.640,66.481,%
Batch: 40 | Loss: 5.229 | Acc: 41.120,60.290,65.568,%
Batch: 60 | Loss: 5.257 | Acc: 40.484,60.220,65.587,%
Train classifier parameters

Epoch: 138
Batch: 0 | Loss: 3.855 | Acc: 43.750,67.188,86.719,%
Batch: 20 | Loss: 3.640 | Acc: 46.912,70.833,87.426,%
Batch: 40 | Loss: 3.715 | Acc: 46.284,70.503,87.024,%
Batch: 60 | Loss: 3.723 | Acc: 46.107,69.749,87.141,%
Batch: 80 | Loss: 3.757 | Acc: 45.698,70.033,87.056,%
Batch: 100 | Loss: 3.748 | Acc: 45.784,69.872,87.152,%
Batch: 120 | Loss: 3.777 | Acc: 45.590,69.970,87.313,%
Batch: 140 | Loss: 3.778 | Acc: 45.551,70.047,87.234,%
Batch: 160 | Loss: 3.781 | Acc: 45.298,69.827,87.000,%
Batch: 180 | Loss: 3.792 | Acc: 45.179,69.618,86.948,%
Batch: 200 | Loss: 3.797 | Acc: 45.114,69.823,86.913,%
Batch: 220 | Loss: 3.790 | Acc: 45.224,69.715,86.899,%
Batch: 240 | Loss: 3.790 | Acc: 45.261,69.684,86.845,%
Batch: 260 | Loss: 3.792 | Acc: 45.268,69.750,86.835,%
Batch: 280 | Loss: 3.793 | Acc: 45.279,69.681,86.755,%
Batch: 300 | Loss: 3.790 | Acc: 45.351,69.557,86.758,%
Batch: 320 | Loss: 3.793 | Acc: 45.342,69.468,86.687,%
Batch: 340 | Loss: 3.803 | Acc: 45.267,69.396,86.700,%
Batch: 360 | Loss: 3.809 | Acc: 45.202,69.339,86.637,%
Batch: 380 | Loss: 3.816 | Acc: 45.077,69.314,86.624,%
Batch: 0 | Loss: 5.041 | Acc: 45.312,64.062,68.750,%
Batch: 20 | Loss: 5.233 | Acc: 41.146,60.714,66.034,%
Batch: 40 | Loss: 5.220 | Acc: 41.273,60.366,65.396,%
Batch: 60 | Loss: 5.246 | Acc: 40.535,59.990,65.471,%
Train classifier parameters

Epoch: 139
Batch: 0 | Loss: 3.798 | Acc: 38.281,68.750,88.281,%
Batch: 20 | Loss: 3.666 | Acc: 45.350,71.094,86.979,%
Batch: 40 | Loss: 3.671 | Acc: 46.018,70.617,87.043,%
Batch: 60 | Loss: 3.666 | Acc: 46.158,70.530,87.167,%
Batch: 80 | Loss: 3.681 | Acc: 45.804,70.341,87.317,%
Batch: 100 | Loss: 3.686 | Acc: 45.753,70.398,87.338,%
Batch: 120 | Loss: 3.703 | Acc: 45.784,70.332,87.158,%
Batch: 140 | Loss: 3.727 | Acc: 45.745,70.063,87.035,%
Batch: 160 | Loss: 3.732 | Acc: 45.434,69.958,87.000,%
Batch: 180 | Loss: 3.720 | Acc: 45.563,70.075,87.073,%
Batch: 200 | Loss: 3.715 | Acc: 45.849,70.029,87.026,%
Batch: 220 | Loss: 3.723 | Acc: 45.853,70.030,87.009,%
Batch: 240 | Loss: 3.736 | Acc: 45.663,69.907,86.926,%
Batch: 260 | Loss: 3.736 | Acc: 45.711,69.861,86.940,%
Batch: 280 | Loss: 3.746 | Acc: 45.730,69.829,86.849,%
Batch: 300 | Loss: 3.764 | Acc: 45.497,69.741,86.828,%
Batch: 320 | Loss: 3.758 | Acc: 45.585,69.831,86.877,%
Batch: 340 | Loss: 3.764 | Acc: 45.526,69.703,86.909,%
Batch: 360 | Loss: 3.769 | Acc: 45.453,69.668,86.922,%
Batch: 380 | Loss: 3.771 | Acc: 45.464,69.578,86.885,%
Batch: 0 | Loss: 5.087 | Acc: 45.312,66.406,65.625,%
Batch: 20 | Loss: 5.229 | Acc: 40.104,61.161,66.332,%
Batch: 40 | Loss: 5.206 | Acc: 40.796,60.899,65.244,%
Batch: 60 | Loss: 5.225 | Acc: 40.318,60.566,65.471,%
Train classifier parameters

Epoch: 140
Batch: 0 | Loss: 3.805 | Acc: 48.438,76.562,92.969,%
Batch: 20 | Loss: 3.757 | Acc: 44.940,70.759,88.021,%
Batch: 40 | Loss: 3.720 | Acc: 45.217,70.503,88.053,%
Batch: 60 | Loss: 3.714 | Acc: 45.863,70.248,87.782,%
Batch: 80 | Loss: 3.700 | Acc: 45.910,70.245,87.867,%
Batch: 100 | Loss: 3.713 | Acc: 45.684,70.088,87.601,%
Batch: 120 | Loss: 3.713 | Acc: 45.706,70.132,87.661,%
Batch: 140 | Loss: 3.733 | Acc: 45.479,70.008,87.639,%
Batch: 160 | Loss: 3.753 | Acc: 45.356,69.788,87.650,%
Batch: 180 | Loss: 3.764 | Acc: 45.356,69.609,87.496,%
Batch: 200 | Loss: 3.769 | Acc: 45.347,69.535,87.446,%
Batch: 220 | Loss: 3.765 | Acc: 45.337,69.538,87.447,%
Batch: 240 | Loss: 3.754 | Acc: 45.449,69.622,87.481,%
Batch: 260 | Loss: 3.747 | Acc: 45.429,69.573,87.488,%
Batch: 280 | Loss: 3.737 | Acc: 45.566,69.687,87.472,%
Batch: 300 | Loss: 3.738 | Acc: 45.658,69.747,87.443,%
Batch: 320 | Loss: 3.735 | Acc: 45.634,69.787,87.439,%
Batch: 340 | Loss: 3.731 | Acc: 45.642,69.751,87.475,%
Batch: 360 | Loss: 3.739 | Acc: 45.503,69.702,87.405,%
Batch: 380 | Loss: 3.742 | Acc: 45.491,69.675,87.332,%
Batch: 0 | Loss: 5.030 | Acc: 45.312,64.062,67.188,%
Batch: 20 | Loss: 5.217 | Acc: 41.109,60.863,66.592,%
Batch: 40 | Loss: 5.190 | Acc: 41.502,60.766,65.949,%
Batch: 60 | Loss: 5.205 | Acc: 41.137,60.694,65.817,%
Train classifier parameters

Epoch: 141
Batch: 0 | Loss: 3.010 | Acc: 50.781,75.000,88.281,%
Batch: 20 | Loss: 3.753 | Acc: 45.461,72.210,88.728,%
Batch: 40 | Loss: 3.698 | Acc: 46.380,72.085,88.281,%
Batch: 60 | Loss: 3.660 | Acc: 46.465,71.504,88.307,%
Batch: 80 | Loss: 3.673 | Acc: 46.055,71.152,87.992,%
Batch: 100 | Loss: 3.706 | Acc: 45.753,70.684,87.894,%
Batch: 120 | Loss: 3.705 | Acc: 45.642,70.442,88.055,%
Batch: 140 | Loss: 3.687 | Acc: 45.933,70.540,88.082,%
Batch: 160 | Loss: 3.675 | Acc: 46.108,70.613,88.155,%
Batch: 180 | Loss: 3.677 | Acc: 46.068,70.425,88.001,%
Batch: 200 | Loss: 3.678 | Acc: 45.985,70.417,87.904,%
Batch: 220 | Loss: 3.683 | Acc: 46.012,70.323,87.914,%
Batch: 240 | Loss: 3.696 | Acc: 45.971,70.274,87.853,%
Batch: 260 | Loss: 3.702 | Acc: 45.947,70.091,87.760,%
Batch: 280 | Loss: 3.709 | Acc: 45.857,70.046,87.770,%
Batch: 300 | Loss: 3.716 | Acc: 45.712,69.988,87.747,%
Batch: 320 | Loss: 3.718 | Acc: 45.651,69.950,87.746,%
Batch: 340 | Loss: 3.714 | Acc: 45.629,70.104,87.704,%
Batch: 360 | Loss: 3.725 | Acc: 45.585,70.018,87.643,%
Batch: 380 | Loss: 3.735 | Acc: 45.583,69.855,87.572,%
Batch: 0 | Loss: 5.011 | Acc: 41.406,64.062,68.750,%
Batch: 20 | Loss: 5.219 | Acc: 40.402,60.751,66.555,%
Batch: 40 | Loss: 5.205 | Acc: 40.758,60.652,65.911,%
Batch: 60 | Loss: 5.234 | Acc: 40.484,60.592,65.599,%
Train classifier parameters

Epoch: 142
Batch: 0 | Loss: 4.183 | Acc: 37.500,67.969,85.156,%
Batch: 20 | Loss: 3.715 | Acc: 46.205,71.615,88.542,%
Batch: 40 | Loss: 3.709 | Acc: 45.332,71.932,88.586,%
Batch: 60 | Loss: 3.672 | Acc: 46.363,71.785,88.332,%
Batch: 80 | Loss: 3.723 | Acc: 45.476,71.219,87.828,%
Batch: 100 | Loss: 3.707 | Acc: 45.831,71.303,88.003,%
Batch: 120 | Loss: 3.696 | Acc: 45.926,71.120,87.810,%
Batch: 140 | Loss: 3.702 | Acc: 45.861,70.916,87.810,%
Batch: 160 | Loss: 3.694 | Acc: 45.977,70.575,87.602,%
Batch: 180 | Loss: 3.692 | Acc: 45.925,70.602,87.625,%
Batch: 200 | Loss: 3.692 | Acc: 45.861,70.460,87.605,%
Batch: 220 | Loss: 3.714 | Acc: 45.814,70.419,87.532,%
Batch: 240 | Loss: 3.726 | Acc: 45.705,70.410,87.539,%
Batch: 260 | Loss: 3.734 | Acc: 45.663,70.304,87.665,%
Batch: 280 | Loss: 3.729 | Acc: 45.654,70.357,87.636,%
Batch: 300 | Loss: 3.742 | Acc: 45.471,70.235,87.560,%
Batch: 320 | Loss: 3.744 | Acc: 45.461,70.264,87.527,%
Batch: 340 | Loss: 3.739 | Acc: 45.500,70.283,87.470,%
Batch: 360 | Loss: 3.734 | Acc: 45.509,70.274,87.483,%
Batch: 380 | Loss: 3.737 | Acc: 45.423,70.247,87.486,%
Batch: 0 | Loss: 4.993 | Acc: 42.969,66.406,68.750,%
Batch: 20 | Loss: 5.188 | Acc: 41.146,60.528,66.964,%
Batch: 40 | Loss: 5.156 | Acc: 41.368,60.556,66.025,%
Batch: 60 | Loss: 5.180 | Acc: 41.304,60.425,65.830,%
Train classifier parameters

Epoch: 143
Batch: 0 | Loss: 4.369 | Acc: 40.625,64.062,83.594,%
Batch: 20 | Loss: 3.814 | Acc: 44.159,70.610,88.318,%
Batch: 40 | Loss: 3.761 | Acc: 44.607,70.236,87.938,%
Batch: 60 | Loss: 3.748 | Acc: 45.274,69.762,87.948,%
Batch: 80 | Loss: 3.704 | Acc: 45.592,70.139,87.828,%
Batch: 100 | Loss: 3.658 | Acc: 45.900,70.568,88.103,%
Batch: 120 | Loss: 3.640 | Acc: 46.307,70.590,88.197,%
Batch: 140 | Loss: 3.651 | Acc: 46.343,70.567,88.049,%
Batch: 160 | Loss: 3.660 | Acc: 46.390,70.317,87.922,%
Batch: 180 | Loss: 3.685 | Acc: 45.925,69.993,87.811,%
Batch: 200 | Loss: 3.686 | Acc: 45.857,70.075,87.908,%
Batch: 220 | Loss: 3.685 | Acc: 45.804,70.125,87.875,%
Batch: 240 | Loss: 3.689 | Acc: 45.711,70.121,87.759,%
Batch: 260 | Loss: 3.703 | Acc: 45.549,70.070,87.671,%
Batch: 280 | Loss: 3.700 | Acc: 45.663,70.059,87.653,%
Batch: 300 | Loss: 3.691 | Acc: 45.707,70.216,87.705,%
Batch: 320 | Loss: 3.696 | Acc: 45.709,70.132,87.680,%
Batch: 340 | Loss: 3.691 | Acc: 45.807,70.104,87.695,%
Batch: 360 | Loss: 3.695 | Acc: 45.782,70.057,87.682,%
Batch: 380 | Loss: 3.695 | Acc: 45.790,70.046,87.701,%
Batch: 0 | Loss: 4.953 | Acc: 42.188,64.844,68.750,%
Batch: 20 | Loss: 5.180 | Acc: 40.216,61.198,66.592,%
Batch: 40 | Loss: 5.171 | Acc: 40.587,60.918,65.549,%
Batch: 60 | Loss: 5.190 | Acc: 40.254,60.720,65.766,%
Train classifier parameters

Epoch: 144
Batch: 0 | Loss: 3.050 | Acc: 50.781,74.219,88.281,%
Batch: 20 | Loss: 3.608 | Acc: 45.871,70.312,87.723,%
Batch: 40 | Loss: 3.615 | Acc: 45.998,70.617,88.053,%
Batch: 60 | Loss: 3.630 | Acc: 45.710,70.287,88.204,%
Batch: 80 | Loss: 3.613 | Acc: 45.988,70.544,88.281,%
Batch: 100 | Loss: 3.627 | Acc: 45.931,70.614,88.150,%
Batch: 120 | Loss: 3.632 | Acc: 45.945,70.435,88.126,%
Batch: 140 | Loss: 3.645 | Acc: 45.662,70.268,88.054,%
Batch: 160 | Loss: 3.663 | Acc: 45.477,70.225,87.995,%
Batch: 180 | Loss: 3.669 | Acc: 45.761,70.192,87.983,%
Batch: 200 | Loss: 3.660 | Acc: 45.841,70.231,87.986,%
Batch: 220 | Loss: 3.666 | Acc: 45.666,70.210,87.896,%
Batch: 240 | Loss: 3.672 | Acc: 45.737,70.248,87.850,%
Batch: 260 | Loss: 3.682 | Acc: 45.669,70.235,87.862,%
Batch: 280 | Loss: 3.683 | Acc: 45.638,70.282,87.811,%
Batch: 300 | Loss: 3.682 | Acc: 45.725,70.214,87.806,%
Batch: 320 | Loss: 3.676 | Acc: 45.814,70.162,87.816,%
Batch: 340 | Loss: 3.682 | Acc: 45.791,70.049,87.798,%
Batch: 360 | Loss: 3.674 | Acc: 45.916,70.083,87.781,%
Batch: 380 | Loss: 3.678 | Acc: 45.909,70.058,87.777,%
Batch: 0 | Loss: 5.031 | Acc: 42.969,64.844,67.188,%
Batch: 20 | Loss: 5.193 | Acc: 40.960,61.086,67.150,%
Batch: 40 | Loss: 5.179 | Acc: 41.216,60.747,66.101,%
Batch: 60 | Loss: 5.207 | Acc: 40.894,60.464,65.932,%
Train classifier parameters

Epoch: 145
Batch: 0 | Loss: 3.746 | Acc: 47.656,66.406,89.844,%
Batch: 20 | Loss: 3.690 | Acc: 45.275,69.829,88.690,%
Batch: 40 | Loss: 3.640 | Acc: 45.598,70.560,88.205,%
Batch: 60 | Loss: 3.672 | Acc: 45.569,70.172,87.551,%
Batch: 80 | Loss: 3.709 | Acc: 45.351,69.859,87.924,%
Batch: 100 | Loss: 3.711 | Acc: 45.382,69.980,88.103,%
Batch: 120 | Loss: 3.705 | Acc: 45.358,69.990,88.139,%
Batch: 140 | Loss: 3.708 | Acc: 45.379,70.096,88.276,%
Batch: 160 | Loss: 3.710 | Acc: 45.482,70.026,88.150,%
Batch: 180 | Loss: 3.694 | Acc: 45.546,70.075,88.134,%
Batch: 200 | Loss: 3.706 | Acc: 45.487,70.091,88.176,%
Batch: 220 | Loss: 3.696 | Acc: 45.673,70.122,88.193,%
Batch: 240 | Loss: 3.697 | Acc: 45.604,70.180,88.220,%
Batch: 260 | Loss: 3.698 | Acc: 45.567,70.208,88.275,%
Batch: 280 | Loss: 3.691 | Acc: 45.643,70.299,88.326,%
Batch: 300 | Loss: 3.692 | Acc: 45.655,70.266,88.227,%
Batch: 320 | Loss: 3.696 | Acc: 45.614,70.237,88.225,%
Batch: 340 | Loss: 3.699 | Acc: 45.576,70.244,88.176,%
Batch: 360 | Loss: 3.705 | Acc: 45.522,70.243,88.186,%
Batch: 380 | Loss: 3.715 | Acc: 45.468,70.146,88.162,%
Batch: 0 | Loss: 4.969 | Acc: 44.531,66.406,71.094,%
Batch: 20 | Loss: 5.179 | Acc: 41.183,60.751,67.299,%
Batch: 40 | Loss: 5.147 | Acc: 41.216,60.938,66.178,%
Batch: 60 | Loss: 5.164 | Acc: 40.727,60.835,66.317,%
Train classifier parameters

Epoch: 146
Batch: 0 | Loss: 3.641 | Acc: 42.969,80.469,90.625,%
Batch: 20 | Loss: 3.576 | Acc: 46.094,72.024,89.583,%
Batch: 40 | Loss: 3.627 | Acc: 45.427,70.713,88.796,%
Batch: 60 | Loss: 3.642 | Acc: 45.389,70.543,88.704,%
Batch: 80 | Loss: 3.693 | Acc: 44.927,70.592,88.503,%
Batch: 100 | Loss: 3.681 | Acc: 45.057,70.668,88.475,%
Batch: 120 | Loss: 3.651 | Acc: 45.616,70.797,88.591,%
Batch: 140 | Loss: 3.666 | Acc: 45.362,70.850,88.669,%
Batch: 160 | Loss: 3.665 | Acc: 45.453,70.730,88.538,%
Batch: 180 | Loss: 3.643 | Acc: 45.787,70.805,88.579,%
Batch: 200 | Loss: 3.643 | Acc: 45.802,70.740,88.534,%
Batch: 220 | Loss: 3.645 | Acc: 45.871,70.896,88.483,%
Batch: 240 | Loss: 3.649 | Acc: 45.925,70.909,88.492,%
Batch: 260 | Loss: 3.664 | Acc: 45.842,70.812,88.458,%
Batch: 280 | Loss: 3.670 | Acc: 45.788,70.685,88.431,%
Batch: 300 | Loss: 3.672 | Acc: 45.886,70.627,88.382,%
Batch: 320 | Loss: 3.673 | Acc: 45.877,70.602,88.345,%
Batch: 340 | Loss: 3.670 | Acc: 45.794,70.633,88.334,%
Batch: 360 | Loss: 3.667 | Acc: 45.819,70.687,88.266,%
Batch: 380 | Loss: 3.679 | Acc: 45.776,70.659,88.177,%
Batch: 0 | Loss: 5.024 | Acc: 43.750,64.844,68.750,%
Batch: 20 | Loss: 5.184 | Acc: 41.109,61.049,67.150,%
Batch: 40 | Loss: 5.162 | Acc: 41.292,60.995,66.463,%
Batch: 60 | Loss: 5.176 | Acc: 40.753,60.835,66.598,%
Train classifier parameters

Epoch: 147
Batch: 0 | Loss: 4.327 | Acc: 46.094,72.656,88.281,%
Batch: 20 | Loss: 3.489 | Acc: 48.735,71.726,89.546,%
Batch: 40 | Loss: 3.586 | Acc: 46.799,70.732,88.281,%
Batch: 60 | Loss: 3.617 | Acc: 46.504,70.838,88.448,%
Batch: 80 | Loss: 3.627 | Acc: 46.537,70.901,88.320,%
Batch: 100 | Loss: 3.629 | Acc: 46.171,70.877,88.444,%
Batch: 120 | Loss: 3.652 | Acc: 45.881,70.455,88.385,%
Batch: 140 | Loss: 3.638 | Acc: 46.155,70.434,88.270,%
Batch: 160 | Loss: 3.627 | Acc: 46.220,70.405,88.252,%
Batch: 180 | Loss: 3.633 | Acc: 46.210,70.459,88.238,%
Batch: 200 | Loss: 3.630 | Acc: 46.241,70.468,88.184,%
Batch: 220 | Loss: 3.634 | Acc: 46.200,70.475,88.129,%
Batch: 240 | Loss: 3.633 | Acc: 46.123,70.374,88.093,%
Batch: 260 | Loss: 3.650 | Acc: 45.965,70.411,88.042,%
Batch: 280 | Loss: 3.647 | Acc: 45.952,70.549,88.062,%
Batch: 300 | Loss: 3.652 | Acc: 46.044,70.468,87.965,%
Batch: 320 | Loss: 3.663 | Acc: 45.974,70.420,87.984,%
Batch: 340 | Loss: 3.661 | Acc: 45.929,70.491,88.009,%
Batch: 360 | Loss: 3.666 | Acc: 45.854,70.460,87.961,%
Batch: 380 | Loss: 3.670 | Acc: 45.829,70.448,87.984,%
Batch: 0 | Loss: 4.947 | Acc: 44.531,67.188,67.969,%
Batch: 20 | Loss: 5.164 | Acc: 40.997,61.384,66.815,%
Batch: 40 | Loss: 5.143 | Acc: 41.330,61.128,66.178,%
Batch: 60 | Loss: 5.162 | Acc: 40.894,61.130,66.291,%
Train classifier parameters

Epoch: 148
Batch: 0 | Loss: 3.717 | Acc: 39.062,77.344,88.281,%
Batch: 20 | Loss: 3.701 | Acc: 45.164,70.759,88.951,%
Batch: 40 | Loss: 3.670 | Acc: 45.465,70.694,88.567,%
Batch: 60 | Loss: 3.672 | Acc: 45.658,70.389,88.320,%
Batch: 80 | Loss: 3.686 | Acc: 45.341,70.014,88.166,%
Batch: 100 | Loss: 3.675 | Acc: 45.545,70.297,88.235,%
Batch: 120 | Loss: 3.667 | Acc: 46.042,70.487,88.507,%
Batch: 140 | Loss: 3.661 | Acc: 46.049,70.495,88.586,%
Batch: 160 | Loss: 3.662 | Acc: 45.997,70.434,88.514,%
Batch: 180 | Loss: 3.659 | Acc: 45.921,70.507,88.583,%
Batch: 200 | Loss: 3.659 | Acc: 46.012,70.620,88.546,%
Batch: 220 | Loss: 3.670 | Acc: 46.016,70.546,88.430,%
Batch: 240 | Loss: 3.665 | Acc: 46.003,70.604,88.343,%
Batch: 260 | Loss: 3.663 | Acc: 46.082,70.552,88.242,%
Batch: 280 | Loss: 3.671 | Acc: 46.008,70.488,88.209,%
Batch: 300 | Loss: 3.674 | Acc: 45.946,70.486,88.224,%
Batch: 320 | Loss: 3.679 | Acc: 45.933,70.281,88.160,%
Batch: 340 | Loss: 3.683 | Acc: 45.924,70.319,88.176,%
Batch: 360 | Loss: 3.675 | Acc: 46.009,70.310,88.186,%
Batch: 380 | Loss: 3.674 | Acc: 46.020,70.274,88.134,%
Batch: 0 | Loss: 5.029 | Acc: 43.750,63.281,65.625,%
Batch: 20 | Loss: 5.160 | Acc: 41.220,61.570,67.039,%
Batch: 40 | Loss: 5.129 | Acc: 41.368,61.414,66.387,%
Batch: 60 | Loss: 5.152 | Acc: 40.779,61.142,66.419,%
Train classifier parameters

Epoch: 149
Batch: 0 | Loss: 3.253 | Acc: 50.000,71.094,85.938,%
Batch: 20 | Loss: 3.539 | Acc: 45.833,72.024,89.769,%
Batch: 40 | Loss: 3.547 | Acc: 46.151,72.142,89.615,%
Batch: 60 | Loss: 3.571 | Acc: 46.363,72.029,89.229,%
Batch: 80 | Loss: 3.580 | Acc: 46.123,71.644,89.014,%
Batch: 100 | Loss: 3.594 | Acc: 46.063,71.465,88.823,%
Batch: 120 | Loss: 3.626 | Acc: 45.545,71.023,88.578,%
Batch: 140 | Loss: 3.626 | Acc: 45.717,70.928,88.381,%
Batch: 160 | Loss: 3.635 | Acc: 45.642,71.065,88.373,%
Batch: 180 | Loss: 3.630 | Acc: 45.783,70.878,88.324,%
Batch: 200 | Loss: 3.652 | Acc: 45.678,70.546,88.211,%
Batch: 220 | Loss: 3.654 | Acc: 45.691,70.634,88.232,%
Batch: 240 | Loss: 3.646 | Acc: 45.640,70.805,88.272,%
Batch: 260 | Loss: 3.647 | Acc: 45.621,70.741,88.200,%
Batch: 280 | Loss: 3.646 | Acc: 45.716,70.830,88.176,%
Batch: 300 | Loss: 3.645 | Acc: 45.829,70.800,88.219,%
Batch: 320 | Loss: 3.646 | Acc: 45.787,70.734,88.254,%
Batch: 340 | Loss: 3.645 | Acc: 45.927,70.780,88.316,%
Batch: 360 | Loss: 3.653 | Acc: 45.880,70.659,88.316,%
Batch: 380 | Loss: 3.657 | Acc: 45.811,70.573,88.316,%
Batch: 0 | Loss: 5.041 | Acc: 44.531,64.844,65.625,%
Batch: 20 | Loss: 5.168 | Acc: 41.220,61.161,66.667,%
Batch: 40 | Loss: 5.149 | Acc: 41.482,61.528,65.930,%
Batch: 60 | Loss: 5.169 | Acc: 40.881,61.027,65.868,%
Train all parameters

Epoch: 150
Batch: 0 | Loss: 2.722 | Acc: 56.250,70.312,90.625,%
Batch: 20 | Loss: 3.364 | Acc: 46.875,72.768,91.481,%
Batch: 40 | Loss: 3.315 | Acc: 46.875,73.666,92.702,%
Batch: 60 | Loss: 3.264 | Acc: 46.644,74.027,93.379,%
Batch: 80 | Loss: 3.224 | Acc: 47.116,74.171,93.827,%
Batch: 100 | Loss: 3.212 | Acc: 47.355,74.683,94.230,%
Batch: 120 | Loss: 3.193 | Acc: 47.346,74.923,94.576,%
Batch: 140 | Loss: 3.163 | Acc: 47.773,75.183,94.697,%
Batch: 160 | Loss: 3.129 | Acc: 48.185,75.228,94.827,%
Batch: 180 | Loss: 3.124 | Acc: 48.230,75.436,94.976,%
Batch: 200 | Loss: 3.130 | Acc: 48.115,75.404,95.099,%
Batch: 220 | Loss: 3.137 | Acc: 48.052,75.477,95.150,%
Batch: 240 | Loss: 3.130 | Acc: 48.165,75.603,95.277,%
Batch: 260 | Loss: 3.122 | Acc: 48.177,75.787,95.336,%
Batch: 280 | Loss: 3.114 | Acc: 48.123,75.923,95.393,%
Batch: 300 | Loss: 3.097 | Acc: 48.354,76.028,95.484,%
Batch: 320 | Loss: 3.086 | Acc: 48.430,76.146,95.544,%
Batch: 340 | Loss: 3.081 | Acc: 48.451,76.196,95.613,%
Batch: 360 | Loss: 3.073 | Acc: 48.561,76.266,95.654,%
Batch: 380 | Loss: 3.076 | Acc: 48.565,76.226,95.708,%
Batch: 0 | Loss: 4.408 | Acc: 53.125,69.531,72.656,%
Batch: 20 | Loss: 4.687 | Acc: 42.485,65.104,72.061,%
Batch: 40 | Loss: 4.671 | Acc: 43.102,65.015,71.551,%
Batch: 60 | Loss: 4.676 | Acc: 42.674,64.869,71.465,%
Train all parameters

Epoch: 151
Batch: 0 | Loss: 2.906 | Acc: 46.094,80.469,99.219,%
Batch: 20 | Loss: 2.797 | Acc: 51.339,80.320,97.991,%
Batch: 40 | Loss: 2.900 | Acc: 49.867,78.868,97.847,%
Batch: 60 | Loss: 2.903 | Acc: 49.552,78.637,97.848,%
Batch: 80 | Loss: 2.862 | Acc: 49.875,78.964,97.811,%
Batch: 100 | Loss: 2.851 | Acc: 49.667,78.929,97.765,%
Batch: 120 | Loss: 2.861 | Acc: 49.567,78.784,97.740,%
Batch: 140 | Loss: 2.877 | Acc: 49.751,78.651,97.795,%
Batch: 160 | Loss: 2.880 | Acc: 49.743,78.547,97.758,%
Batch: 180 | Loss: 2.864 | Acc: 49.953,78.518,97.743,%
Batch: 200 | Loss: 2.855 | Acc: 49.961,78.654,97.773,%
Batch: 220 | Loss: 2.876 | Acc: 49.671,78.510,97.727,%
Batch: 240 | Loss: 2.870 | Acc: 49.818,78.410,97.724,%
Batch: 260 | Loss: 2.874 | Acc: 49.841,78.311,97.704,%
Batch: 280 | Loss: 2.881 | Acc: 49.661,78.239,97.701,%
Batch: 300 | Loss: 2.878 | Acc: 49.603,78.291,97.700,%
Batch: 320 | Loss: 2.877 | Acc: 49.615,78.286,97.688,%
Batch: 340 | Loss: 2.887 | Acc: 49.608,78.297,97.704,%
Batch: 360 | Loss: 2.892 | Acc: 49.550,78.270,97.717,%
Batch: 380 | Loss: 2.897 | Acc: 49.498,78.195,97.710,%
Batch: 0 | Loss: 4.412 | Acc: 48.438,72.656,71.875,%
Batch: 20 | Loss: 4.649 | Acc: 44.196,65.439,71.391,%
Batch: 40 | Loss: 4.626 | Acc: 44.417,65.854,70.960,%
Batch: 60 | Loss: 4.637 | Acc: 44.045,65.727,71.107,%
Train all parameters

Epoch: 152
Batch: 0 | Loss: 2.950 | Acc: 51.562,82.031,98.438,%
Batch: 20 | Loss: 2.970 | Acc: 48.438,79.948,97.991,%
Batch: 40 | Loss: 2.802 | Acc: 50.553,80.545,98.399,%
Batch: 60 | Loss: 2.785 | Acc: 50.115,80.213,98.348,%
Batch: 80 | Loss: 2.770 | Acc: 50.511,80.392,98.428,%
Batch: 100 | Loss: 2.791 | Acc: 50.232,79.950,98.399,%
Batch: 120 | Loss: 2.799 | Acc: 50.142,79.901,98.425,%
Batch: 140 | Loss: 2.816 | Acc: 49.817,79.532,98.404,%
Batch: 160 | Loss: 2.810 | Acc: 49.675,79.508,98.408,%
Batch: 180 | Loss: 2.819 | Acc: 49.650,79.334,98.442,%
Batch: 200 | Loss: 2.825 | Acc: 49.666,79.132,98.430,%
Batch: 220 | Loss: 2.811 | Acc: 49.869,79.256,98.416,%
Batch: 240 | Loss: 2.807 | Acc: 49.861,79.133,98.379,%
Batch: 260 | Loss: 2.818 | Acc: 49.767,79.095,98.411,%
Batch: 280 | Loss: 2.816 | Acc: 49.761,79.095,98.390,%
Batch: 300 | Loss: 2.811 | Acc: 49.912,78.987,98.367,%
Batch: 320 | Loss: 2.805 | Acc: 50.019,79.052,98.338,%
Batch: 340 | Loss: 2.798 | Acc: 50.092,79.025,98.311,%
Batch: 360 | Loss: 2.804 | Acc: 50.143,78.926,98.303,%
Batch: 380 | Loss: 2.800 | Acc: 50.254,78.927,98.302,%
Batch: 0 | Loss: 4.351 | Acc: 50.000,70.312,76.562,%
Batch: 20 | Loss: 4.640 | Acc: 43.564,65.439,72.507,%
Batch: 40 | Loss: 4.623 | Acc: 44.169,65.454,71.780,%
Batch: 60 | Loss: 4.630 | Acc: 43.878,65.330,71.644,%
Train all parameters

Epoch: 153
Batch: 0 | Loss: 2.135 | Acc: 57.812,85.156,99.219,%
Batch: 20 | Loss: 2.701 | Acc: 49.777,80.990,98.251,%
Batch: 40 | Loss: 2.752 | Acc: 50.057,79.554,98.342,%
Batch: 60 | Loss: 2.752 | Acc: 50.166,79.572,98.476,%
Batch: 80 | Loss: 2.800 | Acc: 49.788,79.408,98.495,%
Batch: 100 | Loss: 2.784 | Acc: 50.178,79.394,98.530,%
Batch: 120 | Loss: 2.752 | Acc: 50.413,79.558,98.580,%
Batch: 140 | Loss: 2.769 | Acc: 50.199,79.527,98.570,%
Batch: 160 | Loss: 2.748 | Acc: 50.616,79.833,98.583,%
Batch: 180 | Loss: 2.754 | Acc: 50.406,79.959,98.614,%
Batch: 200 | Loss: 2.761 | Acc: 50.365,80.061,98.636,%
Batch: 220 | Loss: 2.755 | Acc: 50.396,80.115,98.650,%
Batch: 240 | Loss: 2.767 | Acc: 50.266,80.005,98.606,%
Batch: 260 | Loss: 2.761 | Acc: 50.359,79.930,98.602,%
Batch: 280 | Loss: 2.761 | Acc: 50.384,79.904,98.602,%
Batch: 300 | Loss: 2.770 | Acc: 50.306,79.903,98.591,%
Batch: 320 | Loss: 2.775 | Acc: 50.282,79.814,98.576,%
Batch: 340 | Loss: 2.777 | Acc: 50.225,79.809,98.570,%
Batch: 360 | Loss: 2.772 | Acc: 50.338,79.724,98.565,%
Batch: 380 | Loss: 2.771 | Acc: 50.351,79.673,98.530,%
Batch: 0 | Loss: 4.393 | Acc: 49.219,69.531,74.219,%
Batch: 20 | Loss: 4.608 | Acc: 43.229,65.885,72.061,%
Batch: 40 | Loss: 4.612 | Acc: 43.579,65.606,71.475,%
Batch: 60 | Loss: 4.626 | Acc: 43.122,65.164,71.427,%
Train all parameters

Epoch: 154
Batch: 0 | Loss: 2.201 | Acc: 67.188,82.031,99.219,%
Batch: 20 | Loss: 2.774 | Acc: 51.414,78.385,98.847,%
Batch: 40 | Loss: 2.738 | Acc: 51.220,79.764,98.952,%
Batch: 60 | Loss: 2.771 | Acc: 50.884,79.393,98.950,%
Batch: 80 | Loss: 2.741 | Acc: 51.013,79.543,98.997,%
Batch: 100 | Loss: 2.721 | Acc: 51.129,79.440,98.948,%
Batch: 120 | Loss: 2.721 | Acc: 51.059,79.610,98.967,%
Batch: 140 | Loss: 2.722 | Acc: 50.765,79.793,98.953,%
Batch: 160 | Loss: 2.723 | Acc: 50.864,79.746,98.932,%
Batch: 180 | Loss: 2.722 | Acc: 50.786,79.869,98.904,%
Batch: 200 | Loss: 2.727 | Acc: 50.715,79.859,98.853,%
Batch: 220 | Loss: 2.725 | Acc: 50.806,79.741,98.858,%
Batch: 240 | Loss: 2.737 | Acc: 50.729,79.619,98.843,%
Batch: 260 | Loss: 2.732 | Acc: 50.889,79.559,98.851,%
Batch: 280 | Loss: 2.727 | Acc: 50.981,79.610,98.807,%
Batch: 300 | Loss: 2.724 | Acc: 51.064,79.630,98.762,%
Batch: 320 | Loss: 2.734 | Acc: 50.964,79.688,98.747,%
Batch: 340 | Loss: 2.739 | Acc: 50.829,79.678,98.751,%
Batch: 360 | Loss: 2.746 | Acc: 50.697,79.722,98.749,%
Batch: 380 | Loss: 2.744 | Acc: 50.726,79.708,98.735,%
Batch: 0 | Loss: 4.368 | Acc: 50.000,71.875,77.344,%
Batch: 20 | Loss: 4.627 | Acc: 43.973,66.109,71.801,%
Batch: 40 | Loss: 4.627 | Acc: 43.921,65.473,71.246,%
Batch: 60 | Loss: 4.644 | Acc: 43.353,65.292,71.363,%
Train all parameters

Epoch: 155
Batch: 0 | Loss: 2.312 | Acc: 46.094,82.812,99.219,%
Batch: 20 | Loss: 2.826 | Acc: 49.330,81.250,99.368,%
Batch: 40 | Loss: 2.828 | Acc: 49.295,80.697,99.123,%
Batch: 60 | Loss: 2.769 | Acc: 49.898,80.546,99.052,%
Batch: 80 | Loss: 2.788 | Acc: 49.855,80.382,99.064,%
Batch: 100 | Loss: 2.763 | Acc: 50.070,80.577,99.041,%
Batch: 120 | Loss: 2.761 | Acc: 50.349,80.449,98.986,%
Batch: 140 | Loss: 2.756 | Acc: 50.488,80.557,98.980,%
Batch: 160 | Loss: 2.751 | Acc: 50.466,80.629,98.971,%
Batch: 180 | Loss: 2.747 | Acc: 50.496,80.551,98.943,%
Batch: 200 | Loss: 2.759 | Acc: 50.358,80.340,98.900,%
Batch: 220 | Loss: 2.744 | Acc: 50.502,80.306,98.894,%
Batch: 240 | Loss: 2.745 | Acc: 50.506,80.310,98.898,%
Batch: 260 | Loss: 2.742 | Acc: 50.458,80.385,98.881,%
Batch: 280 | Loss: 2.740 | Acc: 50.517,80.341,98.882,%
Batch: 300 | Loss: 2.744 | Acc: 50.436,80.251,98.853,%
Batch: 320 | Loss: 2.744 | Acc: 50.504,80.150,98.866,%
Batch: 340 | Loss: 2.743 | Acc: 50.513,80.150,98.859,%
Batch: 360 | Loss: 2.739 | Acc: 50.623,80.112,98.851,%
Batch: 380 | Loss: 2.743 | Acc: 50.660,79.983,98.844,%
Batch: 0 | Loss: 4.371 | Acc: 52.344,70.312,74.219,%
Batch: 20 | Loss: 4.604 | Acc: 44.420,66.295,71.987,%
Batch: 40 | Loss: 4.600 | Acc: 44.341,65.949,71.341,%
Batch: 60 | Loss: 4.602 | Acc: 43.929,65.881,71.491,%
Train all parameters

Epoch: 156
Batch: 0 | Loss: 2.182 | Acc: 54.688,82.031,96.094,%
Batch: 20 | Loss: 2.726 | Acc: 51.786,79.204,98.921,%
Batch: 40 | Loss: 2.768 | Acc: 51.258,79.611,99.066,%
Batch: 60 | Loss: 2.795 | Acc: 50.295,80.097,99.129,%
Batch: 80 | Loss: 2.761 | Acc: 50.405,80.054,99.122,%
Batch: 100 | Loss: 2.762 | Acc: 50.124,80.144,99.056,%
Batch: 120 | Loss: 2.769 | Acc: 49.929,79.939,98.993,%
Batch: 140 | Loss: 2.759 | Acc: 50.039,80.186,99.025,%
Batch: 160 | Loss: 2.772 | Acc: 49.932,80.187,98.996,%
Batch: 180 | Loss: 2.764 | Acc: 50.147,80.257,98.994,%
Batch: 200 | Loss: 2.749 | Acc: 50.470,80.216,98.993,%
Batch: 220 | Loss: 2.758 | Acc: 50.410,80.140,98.993,%
Batch: 240 | Loss: 2.749 | Acc: 50.541,80.290,98.963,%
Batch: 260 | Loss: 2.749 | Acc: 50.530,80.247,98.961,%
Batch: 280 | Loss: 2.753 | Acc: 50.525,80.118,98.952,%
Batch: 300 | Loss: 2.759 | Acc: 50.524,80.040,98.949,%
Batch: 320 | Loss: 2.758 | Acc: 50.489,80.040,98.946,%
Batch: 340 | Loss: 2.753 | Acc: 50.444,80.107,98.944,%
Batch: 360 | Loss: 2.755 | Acc: 50.422,80.029,98.931,%
Batch: 380 | Loss: 2.756 | Acc: 50.396,80.050,98.942,%
Batch: 0 | Loss: 4.252 | Acc: 50.781,71.875,79.688,%
Batch: 20 | Loss: 4.608 | Acc: 43.787,65.551,72.582,%
Batch: 40 | Loss: 4.592 | Acc: 44.284,65.282,71.341,%
Batch: 60 | Loss: 4.601 | Acc: 43.916,64.895,71.363,%
Train all parameters

Epoch: 157
Batch: 0 | Loss: 2.970 | Acc: 44.531,78.906,100.000,%
Batch: 20 | Loss: 2.757 | Acc: 49.442,80.580,99.182,%
Batch: 40 | Loss: 2.626 | Acc: 51.658,81.879,99.066,%
Batch: 60 | Loss: 2.618 | Acc: 51.537,81.583,99.078,%
Batch: 80 | Loss: 2.658 | Acc: 51.109,81.539,99.026,%
Batch: 100 | Loss: 2.656 | Acc: 51.013,81.784,99.018,%
Batch: 120 | Loss: 2.653 | Acc: 51.182,81.696,99.019,%
Batch: 140 | Loss: 2.671 | Acc: 50.964,81.499,99.036,%
Batch: 160 | Loss: 2.682 | Acc: 50.772,81.381,99.030,%
Batch: 180 | Loss: 2.672 | Acc: 50.971,81.392,98.986,%
Batch: 200 | Loss: 2.659 | Acc: 51.178,81.316,99.017,%
Batch: 220 | Loss: 2.676 | Acc: 50.901,81.268,98.993,%
Batch: 240 | Loss: 2.693 | Acc: 50.668,81.130,98.966,%
Batch: 260 | Loss: 2.691 | Acc: 50.748,81.169,98.964,%
Batch: 280 | Loss: 2.689 | Acc: 50.870,81.094,98.955,%
Batch: 300 | Loss: 2.687 | Acc: 50.885,81.048,98.946,%
Batch: 320 | Loss: 2.704 | Acc: 50.720,80.924,98.922,%
Batch: 340 | Loss: 2.702 | Acc: 50.804,80.973,98.912,%
Batch: 360 | Loss: 2.702 | Acc: 50.926,80.804,98.898,%
Batch: 380 | Loss: 2.695 | Acc: 50.976,80.840,98.907,%
Batch: 0 | Loss: 4.337 | Acc: 48.438,71.094,74.219,%
Batch: 20 | Loss: 4.586 | Acc: 43.787,65.513,72.619,%
Batch: 40 | Loss: 4.576 | Acc: 44.455,65.415,71.684,%
Batch: 60 | Loss: 4.590 | Acc: 44.057,65.484,71.363,%
Train all parameters

Epoch: 158
Batch: 0 | Loss: 2.298 | Acc: 61.719,80.469,100.000,%
Batch: 20 | Loss: 2.655 | Acc: 51.637,82.254,99.368,%
Batch: 40 | Loss: 2.684 | Acc: 50.724,81.879,99.257,%
Batch: 60 | Loss: 2.692 | Acc: 50.282,82.185,99.334,%
Batch: 80 | Loss: 2.667 | Acc: 50.444,82.494,99.334,%
Batch: 100 | Loss: 2.669 | Acc: 50.820,82.201,99.312,%
Batch: 120 | Loss: 2.678 | Acc: 50.529,82.076,99.238,%
Batch: 140 | Loss: 2.689 | Acc: 50.515,81.871,99.197,%
Batch: 160 | Loss: 2.690 | Acc: 50.466,81.750,99.199,%
Batch: 180 | Loss: 2.683 | Acc: 50.729,81.617,99.180,%
Batch: 200 | Loss: 2.681 | Acc: 50.731,81.433,99.157,%
Batch: 220 | Loss: 2.690 | Acc: 50.668,81.324,99.130,%
Batch: 240 | Loss: 2.690 | Acc: 50.603,81.344,99.134,%
Batch: 260 | Loss: 2.690 | Acc: 50.754,81.256,99.138,%
Batch: 280 | Loss: 2.688 | Acc: 50.812,81.242,99.144,%
Batch: 300 | Loss: 2.692 | Acc: 50.799,81.172,99.120,%
Batch: 320 | Loss: 2.697 | Acc: 50.772,81.114,99.099,%
Batch: 340 | Loss: 2.704 | Acc: 50.797,81.046,99.081,%
Batch: 360 | Loss: 2.715 | Acc: 50.734,80.869,99.082,%
Batch: 380 | Loss: 2.715 | Acc: 50.771,80.830,99.040,%
Batch: 0 | Loss: 4.272 | Acc: 51.562,72.656,75.000,%
Batch: 20 | Loss: 4.580 | Acc: 43.638,65.625,72.842,%
Batch: 40 | Loss: 4.554 | Acc: 44.569,65.930,71.761,%
Batch: 60 | Loss: 4.564 | Acc: 44.211,65.663,71.734,%
Train all parameters

Epoch: 159
Batch: 0 | Loss: 3.140 | Acc: 50.781,75.781,100.000,%
Batch: 20 | Loss: 2.760 | Acc: 50.409,82.775,99.368,%
Batch: 40 | Loss: 2.696 | Acc: 50.953,81.822,99.257,%
Batch: 60 | Loss: 2.675 | Acc: 51.716,81.378,99.232,%
Batch: 80 | Loss: 2.682 | Acc: 51.379,80.883,99.171,%
Batch: 100 | Loss: 2.662 | Acc: 51.562,81.026,99.134,%
Batch: 120 | Loss: 2.673 | Acc: 51.440,80.934,99.096,%
Batch: 140 | Loss: 2.685 | Acc: 51.252,81.051,99.047,%
Batch: 160 | Loss: 2.673 | Acc: 51.558,81.119,99.073,%
Batch: 180 | Loss: 2.684 | Acc: 51.200,81.086,99.081,%
Batch: 200 | Loss: 2.673 | Acc: 51.325,81.133,99.110,%
Batch: 220 | Loss: 2.671 | Acc: 51.244,81.091,99.113,%
Batch: 240 | Loss: 2.664 | Acc: 51.287,81.101,99.118,%
Batch: 260 | Loss: 2.650 | Acc: 51.353,81.151,99.099,%
Batch: 280 | Loss: 2.649 | Acc: 51.298,81.133,99.096,%
Batch: 300 | Loss: 2.658 | Acc: 51.119,81.089,99.089,%
Batch: 320 | Loss: 2.655 | Acc: 51.151,81.252,99.107,%
Batch: 340 | Loss: 2.660 | Acc: 51.118,81.149,99.104,%
Batch: 360 | Loss: 2.660 | Acc: 51.182,81.127,99.115,%
Batch: 380 | Loss: 2.667 | Acc: 51.134,81.080,99.100,%
Batch: 0 | Loss: 4.164 | Acc: 49.219,71.875,76.562,%
Batch: 20 | Loss: 4.564 | Acc: 44.792,65.439,72.619,%
Batch: 40 | Loss: 4.561 | Acc: 45.084,65.701,71.646,%
Batch: 60 | Loss: 4.571 | Acc: 44.634,65.612,71.734,%
Train all parameters

Epoch: 160
Batch: 0 | Loss: 2.554 | Acc: 55.469,78.125,98.438,%
Batch: 20 | Loss: 2.818 | Acc: 50.595,79.427,99.219,%
Batch: 40 | Loss: 2.702 | Acc: 51.124,80.926,99.276,%
Batch: 60 | Loss: 2.682 | Acc: 50.897,81.173,99.232,%
Batch: 80 | Loss: 2.672 | Acc: 51.292,81.289,99.190,%
Batch: 100 | Loss: 2.656 | Acc: 51.663,81.498,99.226,%
Batch: 120 | Loss: 2.650 | Acc: 51.401,81.612,99.193,%
Batch: 140 | Loss: 2.658 | Acc: 51.313,81.654,99.102,%
Batch: 160 | Loss: 2.649 | Acc: 51.349,81.760,99.131,%
Batch: 180 | Loss: 2.661 | Acc: 51.243,81.595,99.119,%
Batch: 200 | Loss: 2.665 | Acc: 51.096,81.503,99.102,%
Batch: 220 | Loss: 2.674 | Acc: 50.944,81.381,99.074,%
Batch: 240 | Loss: 2.674 | Acc: 51.021,81.360,99.079,%
Batch: 260 | Loss: 2.684 | Acc: 50.802,81.271,99.096,%
Batch: 280 | Loss: 2.684 | Acc: 50.856,81.139,99.083,%
Batch: 300 | Loss: 2.689 | Acc: 50.825,81.125,99.092,%
Batch: 320 | Loss: 2.682 | Acc: 50.888,81.226,99.085,%
Batch: 340 | Loss: 2.680 | Acc: 50.836,81.275,99.086,%
Batch: 360 | Loss: 2.679 | Acc: 50.857,81.293,99.089,%
Batch: 380 | Loss: 2.687 | Acc: 50.857,81.166,99.077,%
Batch: 0 | Loss: 4.226 | Acc: 50.781,71.094,74.219,%
Batch: 20 | Loss: 4.576 | Acc: 44.159,65.960,71.875,%
Batch: 40 | Loss: 4.569 | Acc: 44.417,65.796,71.418,%
Batch: 60 | Loss: 4.577 | Acc: 44.185,65.651,71.388,%
Train all parameters

Epoch: 161
Batch: 0 | Loss: 2.853 | Acc: 42.188,85.938,99.219,%
Batch: 20 | Loss: 2.701 | Acc: 49.888,83.854,99.293,%
Batch: 40 | Loss: 2.652 | Acc: 51.067,82.832,99.390,%
Batch: 60 | Loss: 2.633 | Acc: 51.306,82.441,99.372,%
Batch: 80 | Loss: 2.627 | Acc: 50.993,82.128,99.354,%
Batch: 100 | Loss: 2.652 | Acc: 51.006,81.675,99.288,%
Batch: 120 | Loss: 2.633 | Acc: 51.227,81.734,99.257,%
Batch: 140 | Loss: 2.644 | Acc: 50.953,81.893,99.274,%
Batch: 160 | Loss: 2.636 | Acc: 51.116,81.949,99.287,%
Batch: 180 | Loss: 2.627 | Acc: 51.407,81.880,99.296,%
Batch: 200 | Loss: 2.625 | Acc: 51.275,81.895,99.312,%
Batch: 220 | Loss: 2.638 | Acc: 51.237,81.713,99.289,%
Batch: 240 | Loss: 2.656 | Acc: 51.177,81.571,99.293,%
Batch: 260 | Loss: 2.668 | Acc: 51.039,81.477,99.300,%
Batch: 280 | Loss: 2.672 | Acc: 50.962,81.461,99.274,%
Batch: 300 | Loss: 2.679 | Acc: 50.880,81.481,99.278,%
Batch: 320 | Loss: 2.678 | Acc: 50.918,81.420,99.265,%
Batch: 340 | Loss: 2.670 | Acc: 51.003,81.532,99.251,%
Batch: 360 | Loss: 2.679 | Acc: 50.970,81.479,99.234,%
Batch: 380 | Loss: 2.682 | Acc: 50.953,81.467,99.241,%
Batch: 0 | Loss: 4.193 | Acc: 52.344,71.094,74.219,%
Batch: 20 | Loss: 4.563 | Acc: 44.234,66.257,71.652,%
Batch: 40 | Loss: 4.565 | Acc: 44.455,65.720,71.189,%
Batch: 60 | Loss: 4.579 | Acc: 44.301,65.292,71.299,%
Train all parameters

Epoch: 162
Batch: 0 | Loss: 2.066 | Acc: 59.375,82.812,99.219,%
Batch: 20 | Loss: 2.681 | Acc: 52.381,81.696,99.442,%
Batch: 40 | Loss: 2.676 | Acc: 52.268,82.412,99.371,%
Batch: 60 | Loss: 2.683 | Acc: 51.857,81.954,99.347,%
Batch: 80 | Loss: 2.650 | Acc: 51.746,82.108,99.277,%
Batch: 100 | Loss: 2.621 | Acc: 51.648,82.225,99.257,%
Batch: 120 | Loss: 2.632 | Acc: 51.259,82.147,99.257,%
Batch: 140 | Loss: 2.649 | Acc: 51.097,81.954,99.241,%
Batch: 160 | Loss: 2.635 | Acc: 51.063,82.012,99.238,%
Batch: 180 | Loss: 2.633 | Acc: 51.075,82.040,99.245,%
Batch: 200 | Loss: 2.647 | Acc: 50.882,81.961,99.215,%
Batch: 220 | Loss: 2.656 | Acc: 50.813,81.964,99.194,%
Batch: 240 | Loss: 2.665 | Acc: 50.742,81.691,99.193,%
Batch: 260 | Loss: 2.672 | Acc: 50.682,81.675,99.195,%
Batch: 280 | Loss: 2.672 | Acc: 50.706,81.631,99.202,%
Batch: 300 | Loss: 2.668 | Acc: 50.766,81.559,99.198,%
Batch: 320 | Loss: 2.669 | Acc: 50.696,81.615,99.185,%
Batch: 340 | Loss: 2.664 | Acc: 50.735,81.665,99.189,%
Batch: 360 | Loss: 2.664 | Acc: 50.701,81.642,99.184,%
Batch: 380 | Loss: 2.666 | Acc: 50.646,81.601,99.172,%
Batch: 0 | Loss: 4.245 | Acc: 54.688,71.875,74.219,%
Batch: 20 | Loss: 4.546 | Acc: 44.717,66.183,72.917,%
Batch: 40 | Loss: 4.545 | Acc: 45.293,65.816,72.027,%
Batch: 60 | Loss: 4.556 | Acc: 44.647,65.689,71.811,%
Train all parameters

Epoch: 163
Batch: 0 | Loss: 2.371 | Acc: 52.344,80.469,100.000,%
Batch: 20 | Loss: 2.570 | Acc: 52.865,83.333,99.368,%
Batch: 40 | Loss: 2.547 | Acc: 51.848,83.327,99.276,%
Batch: 60 | Loss: 2.568 | Acc: 51.524,82.582,99.193,%
Batch: 80 | Loss: 2.622 | Acc: 50.829,82.436,99.180,%
Batch: 100 | Loss: 2.644 | Acc: 50.657,82.039,99.165,%
Batch: 120 | Loss: 2.636 | Acc: 50.775,81.799,99.251,%
Batch: 140 | Loss: 2.630 | Acc: 50.693,81.865,99.235,%
Batch: 160 | Loss: 2.624 | Acc: 50.985,81.910,99.258,%
Batch: 180 | Loss: 2.626 | Acc: 51.023,81.906,99.262,%
Batch: 200 | Loss: 2.627 | Acc: 50.999,81.957,99.230,%
Batch: 220 | Loss: 2.626 | Acc: 51.004,82.007,99.229,%
Batch: 240 | Loss: 2.622 | Acc: 51.054,82.047,99.222,%
Batch: 260 | Loss: 2.637 | Acc: 50.985,81.909,99.237,%
Batch: 280 | Loss: 2.640 | Acc: 50.954,81.837,99.238,%
Batch: 300 | Loss: 2.642 | Acc: 51.023,81.741,99.221,%
Batch: 320 | Loss: 2.643 | Acc: 51.081,81.671,99.185,%
Batch: 340 | Loss: 2.638 | Acc: 51.107,81.621,99.171,%
Batch: 360 | Loss: 2.640 | Acc: 51.175,81.575,99.178,%
Batch: 380 | Loss: 2.638 | Acc: 51.226,81.605,99.161,%
Batch: 0 | Loss: 4.093 | Acc: 50.781,75.000,76.562,%
Batch: 20 | Loss: 4.534 | Acc: 44.680,66.034,72.470,%
Batch: 40 | Loss: 4.533 | Acc: 44.893,65.892,71.665,%
Batch: 60 | Loss: 4.552 | Acc: 44.595,65.625,71.580,%
Train all parameters

Epoch: 164
Batch: 0 | Loss: 3.349 | Acc: 47.656,74.219,100.000,%
Batch: 20 | Loss: 2.626 | Acc: 50.781,82.068,99.219,%
Batch: 40 | Loss: 2.561 | Acc: 52.058,82.774,99.524,%
Batch: 60 | Loss: 2.595 | Acc: 51.972,82.723,99.539,%
Batch: 80 | Loss: 2.583 | Acc: 51.929,82.687,99.547,%
Batch: 100 | Loss: 2.602 | Acc: 51.609,82.433,99.459,%
Batch: 120 | Loss: 2.614 | Acc: 51.479,82.386,99.425,%
Batch: 140 | Loss: 2.616 | Acc: 51.457,82.325,99.379,%
Batch: 160 | Loss: 2.627 | Acc: 51.368,82.046,99.340,%
Batch: 180 | Loss: 2.627 | Acc: 51.433,82.049,99.292,%
Batch: 200 | Loss: 2.628 | Acc: 51.430,81.864,99.296,%
Batch: 220 | Loss: 2.617 | Acc: 51.485,81.975,99.268,%
Batch: 240 | Loss: 2.615 | Acc: 51.553,81.963,99.238,%
Batch: 260 | Loss: 2.612 | Acc: 51.670,81.971,99.237,%
Batch: 280 | Loss: 2.612 | Acc: 51.727,82.048,99.224,%
Batch: 300 | Loss: 2.616 | Acc: 51.599,82.034,99.211,%
Batch: 320 | Loss: 2.609 | Acc: 51.682,82.063,99.221,%
Batch: 340 | Loss: 2.609 | Acc: 51.606,82.018,99.216,%
Batch: 360 | Loss: 2.616 | Acc: 51.582,81.997,99.206,%
Batch: 380 | Loss: 2.615 | Acc: 51.573,81.982,99.196,%
Batch: 0 | Loss: 4.337 | Acc: 48.438,69.531,74.219,%
Batch: 20 | Loss: 4.621 | Acc: 43.936,64.509,71.577,%
Batch: 40 | Loss: 4.614 | Acc: 44.341,64.405,70.560,%
Batch: 60 | Loss: 4.617 | Acc: 43.916,64.639,70.761,%
Train all parameters

Epoch: 165
Batch: 0 | Loss: 3.159 | Acc: 52.344,81.250,99.219,%
Batch: 20 | Loss: 2.527 | Acc: 52.493,82.589,99.144,%
Batch: 40 | Loss: 2.602 | Acc: 52.287,82.927,99.143,%
Batch: 60 | Loss: 2.645 | Acc: 51.831,82.428,99.078,%
Batch: 80 | Loss: 2.669 | Acc: 51.292,82.311,99.199,%
Batch: 100 | Loss: 2.636 | Acc: 51.570,82.449,99.226,%
Batch: 120 | Loss: 2.651 | Acc: 51.214,82.367,99.277,%
Batch: 140 | Loss: 2.647 | Acc: 51.341,82.164,99.291,%
Batch: 160 | Loss: 2.660 | Acc: 51.417,81.973,99.277,%
Batch: 180 | Loss: 2.663 | Acc: 51.485,81.854,99.271,%
Batch: 200 | Loss: 2.657 | Acc: 51.531,81.845,99.269,%
Batch: 220 | Loss: 2.666 | Acc: 51.312,81.702,99.243,%
Batch: 240 | Loss: 2.663 | Acc: 51.287,81.714,99.248,%
Batch: 260 | Loss: 2.659 | Acc: 51.278,81.744,99.234,%
Batch: 280 | Loss: 2.647 | Acc: 51.360,81.995,99.230,%
Batch: 300 | Loss: 2.647 | Acc: 51.376,81.930,99.247,%
Batch: 320 | Loss: 2.650 | Acc: 51.314,81.880,99.226,%
Batch: 340 | Loss: 2.651 | Acc: 51.336,81.823,99.207,%
Batch: 360 | Loss: 2.647 | Acc: 51.342,81.891,99.208,%
Batch: 380 | Loss: 2.647 | Acc: 51.300,81.828,99.186,%
Batch: 0 | Loss: 4.189 | Acc: 50.781,69.531,75.781,%
Batch: 20 | Loss: 4.578 | Acc: 43.787,65.141,71.949,%
Batch: 40 | Loss: 4.570 | Acc: 44.588,65.263,71.208,%
Batch: 60 | Loss: 4.583 | Acc: 44.160,65.202,71.030,%
Train all parameters

Epoch: 166
Batch: 0 | Loss: 2.309 | Acc: 56.250,81.250,99.219,%
Batch: 20 | Loss: 2.451 | Acc: 52.307,83.519,99.219,%
Batch: 40 | Loss: 2.467 | Acc: 52.611,83.403,99.352,%
Batch: 60 | Loss: 2.527 | Acc: 52.446,83.543,99.436,%
Batch: 80 | Loss: 2.528 | Acc: 52.488,83.218,99.450,%
Batch: 100 | Loss: 2.565 | Acc: 52.104,82.805,99.459,%
Batch: 120 | Loss: 2.560 | Acc: 52.131,82.677,99.496,%
Batch: 140 | Loss: 2.558 | Acc: 52.133,82.602,99.490,%
Batch: 160 | Loss: 2.556 | Acc: 52.290,82.618,99.505,%
Batch: 180 | Loss: 2.578 | Acc: 51.942,82.299,99.482,%
Batch: 200 | Loss: 2.602 | Acc: 51.679,82.292,99.471,%
Batch: 220 | Loss: 2.590 | Acc: 51.916,82.346,99.480,%
Batch: 240 | Loss: 2.592 | Acc: 52.003,82.258,99.436,%
Batch: 260 | Loss: 2.603 | Acc: 51.826,82.157,99.407,%
Batch: 280 | Loss: 2.604 | Acc: 51.818,82.001,99.405,%
Batch: 300 | Loss: 2.602 | Acc: 51.765,81.951,99.377,%
Batch: 320 | Loss: 2.598 | Acc: 51.813,81.934,99.367,%
Batch: 340 | Loss: 2.598 | Acc: 51.762,81.944,99.363,%
Batch: 360 | Loss: 2.595 | Acc: 51.792,81.934,99.364,%
Batch: 380 | Loss: 2.591 | Acc: 51.796,81.904,99.346,%
Batch: 0 | Loss: 4.297 | Acc: 52.344,71.094,76.562,%
Batch: 20 | Loss: 4.562 | Acc: 45.052,66.109,71.987,%
Batch: 40 | Loss: 4.551 | Acc: 45.484,66.025,71.532,%
Batch: 60 | Loss: 4.558 | Acc: 45.172,65.881,71.644,%
Train all parameters

Epoch: 167
Batch: 0 | Loss: 3.181 | Acc: 51.562,83.594,99.219,%
Batch: 20 | Loss: 2.583 | Acc: 51.860,81.362,99.182,%
Batch: 40 | Loss: 2.639 | Acc: 51.143,81.536,99.238,%
Batch: 60 | Loss: 2.578 | Acc: 51.767,81.737,99.244,%
Batch: 80 | Loss: 2.560 | Acc: 51.977,81.964,99.344,%
Batch: 100 | Loss: 2.583 | Acc: 51.996,82.054,99.404,%
Batch: 120 | Loss: 2.592 | Acc: 51.795,82.122,99.432,%
Batch: 140 | Loss: 2.597 | Acc: 51.718,81.959,99.451,%
Batch: 160 | Loss: 2.590 | Acc: 51.805,82.026,99.457,%
Batch: 180 | Loss: 2.580 | Acc: 51.895,82.100,99.435,%
Batch: 200 | Loss: 2.586 | Acc: 51.866,82.016,99.401,%
Batch: 220 | Loss: 2.598 | Acc: 51.828,81.957,99.403,%
Batch: 240 | Loss: 2.591 | Acc: 51.903,82.070,99.384,%
Batch: 260 | Loss: 2.589 | Acc: 51.898,82.025,99.374,%
Batch: 280 | Loss: 2.591 | Acc: 51.935,82.034,99.352,%
Batch: 300 | Loss: 2.590 | Acc: 51.890,82.109,99.338,%
Batch: 320 | Loss: 2.583 | Acc: 51.984,82.082,99.328,%
Batch: 340 | Loss: 2.591 | Acc: 51.808,81.999,99.320,%
Batch: 360 | Loss: 2.593 | Acc: 51.770,81.899,99.294,%
Batch: 380 | Loss: 2.596 | Acc: 51.727,81.933,99.266,%
Batch: 0 | Loss: 4.218 | Acc: 51.562,73.438,75.781,%
Batch: 20 | Loss: 4.605 | Acc: 44.234,65.923,72.173,%
Batch: 40 | Loss: 4.568 | Acc: 44.855,65.758,71.532,%
Batch: 60 | Loss: 4.582 | Acc: 44.390,65.407,71.555,%
Train all parameters

Epoch: 168
Batch: 0 | Loss: 2.874 | Acc: 50.000,85.156,98.438,%
Batch: 20 | Loss: 2.681 | Acc: 51.935,81.324,98.996,%
Batch: 40 | Loss: 2.660 | Acc: 51.639,81.517,99.104,%
Batch: 60 | Loss: 2.622 | Acc: 51.575,81.583,99.078,%
Batch: 80 | Loss: 2.630 | Acc: 51.061,81.993,99.132,%
Batch: 100 | Loss: 2.612 | Acc: 51.261,82.116,99.110,%
Batch: 120 | Loss: 2.601 | Acc: 51.246,82.283,99.057,%
Batch: 140 | Loss: 2.602 | Acc: 51.335,82.425,99.097,%
Batch: 160 | Loss: 2.591 | Acc: 51.514,82.347,99.088,%
Batch: 180 | Loss: 2.604 | Acc: 51.476,82.264,99.128,%
Batch: 200 | Loss: 2.608 | Acc: 51.508,82.187,99.172,%
Batch: 220 | Loss: 2.618 | Acc: 51.421,82.190,99.173,%
Batch: 240 | Loss: 2.619 | Acc: 51.462,82.174,99.164,%
Batch: 260 | Loss: 2.617 | Acc: 51.467,82.262,99.183,%
Batch: 280 | Loss: 2.612 | Acc: 51.446,82.334,99.202,%
Batch: 300 | Loss: 2.612 | Acc: 51.464,82.338,99.206,%
Batch: 320 | Loss: 2.619 | Acc: 51.385,82.292,99.214,%
Batch: 340 | Loss: 2.619 | Acc: 51.478,82.192,99.205,%
Batch: 360 | Loss: 2.611 | Acc: 51.595,82.172,99.206,%
Batch: 380 | Loss: 2.604 | Acc: 51.622,82.210,99.215,%
Batch: 0 | Loss: 4.151 | Acc: 53.125,72.656,75.000,%
Batch: 20 | Loss: 4.575 | Acc: 44.159,65.476,72.024,%
Batch: 40 | Loss: 4.584 | Acc: 44.245,65.358,71.227,%
Batch: 60 | Loss: 4.597 | Acc: 43.993,64.946,71.196,%
Train all parameters

Epoch: 169
Batch: 0 | Loss: 2.784 | Acc: 51.562,86.719,99.219,%
Batch: 20 | Loss: 2.489 | Acc: 53.795,83.557,99.219,%
Batch: 40 | Loss: 2.542 | Acc: 52.248,83.460,99.276,%
Batch: 60 | Loss: 2.576 | Acc: 52.164,82.812,99.283,%
Batch: 80 | Loss: 2.582 | Acc: 52.016,82.436,99.296,%
Batch: 100 | Loss: 2.603 | Acc: 51.880,82.573,99.296,%
Batch: 120 | Loss: 2.591 | Acc: 51.905,82.477,99.309,%
Batch: 140 | Loss: 2.591 | Acc: 51.873,82.613,99.324,%
Batch: 160 | Loss: 2.606 | Acc: 51.791,82.580,99.335,%
Batch: 180 | Loss: 2.609 | Acc: 51.679,82.515,99.353,%
Batch: 200 | Loss: 2.601 | Acc: 51.714,82.622,99.363,%
Batch: 220 | Loss: 2.607 | Acc: 51.707,82.600,99.364,%
Batch: 240 | Loss: 2.610 | Acc: 51.702,82.443,99.352,%
Batch: 260 | Loss: 2.616 | Acc: 51.551,82.387,99.359,%
Batch: 280 | Loss: 2.616 | Acc: 51.610,82.276,99.361,%
Batch: 300 | Loss: 2.613 | Acc: 51.505,82.275,99.351,%
Batch: 320 | Loss: 2.615 | Acc: 51.441,82.138,99.323,%
Batch: 340 | Loss: 2.616 | Acc: 51.386,82.189,99.320,%
Batch: 360 | Loss: 2.609 | Acc: 51.500,82.207,99.327,%
Batch: 380 | Loss: 2.610 | Acc: 51.439,82.158,99.319,%
Batch: 0 | Loss: 4.143 | Acc: 51.562,74.219,77.344,%
Batch: 20 | Loss: 4.562 | Acc: 44.568,66.146,71.577,%
Batch: 40 | Loss: 4.551 | Acc: 45.236,65.587,71.037,%
Batch: 60 | Loss: 4.566 | Acc: 44.672,65.356,71.196,%
Train all parameters

Epoch: 170
Batch: 0 | Loss: 2.798 | Acc: 42.188,81.250,98.438,%
Batch: 20 | Loss: 2.522 | Acc: 51.935,83.110,99.182,%
Batch: 40 | Loss: 2.572 | Acc: 51.334,83.365,99.257,%
Batch: 60 | Loss: 2.557 | Acc: 51.665,83.261,99.257,%
Batch: 80 | Loss: 2.579 | Acc: 51.611,83.005,99.286,%
Batch: 100 | Loss: 2.596 | Acc: 51.416,83.114,99.234,%
Batch: 120 | Loss: 2.585 | Acc: 51.433,83.103,99.232,%
Batch: 140 | Loss: 2.562 | Acc: 51.773,83.051,99.213,%
Batch: 160 | Loss: 2.574 | Acc: 51.543,82.822,99.214,%
Batch: 180 | Loss: 2.584 | Acc: 51.511,82.705,99.180,%
Batch: 200 | Loss: 2.586 | Acc: 51.555,82.641,99.180,%
Batch: 220 | Loss: 2.593 | Acc: 51.485,82.625,99.183,%
Batch: 240 | Loss: 2.607 | Acc: 51.349,82.534,99.186,%
Batch: 260 | Loss: 2.623 | Acc: 51.170,82.405,99.165,%
Batch: 280 | Loss: 2.624 | Acc: 51.134,82.432,99.160,%
Batch: 300 | Loss: 2.624 | Acc: 51.222,82.325,99.159,%
Batch: 320 | Loss: 2.607 | Acc: 51.436,82.353,99.163,%
Batch: 340 | Loss: 2.610 | Acc: 51.448,82.297,99.159,%
Batch: 360 | Loss: 2.614 | Acc: 51.402,82.256,99.167,%
Batch: 380 | Loss: 2.612 | Acc: 51.413,82.245,99.167,%
Batch: 0 | Loss: 4.222 | Acc: 50.781,71.094,76.562,%
Batch: 20 | Loss: 4.607 | Acc: 43.862,65.476,71.987,%
Batch: 40 | Loss: 4.609 | Acc: 44.074,64.977,70.941,%
Batch: 60 | Loss: 4.613 | Acc: 43.827,64.664,71.299,%
Train all parameters

Epoch: 171
Batch: 0 | Loss: 2.046 | Acc: 53.906,88.281,99.219,%
Batch: 20 | Loss: 2.615 | Acc: 51.600,82.254,99.442,%
Batch: 40 | Loss: 2.661 | Acc: 51.505,82.736,99.390,%
Batch: 60 | Loss: 2.626 | Acc: 51.524,82.710,99.449,%
Batch: 80 | Loss: 2.628 | Acc: 51.476,82.851,99.479,%
Batch: 100 | Loss: 2.666 | Acc: 50.882,82.325,99.451,%
Batch: 120 | Loss: 2.662 | Acc: 51.001,82.438,99.458,%
Batch: 140 | Loss: 2.646 | Acc: 50.947,82.668,99.435,%
Batch: 160 | Loss: 2.646 | Acc: 51.106,82.478,99.423,%
Batch: 180 | Loss: 2.635 | Acc: 51.308,82.476,99.417,%
Batch: 200 | Loss: 2.624 | Acc: 51.493,82.486,99.374,%
Batch: 220 | Loss: 2.629 | Acc: 51.506,82.455,99.357,%
Batch: 240 | Loss: 2.623 | Acc: 51.572,82.492,99.332,%
Batch: 260 | Loss: 2.611 | Acc: 51.565,82.465,99.350,%
Batch: 280 | Loss: 2.610 | Acc: 51.479,82.496,99.352,%
Batch: 300 | Loss: 2.613 | Acc: 51.565,82.465,99.343,%
Batch: 320 | Loss: 2.615 | Acc: 51.545,82.328,99.340,%
Batch: 340 | Loss: 2.612 | Acc: 51.592,82.338,99.329,%
Batch: 360 | Loss: 2.621 | Acc: 51.459,82.293,99.316,%
Batch: 380 | Loss: 2.626 | Acc: 51.374,82.349,99.305,%
Batch: 0 | Loss: 4.174 | Acc: 50.000,71.094,78.125,%
Batch: 20 | Loss: 4.540 | Acc: 44.978,65.699,71.838,%
Batch: 40 | Loss: 4.553 | Acc: 45.312,65.682,71.189,%
Batch: 60 | Loss: 4.556 | Acc: 44.903,65.510,71.388,%
Train all parameters

Epoch: 172
Batch: 0 | Loss: 2.193 | Acc: 60.156,80.469,100.000,%
Batch: 20 | Loss: 2.589 | Acc: 51.897,83.333,99.479,%
Batch: 40 | Loss: 2.599 | Acc: 51.410,83.365,99.333,%
Batch: 60 | Loss: 2.599 | Acc: 51.665,83.478,99.308,%
Batch: 80 | Loss: 2.607 | Acc: 51.524,83.420,99.334,%
Batch: 100 | Loss: 2.595 | Acc: 51.431,83.369,99.389,%
Batch: 120 | Loss: 2.592 | Acc: 51.498,83.323,99.419,%
Batch: 140 | Loss: 2.617 | Acc: 51.297,83.067,99.440,%
Batch: 160 | Loss: 2.617 | Acc: 51.213,82.861,99.384,%
Batch: 180 | Loss: 2.631 | Acc: 51.114,82.830,99.378,%
Batch: 200 | Loss: 2.628 | Acc: 51.158,82.789,99.347,%
Batch: 220 | Loss: 2.619 | Acc: 51.312,82.759,99.342,%
Batch: 240 | Loss: 2.606 | Acc: 51.362,82.835,99.342,%
Batch: 260 | Loss: 2.599 | Acc: 51.428,82.908,99.306,%
Batch: 280 | Loss: 2.607 | Acc: 51.301,82.851,99.319,%
Batch: 300 | Loss: 2.612 | Acc: 51.207,82.740,99.312,%
Batch: 320 | Loss: 2.616 | Acc: 51.283,82.596,99.309,%
Batch: 340 | Loss: 2.614 | Acc: 51.404,82.515,99.313,%
Batch: 360 | Loss: 2.610 | Acc: 51.435,82.462,99.297,%
Batch: 380 | Loss: 2.613 | Acc: 51.368,82.380,99.276,%
Batch: 0 | Loss: 4.181 | Acc: 52.344,70.312,77.344,%
Batch: 20 | Loss: 4.530 | Acc: 44.829,66.629,72.470,%
Batch: 40 | Loss: 4.539 | Acc: 45.370,66.006,71.494,%
Batch: 60 | Loss: 4.554 | Acc: 45.005,65.651,71.683,%
Train all parameters

Epoch: 173
Batch: 0 | Loss: 3.043 | Acc: 49.219,84.375,98.438,%
Batch: 20 | Loss: 2.484 | Acc: 52.939,84.263,99.702,%
Batch: 40 | Loss: 2.562 | Acc: 51.810,83.498,99.638,%
Batch: 60 | Loss: 2.542 | Acc: 51.473,83.197,99.590,%
Batch: 80 | Loss: 2.547 | Acc: 51.717,82.957,99.518,%
Batch: 100 | Loss: 2.538 | Acc: 52.197,83.137,99.474,%
Batch: 120 | Loss: 2.584 | Acc: 51.679,82.948,99.496,%
Batch: 140 | Loss: 2.566 | Acc: 52.133,83.045,99.501,%
Batch: 160 | Loss: 2.566 | Acc: 52.169,82.764,99.466,%
Batch: 180 | Loss: 2.563 | Acc: 52.175,82.748,99.443,%
Batch: 200 | Loss: 2.571 | Acc: 52.064,82.739,99.425,%
Batch: 220 | Loss: 2.574 | Acc: 51.994,82.678,99.410,%
Batch: 240 | Loss: 2.579 | Acc: 51.939,82.702,99.407,%
Batch: 260 | Loss: 2.571 | Acc: 52.086,82.603,99.383,%
Batch: 280 | Loss: 2.575 | Acc: 52.035,82.559,99.383,%
Batch: 300 | Loss: 2.572 | Acc: 52.006,82.561,99.377,%
Batch: 320 | Loss: 2.573 | Acc: 51.872,82.496,99.348,%
Batch: 340 | Loss: 2.577 | Acc: 51.872,82.517,99.352,%
Batch: 360 | Loss: 2.576 | Acc: 51.924,82.492,99.327,%
Batch: 380 | Loss: 2.584 | Acc: 51.841,82.472,99.321,%
Batch: 0 | Loss: 4.294 | Acc: 52.344,72.656,74.219,%
Batch: 20 | Loss: 4.573 | Acc: 44.792,65.848,71.875,%
Batch: 40 | Loss: 4.580 | Acc: 44.398,65.149,71.265,%
Batch: 60 | Loss: 4.591 | Acc: 44.147,64.972,71.158,%
Train all parameters

Epoch: 174
Batch: 0 | Loss: 2.187 | Acc: 57.812,84.375,99.219,%
Batch: 20 | Loss: 2.485 | Acc: 53.013,82.292,99.182,%
Batch: 40 | Loss: 2.487 | Acc: 52.992,82.851,99.390,%
Batch: 60 | Loss: 2.501 | Acc: 52.792,83.094,99.398,%
Batch: 80 | Loss: 2.567 | Acc: 51.977,82.957,99.392,%
Batch: 100 | Loss: 2.553 | Acc: 52.065,83.130,99.404,%
Batch: 120 | Loss: 2.571 | Acc: 51.956,83.122,99.412,%
Batch: 140 | Loss: 2.571 | Acc: 51.828,83.029,99.429,%
Batch: 160 | Loss: 2.586 | Acc: 51.616,82.866,99.452,%
Batch: 180 | Loss: 2.576 | Acc: 51.813,82.869,99.426,%
Batch: 200 | Loss: 2.558 | Acc: 52.052,83.011,99.425,%
Batch: 220 | Loss: 2.548 | Acc: 52.241,82.982,99.413,%
Batch: 240 | Loss: 2.542 | Acc: 52.396,82.939,99.404,%
Batch: 260 | Loss: 2.546 | Acc: 52.308,82.839,99.380,%
Batch: 280 | Loss: 2.559 | Acc: 52.208,82.618,99.386,%
Batch: 300 | Loss: 2.572 | Acc: 52.048,82.607,99.393,%
Batch: 320 | Loss: 2.576 | Acc: 51.954,82.579,99.394,%
Batch: 340 | Loss: 2.581 | Acc: 51.936,82.597,99.379,%
Batch: 360 | Loss: 2.593 | Acc: 51.913,82.440,99.349,%
Batch: 380 | Loss: 2.591 | Acc: 51.925,82.372,99.348,%
Batch: 0 | Loss: 4.169 | Acc: 52.344,71.875,75.000,%
Batch: 20 | Loss: 4.500 | Acc: 45.424,66.295,72.284,%
Batch: 40 | Loss: 4.511 | Acc: 45.732,66.025,71.284,%
Batch: 60 | Loss: 4.518 | Acc: 45.389,65.856,71.427,%
Train all parameters

Epoch: 175
Batch: 0 | Loss: 2.948 | Acc: 50.000,77.344,99.219,%
Batch: 20 | Loss: 2.443 | Acc: 51.935,85.305,99.330,%
Batch: 40 | Loss: 2.527 | Acc: 52.039,84.070,99.257,%
Batch: 60 | Loss: 2.499 | Acc: 52.497,84.119,99.270,%
Batch: 80 | Loss: 2.531 | Acc: 52.459,83.796,99.267,%
Batch: 100 | Loss: 2.520 | Acc: 52.661,83.725,99.312,%
Batch: 120 | Loss: 2.520 | Acc: 52.608,83.478,99.348,%
Batch: 140 | Loss: 2.528 | Acc: 52.272,83.494,99.335,%
Batch: 160 | Loss: 2.546 | Acc: 52.023,83.419,99.335,%
Batch: 180 | Loss: 2.564 | Acc: 51.921,83.227,99.353,%
Batch: 200 | Loss: 2.570 | Acc: 51.885,83.201,99.355,%
Batch: 220 | Loss: 2.560 | Acc: 51.941,83.198,99.360,%
Batch: 240 | Loss: 2.562 | Acc: 51.874,83.130,99.348,%
Batch: 260 | Loss: 2.561 | Acc: 51.859,83.148,99.344,%
Batch: 280 | Loss: 2.567 | Acc: 51.857,83.060,99.316,%
Batch: 300 | Loss: 2.562 | Acc: 51.900,82.984,99.310,%
Batch: 320 | Loss: 2.561 | Acc: 51.957,82.959,99.311,%
Batch: 340 | Loss: 2.566 | Acc: 51.918,82.884,99.306,%
Batch: 360 | Loss: 2.571 | Acc: 51.896,82.877,99.307,%
Batch: 380 | Loss: 2.575 | Acc: 51.870,82.903,99.303,%
Batch: 0 | Loss: 4.216 | Acc: 50.781,69.531,76.562,%
Batch: 20 | Loss: 4.576 | Acc: 44.866,65.253,71.949,%
Batch: 40 | Loss: 4.575 | Acc: 45.084,65.263,70.960,%
Batch: 60 | Loss: 4.587 | Acc: 44.813,65.164,70.799,%
Train all parameters

Epoch: 176
Batch: 0 | Loss: 1.870 | Acc: 53.906,89.844,100.000,%
Batch: 20 | Loss: 2.497 | Acc: 52.418,85.007,99.702,%
Batch: 40 | Loss: 2.499 | Acc: 51.429,84.070,99.581,%
Batch: 60 | Loss: 2.551 | Acc: 51.332,83.389,99.565,%
Batch: 80 | Loss: 2.533 | Acc: 51.591,83.690,99.489,%
Batch: 100 | Loss: 2.521 | Acc: 51.887,83.648,99.459,%
Batch: 120 | Loss: 2.527 | Acc: 51.956,83.626,99.464,%
Batch: 140 | Loss: 2.534 | Acc: 51.834,83.738,99.457,%
Batch: 160 | Loss: 2.518 | Acc: 51.985,83.812,99.432,%
Batch: 180 | Loss: 2.527 | Acc: 52.076,83.576,99.391,%
Batch: 200 | Loss: 2.539 | Acc: 51.807,83.477,99.394,%
Batch: 220 | Loss: 2.539 | Acc: 51.821,83.353,99.410,%
Batch: 240 | Loss: 2.539 | Acc: 51.958,83.289,99.407,%
Batch: 260 | Loss: 2.534 | Acc: 52.134,83.169,99.407,%
Batch: 280 | Loss: 2.547 | Acc: 51.996,83.021,99.408,%
Batch: 300 | Loss: 2.542 | Acc: 52.115,83.031,99.390,%
Batch: 320 | Loss: 2.543 | Acc: 52.154,83.002,99.379,%
Batch: 340 | Loss: 2.550 | Acc: 52.055,82.982,99.372,%
Batch: 360 | Loss: 2.554 | Acc: 52.010,82.932,99.368,%
Batch: 380 | Loss: 2.553 | Acc: 51.917,82.915,99.336,%
Batch: 0 | Loss: 4.218 | Acc: 54.688,72.656,74.219,%
Batch: 20 | Loss: 4.530 | Acc: 45.089,66.369,71.801,%
Batch: 40 | Loss: 4.554 | Acc: 45.084,65.816,70.732,%
Batch: 60 | Loss: 4.578 | Acc: 44.480,65.459,70.658,%
Train all parameters

Epoch: 177
Batch: 0 | Loss: 2.469 | Acc: 53.906,83.594,99.219,%
Batch: 20 | Loss: 2.540 | Acc: 51.860,83.445,99.554,%
Batch: 40 | Loss: 2.537 | Acc: 52.344,83.518,99.486,%
Batch: 60 | Loss: 2.572 | Acc: 52.382,82.953,99.501,%
Batch: 80 | Loss: 2.536 | Acc: 52.382,83.603,99.508,%
Batch: 100 | Loss: 2.534 | Acc: 52.351,83.455,99.404,%
Batch: 120 | Loss: 2.539 | Acc: 52.118,83.542,99.419,%
Batch: 140 | Loss: 2.559 | Acc: 51.934,83.306,99.413,%
Batch: 160 | Loss: 2.552 | Acc: 52.193,83.094,99.423,%
Batch: 180 | Loss: 2.542 | Acc: 52.344,83.184,99.391,%
Batch: 200 | Loss: 2.535 | Acc: 52.371,83.228,99.401,%
Batch: 220 | Loss: 2.525 | Acc: 52.404,83.286,99.374,%
Batch: 240 | Loss: 2.524 | Acc: 52.435,83.263,99.365,%
Batch: 260 | Loss: 2.520 | Acc: 52.416,83.241,99.368,%
Batch: 280 | Loss: 2.536 | Acc: 52.224,83.093,99.369,%
Batch: 300 | Loss: 2.535 | Acc: 52.230,83.145,99.349,%
Batch: 320 | Loss: 2.547 | Acc: 52.125,83.010,99.345,%
Batch: 340 | Loss: 2.556 | Acc: 52.060,82.943,99.349,%
Batch: 360 | Loss: 2.553 | Acc: 52.028,82.986,99.342,%
Batch: 380 | Loss: 2.553 | Acc: 52.112,82.925,99.350,%
Batch: 0 | Loss: 4.072 | Acc: 52.344,72.656,78.906,%
Batch: 20 | Loss: 4.569 | Acc: 45.424,65.625,71.838,%
Batch: 40 | Loss: 4.572 | Acc: 45.389,65.492,70.998,%
Batch: 60 | Loss: 4.570 | Acc: 45.300,65.484,71.132,%
Train all parameters

Epoch: 178
Batch: 0 | Loss: 2.872 | Acc: 40.625,85.938,99.219,%
Batch: 20 | Loss: 2.483 | Acc: 51.376,84.077,99.479,%
Batch: 40 | Loss: 2.507 | Acc: 51.582,84.127,99.371,%
Batch: 60 | Loss: 2.512 | Acc: 52.024,83.824,99.411,%
Batch: 80 | Loss: 2.524 | Acc: 51.900,83.777,99.421,%
Batch: 100 | Loss: 2.509 | Acc: 52.344,83.679,99.451,%
Batch: 120 | Loss: 2.516 | Acc: 52.144,83.891,99.425,%
Batch: 140 | Loss: 2.517 | Acc: 51.995,83.887,99.429,%
Batch: 160 | Loss: 2.499 | Acc: 52.125,83.987,99.442,%
Batch: 180 | Loss: 2.494 | Acc: 52.188,83.995,99.422,%
Batch: 200 | Loss: 2.512 | Acc: 52.013,83.780,99.401,%
Batch: 220 | Loss: 2.526 | Acc: 51.898,83.626,99.399,%
Batch: 240 | Loss: 2.525 | Acc: 51.948,83.603,99.420,%
Batch: 260 | Loss: 2.529 | Acc: 51.928,83.543,99.401,%
Batch: 280 | Loss: 2.537 | Acc: 51.882,83.460,99.405,%
Batch: 300 | Loss: 2.540 | Acc: 52.022,83.355,99.413,%
Batch: 320 | Loss: 2.545 | Acc: 51.947,83.348,99.399,%
Batch: 340 | Loss: 2.553 | Acc: 51.897,83.294,99.391,%
Batch: 360 | Loss: 2.562 | Acc: 51.842,83.219,99.392,%
Batch: 380 | Loss: 2.567 | Acc: 51.903,83.098,99.377,%
Batch: 0 | Loss: 4.136 | Acc: 50.781,73.438,78.125,%
Batch: 20 | Loss: 4.546 | Acc: 43.936,66.964,72.879,%
Batch: 40 | Loss: 4.542 | Acc: 44.665,66.139,71.970,%
Batch: 60 | Loss: 4.546 | Acc: 44.416,65.996,71.773,%
Train all parameters

Epoch: 179
Batch: 0 | Loss: 3.138 | Acc: 51.562,82.031,99.219,%
Batch: 20 | Loss: 2.440 | Acc: 52.753,84.673,99.368,%
Batch: 40 | Loss: 2.478 | Acc: 52.172,84.299,99.390,%
Batch: 60 | Loss: 2.508 | Acc: 52.267,84.119,99.385,%
Batch: 80 | Loss: 2.525 | Acc: 51.939,83.864,99.412,%
Batch: 100 | Loss: 2.540 | Acc: 51.926,83.733,99.397,%
Batch: 120 | Loss: 2.543 | Acc: 51.931,83.529,99.419,%
Batch: 140 | Loss: 2.546 | Acc: 52.078,83.428,99.391,%
Batch: 160 | Loss: 2.563 | Acc: 51.689,83.337,99.398,%
Batch: 180 | Loss: 2.577 | Acc: 51.411,83.374,99.357,%
Batch: 200 | Loss: 2.567 | Acc: 51.586,83.345,99.339,%
Batch: 220 | Loss: 2.564 | Acc: 51.605,83.304,99.318,%
Batch: 240 | Loss: 2.573 | Acc: 51.627,83.127,99.287,%
Batch: 260 | Loss: 2.568 | Acc: 51.664,83.124,99.279,%
Batch: 280 | Loss: 2.572 | Acc: 51.693,83.116,99.280,%
Batch: 300 | Loss: 2.575 | Acc: 51.625,83.018,99.265,%
Batch: 320 | Loss: 2.574 | Acc: 51.648,82.871,99.265,%
Batch: 340 | Loss: 2.574 | Acc: 51.590,82.838,99.265,%
Batch: 360 | Loss: 2.581 | Acc: 51.422,82.767,99.262,%
Batch: 380 | Loss: 2.587 | Acc: 51.306,82.708,99.262,%
Batch: 0 | Loss: 4.100 | Acc: 53.125,74.219,76.562,%
Batch: 20 | Loss: 4.597 | Acc: 44.085,65.960,71.391,%
Batch: 40 | Loss: 4.585 | Acc: 44.512,65.511,70.655,%
Batch: 60 | Loss: 4.597 | Acc: 44.275,65.279,70.671,%
Train all parameters

Epoch: 180
Batch: 0 | Loss: 1.788 | Acc: 56.250,92.188,100.000,%
Batch: 20 | Loss: 2.383 | Acc: 52.641,86.384,99.479,%
Batch: 40 | Loss: 2.370 | Acc: 53.277,85.480,99.447,%
Batch: 60 | Loss: 2.414 | Acc: 53.356,84.503,99.462,%
Batch: 80 | Loss: 2.419 | Acc: 53.347,84.423,99.498,%
Batch: 100 | Loss: 2.428 | Acc: 53.357,84.089,99.474,%
Batch: 120 | Loss: 2.418 | Acc: 53.564,84.155,99.458,%
Batch: 140 | Loss: 2.421 | Acc: 53.640,84.026,99.451,%
Batch: 160 | Loss: 2.451 | Acc: 53.329,83.841,99.461,%
Batch: 180 | Loss: 2.457 | Acc: 53.207,83.728,99.439,%
Batch: 200 | Loss: 2.468 | Acc: 53.148,83.749,99.436,%
Batch: 220 | Loss: 2.478 | Acc: 52.966,83.626,99.434,%
Batch: 240 | Loss: 2.491 | Acc: 52.788,83.467,99.436,%
Batch: 260 | Loss: 2.512 | Acc: 52.637,83.330,99.425,%
Batch: 280 | Loss: 2.529 | Acc: 52.591,83.213,99.399,%
Batch: 300 | Loss: 2.529 | Acc: 52.627,83.140,99.393,%
Batch: 320 | Loss: 2.533 | Acc: 52.575,83.063,99.387,%
Batch: 340 | Loss: 2.536 | Acc: 52.488,83.120,99.381,%
Batch: 360 | Loss: 2.523 | Acc: 52.523,83.133,99.362,%
Batch: 380 | Loss: 2.524 | Acc: 52.545,83.067,99.340,%
Batch: 0 | Loss: 4.088 | Acc: 53.125,71.875,77.344,%
Batch: 20 | Loss: 4.539 | Acc: 44.829,65.625,72.545,%
Batch: 40 | Loss: 4.536 | Acc: 45.332,65.720,71.704,%
Batch: 60 | Loss: 4.545 | Acc: 45.005,65.574,71.670,%
Train all parameters

Epoch: 181
Batch: 0 | Loss: 2.322 | Acc: 53.906,82.812,99.219,%
Batch: 20 | Loss: 2.528 | Acc: 52.641,82.887,99.256,%
Batch: 40 | Loss: 2.500 | Acc: 52.954,84.223,99.371,%
Batch: 60 | Loss: 2.530 | Acc: 52.139,83.978,99.372,%
Batch: 80 | Loss: 2.570 | Acc: 51.447,83.546,99.354,%
Batch: 100 | Loss: 2.520 | Acc: 52.205,83.609,99.389,%
Batch: 120 | Loss: 2.526 | Acc: 52.073,83.613,99.432,%
Batch: 140 | Loss: 2.538 | Acc: 51.967,83.461,99.363,%
Batch: 160 | Loss: 2.530 | Acc: 52.184,83.380,99.258,%
Batch: 180 | Loss: 2.533 | Acc: 52.137,83.369,99.275,%
Batch: 200 | Loss: 2.522 | Acc: 52.328,83.396,99.273,%
Batch: 220 | Loss: 2.518 | Acc: 52.425,83.251,99.279,%
Batch: 240 | Loss: 2.506 | Acc: 52.661,83.166,99.271,%
Batch: 260 | Loss: 2.509 | Acc: 52.619,83.088,99.267,%
Batch: 280 | Loss: 2.517 | Acc: 52.538,83.029,99.272,%
Batch: 300 | Loss: 2.516 | Acc: 52.544,82.922,99.273,%
Batch: 320 | Loss: 2.525 | Acc: 52.434,82.832,99.250,%
Batch: 340 | Loss: 2.535 | Acc: 52.261,82.801,99.260,%
Batch: 360 | Loss: 2.535 | Acc: 52.339,82.804,99.260,%
Batch: 380 | Loss: 2.535 | Acc: 52.375,82.728,99.264,%
Batch: 0 | Loss: 4.236 | Acc: 50.781,69.531,74.219,%
Batch: 20 | Loss: 4.588 | Acc: 44.643,65.179,71.205,%
Batch: 40 | Loss: 4.600 | Acc: 44.531,64.920,70.522,%
Batch: 60 | Loss: 4.596 | Acc: 44.224,64.831,70.876,%
Train all parameters

Epoch: 182
Batch: 0 | Loss: 2.600 | Acc: 53.906,78.906,98.438,%
Batch: 20 | Loss: 2.472 | Acc: 51.786,84.152,99.330,%
Batch: 40 | Loss: 2.442 | Acc: 52.820,84.413,99.447,%
Batch: 60 | Loss: 2.466 | Acc: 52.690,84.106,99.398,%
Batch: 80 | Loss: 2.449 | Acc: 53.096,84.365,99.373,%
Batch: 100 | Loss: 2.481 | Acc: 52.831,83.965,99.389,%
Batch: 120 | Loss: 2.499 | Acc: 52.718,83.833,99.419,%
Batch: 140 | Loss: 2.499 | Acc: 52.770,83.926,99.429,%
Batch: 160 | Loss: 2.523 | Acc: 52.286,83.739,99.403,%
Batch: 180 | Loss: 2.531 | Acc: 52.214,83.615,99.417,%
Batch: 200 | Loss: 2.536 | Acc: 52.212,83.594,99.386,%
Batch: 220 | Loss: 2.528 | Acc: 52.118,83.672,99.392,%
Batch: 240 | Loss: 2.537 | Acc: 52.084,83.623,99.378,%
Batch: 260 | Loss: 2.538 | Acc: 52.101,83.483,99.365,%
Batch: 280 | Loss: 2.539 | Acc: 52.060,83.432,99.366,%
Batch: 300 | Loss: 2.546 | Acc: 51.954,83.360,99.364,%
Batch: 320 | Loss: 2.558 | Acc: 51.893,83.214,99.367,%
Batch: 340 | Loss: 2.552 | Acc: 51.924,83.268,99.372,%
Batch: 360 | Loss: 2.559 | Acc: 51.818,83.206,99.362,%
Batch: 380 | Loss: 2.553 | Acc: 51.905,83.260,99.360,%
Batch: 0 | Loss: 4.184 | Acc: 51.562,67.969,73.438,%
Batch: 20 | Loss: 4.545 | Acc: 44.829,65.885,71.652,%
Batch: 40 | Loss: 4.541 | Acc: 45.122,65.625,70.960,%
Batch: 60 | Loss: 4.548 | Acc: 44.941,65.433,71.068,%
Train all parameters

Epoch: 183
Batch: 0 | Loss: 2.887 | Acc: 53.125,81.250,99.219,%
Batch: 20 | Loss: 2.537 | Acc: 51.265,83.966,99.516,%
Batch: 40 | Loss: 2.503 | Acc: 52.306,84.204,99.524,%
Batch: 60 | Loss: 2.469 | Acc: 52.587,84.068,99.565,%
Batch: 80 | Loss: 2.488 | Acc: 52.459,84.008,99.508,%
Batch: 100 | Loss: 2.495 | Acc: 52.321,83.725,99.466,%
Batch: 120 | Loss: 2.492 | Acc: 52.234,83.626,99.432,%
Batch: 140 | Loss: 2.509 | Acc: 51.973,83.616,99.440,%
Batch: 160 | Loss: 2.515 | Acc: 51.912,83.599,99.432,%
Batch: 180 | Loss: 2.514 | Acc: 51.856,83.732,99.404,%
Batch: 200 | Loss: 2.510 | Acc: 51.994,83.691,99.390,%
Batch: 220 | Loss: 2.521 | Acc: 51.895,83.664,99.403,%
Batch: 240 | Loss: 2.528 | Acc: 51.906,83.607,99.391,%
Batch: 260 | Loss: 2.530 | Acc: 51.802,83.618,99.380,%
Batch: 280 | Loss: 2.528 | Acc: 51.927,83.566,99.397,%
Batch: 300 | Loss: 2.535 | Acc: 51.822,83.518,99.390,%
Batch: 320 | Loss: 2.535 | Acc: 51.816,83.492,99.392,%
Batch: 340 | Loss: 2.538 | Acc: 51.858,83.433,99.388,%
Batch: 360 | Loss: 2.535 | Acc: 51.950,83.479,99.381,%
Batch: 380 | Loss: 2.541 | Acc: 51.884,83.473,99.385,%
Batch: 0 | Loss: 4.225 | Acc: 50.781,70.312,77.344,%
Batch: 20 | Loss: 4.570 | Acc: 45.908,65.848,70.461,%
Batch: 40 | Loss: 4.560 | Acc: 46.322,65.644,70.694,%
Batch: 60 | Loss: 4.565 | Acc: 45.735,65.228,71.043,%
Train all parameters

Epoch: 184
Batch: 0 | Loss: 3.005 | Acc: 46.094,84.375,99.219,%
Batch: 20 | Loss: 2.554 | Acc: 52.455,83.668,99.182,%
Batch: 40 | Loss: 2.418 | Acc: 54.440,83.899,99.162,%
Batch: 60 | Loss: 2.430 | Acc: 54.086,83.453,99.270,%
Batch: 80 | Loss: 2.473 | Acc: 53.443,83.439,99.267,%
Batch: 100 | Loss: 2.489 | Acc: 53.496,83.222,99.288,%
Batch: 120 | Loss: 2.493 | Acc: 53.487,83.445,99.283,%
Batch: 140 | Loss: 2.504 | Acc: 53.374,83.544,99.341,%
Batch: 160 | Loss: 2.513 | Acc: 53.251,83.419,99.311,%
Batch: 180 | Loss: 2.516 | Acc: 53.121,83.395,99.327,%
Batch: 200 | Loss: 2.510 | Acc: 53.047,83.442,99.293,%
Batch: 220 | Loss: 2.512 | Acc: 53.093,83.251,99.289,%
Batch: 240 | Loss: 2.514 | Acc: 52.937,83.286,99.284,%
Batch: 260 | Loss: 2.523 | Acc: 52.766,83.241,99.294,%
Batch: 280 | Loss: 2.534 | Acc: 52.502,83.149,99.291,%
Batch: 300 | Loss: 2.536 | Acc: 52.487,83.088,99.281,%
Batch: 320 | Loss: 2.540 | Acc: 52.560,83.083,99.272,%
Batch: 340 | Loss: 2.550 | Acc: 52.328,82.989,99.271,%
Batch: 360 | Loss: 2.549 | Acc: 52.404,83.012,99.284,%
Batch: 380 | Loss: 2.541 | Acc: 52.473,83.061,99.297,%
Batch: 0 | Loss: 4.139 | Acc: 53.125,72.656,75.781,%
Batch: 20 | Loss: 4.550 | Acc: 45.089,65.216,71.801,%
Batch: 40 | Loss: 4.569 | Acc: 45.217,65.320,70.846,%
Batch: 60 | Loss: 4.570 | Acc: 45.159,65.202,70.940,%
Train classifier parameters

Epoch: 185
Batch: 0 | Loss: 2.394 | Acc: 59.375,81.250,98.438,%
Batch: 20 | Loss: 2.563 | Acc: 53.534,82.589,99.368,%
Batch: 40 | Loss: 2.664 | Acc: 51.848,82.260,99.314,%
Batch: 60 | Loss: 2.633 | Acc: 51.934,82.569,99.257,%
Batch: 80 | Loss: 2.575 | Acc: 52.643,82.784,99.161,%
Batch: 100 | Loss: 2.587 | Acc: 52.166,82.565,99.072,%
Batch: 120 | Loss: 2.592 | Acc: 52.098,82.457,99.077,%
Batch: 140 | Loss: 2.586 | Acc: 52.050,82.668,99.091,%
Batch: 160 | Loss: 2.589 | Acc: 52.150,82.725,99.068,%
Batch: 180 | Loss: 2.579 | Acc: 52.193,82.787,99.072,%
Batch: 200 | Loss: 2.561 | Acc: 52.390,82.914,99.052,%
Batch: 220 | Loss: 2.576 | Acc: 52.259,82.805,99.049,%
Batch: 240 | Loss: 2.572 | Acc: 52.298,82.877,99.066,%
Batch: 260 | Loss: 2.566 | Acc: 52.314,82.938,99.066,%
Batch: 280 | Loss: 2.569 | Acc: 52.260,82.907,99.057,%
Batch: 300 | Loss: 2.570 | Acc: 52.240,82.924,99.058,%
Batch: 320 | Loss: 2.562 | Acc: 52.293,82.983,99.056,%
Batch: 340 | Loss: 2.570 | Acc: 52.227,83.019,99.070,%
Batch: 360 | Loss: 2.569 | Acc: 52.220,83.022,99.069,%
Batch: 380 | Loss: 2.566 | Acc: 52.229,83.054,99.075,%
Batch: 0 | Loss: 4.244 | Acc: 53.906,71.875,76.562,%
Batch: 20 | Loss: 4.593 | Acc: 45.387,65.699,71.615,%
Batch: 40 | Loss: 4.606 | Acc: 45.198,65.473,70.808,%
Batch: 60 | Loss: 4.607 | Acc: 44.890,65.535,70.722,%
Train classifier parameters

Epoch: 186
Batch: 0 | Loss: 2.046 | Acc: 57.812,86.719,99.219,%
Batch: 20 | Loss: 2.657 | Acc: 51.042,83.743,99.256,%
Batch: 40 | Loss: 2.579 | Acc: 51.848,83.537,99.238,%
Batch: 60 | Loss: 2.552 | Acc: 52.139,83.427,99.193,%
Batch: 80 | Loss: 2.553 | Acc: 52.218,83.237,99.209,%
Batch: 100 | Loss: 2.544 | Acc: 52.468,83.052,99.203,%
Batch: 120 | Loss: 2.516 | Acc: 52.318,83.335,99.154,%
Batch: 140 | Loss: 2.510 | Acc: 52.527,83.389,99.158,%
Batch: 160 | Loss: 2.521 | Acc: 52.557,83.293,99.170,%
Batch: 180 | Loss: 2.535 | Acc: 52.266,83.374,99.154,%
Batch: 200 | Loss: 2.543 | Acc: 52.223,83.182,99.153,%
Batch: 220 | Loss: 2.546 | Acc: 52.294,83.159,99.155,%
Batch: 240 | Loss: 2.547 | Acc: 52.341,83.227,99.167,%
Batch: 260 | Loss: 2.550 | Acc: 52.233,83.166,99.168,%
Batch: 280 | Loss: 2.551 | Acc: 52.210,83.218,99.199,%
Batch: 300 | Loss: 2.566 | Acc: 51.926,83.236,99.214,%
Batch: 320 | Loss: 2.563 | Acc: 51.969,83.270,99.199,%
Batch: 340 | Loss: 2.564 | Acc: 51.984,83.378,99.189,%
Batch: 360 | Loss: 2.567 | Acc: 51.961,83.410,99.195,%
Batch: 380 | Loss: 2.564 | Acc: 52.005,83.409,99.202,%
Batch: 0 | Loss: 4.205 | Acc: 53.125,71.875,76.562,%
Batch: 20 | Loss: 4.579 | Acc: 45.201,65.885,71.652,%
Batch: 40 | Loss: 4.591 | Acc: 45.027,65.434,70.941,%
Batch: 60 | Loss: 4.592 | Acc: 44.864,65.548,70.838,%
Train classifier parameters

Epoch: 187
Batch: 0 | Loss: 2.668 | Acc: 48.438,82.812,98.438,%
Batch: 20 | Loss: 2.532 | Acc: 52.790,83.929,99.107,%
Batch: 40 | Loss: 2.547 | Acc: 52.363,83.803,99.181,%
Batch: 60 | Loss: 2.533 | Acc: 52.561,83.389,99.052,%
Batch: 80 | Loss: 2.501 | Acc: 52.874,83.632,99.093,%
Batch: 100 | Loss: 2.510 | Acc: 52.877,83.586,99.110,%
Batch: 120 | Loss: 2.539 | Acc: 52.602,83.510,99.109,%
Batch: 140 | Loss: 2.545 | Acc: 52.394,83.455,99.113,%
Batch: 160 | Loss: 2.551 | Acc: 52.208,83.361,99.097,%
Batch: 180 | Loss: 2.545 | Acc: 52.491,83.309,99.072,%
Batch: 200 | Loss: 2.560 | Acc: 52.289,83.294,99.063,%
Batch: 220 | Loss: 2.567 | Acc: 52.110,83.201,99.084,%
Batch: 240 | Loss: 2.575 | Acc: 52.097,83.195,99.083,%
Batch: 260 | Loss: 2.572 | Acc: 52.068,83.324,99.090,%
Batch: 280 | Loss: 2.574 | Acc: 52.144,83.341,99.099,%
Batch: 300 | Loss: 2.584 | Acc: 52.087,83.272,99.089,%
Batch: 320 | Loss: 2.579 | Acc: 52.144,83.309,99.087,%
Batch: 340 | Loss: 2.576 | Acc: 52.158,83.349,99.097,%
Batch: 360 | Loss: 2.584 | Acc: 52.030,83.308,99.108,%
Batch: 380 | Loss: 2.588 | Acc: 51.962,83.264,99.106,%
Batch: 0 | Loss: 4.197 | Acc: 53.125,71.094,76.562,%
Batch: 20 | Loss: 4.571 | Acc: 45.387,65.885,71.429,%
Batch: 40 | Loss: 4.582 | Acc: 45.065,65.549,70.675,%
Batch: 60 | Loss: 4.584 | Acc: 44.749,65.612,70.812,%
Train classifier parameters

Epoch: 188
Batch: 0 | Loss: 2.441 | Acc: 54.688,79.688,97.656,%
Batch: 20 | Loss: 2.609 | Acc: 51.860,84.003,99.368,%
Batch: 40 | Loss: 2.638 | Acc: 51.181,83.251,99.333,%
Batch: 60 | Loss: 2.599 | Acc: 51.652,83.338,99.360,%
Batch: 80 | Loss: 2.577 | Acc: 52.180,83.324,99.344,%
Batch: 100 | Loss: 2.586 | Acc: 51.911,83.346,99.296,%
Batch: 120 | Loss: 2.596 | Acc: 51.730,83.490,99.309,%
Batch: 140 | Loss: 2.614 | Acc: 51.407,83.272,99.274,%
Batch: 160 | Loss: 2.627 | Acc: 51.412,83.249,99.296,%
Batch: 180 | Loss: 2.622 | Acc: 51.360,83.391,99.288,%
Batch: 200 | Loss: 2.605 | Acc: 51.500,83.605,99.304,%
Batch: 220 | Loss: 2.600 | Acc: 51.718,83.516,99.286,%
Batch: 240 | Loss: 2.588 | Acc: 51.864,83.441,99.290,%
Batch: 260 | Loss: 2.581 | Acc: 51.877,83.534,99.276,%
Batch: 280 | Loss: 2.584 | Acc: 51.763,83.491,99.258,%
Batch: 300 | Loss: 2.589 | Acc: 51.700,83.464,99.247,%
Batch: 320 | Loss: 2.587 | Acc: 51.784,83.494,99.236,%
Batch: 340 | Loss: 2.585 | Acc: 51.812,83.534,99.230,%
Batch: 360 | Loss: 2.577 | Acc: 51.881,83.618,99.225,%
Batch: 380 | Loss: 2.578 | Acc: 51.936,83.600,99.225,%
Batch: 0 | Loss: 4.178 | Acc: 51.562,70.312,75.781,%
Batch: 20 | Loss: 4.561 | Acc: 45.238,65.662,71.652,%
Batch: 40 | Loss: 4.569 | Acc: 44.989,65.454,71.018,%
Batch: 60 | Loss: 4.571 | Acc: 44.890,65.587,70.902,%
Train classifier parameters

Epoch: 189
Batch: 0 | Loss: 2.919 | Acc: 51.562,79.688,100.000,%
Batch: 20 | Loss: 2.633 | Acc: 50.707,82.924,98.884,%
Batch: 40 | Loss: 2.539 | Acc: 51.829,83.727,99.085,%
Batch: 60 | Loss: 2.532 | Acc: 51.895,84.042,99.065,%
Batch: 80 | Loss: 2.526 | Acc: 52.324,83.738,99.122,%
Batch: 100 | Loss: 2.516 | Acc: 52.251,84.019,99.172,%
Batch: 120 | Loss: 2.500 | Acc: 52.376,84.130,99.180,%
Batch: 140 | Loss: 2.502 | Acc: 52.488,84.270,99.197,%
Batch: 160 | Loss: 2.522 | Acc: 52.344,84.050,99.156,%
Batch: 180 | Loss: 2.514 | Acc: 52.465,84.051,99.167,%
Batch: 200 | Loss: 2.522 | Acc: 52.313,84.091,99.168,%
Batch: 220 | Loss: 2.531 | Acc: 52.227,84.163,99.176,%
Batch: 240 | Loss: 2.532 | Acc: 52.305,84.028,99.183,%
Batch: 260 | Loss: 2.532 | Acc: 52.257,84.037,99.156,%
Batch: 280 | Loss: 2.536 | Acc: 52.263,83.930,99.174,%
Batch: 300 | Loss: 2.533 | Acc: 52.313,83.796,99.190,%
Batch: 320 | Loss: 2.536 | Acc: 52.280,83.842,99.197,%
Batch: 340 | Loss: 2.542 | Acc: 52.225,83.857,99.203,%
Batch: 360 | Loss: 2.541 | Acc: 52.192,83.886,99.206,%
Batch: 380 | Loss: 2.532 | Acc: 52.241,83.942,99.215,%
Batch: 0 | Loss: 4.220 | Acc: 53.906,71.094,76.562,%
Batch: 20 | Loss: 4.566 | Acc: 45.499,65.848,71.801,%
Batch: 40 | Loss: 4.571 | Acc: 45.198,65.530,70.960,%
Batch: 60 | Loss: 4.572 | Acc: 44.980,65.689,70.953,%
Train classifier parameters

Epoch: 190
Batch: 0 | Loss: 2.532 | Acc: 40.625,85.938,100.000,%
Batch: 20 | Loss: 2.649 | Acc: 49.926,83.743,99.070,%
Batch: 40 | Loss: 2.582 | Acc: 51.448,83.079,99.219,%
Batch: 60 | Loss: 2.571 | Acc: 51.716,83.171,99.244,%
Batch: 80 | Loss: 2.593 | Acc: 51.321,83.410,99.267,%
Batch: 100 | Loss: 2.600 | Acc: 51.238,83.168,99.234,%
Batch: 120 | Loss: 2.604 | Acc: 51.214,83.026,99.257,%
Batch: 140 | Loss: 2.592 | Acc: 51.463,83.095,99.274,%
Batch: 160 | Loss: 2.577 | Acc: 51.732,83.264,99.272,%
Batch: 180 | Loss: 2.564 | Acc: 51.955,83.607,99.266,%
Batch: 200 | Loss: 2.554 | Acc: 52.029,83.706,99.277,%
Batch: 220 | Loss: 2.557 | Acc: 52.061,83.622,99.272,%
Batch: 240 | Loss: 2.552 | Acc: 52.162,83.720,99.280,%
Batch: 260 | Loss: 2.541 | Acc: 52.224,83.854,99.285,%
Batch: 280 | Loss: 2.532 | Acc: 52.266,83.900,99.294,%
Batch: 300 | Loss: 2.536 | Acc: 52.224,83.884,99.289,%
Batch: 320 | Loss: 2.538 | Acc: 52.193,83.896,99.267,%
Batch: 340 | Loss: 2.536 | Acc: 52.236,83.887,99.249,%
Batch: 360 | Loss: 2.544 | Acc: 52.216,83.802,99.234,%
Batch: 380 | Loss: 2.558 | Acc: 52.061,83.723,99.243,%
Batch: 0 | Loss: 4.191 | Acc: 52.344,71.875,75.781,%
Batch: 20 | Loss: 4.559 | Acc: 45.201,65.923,71.577,%
Batch: 40 | Loss: 4.568 | Acc: 45.046,65.758,70.636,%
Batch: 60 | Loss: 4.572 | Acc: 44.967,65.791,70.607,%
Train classifier parameters

Epoch: 191
Batch: 0 | Loss: 2.218 | Acc: 60.156,83.594,100.000,%
Batch: 20 | Loss: 2.592 | Acc: 52.567,83.557,99.256,%
Batch: 40 | Loss: 2.575 | Acc: 52.020,84.146,99.219,%
Batch: 60 | Loss: 2.563 | Acc: 52.164,84.132,99.232,%
Batch: 80 | Loss: 2.548 | Acc: 52.257,84.201,99.180,%
Batch: 100 | Loss: 2.558 | Acc: 52.011,84.282,99.196,%
Batch: 120 | Loss: 2.575 | Acc: 51.840,84.194,99.141,%
Batch: 140 | Loss: 2.591 | Acc: 51.568,84.087,99.125,%
Batch: 160 | Loss: 2.601 | Acc: 51.708,83.870,99.156,%
Batch: 180 | Loss: 2.593 | Acc: 51.973,83.749,99.137,%
Batch: 200 | Loss: 2.594 | Acc: 51.978,83.776,99.164,%
Batch: 220 | Loss: 2.591 | Acc: 52.036,83.778,99.183,%
Batch: 240 | Loss: 2.583 | Acc: 52.140,83.889,99.173,%
Batch: 260 | Loss: 2.589 | Acc: 52.041,83.833,99.159,%
Batch: 280 | Loss: 2.585 | Acc: 51.938,83.825,99.155,%
Batch: 300 | Loss: 2.587 | Acc: 51.944,83.825,99.172,%
Batch: 320 | Loss: 2.595 | Acc: 51.813,83.750,99.187,%
Batch: 340 | Loss: 2.596 | Acc: 51.782,83.743,99.182,%
Batch: 360 | Loss: 2.581 | Acc: 51.920,83.776,99.195,%
Batch: 380 | Loss: 2.582 | Acc: 51.958,83.768,99.192,%
Batch: 0 | Loss: 4.181 | Acc: 51.562,72.656,76.562,%
Batch: 20 | Loss: 4.544 | Acc: 45.499,65.923,71.540,%
Batch: 40 | Loss: 4.558 | Acc: 45.198,65.854,70.884,%
Batch: 60 | Loss: 4.560 | Acc: 45.082,65.843,70.902,%
Train classifier parameters

Epoch: 192
Batch: 0 | Loss: 3.240 | Acc: 49.219,78.125,100.000,%
Batch: 20 | Loss: 2.640 | Acc: 52.046,83.482,99.182,%
Batch: 40 | Loss: 2.592 | Acc: 51.486,82.889,99.181,%
Batch: 60 | Loss: 2.571 | Acc: 52.011,82.812,99.155,%
Batch: 80 | Loss: 2.523 | Acc: 52.643,83.333,99.199,%
Batch: 100 | Loss: 2.484 | Acc: 53.110,83.601,99.265,%
Batch: 120 | Loss: 2.496 | Acc: 52.544,83.878,99.264,%
Batch: 140 | Loss: 2.502 | Acc: 52.338,83.893,99.235,%
Batch: 160 | Loss: 2.512 | Acc: 52.193,83.904,99.214,%
Batch: 180 | Loss: 2.514 | Acc: 52.011,84.034,99.236,%
Batch: 200 | Loss: 2.535 | Acc: 51.722,83.979,99.238,%
Batch: 220 | Loss: 2.549 | Acc: 51.637,83.930,99.247,%
Batch: 240 | Loss: 2.534 | Acc: 51.903,83.873,99.235,%
Batch: 260 | Loss: 2.530 | Acc: 51.928,83.884,99.228,%
Batch: 280 | Loss: 2.514 | Acc: 52.152,83.961,99.235,%
Batch: 300 | Loss: 2.508 | Acc: 52.292,84.019,99.252,%
Batch: 320 | Loss: 2.502 | Acc: 52.361,84.090,99.270,%
Batch: 340 | Loss: 2.507 | Acc: 52.282,84.100,99.262,%
Batch: 360 | Loss: 2.503 | Acc: 52.374,84.126,99.247,%
Batch: 380 | Loss: 2.512 | Acc: 52.315,84.065,99.256,%
Batch: 0 | Loss: 4.185 | Acc: 53.125,73.438,76.562,%
Batch: 20 | Loss: 4.547 | Acc: 45.238,65.774,72.135,%
Batch: 40 | Loss: 4.556 | Acc: 44.989,65.473,71.265,%
Batch: 60 | Loss: 4.558 | Acc: 44.851,65.868,71.183,%
Train classifier parameters

Epoch: 193
Batch: 0 | Loss: 2.949 | Acc: 45.312,89.062,100.000,%
Batch: 20 | Loss: 2.346 | Acc: 54.836,86.012,99.516,%
Batch: 40 | Loss: 2.416 | Acc: 55.107,85.194,99.428,%
Batch: 60 | Loss: 2.428 | Acc: 54.265,84.862,99.321,%
Batch: 80 | Loss: 2.457 | Acc: 54.051,84.201,99.296,%
Batch: 100 | Loss: 2.463 | Acc: 53.628,84.321,99.211,%
Batch: 120 | Loss: 2.486 | Acc: 53.202,84.188,99.219,%
Batch: 140 | Loss: 2.472 | Acc: 53.341,84.430,99.241,%
Batch: 160 | Loss: 2.488 | Acc: 53.023,84.225,99.238,%
Batch: 180 | Loss: 2.500 | Acc: 52.849,84.362,99.245,%
Batch: 200 | Loss: 2.496 | Acc: 52.818,84.398,99.265,%
Batch: 220 | Loss: 2.495 | Acc: 52.782,84.435,99.293,%
Batch: 240 | Loss: 2.507 | Acc: 52.765,84.216,99.287,%
Batch: 260 | Loss: 2.516 | Acc: 52.643,84.139,99.303,%
Batch: 280 | Loss: 2.524 | Acc: 52.669,84.039,99.305,%
Batch: 300 | Loss: 2.518 | Acc: 52.588,84.115,99.302,%
Batch: 320 | Loss: 2.520 | Acc: 52.553,84.136,99.294,%
Batch: 340 | Loss: 2.518 | Acc: 52.550,84.164,99.317,%
Batch: 360 | Loss: 2.523 | Acc: 52.480,84.154,99.327,%
Batch: 380 | Loss: 2.534 | Acc: 52.383,84.053,99.313,%
Batch: 0 | Loss: 4.180 | Acc: 52.344,71.094,76.562,%
Batch: 20 | Loss: 4.546 | Acc: 44.940,65.960,71.949,%
Batch: 40 | Loss: 4.552 | Acc: 44.989,65.816,71.094,%
Batch: 60 | Loss: 4.556 | Acc: 44.851,65.804,70.914,%
Train classifier parameters

Epoch: 194
Batch: 0 | Loss: 2.658 | Acc: 50.000,85.156,100.000,%
Batch: 20 | Loss: 2.505 | Acc: 53.943,83.557,99.293,%
Batch: 40 | Loss: 2.514 | Acc: 53.373,83.460,99.295,%
Batch: 60 | Loss: 2.500 | Acc: 52.959,84.119,99.334,%
Batch: 80 | Loss: 2.459 | Acc: 53.443,84.279,99.315,%
Batch: 100 | Loss: 2.448 | Acc: 53.465,84.460,99.242,%
Batch: 120 | Loss: 2.476 | Acc: 53.254,84.214,99.174,%
Batch: 140 | Loss: 2.479 | Acc: 53.059,84.225,99.136,%
Batch: 160 | Loss: 2.480 | Acc: 53.047,84.181,99.146,%
Batch: 180 | Loss: 2.483 | Acc: 53.142,84.133,99.154,%
Batch: 200 | Loss: 2.486 | Acc: 53.078,84.021,99.176,%
Batch: 220 | Loss: 2.498 | Acc: 53.068,83.778,99.180,%
Batch: 240 | Loss: 2.497 | Acc: 53.044,83.704,99.186,%
Batch: 260 | Loss: 2.501 | Acc: 53.002,83.582,99.174,%
Batch: 280 | Loss: 2.510 | Acc: 52.939,83.471,99.169,%
Batch: 300 | Loss: 2.509 | Acc: 52.847,83.581,99.172,%
Batch: 320 | Loss: 2.504 | Acc: 52.867,83.628,99.173,%
Batch: 340 | Loss: 2.506 | Acc: 52.841,83.649,99.189,%
Batch: 360 | Loss: 2.513 | Acc: 52.790,83.672,99.199,%
Batch: 380 | Loss: 2.505 | Acc: 52.920,83.674,99.208,%
Batch: 0 | Loss: 4.155 | Acc: 53.125,71.094,76.562,%
Batch: 20 | Loss: 4.539 | Acc: 45.275,66.220,71.801,%
Batch: 40 | Loss: 4.550 | Acc: 44.950,65.873,70.903,%
Batch: 60 | Loss: 4.551 | Acc: 44.903,65.907,70.902,%
Train classifier parameters

Epoch: 195
Batch: 0 | Loss: 2.182 | Acc: 50.000,84.375,99.219,%
Batch: 20 | Loss: 2.409 | Acc: 53.609,84.673,99.405,%
Batch: 40 | Loss: 2.432 | Acc: 53.468,84.318,99.390,%
Batch: 60 | Loss: 2.459 | Acc: 52.523,84.490,99.360,%
Batch: 80 | Loss: 2.458 | Acc: 52.807,84.510,99.334,%
Batch: 100 | Loss: 2.469 | Acc: 52.792,84.491,99.350,%
Batch: 120 | Loss: 2.449 | Acc: 52.983,84.549,99.354,%
Batch: 140 | Loss: 2.487 | Acc: 52.527,84.087,99.357,%
Batch: 160 | Loss: 2.492 | Acc: 52.523,84.001,99.359,%
Batch: 180 | Loss: 2.498 | Acc: 52.447,83.896,99.370,%
Batch: 200 | Loss: 2.504 | Acc: 52.340,83.986,99.366,%
Batch: 220 | Loss: 2.512 | Acc: 52.273,84.046,99.353,%
Batch: 240 | Loss: 2.517 | Acc: 52.175,84.074,99.374,%
Batch: 260 | Loss: 2.513 | Acc: 52.122,84.127,99.368,%
Batch: 280 | Loss: 2.504 | Acc: 52.291,84.175,99.377,%
Batch: 300 | Loss: 2.502 | Acc: 52.331,84.245,99.367,%
Batch: 320 | Loss: 2.501 | Acc: 52.356,84.200,99.365,%
Batch: 340 | Loss: 2.507 | Acc: 52.337,84.006,99.381,%
Batch: 360 | Loss: 2.504 | Acc: 52.422,83.977,99.385,%
Batch: 380 | Loss: 2.504 | Acc: 52.481,83.969,99.373,%
Batch: 0 | Loss: 4.189 | Acc: 52.344,71.875,75.000,%
Batch: 20 | Loss: 4.539 | Acc: 45.424,65.885,72.135,%
Batch: 40 | Loss: 4.554 | Acc: 45.141,65.396,71.189,%
Batch: 60 | Loss: 4.553 | Acc: 44.992,65.599,71.068,%
Train classifier parameters

Epoch: 196
Batch: 0 | Loss: 2.635 | Acc: 43.750,85.938,98.438,%
Batch: 20 | Loss: 2.447 | Acc: 50.893,84.821,99.368,%
Batch: 40 | Loss: 2.584 | Acc: 50.495,84.146,99.371,%
Batch: 60 | Loss: 2.567 | Acc: 51.383,84.004,99.308,%
Batch: 80 | Loss: 2.563 | Acc: 51.090,84.008,99.392,%
Batch: 100 | Loss: 2.565 | Acc: 51.276,83.981,99.350,%
Batch: 120 | Loss: 2.572 | Acc: 51.362,83.936,99.322,%
Batch: 140 | Loss: 2.539 | Acc: 51.679,84.342,99.313,%
Batch: 160 | Loss: 2.553 | Acc: 51.684,84.191,99.340,%
Batch: 180 | Loss: 2.562 | Acc: 51.610,84.138,99.340,%
Batch: 200 | Loss: 2.565 | Acc: 51.656,84.150,99.366,%
Batch: 220 | Loss: 2.556 | Acc: 51.669,84.103,99.357,%
Batch: 240 | Loss: 2.557 | Acc: 51.666,83.947,99.352,%
Batch: 260 | Loss: 2.564 | Acc: 51.673,83.926,99.353,%
Batch: 280 | Loss: 2.565 | Acc: 51.640,84.016,99.358,%
Batch: 300 | Loss: 2.558 | Acc: 51.736,84.061,99.377,%
Batch: 320 | Loss: 2.555 | Acc: 51.838,84.039,99.355,%
Batch: 340 | Loss: 2.558 | Acc: 51.815,84.002,99.342,%
Batch: 360 | Loss: 2.558 | Acc: 51.924,84.014,99.325,%
Batch: 380 | Loss: 2.551 | Acc: 52.051,84.059,99.334,%
Batch: 0 | Loss: 4.182 | Acc: 53.125,69.531,76.562,%
Batch: 20 | Loss: 4.543 | Acc: 45.461,65.588,72.135,%
Batch: 40 | Loss: 4.550 | Acc: 45.389,65.339,71.151,%
Batch: 60 | Loss: 4.550 | Acc: 45.095,65.548,71.094,%
Train classifier parameters

Epoch: 197
Batch: 0 | Loss: 2.793 | Acc: 49.219,83.594,100.000,%
Batch: 20 | Loss: 2.600 | Acc: 52.604,83.891,99.256,%
Batch: 40 | Loss: 2.509 | Acc: 53.144,83.918,99.333,%
Batch: 60 | Loss: 2.490 | Acc: 52.882,84.247,99.244,%
Batch: 80 | Loss: 2.467 | Acc: 53.453,84.703,99.277,%
Batch: 100 | Loss: 2.504 | Acc: 52.885,84.205,99.273,%
Batch: 120 | Loss: 2.520 | Acc: 52.667,83.981,99.212,%
Batch: 140 | Loss: 2.512 | Acc: 52.721,84.054,99.291,%
Batch: 160 | Loss: 2.536 | Acc: 52.286,84.001,99.316,%
Batch: 180 | Loss: 2.515 | Acc: 52.404,84.254,99.318,%
Batch: 200 | Loss: 2.510 | Acc: 52.538,84.165,99.320,%
Batch: 220 | Loss: 2.521 | Acc: 52.418,84.163,99.339,%
Batch: 240 | Loss: 2.521 | Acc: 52.402,84.145,99.342,%
Batch: 260 | Loss: 2.530 | Acc: 52.260,84.088,99.353,%
Batch: 280 | Loss: 2.534 | Acc: 52.208,84.086,99.358,%
Batch: 300 | Loss: 2.541 | Acc: 52.128,83.980,99.351,%
Batch: 320 | Loss: 2.540 | Acc: 52.176,83.952,99.340,%
Batch: 340 | Loss: 2.537 | Acc: 52.195,84.063,99.342,%
Batch: 360 | Loss: 2.533 | Acc: 52.331,83.972,99.325,%
Batch: 380 | Loss: 2.537 | Acc: 52.258,83.907,99.317,%
Batch: 0 | Loss: 4.160 | Acc: 53.125,71.094,76.562,%
Batch: 20 | Loss: 4.541 | Acc: 45.647,65.811,71.689,%
Batch: 40 | Loss: 4.544 | Acc: 45.408,65.606,70.979,%
Batch: 60 | Loss: 4.547 | Acc: 45.082,65.779,71.119,%
Train classifier parameters

Epoch: 198
Batch: 0 | Loss: 1.691 | Acc: 64.062,90.625,100.000,%
Batch: 20 | Loss: 2.324 | Acc: 55.766,85.379,99.033,%
Batch: 40 | Loss: 2.477 | Acc: 53.620,84.470,99.162,%
Batch: 60 | Loss: 2.519 | Acc: 53.035,84.298,99.270,%
Batch: 80 | Loss: 2.527 | Acc: 53.096,84.057,99.286,%
Batch: 100 | Loss: 2.516 | Acc: 53.295,84.135,99.335,%
Batch: 120 | Loss: 2.515 | Acc: 53.106,84.214,99.354,%
Batch: 140 | Loss: 2.537 | Acc: 52.643,84.104,99.357,%
Batch: 160 | Loss: 2.540 | Acc: 52.441,84.035,99.364,%
Batch: 180 | Loss: 2.535 | Acc: 52.577,84.112,99.370,%
Batch: 200 | Loss: 2.548 | Acc: 52.519,84.150,99.343,%
Batch: 220 | Loss: 2.545 | Acc: 52.549,84.085,99.353,%
Batch: 240 | Loss: 2.542 | Acc: 52.503,84.142,99.339,%
Batch: 260 | Loss: 2.533 | Acc: 52.508,84.189,99.306,%
Batch: 280 | Loss: 2.536 | Acc: 52.483,84.166,99.302,%
Batch: 300 | Loss: 2.535 | Acc: 52.497,84.157,99.310,%
Batch: 320 | Loss: 2.527 | Acc: 52.541,84.175,99.297,%
Batch: 340 | Loss: 2.526 | Acc: 52.621,84.095,99.308,%
Batch: 360 | Loss: 2.531 | Acc: 52.582,84.057,99.310,%
Batch: 380 | Loss: 2.526 | Acc: 52.606,84.129,99.321,%
Batch: 0 | Loss: 4.146 | Acc: 55.469,70.312,76.562,%
Batch: 20 | Loss: 4.535 | Acc: 45.796,65.774,71.801,%
Batch: 40 | Loss: 4.538 | Acc: 45.179,65.644,71.132,%
Batch: 60 | Loss: 4.541 | Acc: 45.082,65.779,71.183,%
Train classifier parameters

Epoch: 199
Batch: 0 | Loss: 2.441 | Acc: 57.031,80.469,100.000,%
Batch: 20 | Loss: 2.654 | Acc: 50.930,82.068,99.368,%
Batch: 40 | Loss: 2.637 | Acc: 52.172,83.079,99.428,%
Batch: 60 | Loss: 2.584 | Acc: 52.280,83.824,99.424,%
Batch: 80 | Loss: 2.551 | Acc: 52.643,84.037,99.412,%
Batch: 100 | Loss: 2.589 | Acc: 51.957,83.957,99.412,%
Batch: 120 | Loss: 2.613 | Acc: 51.698,83.968,99.393,%
Batch: 140 | Loss: 2.612 | Acc: 51.690,83.815,99.368,%
Batch: 160 | Loss: 2.585 | Acc: 51.917,83.948,99.384,%
Batch: 180 | Loss: 2.571 | Acc: 51.955,84.051,99.370,%
Batch: 200 | Loss: 2.551 | Acc: 52.215,83.994,99.366,%
Batch: 220 | Loss: 2.561 | Acc: 52.114,84.036,99.350,%
Batch: 240 | Loss: 2.576 | Acc: 52.058,83.937,99.342,%
Batch: 260 | Loss: 2.574 | Acc: 52.134,83.770,99.371,%
Batch: 280 | Loss: 2.564 | Acc: 52.096,83.994,99.347,%
Batch: 300 | Loss: 2.555 | Acc: 52.302,83.918,99.343,%
Batch: 320 | Loss: 2.547 | Acc: 52.327,83.917,99.343,%
Batch: 340 | Loss: 2.539 | Acc: 52.364,83.967,99.352,%
Batch: 360 | Loss: 2.537 | Acc: 52.394,83.899,99.342,%
Batch: 380 | Loss: 2.533 | Acc: 52.440,83.864,99.340,%
Batch: 0 | Loss: 4.152 | Acc: 51.562,71.094,75.781,%
Batch: 20 | Loss: 4.531 | Acc: 45.424,66.109,71.949,%
Batch: 40 | Loss: 4.539 | Acc: 45.236,65.549,71.075,%
Batch: 60 | Loss: 4.541 | Acc: 45.108,65.676,71.119,%
Train all parameters

Epoch: 200
Batch: 0 | Loss: 3.503 | Acc: 46.875,75.781,99.219,%
Batch: 20 | Loss: 2.492 | Acc: 51.600,85.751,99.405,%
Batch: 40 | Loss: 2.471 | Acc: 52.534,84.756,99.371,%
Batch: 60 | Loss: 2.480 | Acc: 52.254,84.567,99.296,%
Batch: 80 | Loss: 2.459 | Acc: 52.604,84.539,99.257,%
Batch: 100 | Loss: 2.468 | Acc: 52.823,84.112,99.288,%
Batch: 120 | Loss: 2.468 | Acc: 52.660,84.117,99.270,%
Batch: 140 | Loss: 2.504 | Acc: 52.322,83.771,99.224,%
Batch: 160 | Loss: 2.523 | Acc: 52.096,83.734,99.190,%
Batch: 180 | Loss: 2.532 | Acc: 52.106,83.641,99.214,%
Batch: 200 | Loss: 2.533 | Acc: 52.041,83.497,99.195,%
Batch: 220 | Loss: 2.533 | Acc: 52.118,83.558,99.162,%
Batch: 240 | Loss: 2.535 | Acc: 52.097,83.493,99.151,%
Batch: 260 | Loss: 2.533 | Acc: 52.200,83.354,99.144,%
Batch: 280 | Loss: 2.542 | Acc: 52.166,83.221,99.138,%
Batch: 300 | Loss: 2.550 | Acc: 52.113,83.160,99.123,%
Batch: 320 | Loss: 2.548 | Acc: 52.159,83.022,99.090,%
Batch: 340 | Loss: 2.551 | Acc: 52.154,82.861,99.077,%
Batch: 360 | Loss: 2.551 | Acc: 52.114,82.828,99.065,%
Batch: 380 | Loss: 2.561 | Acc: 51.973,82.696,99.069,%
Batch: 0 | Loss: 4.212 | Acc: 53.906,70.312,75.000,%
Batch: 20 | Loss: 4.627 | Acc: 45.201,65.365,71.205,%
Batch: 40 | Loss: 4.616 | Acc: 45.389,64.729,70.751,%
Batch: 60 | Loss: 4.614 | Acc: 45.095,64.677,70.607,%
Train all parameters

Epoch: 201
Batch: 0 | Loss: 2.998 | Acc: 57.812,79.688,97.656,%
Batch: 20 | Loss: 2.469 | Acc: 53.274,83.557,99.219,%
Batch: 40 | Loss: 2.570 | Acc: 51.867,82.908,99.123,%
Batch: 60 | Loss: 2.544 | Acc: 52.062,82.723,99.168,%
Batch: 80 | Loss: 2.531 | Acc: 52.450,82.870,99.093,%
Batch: 100 | Loss: 2.534 | Acc: 52.468,82.843,99.087,%
Batch: 120 | Loss: 2.547 | Acc: 52.447,82.800,99.032,%
Batch: 140 | Loss: 2.558 | Acc: 52.316,82.957,99.069,%
Batch: 160 | Loss: 2.550 | Acc: 52.417,82.812,99.034,%
Batch: 180 | Loss: 2.569 | Acc: 52.292,82.687,98.990,%
Batch: 200 | Loss: 2.585 | Acc: 52.076,82.502,99.009,%
Batch: 220 | Loss: 2.579 | Acc: 52.287,82.480,99.024,%
Batch: 240 | Loss: 2.571 | Acc: 52.428,82.404,99.024,%
Batch: 260 | Loss: 2.564 | Acc: 52.457,82.549,99.033,%
Batch: 280 | Loss: 2.570 | Acc: 52.408,82.496,99.024,%
Batch: 300 | Loss: 2.581 | Acc: 52.333,82.428,99.001,%
Batch: 320 | Loss: 2.576 | Acc: 52.475,82.335,98.990,%
Batch: 340 | Loss: 2.584 | Acc: 52.438,82.148,98.976,%
Batch: 360 | Loss: 2.584 | Acc: 52.424,82.170,98.989,%
Batch: 380 | Loss: 2.584 | Acc: 52.354,82.255,98.979,%
Batch: 0 | Loss: 4.093 | Acc: 52.344,71.094,76.562,%
Batch: 20 | Loss: 4.612 | Acc: 44.792,64.918,70.833,%
Batch: 40 | Loss: 4.582 | Acc: 45.217,64.710,70.598,%
Batch: 60 | Loss: 4.576 | Acc: 45.082,64.613,70.914,%
Train all parameters

Epoch: 202
Batch: 0 | Loss: 2.175 | Acc: 50.000,85.156,100.000,%
Batch: 20 | Loss: 2.612 | Acc: 50.484,82.887,98.996,%
Batch: 40 | Loss: 2.524 | Acc: 52.134,83.899,99.047,%
Batch: 60 | Loss: 2.568 | Acc: 52.139,83.210,98.988,%
Batch: 80 | Loss: 2.571 | Acc: 52.083,83.150,98.881,%
Batch: 100 | Loss: 2.578 | Acc: 52.135,82.898,98.940,%
Batch: 120 | Loss: 2.588 | Acc: 52.240,82.806,98.928,%
Batch: 140 | Loss: 2.594 | Acc: 51.900,82.857,98.964,%
Batch: 160 | Loss: 2.578 | Acc: 52.072,82.866,98.996,%
Batch: 180 | Loss: 2.571 | Acc: 52.111,82.778,99.020,%
Batch: 200 | Loss: 2.565 | Acc: 52.107,82.984,99.028,%
Batch: 220 | Loss: 2.549 | Acc: 52.376,82.936,99.049,%
Batch: 240 | Loss: 2.555 | Acc: 52.315,82.890,99.070,%
Batch: 260 | Loss: 2.570 | Acc: 52.182,82.795,99.084,%
Batch: 280 | Loss: 2.569 | Acc: 52.191,82.646,99.083,%
Batch: 300 | Loss: 2.570 | Acc: 52.066,82.628,99.089,%
Batch: 320 | Loss: 2.569 | Acc: 52.105,82.615,99.090,%
Batch: 340 | Loss: 2.563 | Acc: 52.160,82.680,99.100,%
Batch: 360 | Loss: 2.564 | Acc: 52.181,82.587,99.085,%
Batch: 380 | Loss: 2.573 | Acc: 52.147,82.515,99.075,%
Batch: 0 | Loss: 4.219 | Acc: 50.781,71.094,76.562,%
Batch: 20 | Loss: 4.654 | Acc: 45.201,64.397,70.722,%
Batch: 40 | Loss: 4.639 | Acc: 45.332,64.120,69.798,%
Batch: 60 | Loss: 4.630 | Acc: 44.595,64.267,70.005,%
Train all parameters

Epoch: 203
Batch: 0 | Loss: 2.558 | Acc: 55.469,82.812,99.219,%
Batch: 20 | Loss: 2.561 | Acc: 53.013,81.994,99.442,%
Batch: 40 | Loss: 2.544 | Acc: 52.229,83.232,99.371,%
Batch: 60 | Loss: 2.507 | Acc: 52.446,83.927,99.360,%
Batch: 80 | Loss: 2.517 | Acc: 52.411,83.642,99.296,%
Batch: 100 | Loss: 2.512 | Acc: 52.328,83.354,99.288,%
Batch: 120 | Loss: 2.521 | Acc: 52.279,83.387,99.212,%
Batch: 140 | Loss: 2.485 | Acc: 52.532,83.572,99.202,%
Batch: 160 | Loss: 2.495 | Acc: 52.548,83.574,99.170,%
Batch: 180 | Loss: 2.491 | Acc: 52.534,83.615,99.141,%
Batch: 200 | Loss: 2.521 | Acc: 52.309,83.442,99.172,%
Batch: 220 | Loss: 2.521 | Acc: 52.188,83.360,99.194,%
Batch: 240 | Loss: 2.526 | Acc: 52.318,83.169,99.154,%
Batch: 260 | Loss: 2.528 | Acc: 52.224,83.133,99.153,%
Batch: 280 | Loss: 2.532 | Acc: 52.397,82.965,99.152,%
Batch: 300 | Loss: 2.539 | Acc: 52.318,82.919,99.169,%
Batch: 320 | Loss: 2.528 | Acc: 52.424,82.890,99.175,%
Batch: 340 | Loss: 2.526 | Acc: 52.488,82.831,99.166,%
Batch: 360 | Loss: 2.530 | Acc: 52.478,82.717,99.165,%
Batch: 380 | Loss: 2.540 | Acc: 52.377,82.765,99.157,%
Batch: 0 | Loss: 4.150 | Acc: 52.344,71.875,76.562,%
Batch: 20 | Loss: 4.567 | Acc: 45.610,65.551,71.354,%
Batch: 40 | Loss: 4.562 | Acc: 45.694,64.863,70.598,%
Batch: 60 | Loss: 4.574 | Acc: 45.325,64.703,70.620,%
Train all parameters

Epoch: 204
Batch: 0 | Loss: 1.949 | Acc: 57.812,86.719,98.438,%
Batch: 20 | Loss: 2.363 | Acc: 54.501,85.900,99.554,%
Batch: 40 | Loss: 2.447 | Acc: 53.106,84.756,99.428,%
Batch: 60 | Loss: 2.496 | Acc: 53.023,84.106,99.385,%
Batch: 80 | Loss: 2.485 | Acc: 53.000,83.681,99.383,%
Batch: 100 | Loss: 2.500 | Acc: 52.676,83.586,99.343,%
Batch: 120 | Loss: 2.506 | Acc: 52.531,83.652,99.361,%
Batch: 140 | Loss: 2.524 | Acc: 52.311,83.433,99.318,%
Batch: 160 | Loss: 2.536 | Acc: 52.193,83.225,99.311,%
Batch: 180 | Loss: 2.525 | Acc: 52.396,83.244,99.288,%
Batch: 200 | Loss: 2.535 | Acc: 52.476,83.244,99.265,%
Batch: 220 | Loss: 2.538 | Acc: 52.531,83.191,99.243,%
Batch: 240 | Loss: 2.554 | Acc: 52.473,83.020,99.228,%
Batch: 260 | Loss: 2.554 | Acc: 52.413,82.971,99.198,%
Batch: 280 | Loss: 2.551 | Acc: 52.355,83.007,99.205,%
Batch: 300 | Loss: 2.548 | Acc: 52.359,83.062,99.188,%
Batch: 320 | Loss: 2.546 | Acc: 52.346,82.961,99.165,%
Batch: 340 | Loss: 2.543 | Acc: 52.236,82.961,99.150,%
Batch: 360 | Loss: 2.546 | Acc: 52.184,82.962,99.156,%
Batch: 380 | Loss: 2.547 | Acc: 52.114,82.938,99.143,%
Batch: 0 | Loss: 4.205 | Acc: 53.906,70.312,77.344,%
Batch: 20 | Loss: 4.581 | Acc: 45.647,66.071,71.205,%
Batch: 40 | Loss: 4.580 | Acc: 45.617,65.415,70.884,%
Batch: 60 | Loss: 4.592 | Acc: 45.287,65.446,70.658,%
Train all parameters

Epoch: 205
Batch: 0 | Loss: 1.995 | Acc: 60.156,86.719,100.000,%
Batch: 20 | Loss: 2.619 | Acc: 52.009,83.185,99.479,%
Batch: 40 | Loss: 2.511 | Acc: 53.411,84.108,99.352,%
Batch: 60 | Loss: 2.500 | Acc: 53.215,84.311,99.347,%
Batch: 80 | Loss: 2.530 | Acc: 52.508,84.201,99.306,%
Batch: 100 | Loss: 2.525 | Acc: 52.119,84.259,99.335,%
Batch: 120 | Loss: 2.512 | Acc: 52.466,84.310,99.361,%
Batch: 140 | Loss: 2.501 | Acc: 52.432,84.164,99.363,%
Batch: 160 | Loss: 2.474 | Acc: 52.747,84.210,99.345,%
Batch: 180 | Loss: 2.464 | Acc: 53.142,84.176,99.327,%
Batch: 200 | Loss: 2.468 | Acc: 53.214,84.021,99.304,%
Batch: 220 | Loss: 2.471 | Acc: 53.171,83.972,99.289,%
Batch: 240 | Loss: 2.473 | Acc: 53.213,83.827,99.274,%
Batch: 260 | Loss: 2.480 | Acc: 53.116,83.773,99.276,%
Batch: 280 | Loss: 2.478 | Acc: 53.019,83.744,99.272,%
Batch: 300 | Loss: 2.487 | Acc: 52.819,83.674,99.237,%
Batch: 320 | Loss: 2.496 | Acc: 52.784,83.591,99.226,%
Batch: 340 | Loss: 2.505 | Acc: 52.713,83.534,99.226,%
Batch: 360 | Loss: 2.503 | Acc: 52.746,83.477,99.212,%
Batch: 380 | Loss: 2.508 | Acc: 52.664,83.397,99.200,%
Batch: 0 | Loss: 4.126 | Acc: 52.344,72.656,79.688,%
Batch: 20 | Loss: 4.590 | Acc: 44.494,65.625,70.499,%
Batch: 40 | Loss: 4.578 | Acc: 44.874,65.072,70.084,%
Batch: 60 | Loss: 4.593 | Acc: 44.723,64.524,70.146,%
Train all parameters

Epoch: 206
Batch: 0 | Loss: 2.153 | Acc: 58.594,85.156,100.000,%
Batch: 20 | Loss: 2.491 | Acc: 53.497,83.743,99.219,%
Batch: 40 | Loss: 2.494 | Acc: 53.620,83.784,99.238,%
Batch: 60 | Loss: 2.511 | Acc: 53.381,83.286,99.244,%
Batch: 80 | Loss: 2.514 | Acc: 53.376,83.034,99.306,%
Batch: 100 | Loss: 2.509 | Acc: 52.994,83.099,99.304,%
Batch: 120 | Loss: 2.504 | Acc: 52.918,83.219,99.283,%
Batch: 140 | Loss: 2.504 | Acc: 52.865,83.383,99.285,%
Batch: 160 | Loss: 2.509 | Acc: 52.776,83.293,99.272,%
Batch: 180 | Loss: 2.498 | Acc: 52.762,83.391,99.266,%
Batch: 200 | Loss: 2.502 | Acc: 52.760,83.512,99.273,%
Batch: 220 | Loss: 2.525 | Acc: 52.570,83.389,99.243,%
Batch: 240 | Loss: 2.531 | Acc: 52.486,83.347,99.222,%
Batch: 260 | Loss: 2.533 | Acc: 52.523,83.142,99.237,%
Batch: 280 | Loss: 2.532 | Acc: 52.666,83.154,99.224,%
Batch: 300 | Loss: 2.532 | Acc: 52.629,83.158,99.229,%
Batch: 320 | Loss: 2.530 | Acc: 52.665,83.163,99.211,%
Batch: 340 | Loss: 2.531 | Acc: 52.765,83.115,99.198,%
Batch: 360 | Loss: 2.530 | Acc: 52.703,83.051,99.188,%
Batch: 380 | Loss: 2.538 | Acc: 52.569,83.011,99.182,%
Batch: 0 | Loss: 4.064 | Acc: 53.125,71.875,73.438,%
Batch: 20 | Loss: 4.547 | Acc: 45.238,66.071,70.499,%
Batch: 40 | Loss: 4.559 | Acc: 45.446,65.663,70.122,%
Batch: 60 | Loss: 4.562 | Acc: 45.044,65.459,70.505,%
Train all parameters

Epoch: 207
Batch: 0 | Loss: 2.743 | Acc: 60.156,89.062,100.000,%
Batch: 20 | Loss: 2.740 | Acc: 51.376,83.222,99.070,%
Batch: 40 | Loss: 2.668 | Acc: 51.486,83.575,99.238,%
Batch: 60 | Loss: 2.605 | Acc: 52.075,83.760,99.296,%
Batch: 80 | Loss: 2.594 | Acc: 52.257,83.546,99.257,%
Batch: 100 | Loss: 2.586 | Acc: 52.421,83.331,99.219,%
Batch: 120 | Loss: 2.581 | Acc: 52.324,83.426,99.206,%
Batch: 140 | Loss: 2.566 | Acc: 52.161,83.549,99.213,%
Batch: 160 | Loss: 2.579 | Acc: 52.077,83.244,99.199,%
Batch: 180 | Loss: 2.568 | Acc: 52.344,83.240,99.189,%
Batch: 200 | Loss: 2.568 | Acc: 52.375,83.174,99.168,%
Batch: 220 | Loss: 2.568 | Acc: 52.354,83.226,99.106,%
Batch: 240 | Loss: 2.573 | Acc: 52.289,83.156,99.079,%
Batch: 260 | Loss: 2.566 | Acc: 52.523,83.166,99.072,%
Batch: 280 | Loss: 2.565 | Acc: 52.502,83.077,99.066,%
Batch: 300 | Loss: 2.573 | Acc: 52.385,83.020,99.047,%
Batch: 320 | Loss: 2.577 | Acc: 52.300,82.988,99.053,%
Batch: 340 | Loss: 2.577 | Acc: 52.225,82.991,99.049,%
Batch: 360 | Loss: 2.574 | Acc: 52.218,82.951,99.033,%
Batch: 380 | Loss: 2.573 | Acc: 52.178,82.999,99.042,%
Batch: 0 | Loss: 4.143 | Acc: 53.125,73.438,76.562,%
Batch: 20 | Loss: 4.621 | Acc: 44.680,65.402,70.759,%
Batch: 40 | Loss: 4.634 | Acc: 44.931,64.558,69.531,%
Batch: 60 | Loss: 4.637 | Acc: 44.749,64.408,69.839,%
Train all parameters

Epoch: 208
Batch: 0 | Loss: 3.273 | Acc: 45.312,75.781,100.000,%
Batch: 20 | Loss: 2.533 | Acc: 52.232,83.519,99.442,%
Batch: 40 | Loss: 2.541 | Acc: 51.601,83.727,99.409,%
Batch: 60 | Loss: 2.564 | Acc: 51.691,83.069,99.360,%
Batch: 80 | Loss: 2.564 | Acc: 51.736,83.555,99.402,%
Batch: 100 | Loss: 2.572 | Acc: 51.887,83.671,99.373,%
Batch: 120 | Loss: 2.571 | Acc: 51.717,83.871,99.303,%
Batch: 140 | Loss: 2.559 | Acc: 52.011,83.754,99.269,%
Batch: 160 | Loss: 2.561 | Acc: 52.072,83.594,99.228,%
Batch: 180 | Loss: 2.538 | Acc: 52.460,83.559,99.193,%
Batch: 200 | Loss: 2.535 | Acc: 52.437,83.671,99.188,%
Batch: 220 | Loss: 2.542 | Acc: 52.390,83.640,99.166,%
Batch: 240 | Loss: 2.545 | Acc: 52.525,83.535,99.138,%
Batch: 260 | Loss: 2.547 | Acc: 52.398,83.480,99.117,%
Batch: 280 | Loss: 2.545 | Acc: 52.438,83.513,99.108,%
Batch: 300 | Loss: 2.544 | Acc: 52.375,83.490,99.086,%
Batch: 320 | Loss: 2.560 | Acc: 52.285,83.299,99.078,%
Batch: 340 | Loss: 2.561 | Acc: 52.229,83.234,99.084,%
Batch: 360 | Loss: 2.561 | Acc: 52.264,83.200,99.078,%
Batch: 380 | Loss: 2.565 | Acc: 52.219,83.110,99.085,%
Batch: 0 | Loss: 4.369 | Acc: 49.219,74.219,75.000,%
Batch: 20 | Loss: 4.576 | Acc: 44.420,65.774,70.982,%
Batch: 40 | Loss: 4.592 | Acc: 44.722,64.863,70.122,%
Batch: 60 | Loss: 4.602 | Acc: 44.634,64.498,70.069,%
Train all parameters

Epoch: 209
Batch: 0 | Loss: 2.418 | Acc: 57.031,82.031,98.438,%
Batch: 20 | Loss: 2.519 | Acc: 54.241,82.292,99.368,%
Batch: 40 | Loss: 2.522 | Acc: 53.906,82.470,99.333,%
Batch: 60 | Loss: 2.487 | Acc: 53.471,83.235,99.347,%
Batch: 80 | Loss: 2.490 | Acc: 53.463,83.314,99.392,%
Batch: 100 | Loss: 2.502 | Acc: 53.519,82.967,99.389,%
Batch: 120 | Loss: 2.475 | Acc: 53.564,83.335,99.335,%
Batch: 140 | Loss: 2.478 | Acc: 53.413,83.361,99.302,%
Batch: 160 | Loss: 2.490 | Acc: 53.290,83.288,99.262,%
Batch: 180 | Loss: 2.474 | Acc: 53.466,83.231,99.253,%
Batch: 200 | Loss: 2.479 | Acc: 53.428,83.232,99.238,%
Batch: 220 | Loss: 2.490 | Acc: 53.397,83.173,99.198,%
Batch: 240 | Loss: 2.491 | Acc: 53.365,83.257,99.164,%
Batch: 260 | Loss: 2.495 | Acc: 53.260,83.184,99.159,%
Batch: 280 | Loss: 2.497 | Acc: 53.267,83.154,99.160,%
Batch: 300 | Loss: 2.500 | Acc: 53.148,83.145,99.159,%
Batch: 320 | Loss: 2.504 | Acc: 53.081,83.175,99.153,%
Batch: 340 | Loss: 2.510 | Acc: 52.965,83.195,99.141,%
Batch: 360 | Loss: 2.506 | Acc: 52.986,83.172,99.145,%
Batch: 380 | Loss: 2.510 | Acc: 52.926,83.083,99.165,%
Batch: 0 | Loss: 4.245 | Acc: 48.438,71.875,75.781,%
Batch: 20 | Loss: 4.596 | Acc: 44.085,64.769,70.908,%
Batch: 40 | Loss: 4.574 | Acc: 44.684,64.672,70.922,%
Batch: 60 | Loss: 4.577 | Acc: 44.762,64.908,70.991,%
Train all parameters

Epoch: 210
Batch: 0 | Loss: 2.926 | Acc: 53.906,82.812,100.000,%
Batch: 20 | Loss: 2.404 | Acc: 53.720,84.710,99.293,%
Batch: 40 | Loss: 2.446 | Acc: 53.811,84.184,99.276,%
Batch: 60 | Loss: 2.436 | Acc: 53.637,84.388,99.232,%
Batch: 80 | Loss: 2.455 | Acc: 53.260,84.115,99.306,%
Batch: 100 | Loss: 2.476 | Acc: 53.133,83.834,99.327,%
Batch: 120 | Loss: 2.486 | Acc: 53.190,83.749,99.322,%
Batch: 140 | Loss: 2.479 | Acc: 53.131,83.948,99.357,%
Batch: 160 | Loss: 2.505 | Acc: 52.911,83.725,99.321,%
Batch: 180 | Loss: 2.497 | Acc: 53.052,83.723,99.327,%
Batch: 200 | Loss: 2.511 | Acc: 52.884,83.695,99.316,%
Batch: 220 | Loss: 2.519 | Acc: 52.743,83.597,99.251,%
Batch: 240 | Loss: 2.522 | Acc: 52.733,83.532,99.222,%
Batch: 260 | Loss: 2.521 | Acc: 52.805,83.543,99.213,%
Batch: 280 | Loss: 2.526 | Acc: 52.755,83.460,99.180,%
Batch: 300 | Loss: 2.523 | Acc: 52.858,83.378,99.172,%
Batch: 320 | Loss: 2.523 | Acc: 52.835,83.348,99.160,%
Batch: 340 | Loss: 2.517 | Acc: 52.839,83.344,99.145,%
Batch: 360 | Loss: 2.521 | Acc: 52.718,83.343,99.137,%
Batch: 380 | Loss: 2.521 | Acc: 52.692,83.307,99.114,%
Batch: 0 | Loss: 4.248 | Acc: 53.125,71.094,72.656,%
Batch: 20 | Loss: 4.576 | Acc: 45.722,66.406,71.540,%
Batch: 40 | Loss: 4.574 | Acc: 45.713,65.263,70.865,%
Batch: 60 | Loss: 4.582 | Acc: 44.992,65.138,70.684,%
Train all parameters

Epoch: 211
Batch: 0 | Loss: 2.722 | Acc: 58.594,89.062,100.000,%
Batch: 20 | Loss: 2.374 | Acc: 54.427,85.863,99.479,%
Batch: 40 | Loss: 2.440 | Acc: 54.135,84.261,99.390,%
Batch: 60 | Loss: 2.450 | Acc: 53.522,83.863,99.411,%
Batch: 80 | Loss: 2.493 | Acc: 52.932,83.787,99.325,%
Batch: 100 | Loss: 2.479 | Acc: 52.916,84.058,99.304,%
Batch: 120 | Loss: 2.494 | Acc: 52.912,83.846,99.335,%
Batch: 140 | Loss: 2.481 | Acc: 53.158,83.926,99.335,%
Batch: 160 | Loss: 2.481 | Acc: 53.115,83.943,99.311,%
Batch: 180 | Loss: 2.485 | Acc: 53.086,83.818,99.331,%
Batch: 200 | Loss: 2.490 | Acc: 53.001,83.703,99.316,%
Batch: 220 | Loss: 2.491 | Acc: 53.015,83.693,99.275,%
Batch: 240 | Loss: 2.488 | Acc: 53.057,83.733,99.254,%
Batch: 260 | Loss: 2.496 | Acc: 52.972,83.660,99.243,%
Batch: 280 | Loss: 2.501 | Acc: 52.825,83.566,99.249,%
Batch: 300 | Loss: 2.509 | Acc: 52.712,83.511,99.255,%
Batch: 320 | Loss: 2.508 | Acc: 52.745,83.511,99.226,%
Batch: 340 | Loss: 2.509 | Acc: 52.692,83.463,99.210,%
Batch: 360 | Loss: 2.510 | Acc: 52.658,83.414,99.191,%
Batch: 380 | Loss: 2.505 | Acc: 52.770,83.467,99.192,%
Batch: 0 | Loss: 4.225 | Acc: 50.781,72.656,78.125,%
Batch: 20 | Loss: 4.549 | Acc: 44.866,66.332,71.429,%
Batch: 40 | Loss: 4.551 | Acc: 45.465,65.625,70.541,%
Batch: 60 | Loss: 4.564 | Acc: 44.941,65.279,70.466,%
Train all parameters

Epoch: 212
Batch: 0 | Loss: 2.528 | Acc: 59.375,92.188,100.000,%
Batch: 20 | Loss: 2.513 | Acc: 52.641,85.007,99.479,%
Batch: 40 | Loss: 2.570 | Acc: 51.963,85.252,99.447,%
Batch: 60 | Loss: 2.494 | Acc: 52.741,85.118,99.411,%
Batch: 80 | Loss: 2.530 | Acc: 52.382,84.491,99.431,%
Batch: 100 | Loss: 2.526 | Acc: 51.972,84.437,99.343,%
Batch: 120 | Loss: 2.530 | Acc: 52.292,84.511,99.322,%
Batch: 140 | Loss: 2.525 | Acc: 52.427,84.253,99.285,%
Batch: 160 | Loss: 2.528 | Acc: 52.276,84.157,99.282,%
Batch: 180 | Loss: 2.515 | Acc: 52.585,84.120,99.279,%
Batch: 200 | Loss: 2.507 | Acc: 52.589,84.223,99.273,%
Batch: 220 | Loss: 2.522 | Acc: 52.365,83.965,99.279,%
Batch: 240 | Loss: 2.517 | Acc: 52.473,83.902,99.267,%
Batch: 260 | Loss: 2.517 | Acc: 52.556,83.830,99.255,%
Batch: 280 | Loss: 2.527 | Acc: 52.408,83.730,99.258,%
Batch: 300 | Loss: 2.526 | Acc: 52.429,83.674,99.234,%
Batch: 320 | Loss: 2.533 | Acc: 52.263,83.608,99.219,%
Batch: 340 | Loss: 2.531 | Acc: 52.229,83.683,99.212,%
Batch: 360 | Loss: 2.530 | Acc: 52.175,83.661,99.184,%
Batch: 380 | Loss: 2.532 | Acc: 52.251,83.557,99.202,%
Batch: 0 | Loss: 4.279 | Acc: 53.125,72.656,76.562,%
Batch: 20 | Loss: 4.529 | Acc: 45.499,66.183,71.838,%
Batch: 40 | Loss: 4.520 | Acc: 45.541,65.892,71.246,%
Batch: 60 | Loss: 4.528 | Acc: 45.223,65.881,71.324,%
Train all parameters

Epoch: 213
Batch: 0 | Loss: 2.812 | Acc: 43.750,88.281,99.219,%
Batch: 20 | Loss: 2.412 | Acc: 52.307,86.310,99.293,%
Batch: 40 | Loss: 2.465 | Acc: 52.077,85.499,99.390,%
Batch: 60 | Loss: 2.474 | Acc: 52.203,85.041,99.385,%
Batch: 80 | Loss: 2.527 | Acc: 51.485,84.770,99.354,%
Batch: 100 | Loss: 2.528 | Acc: 51.709,84.553,99.335,%
Batch: 120 | Loss: 2.496 | Acc: 52.215,84.569,99.296,%
Batch: 140 | Loss: 2.481 | Acc: 52.554,84.525,99.269,%
Batch: 160 | Loss: 2.495 | Acc: 52.252,84.341,99.233,%
Batch: 180 | Loss: 2.486 | Acc: 52.344,84.358,99.227,%
Batch: 200 | Loss: 2.497 | Acc: 52.328,84.188,99.230,%
Batch: 220 | Loss: 2.492 | Acc: 52.450,84.071,99.205,%
Batch: 240 | Loss: 2.506 | Acc: 52.399,83.866,99.193,%
Batch: 260 | Loss: 2.509 | Acc: 52.440,83.803,99.177,%
Batch: 280 | Loss: 2.516 | Acc: 52.408,83.736,99.177,%
Batch: 300 | Loss: 2.516 | Acc: 52.403,83.672,99.175,%
Batch: 320 | Loss: 2.525 | Acc: 52.315,83.565,99.182,%
Batch: 340 | Loss: 2.528 | Acc: 52.275,83.518,99.173,%
Batch: 360 | Loss: 2.532 | Acc: 52.352,83.431,99.182,%
Batch: 380 | Loss: 2.532 | Acc: 52.372,83.393,99.174,%
Batch: 0 | Loss: 4.069 | Acc: 53.125,73.438,75.000,%
Batch: 20 | Loss: 4.602 | Acc: 44.754,65.253,70.796,%
Batch: 40 | Loss: 4.575 | Acc: 45.484,64.996,70.751,%
Batch: 60 | Loss: 4.571 | Acc: 45.108,65.215,71.017,%
Train all parameters

Epoch: 214
Batch: 0 | Loss: 2.059 | Acc: 55.469,90.625,97.656,%
Batch: 20 | Loss: 2.499 | Acc: 52.641,84.375,99.033,%
Batch: 40 | Loss: 2.520 | Acc: 52.515,84.127,99.104,%
Batch: 60 | Loss: 2.535 | Acc: 52.382,84.388,99.219,%
Batch: 80 | Loss: 2.509 | Acc: 52.614,84.375,99.315,%
Batch: 100 | Loss: 2.496 | Acc: 52.468,84.514,99.288,%
Batch: 120 | Loss: 2.497 | Acc: 52.570,84.020,99.303,%
Batch: 140 | Loss: 2.501 | Acc: 52.565,84.092,99.280,%
Batch: 160 | Loss: 2.491 | Acc: 52.582,83.972,99.243,%
Batch: 180 | Loss: 2.501 | Acc: 52.568,83.887,99.232,%
Batch: 200 | Loss: 2.492 | Acc: 52.546,83.975,99.250,%
Batch: 220 | Loss: 2.500 | Acc: 52.517,83.732,99.222,%
Batch: 240 | Loss: 2.495 | Acc: 52.554,83.788,99.206,%
Batch: 260 | Loss: 2.501 | Acc: 52.532,83.734,99.192,%
Batch: 280 | Loss: 2.511 | Acc: 52.552,83.602,99.177,%
Batch: 300 | Loss: 2.517 | Acc: 52.440,83.599,99.190,%
Batch: 320 | Loss: 2.529 | Acc: 52.273,83.499,99.187,%
Batch: 340 | Loss: 2.528 | Acc: 52.403,83.477,99.196,%
Batch: 360 | Loss: 2.529 | Acc: 52.426,83.397,99.178,%
Batch: 380 | Loss: 2.531 | Acc: 52.465,83.344,99.172,%
Batch: 0 | Loss: 4.205 | Acc: 53.125,71.875,75.000,%
Batch: 20 | Loss: 4.535 | Acc: 45.833,66.295,71.243,%
Batch: 40 | Loss: 4.540 | Acc: 45.827,65.663,70.655,%
Batch: 60 | Loss: 4.541 | Acc: 45.684,65.510,70.978,%
Train all parameters

Epoch: 215
Batch: 0 | Loss: 2.717 | Acc: 58.594,73.438,99.219,%
Batch: 20 | Loss: 2.326 | Acc: 55.246,84.933,99.107,%
Batch: 40 | Loss: 2.388 | Acc: 53.716,84.661,99.257,%
Batch: 60 | Loss: 2.429 | Acc: 53.663,83.568,99.257,%
Batch: 80 | Loss: 2.466 | Acc: 53.279,83.709,99.248,%
Batch: 100 | Loss: 2.458 | Acc: 53.411,83.694,99.203,%
Batch: 120 | Loss: 2.460 | Acc: 53.370,83.342,99.219,%
Batch: 140 | Loss: 2.468 | Acc: 53.341,83.283,99.230,%
Batch: 160 | Loss: 2.473 | Acc: 53.353,83.293,99.194,%
Batch: 180 | Loss: 2.459 | Acc: 53.483,83.412,99.206,%
Batch: 200 | Loss: 2.464 | Acc: 53.358,83.458,99.211,%
Batch: 220 | Loss: 2.473 | Acc: 53.182,83.449,99.190,%
Batch: 240 | Loss: 2.480 | Acc: 53.138,83.367,99.196,%
Batch: 260 | Loss: 2.480 | Acc: 52.999,83.324,99.150,%
Batch: 280 | Loss: 2.492 | Acc: 52.872,83.243,99.138,%
Batch: 300 | Loss: 2.501 | Acc: 52.705,83.254,99.123,%
Batch: 320 | Loss: 2.496 | Acc: 52.804,83.263,99.119,%
Batch: 340 | Loss: 2.504 | Acc: 52.706,83.273,99.118,%
Batch: 360 | Loss: 2.503 | Acc: 52.718,83.230,99.113,%
Batch: 380 | Loss: 2.505 | Acc: 52.764,83.118,99.088,%
Batch: 0 | Loss: 4.447 | Acc: 50.781,68.750,75.781,%
Batch: 20 | Loss: 4.576 | Acc: 44.717,65.253,70.536,%
Batch: 40 | Loss: 4.553 | Acc: 45.065,65.739,70.389,%
Batch: 60 | Loss: 4.560 | Acc: 44.915,65.292,70.722,%
Train all parameters

Epoch: 216
Batch: 0 | Loss: 2.656 | Acc: 48.438,88.281,97.656,%
Batch: 20 | Loss: 2.559 | Acc: 51.525,84.859,99.368,%
Batch: 40 | Loss: 2.535 | Acc: 52.134,84.108,99.295,%
Batch: 60 | Loss: 2.541 | Acc: 52.369,83.261,99.347,%
Batch: 80 | Loss: 2.516 | Acc: 52.749,83.738,99.334,%
Batch: 100 | Loss: 2.514 | Acc: 52.731,83.702,99.273,%
Batch: 120 | Loss: 2.508 | Acc: 52.628,83.626,99.264,%
Batch: 140 | Loss: 2.524 | Acc: 52.455,83.439,99.258,%
Batch: 160 | Loss: 2.499 | Acc: 52.737,83.569,99.238,%
Batch: 180 | Loss: 2.515 | Acc: 52.508,83.507,99.214,%
Batch: 200 | Loss: 2.519 | Acc: 52.589,83.392,99.199,%
Batch: 220 | Loss: 2.516 | Acc: 52.655,83.336,99.176,%
Batch: 240 | Loss: 2.519 | Acc: 52.597,83.325,99.177,%
Batch: 260 | Loss: 2.515 | Acc: 52.571,83.393,99.141,%
Batch: 280 | Loss: 2.516 | Acc: 52.619,83.419,99.166,%
Batch: 300 | Loss: 2.531 | Acc: 52.510,83.334,99.162,%
Batch: 320 | Loss: 2.532 | Acc: 52.439,83.372,99.168,%
Batch: 340 | Loss: 2.529 | Acc: 52.412,83.424,99.168,%
Batch: 360 | Loss: 2.533 | Acc: 52.402,83.364,99.143,%
Batch: 380 | Loss: 2.533 | Acc: 52.450,83.364,99.126,%
Batch: 0 | Loss: 4.245 | Acc: 50.000,74.219,75.781,%
Batch: 20 | Loss: 4.556 | Acc: 44.829,65.662,70.908,%
Batch: 40 | Loss: 4.549 | Acc: 45.465,65.396,70.522,%
Batch: 60 | Loss: 4.557 | Acc: 45.159,65.343,70.569,%
Train all parameters

Epoch: 217
Batch: 0 | Loss: 2.507 | Acc: 52.344,89.844,98.438,%
Batch: 20 | Loss: 2.435 | Acc: 53.534,85.900,99.070,%
Batch: 40 | Loss: 2.505 | Acc: 52.820,84.413,99.181,%
Batch: 60 | Loss: 2.470 | Acc: 53.381,84.119,99.244,%
Batch: 80 | Loss: 2.494 | Acc: 53.434,84.211,99.199,%
Batch: 100 | Loss: 2.486 | Acc: 53.458,84.158,99.250,%
Batch: 120 | Loss: 2.495 | Acc: 53.325,84.104,99.264,%
Batch: 140 | Loss: 2.507 | Acc: 53.347,83.871,99.269,%
Batch: 160 | Loss: 2.539 | Acc: 53.023,83.676,99.248,%
Batch: 180 | Loss: 2.548 | Acc: 52.922,83.525,99.266,%
Batch: 200 | Loss: 2.534 | Acc: 52.985,83.551,99.238,%
Batch: 220 | Loss: 2.540 | Acc: 52.888,83.403,99.233,%
Batch: 240 | Loss: 2.535 | Acc: 52.956,83.344,99.248,%
Batch: 260 | Loss: 2.526 | Acc: 53.026,83.360,99.222,%
Batch: 280 | Loss: 2.535 | Acc: 52.967,83.296,99.230,%
Batch: 300 | Loss: 2.527 | Acc: 52.936,83.358,99.219,%
Batch: 320 | Loss: 2.527 | Acc: 52.796,83.389,99.214,%
Batch: 340 | Loss: 2.528 | Acc: 52.758,83.376,99.210,%
Batch: 360 | Loss: 2.525 | Acc: 52.757,83.321,99.210,%
Batch: 380 | Loss: 2.527 | Acc: 52.760,83.335,99.194,%
Batch: 0 | Loss: 4.409 | Acc: 46.875,68.750,74.219,%
Batch: 20 | Loss: 4.723 | Acc: 43.378,64.211,69.420,%
Batch: 40 | Loss: 4.703 | Acc: 43.883,63.491,69.169,%
Batch: 60 | Loss: 4.695 | Acc: 43.763,63.512,69.134,%
Train all parameters

Epoch: 218
Batch: 0 | Loss: 2.264 | Acc: 58.594,78.906,99.219,%
Batch: 20 | Loss: 2.439 | Acc: 52.530,83.780,99.405,%
Batch: 40 | Loss: 2.457 | Acc: 52.515,83.841,99.409,%
Batch: 60 | Loss: 2.484 | Acc: 53.087,83.568,99.398,%
Batch: 80 | Loss: 2.492 | Acc: 53.038,83.661,99.354,%
Batch: 100 | Loss: 2.474 | Acc: 53.272,84.042,99.343,%
Batch: 120 | Loss: 2.475 | Acc: 53.196,84.033,99.309,%
Batch: 140 | Loss: 2.466 | Acc: 53.247,84.176,99.285,%
Batch: 160 | Loss: 2.483 | Acc: 52.863,84.123,99.258,%
Batch: 180 | Loss: 2.489 | Acc: 52.680,84.142,99.271,%
Batch: 200 | Loss: 2.495 | Acc: 52.643,83.986,99.285,%
Batch: 220 | Loss: 2.508 | Acc: 52.602,83.845,99.275,%
Batch: 240 | Loss: 2.513 | Acc: 52.606,83.714,99.245,%
Batch: 260 | Loss: 2.507 | Acc: 52.592,83.684,99.231,%
Batch: 280 | Loss: 2.504 | Acc: 52.697,83.688,99.213,%
Batch: 300 | Loss: 2.514 | Acc: 52.676,83.615,99.229,%
Batch: 320 | Loss: 2.520 | Acc: 52.592,83.628,99.214,%
Batch: 340 | Loss: 2.517 | Acc: 52.664,83.612,99.210,%
Batch: 360 | Loss: 2.517 | Acc: 52.679,83.667,99.208,%
Batch: 380 | Loss: 2.519 | Acc: 52.707,83.645,99.190,%
Batch: 0 | Loss: 4.227 | Acc: 51.562,69.531,72.656,%
Batch: 20 | Loss: 4.601 | Acc: 45.796,65.104,71.168,%
Batch: 40 | Loss: 4.606 | Acc: 45.427,64.958,70.103,%
Batch: 60 | Loss: 4.591 | Acc: 45.287,64.562,70.312,%
Train all parameters

Epoch: 219
Batch: 0 | Loss: 1.911 | Acc: 61.719,89.844,100.000,%
Batch: 20 | Loss: 2.327 | Acc: 54.985,86.124,99.479,%
Batch: 40 | Loss: 2.367 | Acc: 54.592,85.290,99.486,%
Batch: 60 | Loss: 2.492 | Acc: 53.215,84.541,99.436,%
Batch: 80 | Loss: 2.465 | Acc: 53.636,84.780,99.460,%
Batch: 100 | Loss: 2.462 | Acc: 53.419,85.017,99.389,%
Batch: 120 | Loss: 2.460 | Acc: 53.183,84.975,99.419,%
Batch: 140 | Loss: 2.495 | Acc: 52.842,84.608,99.396,%
Batch: 160 | Loss: 2.504 | Acc: 52.863,84.394,99.398,%
Batch: 180 | Loss: 2.494 | Acc: 52.983,84.328,99.366,%
Batch: 200 | Loss: 2.498 | Acc: 52.880,84.177,99.370,%
Batch: 220 | Loss: 2.502 | Acc: 52.817,83.968,99.335,%
Batch: 240 | Loss: 2.502 | Acc: 52.752,84.005,99.306,%
Batch: 260 | Loss: 2.500 | Acc: 52.781,83.899,99.291,%
Batch: 280 | Loss: 2.496 | Acc: 52.889,83.761,99.260,%
Batch: 300 | Loss: 2.499 | Acc: 52.790,83.833,99.237,%
Batch: 320 | Loss: 2.513 | Acc: 52.716,83.562,99.228,%
Batch: 340 | Loss: 2.521 | Acc: 52.747,83.417,99.219,%
Batch: 360 | Loss: 2.537 | Acc: 52.627,83.319,99.206,%
Batch: 380 | Loss: 2.545 | Acc: 52.528,83.303,99.174,%
Batch: 0 | Loss: 4.101 | Acc: 52.344,74.219,75.000,%
Batch: 20 | Loss: 4.552 | Acc: 44.866,65.141,70.908,%
Batch: 40 | Loss: 4.546 | Acc: 45.293,65.130,70.617,%
Batch: 60 | Loss: 4.550 | Acc: 45.351,65.087,70.671,%
Train all parameters

Epoch: 220
Batch: 0 | Loss: 2.240 | Acc: 60.938,79.688,99.219,%
Batch: 20 | Loss: 2.349 | Acc: 54.799,84.301,99.219,%
Batch: 40 | Loss: 2.480 | Acc: 53.125,84.356,99.276,%
Batch: 60 | Loss: 2.489 | Acc: 52.690,84.644,99.257,%
Batch: 80 | Loss: 2.487 | Acc: 52.730,84.443,99.257,%
Batch: 100 | Loss: 2.504 | Acc: 52.877,84.336,99.226,%
Batch: 120 | Loss: 2.502 | Acc: 52.893,84.201,99.238,%
Batch: 140 | Loss: 2.512 | Acc: 52.887,83.982,99.269,%
Batch: 160 | Loss: 2.520 | Acc: 52.979,83.662,99.262,%
Batch: 180 | Loss: 2.515 | Acc: 52.983,83.646,99.210,%
Batch: 200 | Loss: 2.517 | Acc: 52.876,83.749,99.211,%
Batch: 220 | Loss: 2.502 | Acc: 53.051,83.788,99.226,%
Batch: 240 | Loss: 2.509 | Acc: 52.914,83.620,99.228,%
Batch: 260 | Loss: 2.509 | Acc: 52.912,83.609,99.228,%
Batch: 280 | Loss: 2.506 | Acc: 52.953,83.602,99.233,%
Batch: 300 | Loss: 2.510 | Acc: 52.769,83.555,99.245,%
Batch: 320 | Loss: 2.517 | Acc: 52.726,83.423,99.226,%
Batch: 340 | Loss: 2.522 | Acc: 52.658,83.454,99.216,%
Batch: 360 | Loss: 2.522 | Acc: 52.692,83.334,99.191,%
Batch: 380 | Loss: 2.530 | Acc: 52.633,83.301,99.182,%
Batch: 0 | Loss: 4.220 | Acc: 50.781,71.875,75.000,%
Batch: 20 | Loss: 4.530 | Acc: 46.243,65.737,71.205,%
Batch: 40 | Loss: 4.537 | Acc: 46.284,65.415,70.484,%
Batch: 60 | Loss: 4.551 | Acc: 45.684,65.266,70.722,%
Train all parameters

Epoch: 221
Batch: 0 | Loss: 1.997 | Acc: 60.156,88.281,99.219,%
Batch: 20 | Loss: 2.461 | Acc: 53.609,85.454,99.107,%
Batch: 40 | Loss: 2.385 | Acc: 53.601,85.213,99.181,%
Batch: 60 | Loss: 2.411 | Acc: 53.701,85.028,99.129,%
Batch: 80 | Loss: 2.431 | Acc: 53.405,84.645,99.190,%
Batch: 100 | Loss: 2.435 | Acc: 53.589,84.669,99.157,%
Batch: 120 | Loss: 2.447 | Acc: 53.493,84.259,99.148,%
Batch: 140 | Loss: 2.446 | Acc: 53.596,83.954,99.163,%
Batch: 160 | Loss: 2.448 | Acc: 53.858,83.773,99.185,%
Batch: 180 | Loss: 2.441 | Acc: 53.902,83.715,99.189,%
Batch: 200 | Loss: 2.442 | Acc: 53.852,83.652,99.195,%
Batch: 220 | Loss: 2.456 | Acc: 53.553,83.527,99.176,%
Batch: 240 | Loss: 2.459 | Acc: 53.579,83.561,99.147,%
Batch: 260 | Loss: 2.465 | Acc: 53.499,83.438,99.144,%
Batch: 280 | Loss: 2.470 | Acc: 53.381,83.421,99.141,%
Batch: 300 | Loss: 2.471 | Acc: 53.390,83.391,99.118,%
Batch: 320 | Loss: 2.474 | Acc: 53.283,83.401,99.099,%
Batch: 340 | Loss: 2.472 | Acc: 53.249,83.452,99.109,%
Batch: 360 | Loss: 2.463 | Acc: 53.244,83.520,99.108,%
Batch: 380 | Loss: 2.469 | Acc: 53.275,83.469,99.088,%
Batch: 0 | Loss: 4.103 | Acc: 50.781,72.656,75.781,%
Batch: 20 | Loss: 4.606 | Acc: 44.829,66.071,70.796,%
Batch: 40 | Loss: 4.604 | Acc: 45.598,65.053,69.950,%
Batch: 60 | Loss: 4.604 | Acc: 45.223,65.215,70.184,%
Train all parameters

Epoch: 222
Batch: 0 | Loss: 3.019 | Acc: 43.750,82.812,99.219,%
Batch: 20 | Loss: 2.480 | Acc: 53.125,83.705,98.884,%
Batch: 40 | Loss: 2.372 | Acc: 53.830,84.527,99.085,%
Batch: 60 | Loss: 2.347 | Acc: 54.265,84.862,99.129,%
Batch: 80 | Loss: 2.379 | Acc: 53.762,84.828,99.113,%
Batch: 100 | Loss: 2.435 | Acc: 53.620,84.336,99.172,%
Batch: 120 | Loss: 2.422 | Acc: 53.680,84.349,99.212,%
Batch: 140 | Loss: 2.414 | Acc: 53.624,84.480,99.246,%
Batch: 160 | Loss: 2.417 | Acc: 53.508,84.535,99.199,%
Batch: 180 | Loss: 2.434 | Acc: 53.034,84.453,99.197,%
Batch: 200 | Loss: 2.439 | Acc: 53.098,84.340,99.215,%
Batch: 220 | Loss: 2.458 | Acc: 52.973,84.078,99.190,%
Batch: 240 | Loss: 2.459 | Acc: 52.999,84.106,99.180,%
Batch: 260 | Loss: 2.470 | Acc: 52.912,83.950,99.195,%
Batch: 280 | Loss: 2.471 | Acc: 52.905,83.905,99.199,%
Batch: 300 | Loss: 2.477 | Acc: 52.876,83.757,99.198,%
Batch: 320 | Loss: 2.473 | Acc: 52.896,83.747,99.214,%
Batch: 340 | Loss: 2.476 | Acc: 52.859,83.724,99.216,%
Batch: 360 | Loss: 2.488 | Acc: 52.755,83.631,99.204,%
Batch: 380 | Loss: 2.501 | Acc: 52.748,83.540,99.186,%
Batch: 0 | Loss: 4.080 | Acc: 53.125,71.875,76.562,%
Batch: 20 | Loss: 4.551 | Acc: 45.238,66.109,71.168,%
Batch: 40 | Loss: 4.562 | Acc: 45.560,65.263,70.274,%
Batch: 60 | Loss: 4.561 | Acc: 45.364,65.215,70.479,%
Train all parameters

Epoch: 223
Batch: 0 | Loss: 2.074 | Acc: 64.844,82.812,99.219,%
Batch: 20 | Loss: 2.459 | Acc: 54.204,84.673,99.368,%
Batch: 40 | Loss: 2.448 | Acc: 54.383,84.623,99.219,%
Batch: 60 | Loss: 2.469 | Acc: 53.407,84.273,99.270,%
Batch: 80 | Loss: 2.432 | Acc: 53.463,84.443,99.325,%
Batch: 100 | Loss: 2.446 | Acc: 53.218,84.429,99.358,%
Batch: 120 | Loss: 2.447 | Acc: 53.048,84.298,99.354,%
Batch: 140 | Loss: 2.448 | Acc: 53.059,84.403,99.280,%
Batch: 160 | Loss: 2.442 | Acc: 53.076,84.229,99.258,%
Batch: 180 | Loss: 2.455 | Acc: 52.922,84.207,99.223,%
Batch: 200 | Loss: 2.466 | Acc: 53.008,84.025,99.227,%
Batch: 220 | Loss: 2.468 | Acc: 52.941,83.891,99.233,%
Batch: 240 | Loss: 2.458 | Acc: 53.018,83.947,99.238,%
Batch: 260 | Loss: 2.453 | Acc: 53.065,83.902,99.231,%
Batch: 280 | Loss: 2.464 | Acc: 52.964,83.672,99.241,%
Batch: 300 | Loss: 2.483 | Acc: 52.777,83.513,99.224,%
Batch: 320 | Loss: 2.493 | Acc: 52.728,83.513,99.202,%
Batch: 340 | Loss: 2.499 | Acc: 52.623,83.449,99.189,%
Batch: 360 | Loss: 2.500 | Acc: 52.608,83.477,99.178,%
Batch: 380 | Loss: 2.502 | Acc: 52.614,83.442,99.178,%
Batch: 0 | Loss: 4.180 | Acc: 52.344,73.438,77.344,%
Batch: 20 | Loss: 4.575 | Acc: 45.350,65.402,70.387,%
Batch: 40 | Loss: 4.582 | Acc: 45.579,65.091,69.817,%
Batch: 60 | Loss: 4.572 | Acc: 45.287,65.241,70.172,%
Train all parameters

Epoch: 224
Batch: 0 | Loss: 3.010 | Acc: 53.906,83.594,99.219,%
Batch: 20 | Loss: 2.489 | Acc: 55.394,85.305,99.070,%
Batch: 40 | Loss: 2.433 | Acc: 54.707,85.499,99.219,%
Batch: 60 | Loss: 2.485 | Acc: 53.983,84.836,99.142,%
Batch: 80 | Loss: 2.496 | Acc: 53.607,84.587,99.122,%
Batch: 100 | Loss: 2.520 | Acc: 53.140,84.360,99.134,%
Batch: 120 | Loss: 2.517 | Acc: 53.060,84.517,99.148,%
Batch: 140 | Loss: 2.515 | Acc: 53.025,84.259,99.169,%
Batch: 160 | Loss: 2.516 | Acc: 52.989,84.089,99.156,%
Batch: 180 | Loss: 2.508 | Acc: 53.056,84.060,99.163,%
Batch: 200 | Loss: 2.515 | Acc: 53.074,83.951,99.145,%
Batch: 220 | Loss: 2.506 | Acc: 53.044,84.131,99.180,%
Batch: 240 | Loss: 2.518 | Acc: 52.969,84.005,99.170,%
Batch: 260 | Loss: 2.502 | Acc: 53.092,84.112,99.183,%
Batch: 280 | Loss: 2.497 | Acc: 53.128,84.058,99.188,%
Batch: 300 | Loss: 2.506 | Acc: 53.021,83.897,99.159,%
Batch: 320 | Loss: 2.519 | Acc: 52.908,83.742,99.158,%
Batch: 340 | Loss: 2.511 | Acc: 52.907,83.807,99.161,%
Batch: 360 | Loss: 2.516 | Acc: 52.865,83.728,99.139,%
Batch: 380 | Loss: 2.514 | Acc: 52.834,83.727,99.120,%
Batch: 0 | Loss: 4.146 | Acc: 52.344,74.219,75.000,%
Batch: 20 | Loss: 4.588 | Acc: 44.717,64.844,71.280,%
Batch: 40 | Loss: 4.600 | Acc: 44.817,64.825,70.636,%
Batch: 60 | Loss: 4.612 | Acc: 44.582,64.588,70.569,%
Train all parameters

Epoch: 225
Batch: 0 | Loss: 2.706 | Acc: 49.219,87.500,100.000,%
Batch: 20 | Loss: 2.452 | Acc: 52.753,85.268,99.591,%
Batch: 40 | Loss: 2.506 | Acc: 52.382,85.023,99.638,%
Batch: 60 | Loss: 2.487 | Acc: 52.293,85.502,99.641,%
Batch: 80 | Loss: 2.457 | Acc: 52.681,85.359,99.614,%
Batch: 100 | Loss: 2.446 | Acc: 52.653,85.613,99.613,%
Batch: 120 | Loss: 2.420 | Acc: 53.086,85.892,99.606,%
Batch: 140 | Loss: 2.426 | Acc: 53.042,85.871,99.596,%
Batch: 160 | Loss: 2.395 | Acc: 53.460,85.986,99.612,%
Batch: 180 | Loss: 2.398 | Acc: 53.522,85.963,99.607,%
Batch: 200 | Loss: 2.389 | Acc: 53.619,86.225,99.607,%
Batch: 220 | Loss: 2.399 | Acc: 53.429,86.135,99.622,%
Batch: 240 | Loss: 2.390 | Acc: 53.537,86.126,99.624,%
Batch: 260 | Loss: 2.396 | Acc: 53.409,86.081,99.632,%
Batch: 280 | Loss: 2.399 | Acc: 53.361,86.249,99.639,%
Batch: 300 | Loss: 2.416 | Acc: 53.374,86.184,99.634,%
Batch: 320 | Loss: 2.403 | Acc: 53.454,86.344,99.650,%
Batch: 340 | Loss: 2.400 | Acc: 53.510,86.329,99.668,%
Batch: 360 | Loss: 2.401 | Acc: 53.499,86.288,99.671,%
Batch: 380 | Loss: 2.399 | Acc: 53.517,86.290,99.674,%
Batch: 0 | Loss: 4.029 | Acc: 55.469,71.875,75.781,%
Batch: 20 | Loss: 4.402 | Acc: 46.689,66.443,72.507,%
Batch: 40 | Loss: 4.396 | Acc: 47.066,66.444,71.932,%
Batch: 60 | Loss: 4.399 | Acc: 46.529,66.509,72.182,%
Train all parameters

Epoch: 226
Batch: 0 | Loss: 1.783 | Acc: 57.031,89.062,100.000,%
Batch: 20 | Loss: 2.276 | Acc: 55.952,85.528,99.702,%
Batch: 40 | Loss: 2.434 | Acc: 53.411,85.347,99.771,%
Batch: 60 | Loss: 2.408 | Acc: 53.304,86.347,99.769,%
Batch: 80 | Loss: 2.376 | Acc: 53.762,86.497,99.759,%
Batch: 100 | Loss: 2.363 | Acc: 53.759,86.719,99.745,%
Batch: 120 | Loss: 2.359 | Acc: 54.055,86.751,99.768,%
Batch: 140 | Loss: 2.349 | Acc: 54.294,87.018,99.767,%
Batch: 160 | Loss: 2.347 | Acc: 54.387,87.044,99.757,%
Batch: 180 | Loss: 2.318 | Acc: 54.731,87.215,99.780,%
Batch: 200 | Loss: 2.337 | Acc: 54.509,87.092,99.751,%
Batch: 220 | Loss: 2.332 | Acc: 54.490,87.143,99.756,%
Batch: 240 | Loss: 2.326 | Acc: 54.503,87.098,99.763,%
Batch: 260 | Loss: 2.328 | Acc: 54.463,87.036,99.770,%
Batch: 280 | Loss: 2.322 | Acc: 54.537,87.152,99.772,%
Batch: 300 | Loss: 2.323 | Acc: 54.563,87.118,99.777,%
Batch: 320 | Loss: 2.337 | Acc: 54.471,87.096,99.786,%
Batch: 340 | Loss: 2.332 | Acc: 54.474,87.122,99.787,%
Batch: 360 | Loss: 2.339 | Acc: 54.421,87.072,99.771,%
Batch: 380 | Loss: 2.339 | Acc: 54.335,87.071,99.768,%
Batch: 0 | Loss: 4.012 | Acc: 56.250,71.875,76.562,%
Batch: 20 | Loss: 4.406 | Acc: 47.024,66.778,72.098,%
Batch: 40 | Loss: 4.395 | Acc: 47.142,66.749,71.837,%
Batch: 60 | Loss: 4.393 | Acc: 46.773,66.662,72.067,%
Train all parameters

Epoch: 227
Batch: 0 | Loss: 2.059 | Acc: 60.156,83.594,99.219,%
Batch: 20 | Loss: 2.303 | Acc: 54.836,86.533,99.888,%
Batch: 40 | Loss: 2.237 | Acc: 55.697,86.566,99.790,%
Batch: 60 | Loss: 2.261 | Acc: 54.675,87.205,99.757,%
Batch: 80 | Loss: 2.278 | Acc: 54.456,87.317,99.759,%
Batch: 100 | Loss: 2.311 | Acc: 53.759,87.183,99.783,%
Batch: 120 | Loss: 2.281 | Acc: 54.087,87.642,99.800,%
Batch: 140 | Loss: 2.285 | Acc: 54.023,87.572,99.795,%
Batch: 160 | Loss: 2.302 | Acc: 53.940,87.563,99.791,%
Batch: 180 | Loss: 2.313 | Acc: 53.915,87.453,99.801,%
Batch: 200 | Loss: 2.311 | Acc: 54.046,87.547,99.806,%
Batch: 220 | Loss: 2.314 | Acc: 53.998,87.691,99.809,%
Batch: 240 | Loss: 2.307 | Acc: 54.273,87.636,99.815,%
Batch: 260 | Loss: 2.314 | Acc: 54.322,87.482,99.811,%
Batch: 280 | Loss: 2.321 | Acc: 54.223,87.330,99.811,%
Batch: 300 | Loss: 2.315 | Acc: 54.280,87.388,99.811,%
Batch: 320 | Loss: 2.320 | Acc: 54.281,87.437,99.810,%
Batch: 340 | Loss: 2.325 | Acc: 54.206,87.468,99.817,%
Batch: 360 | Loss: 2.328 | Acc: 54.216,87.450,99.816,%
Batch: 380 | Loss: 2.326 | Acc: 54.308,87.408,99.820,%
Batch: 0 | Loss: 4.016 | Acc: 55.469,71.094,75.781,%
Batch: 20 | Loss: 4.394 | Acc: 46.987,66.183,72.098,%
Batch: 40 | Loss: 4.385 | Acc: 47.123,66.349,71.742,%
Batch: 60 | Loss: 4.385 | Acc: 46.785,66.163,72.016,%
Train all parameters

Epoch: 228
Batch: 0 | Loss: 2.507 | Acc: 44.531,90.625,100.000,%
Batch: 20 | Loss: 2.244 | Acc: 55.134,88.170,99.926,%
Batch: 40 | Loss: 2.283 | Acc: 55.488,87.805,99.924,%
Batch: 60 | Loss: 2.248 | Acc: 55.635,87.692,99.910,%
Batch: 80 | Loss: 2.244 | Acc: 55.334,87.915,99.923,%
Batch: 100 | Loss: 2.247 | Acc: 55.500,87.925,99.923,%
Batch: 120 | Loss: 2.247 | Acc: 55.411,87.894,99.903,%
Batch: 140 | Loss: 2.255 | Acc: 55.203,87.882,99.884,%
Batch: 160 | Loss: 2.255 | Acc: 55.284,87.942,99.888,%
Batch: 180 | Loss: 2.235 | Acc: 55.477,87.927,99.888,%
Batch: 200 | Loss: 2.242 | Acc: 55.340,87.900,99.899,%
Batch: 220 | Loss: 2.244 | Acc: 55.416,87.783,99.897,%
Batch: 240 | Loss: 2.230 | Acc: 55.504,87.892,99.883,%
Batch: 260 | Loss: 2.234 | Acc: 55.517,87.790,99.886,%
Batch: 280 | Loss: 2.230 | Acc: 55.502,87.797,99.883,%
Batch: 300 | Loss: 2.241 | Acc: 55.290,87.773,99.878,%
Batch: 320 | Loss: 2.243 | Acc: 55.223,87.836,99.876,%
Batch: 340 | Loss: 2.252 | Acc: 55.153,87.816,99.872,%
Batch: 360 | Loss: 2.258 | Acc: 55.125,87.838,99.870,%
Batch: 380 | Loss: 2.269 | Acc: 54.954,87.810,99.875,%
Batch: 0 | Loss: 3.983 | Acc: 55.469,71.875,76.562,%
Batch: 20 | Loss: 4.389 | Acc: 46.838,66.815,72.545,%
Batch: 40 | Loss: 4.379 | Acc: 46.913,66.749,72.027,%
Batch: 60 | Loss: 4.379 | Acc: 46.709,66.714,72.234,%
Train all parameters

Epoch: 229
Batch: 0 | Loss: 1.886 | Acc: 58.594,86.719,100.000,%
Batch: 20 | Loss: 2.159 | Acc: 56.101,89.435,99.888,%
Batch: 40 | Loss: 2.267 | Acc: 54.478,88.034,99.848,%
Batch: 60 | Loss: 2.248 | Acc: 54.444,88.128,99.859,%
Batch: 80 | Loss: 2.245 | Acc: 54.225,88.252,99.846,%
Batch: 100 | Loss: 2.201 | Acc: 54.688,88.668,99.861,%
Batch: 120 | Loss: 2.214 | Acc: 54.688,88.481,99.864,%
Batch: 140 | Loss: 2.226 | Acc: 54.460,88.580,99.867,%
Batch: 160 | Loss: 2.220 | Acc: 54.814,88.480,99.869,%
Batch: 180 | Loss: 2.244 | Acc: 54.675,88.359,99.879,%
Batch: 200 | Loss: 2.240 | Acc: 54.649,88.402,99.883,%
Batch: 220 | Loss: 2.255 | Acc: 54.543,88.302,99.873,%
Batch: 240 | Loss: 2.258 | Acc: 54.503,88.372,99.870,%
Batch: 260 | Loss: 2.261 | Acc: 54.505,88.347,99.874,%
Batch: 280 | Loss: 2.265 | Acc: 54.482,88.334,99.867,%
Batch: 300 | Loss: 2.273 | Acc: 54.444,88.333,99.868,%
Batch: 320 | Loss: 2.281 | Acc: 54.395,88.250,99.869,%
Batch: 340 | Loss: 2.287 | Acc: 54.309,88.208,99.865,%
Batch: 360 | Loss: 2.292 | Acc: 54.278,88.177,99.861,%
Batch: 380 | Loss: 2.295 | Acc: 54.185,88.142,99.859,%
Batch: 0 | Loss: 3.960 | Acc: 57.031,72.656,76.562,%
Batch: 20 | Loss: 4.395 | Acc: 46.689,66.667,72.507,%
Batch: 40 | Loss: 4.383 | Acc: 46.951,66.482,72.199,%
Batch: 60 | Loss: 4.386 | Acc: 46.657,66.381,72.400,%
Train all parameters

Epoch: 230
Batch: 0 | Loss: 2.529 | Acc: 52.344,81.250,100.000,%
Batch: 20 | Loss: 2.297 | Acc: 52.902,88.653,99.851,%
Batch: 40 | Loss: 2.222 | Acc: 54.840,89.177,99.867,%
Batch: 60 | Loss: 2.247 | Acc: 54.726,88.653,99.846,%
Batch: 80 | Loss: 2.248 | Acc: 55.093,88.252,99.865,%
Batch: 100 | Loss: 2.269 | Acc: 55.152,87.864,99.861,%
Batch: 120 | Loss: 2.285 | Acc: 54.791,87.745,99.871,%
Batch: 140 | Loss: 2.281 | Acc: 54.820,87.733,99.873,%
Batch: 160 | Loss: 2.290 | Acc: 54.775,87.723,99.879,%
Batch: 180 | Loss: 2.294 | Acc: 54.523,87.811,99.883,%
Batch: 200 | Loss: 2.299 | Acc: 54.586,87.613,99.880,%
Batch: 220 | Loss: 2.287 | Acc: 54.642,87.663,99.873,%
Batch: 240 | Loss: 2.280 | Acc: 54.681,87.740,99.874,%
Batch: 260 | Loss: 2.273 | Acc: 54.804,87.808,99.871,%
Batch: 280 | Loss: 2.278 | Acc: 54.743,87.792,99.872,%
Batch: 300 | Loss: 2.280 | Acc: 54.711,87.798,99.870,%
Batch: 320 | Loss: 2.273 | Acc: 54.821,87.826,99.869,%
Batch: 340 | Loss: 2.278 | Acc: 54.834,87.892,99.869,%
Batch: 360 | Loss: 2.273 | Acc: 54.941,87.861,99.870,%
Batch: 380 | Loss: 2.272 | Acc: 54.917,87.929,99.867,%
Batch: 0 | Loss: 4.023 | Acc: 53.125,71.875,77.344,%
Batch: 20 | Loss: 4.377 | Acc: 47.061,67.150,72.805,%
Batch: 40 | Loss: 4.370 | Acc: 47.123,66.845,72.389,%
Batch: 60 | Loss: 4.373 | Acc: 46.824,66.586,72.400,%
Train all parameters

Epoch: 231
Batch: 0 | Loss: 2.741 | Acc: 53.125,83.594,100.000,%
Batch: 20 | Loss: 2.277 | Acc: 56.473,87.760,99.814,%
Batch: 40 | Loss: 2.207 | Acc: 56.402,88.319,99.867,%
Batch: 60 | Loss: 2.271 | Acc: 55.213,88.243,99.872,%
Batch: 80 | Loss: 2.284 | Acc: 55.218,87.982,99.865,%
Batch: 100 | Loss: 2.307 | Acc: 55.152,87.709,99.861,%
Batch: 120 | Loss: 2.289 | Acc: 55.424,87.571,99.858,%
Batch: 140 | Loss: 2.296 | Acc: 55.247,87.694,99.861,%
Batch: 160 | Loss: 2.314 | Acc: 55.139,87.529,99.864,%
Batch: 180 | Loss: 2.312 | Acc: 55.041,87.517,99.866,%
Batch: 200 | Loss: 2.310 | Acc: 54.824,87.605,99.864,%
Batch: 220 | Loss: 2.318 | Acc: 54.794,87.595,99.866,%
Batch: 240 | Loss: 2.327 | Acc: 54.587,87.633,99.867,%
Batch: 260 | Loss: 2.322 | Acc: 54.661,87.629,99.874,%
Batch: 280 | Loss: 2.319 | Acc: 54.713,87.667,99.872,%
Batch: 300 | Loss: 2.315 | Acc: 54.771,87.565,99.873,%
Batch: 320 | Loss: 2.325 | Acc: 54.646,87.512,99.871,%
Batch: 340 | Loss: 2.328 | Acc: 54.552,87.518,99.867,%
Batch: 360 | Loss: 2.329 | Acc: 54.484,87.606,99.864,%
Batch: 380 | Loss: 2.331 | Acc: 54.435,87.609,99.865,%
Batch: 0 | Loss: 4.008 | Acc: 54.688,71.875,77.344,%
Batch: 20 | Loss: 4.383 | Acc: 47.098,67.001,72.731,%
Batch: 40 | Loss: 4.374 | Acc: 47.104,66.845,72.161,%
Batch: 60 | Loss: 4.377 | Acc: 46.849,66.496,72.272,%
Train all parameters

Epoch: 232
Batch: 0 | Loss: 2.492 | Acc: 50.781,81.250,100.000,%
Batch: 20 | Loss: 2.247 | Acc: 55.655,86.979,99.963,%
Batch: 40 | Loss: 2.325 | Acc: 54.973,86.871,99.962,%
Batch: 60 | Loss: 2.311 | Acc: 55.072,87.090,99.949,%
Batch: 80 | Loss: 2.326 | Acc: 54.504,87.269,99.932,%
Batch: 100 | Loss: 2.298 | Acc: 54.672,87.307,99.946,%
Batch: 120 | Loss: 2.297 | Acc: 54.765,87.326,99.923,%
Batch: 140 | Loss: 2.289 | Acc: 54.748,87.395,99.911,%
Batch: 160 | Loss: 2.306 | Acc: 54.372,87.466,99.918,%
Batch: 180 | Loss: 2.316 | Acc: 54.359,87.375,99.914,%
Batch: 200 | Loss: 2.326 | Acc: 54.248,87.484,99.899,%
Batch: 220 | Loss: 2.319 | Acc: 54.366,87.465,99.894,%
Batch: 240 | Loss: 2.314 | Acc: 54.383,87.545,99.890,%
Batch: 260 | Loss: 2.314 | Acc: 54.400,87.602,99.883,%
Batch: 280 | Loss: 2.306 | Acc: 54.443,87.614,99.886,%
Batch: 300 | Loss: 2.295 | Acc: 54.553,87.676,99.886,%
Batch: 320 | Loss: 2.295 | Acc: 54.580,87.736,99.888,%
Batch: 340 | Loss: 2.295 | Acc: 54.591,87.674,99.881,%
Batch: 360 | Loss: 2.303 | Acc: 54.456,87.656,99.879,%
Batch: 380 | Loss: 2.298 | Acc: 54.484,87.732,99.879,%
Batch: 0 | Loss: 4.013 | Acc: 55.469,71.875,78.125,%
Batch: 20 | Loss: 4.383 | Acc: 47.135,66.927,72.656,%
Batch: 40 | Loss: 4.380 | Acc: 47.104,66.692,72.294,%
Batch: 60 | Loss: 4.381 | Acc: 46.875,66.573,72.285,%
Train all parameters

Epoch: 233
Batch: 0 | Loss: 1.867 | Acc: 65.625,88.281,100.000,%
Batch: 20 | Loss: 2.318 | Acc: 53.683,88.504,99.888,%
Batch: 40 | Loss: 2.315 | Acc: 52.934,88.720,99.848,%
Batch: 60 | Loss: 2.283 | Acc: 53.663,88.614,99.821,%
Batch: 80 | Loss: 2.315 | Acc: 53.482,88.551,99.846,%
Batch: 100 | Loss: 2.339 | Acc: 53.411,88.475,99.861,%
Batch: 120 | Loss: 2.351 | Acc: 53.383,88.042,99.864,%
Batch: 140 | Loss: 2.346 | Acc: 53.585,87.993,99.873,%
Batch: 160 | Loss: 2.354 | Acc: 53.513,87.951,99.869,%
Batch: 180 | Loss: 2.336 | Acc: 53.509,88.182,99.866,%
Batch: 200 | Loss: 2.324 | Acc: 53.623,88.188,99.848,%
Batch: 220 | Loss: 2.322 | Acc: 53.729,88.257,99.852,%
Batch: 240 | Loss: 2.304 | Acc: 54.020,88.268,99.854,%
Batch: 260 | Loss: 2.296 | Acc: 54.023,88.383,99.847,%
Batch: 280 | Loss: 2.293 | Acc: 54.170,88.312,99.858,%
Batch: 300 | Loss: 2.297 | Acc: 54.205,88.175,99.865,%
Batch: 320 | Loss: 2.298 | Acc: 54.191,88.203,99.859,%
Batch: 340 | Loss: 2.295 | Acc: 54.275,88.187,99.863,%
Batch: 360 | Loss: 2.296 | Acc: 54.348,88.136,99.868,%
Batch: 380 | Loss: 2.300 | Acc: 54.380,88.039,99.863,%
Batch: 0 | Loss: 3.967 | Acc: 56.250,72.656,77.344,%
Batch: 20 | Loss: 4.379 | Acc: 47.247,67.225,72.545,%
Batch: 40 | Loss: 4.370 | Acc: 47.085,66.845,72.237,%
Batch: 60 | Loss: 4.369 | Acc: 46.913,66.701,72.413,%
Train all parameters

Epoch: 234
Batch: 0 | Loss: 1.479 | Acc: 64.844,92.188,99.219,%
Batch: 20 | Loss: 2.197 | Acc: 54.613,89.360,99.777,%
Batch: 40 | Loss: 2.250 | Acc: 54.935,88.377,99.848,%
Batch: 60 | Loss: 2.252 | Acc: 55.136,88.896,99.846,%
Batch: 80 | Loss: 2.247 | Acc: 55.208,88.841,99.855,%
Batch: 100 | Loss: 2.267 | Acc: 55.074,88.405,99.869,%
Batch: 120 | Loss: 2.243 | Acc: 55.333,88.417,99.877,%
Batch: 140 | Loss: 2.238 | Acc: 55.120,88.625,99.867,%
Batch: 160 | Loss: 2.243 | Acc: 55.042,88.655,99.854,%
Batch: 180 | Loss: 2.261 | Acc: 54.938,88.540,99.827,%
Batch: 200 | Loss: 2.271 | Acc: 54.960,88.390,99.825,%
Batch: 220 | Loss: 2.266 | Acc: 54.698,88.497,99.837,%
Batch: 240 | Loss: 2.281 | Acc: 54.542,88.382,99.838,%
Batch: 260 | Loss: 2.277 | Acc: 54.690,88.170,99.844,%
Batch: 280 | Loss: 2.277 | Acc: 54.582,88.228,99.850,%
Batch: 300 | Loss: 2.278 | Acc: 54.656,88.216,99.860,%
Batch: 320 | Loss: 2.281 | Acc: 54.610,88.257,99.856,%
Batch: 340 | Loss: 2.288 | Acc: 54.541,88.199,99.860,%
Batch: 360 | Loss: 2.295 | Acc: 54.330,88.132,99.855,%
Batch: 380 | Loss: 2.298 | Acc: 54.277,88.121,99.859,%
Batch: 0 | Loss: 3.973 | Acc: 55.469,73.438,77.344,%
Batch: 20 | Loss: 4.384 | Acc: 46.838,67.448,72.433,%
Batch: 40 | Loss: 4.375 | Acc: 47.085,66.806,72.085,%
Batch: 60 | Loss: 4.381 | Acc: 46.888,66.547,72.221,%
Train classifier parameters

Epoch: 235
Batch: 0 | Loss: 2.172 | Acc: 57.812,83.594,100.000,%
Batch: 20 | Loss: 2.302 | Acc: 54.129,88.988,99.814,%
Batch: 40 | Loss: 2.282 | Acc: 54.478,87.538,99.829,%
Batch: 60 | Loss: 2.265 | Acc: 54.777,88.051,99.846,%
Batch: 80 | Loss: 2.305 | Acc: 54.369,88.059,99.875,%
Batch: 100 | Loss: 2.315 | Acc: 53.636,88.482,99.884,%
Batch: 120 | Loss: 2.335 | Acc: 53.816,88.210,99.877,%
Batch: 140 | Loss: 2.340 | Acc: 53.812,88.148,99.867,%
Batch: 160 | Loss: 2.308 | Acc: 54.183,88.184,99.879,%
Batch: 180 | Loss: 2.308 | Acc: 54.247,88.173,99.888,%
Batch: 200 | Loss: 2.307 | Acc: 54.229,88.145,99.895,%
Batch: 220 | Loss: 2.308 | Acc: 54.242,88.066,99.890,%
Batch: 240 | Loss: 2.307 | Acc: 54.324,88.126,99.877,%
Batch: 260 | Loss: 2.305 | Acc: 54.313,88.093,99.874,%
Batch: 280 | Loss: 2.290 | Acc: 54.537,88.198,99.875,%
Batch: 300 | Loss: 2.282 | Acc: 54.680,88.190,99.881,%
Batch: 320 | Loss: 2.279 | Acc: 54.702,88.196,99.886,%
Batch: 340 | Loss: 2.278 | Acc: 54.761,88.132,99.881,%
Batch: 360 | Loss: 2.274 | Acc: 54.822,88.231,99.877,%
Batch: 380 | Loss: 2.281 | Acc: 54.770,88.179,99.875,%
Batch: 0 | Loss: 3.979 | Acc: 57.031,72.656,80.469,%
Batch: 20 | Loss: 4.369 | Acc: 47.284,66.927,72.879,%
Batch: 40 | Loss: 4.361 | Acc: 47.313,66.692,72.466,%
Batch: 60 | Loss: 4.366 | Acc: 47.041,66.470,72.477,%
Train classifier parameters

Epoch: 236
Batch: 0 | Loss: 1.839 | Acc: 60.938,89.844,100.000,%
Batch: 20 | Loss: 2.159 | Acc: 56.399,88.430,99.926,%
Batch: 40 | Loss: 2.164 | Acc: 56.650,89.158,99.886,%
Batch: 60 | Loss: 2.192 | Acc: 55.802,88.755,99.885,%
Batch: 80 | Loss: 2.193 | Acc: 56.182,88.764,99.855,%
Batch: 100 | Loss: 2.199 | Acc: 56.119,88.668,99.853,%
Batch: 120 | Loss: 2.195 | Acc: 56.211,88.694,99.864,%
Batch: 140 | Loss: 2.217 | Acc: 55.923,88.586,99.861,%
Batch: 160 | Loss: 2.219 | Acc: 55.770,88.762,99.854,%
Batch: 180 | Loss: 2.222 | Acc: 55.577,88.752,99.871,%
Batch: 200 | Loss: 2.222 | Acc: 55.500,88.732,99.864,%
Batch: 220 | Loss: 2.219 | Acc: 55.490,88.840,99.862,%
Batch: 240 | Loss: 2.242 | Acc: 55.222,88.732,99.861,%
Batch: 260 | Loss: 2.251 | Acc: 55.083,88.748,99.853,%
Batch: 280 | Loss: 2.248 | Acc: 55.185,88.687,99.855,%
Batch: 300 | Loss: 2.254 | Acc: 55.095,88.600,99.862,%
Batch: 320 | Loss: 2.252 | Acc: 55.174,88.610,99.866,%
Batch: 340 | Loss: 2.248 | Acc: 55.187,88.565,99.867,%
Batch: 360 | Loss: 2.246 | Acc: 55.203,88.576,99.868,%
Batch: 380 | Loss: 2.246 | Acc: 55.124,88.572,99.867,%
Batch: 0 | Loss: 3.996 | Acc: 56.250,72.656,77.344,%
Batch: 20 | Loss: 4.377 | Acc: 47.321,67.262,72.433,%
Batch: 40 | Loss: 4.374 | Acc: 47.294,66.806,72.046,%
Batch: 60 | Loss: 4.379 | Acc: 46.977,66.611,72.131,%
Train classifier parameters

Epoch: 237
Batch: 0 | Loss: 2.172 | Acc: 55.469,89.844,100.000,%
Batch: 20 | Loss: 2.253 | Acc: 55.394,88.839,99.926,%
Batch: 40 | Loss: 2.238 | Acc: 54.764,88.986,99.867,%
Batch: 60 | Loss: 2.317 | Acc: 53.624,88.665,99.859,%
Batch: 80 | Loss: 2.321 | Acc: 53.655,88.879,99.836,%
Batch: 100 | Loss: 2.306 | Acc: 53.643,88.861,99.861,%
Batch: 120 | Loss: 2.315 | Acc: 53.680,88.527,99.871,%
Batch: 140 | Loss: 2.282 | Acc: 54.095,88.785,99.873,%
Batch: 160 | Loss: 2.281 | Acc: 54.227,88.771,99.869,%
Batch: 180 | Loss: 2.278 | Acc: 54.338,88.791,99.862,%
Batch: 200 | Loss: 2.271 | Acc: 54.474,88.903,99.864,%
Batch: 220 | Loss: 2.282 | Acc: 54.553,88.667,99.862,%
Batch: 240 | Loss: 2.282 | Acc: 54.480,88.583,99.870,%
Batch: 260 | Loss: 2.284 | Acc: 54.568,88.494,99.874,%
Batch: 280 | Loss: 2.269 | Acc: 54.696,88.582,99.878,%
Batch: 300 | Loss: 2.282 | Acc: 54.407,88.530,99.878,%
Batch: 320 | Loss: 2.279 | Acc: 54.524,88.542,99.876,%
Batch: 340 | Loss: 2.285 | Acc: 54.449,88.577,99.874,%
Batch: 360 | Loss: 2.283 | Acc: 54.445,88.578,99.872,%
Batch: 380 | Loss: 2.284 | Acc: 54.431,88.603,99.871,%
Batch: 0 | Loss: 3.952 | Acc: 57.031,73.438,78.906,%
Batch: 20 | Loss: 4.385 | Acc: 46.726,67.113,72.210,%
Batch: 40 | Loss: 4.376 | Acc: 47.008,66.768,71.951,%
Batch: 60 | Loss: 4.379 | Acc: 46.760,66.573,72.106,%
Train classifier parameters

Epoch: 238
Batch: 0 | Loss: 2.366 | Acc: 50.000,92.188,99.219,%
Batch: 20 | Loss: 2.157 | Acc: 55.729,89.174,99.888,%
Batch: 40 | Loss: 2.195 | Acc: 54.992,89.425,99.848,%
Batch: 60 | Loss: 2.246 | Acc: 54.905,88.934,99.872,%
Batch: 80 | Loss: 2.247 | Acc: 55.044,88.628,99.875,%
Batch: 100 | Loss: 2.254 | Acc: 55.175,88.699,99.869,%
Batch: 120 | Loss: 2.293 | Acc: 54.720,88.378,99.858,%
Batch: 140 | Loss: 2.287 | Acc: 54.915,88.281,99.850,%
Batch: 160 | Loss: 2.273 | Acc: 55.280,88.179,99.850,%
Batch: 180 | Loss: 2.273 | Acc: 55.344,88.040,99.858,%
Batch: 200 | Loss: 2.294 | Acc: 55.030,88.009,99.868,%
Batch: 220 | Loss: 2.294 | Acc: 54.949,88.002,99.866,%
Batch: 240 | Loss: 2.310 | Acc: 54.788,87.944,99.867,%
Batch: 260 | Loss: 2.304 | Acc: 54.711,88.063,99.868,%
Batch: 280 | Loss: 2.299 | Acc: 54.840,88.017,99.875,%
Batch: 300 | Loss: 2.298 | Acc: 54.820,88.183,99.873,%
Batch: 320 | Loss: 2.294 | Acc: 54.843,88.225,99.871,%
Batch: 340 | Loss: 2.287 | Acc: 54.816,88.265,99.876,%
Batch: 360 | Loss: 2.286 | Acc: 54.824,88.244,99.872,%
Batch: 380 | Loss: 2.291 | Acc: 54.696,88.218,99.875,%
Batch: 0 | Loss: 3.961 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.379 | Acc: 46.838,67.225,72.470,%
Batch: 40 | Loss: 4.370 | Acc: 47.180,66.902,72.046,%
Batch: 60 | Loss: 4.374 | Acc: 46.862,66.586,72.195,%
Train classifier parameters

Epoch: 239
Batch: 0 | Loss: 2.160 | Acc: 59.375,82.812,100.000,%
Batch: 20 | Loss: 2.125 | Acc: 57.552,90.030,99.814,%
Batch: 40 | Loss: 2.255 | Acc: 55.545,89.158,99.886,%
Batch: 60 | Loss: 2.214 | Acc: 55.840,89.011,99.885,%
Batch: 80 | Loss: 2.251 | Acc: 55.035,88.773,99.884,%
Batch: 100 | Loss: 2.232 | Acc: 55.237,88.776,99.876,%
Batch: 120 | Loss: 2.250 | Acc: 55.223,88.436,99.890,%
Batch: 140 | Loss: 2.239 | Acc: 55.286,88.353,99.895,%
Batch: 160 | Loss: 2.241 | Acc: 55.347,88.446,99.898,%
Batch: 180 | Loss: 2.243 | Acc: 55.426,88.411,99.905,%
Batch: 200 | Loss: 2.242 | Acc: 55.399,88.378,99.914,%
Batch: 220 | Loss: 2.239 | Acc: 55.320,88.366,99.905,%
Batch: 240 | Loss: 2.242 | Acc: 55.242,88.359,99.890,%
Batch: 260 | Loss: 2.248 | Acc: 55.172,88.407,99.895,%
Batch: 280 | Loss: 2.252 | Acc: 55.152,88.392,99.897,%
Batch: 300 | Loss: 2.249 | Acc: 55.186,88.429,99.901,%
Batch: 320 | Loss: 2.258 | Acc: 55.031,88.335,99.905,%
Batch: 340 | Loss: 2.269 | Acc: 54.912,88.290,99.904,%
Batch: 360 | Loss: 2.268 | Acc: 54.817,88.342,99.907,%
Batch: 380 | Loss: 2.264 | Acc: 54.823,88.417,99.910,%
Batch: 0 | Loss: 3.999 | Acc: 56.250,72.656,77.344,%
Batch: 20 | Loss: 4.381 | Acc: 47.396,67.188,72.359,%
Batch: 40 | Loss: 4.375 | Acc: 47.485,66.673,71.989,%
Batch: 60 | Loss: 4.375 | Acc: 47.157,66.432,72.195,%
Train classifier parameters

Epoch: 240
Batch: 0 | Loss: 2.200 | Acc: 58.594,83.594,100.000,%
Batch: 20 | Loss: 2.165 | Acc: 57.329,87.463,99.851,%
Batch: 40 | Loss: 2.283 | Acc: 55.488,87.900,99.867,%
Batch: 60 | Loss: 2.238 | Acc: 55.328,88.473,99.859,%
Batch: 80 | Loss: 2.237 | Acc: 55.305,88.754,99.875,%
Batch: 100 | Loss: 2.246 | Acc: 55.190,88.683,99.869,%
Batch: 120 | Loss: 2.234 | Acc: 55.282,88.862,99.858,%
Batch: 140 | Loss: 2.248 | Acc: 55.264,88.403,99.873,%
Batch: 160 | Loss: 2.258 | Acc: 55.032,88.441,99.864,%
Batch: 180 | Loss: 2.272 | Acc: 55.067,88.264,99.871,%
Batch: 200 | Loss: 2.265 | Acc: 55.107,88.324,99.880,%
Batch: 220 | Loss: 2.267 | Acc: 55.140,88.239,99.883,%
Batch: 240 | Loss: 2.254 | Acc: 55.219,88.122,99.890,%
Batch: 260 | Loss: 2.249 | Acc: 55.208,88.218,99.886,%
Batch: 280 | Loss: 2.244 | Acc: 55.238,88.320,99.886,%
Batch: 300 | Loss: 2.244 | Acc: 55.240,88.398,99.888,%
Batch: 320 | Loss: 2.258 | Acc: 55.074,88.332,99.890,%
Batch: 340 | Loss: 2.265 | Acc: 54.992,88.293,99.892,%
Batch: 360 | Loss: 2.261 | Acc: 55.045,88.363,99.887,%
Batch: 380 | Loss: 2.262 | Acc: 55.020,88.390,99.887,%
Batch: 0 | Loss: 3.991 | Acc: 55.469,71.875,80.469,%
Batch: 20 | Loss: 4.387 | Acc: 46.987,66.815,72.470,%
Batch: 40 | Loss: 4.373 | Acc: 47.180,66.578,72.161,%
Batch: 60 | Loss: 4.375 | Acc: 46.965,66.342,72.362,%
Train classifier parameters

Epoch: 241
Batch: 0 | Loss: 1.448 | Acc: 64.062,92.969,100.000,%
Batch: 20 | Loss: 2.280 | Acc: 54.241,88.504,99.851,%
Batch: 40 | Loss: 2.261 | Acc: 54.935,88.129,99.867,%
Batch: 60 | Loss: 2.267 | Acc: 54.559,88.499,99.846,%
Batch: 80 | Loss: 2.262 | Acc: 54.668,88.696,99.826,%
Batch: 100 | Loss: 2.252 | Acc: 54.610,88.761,99.838,%
Batch: 120 | Loss: 2.236 | Acc: 54.713,88.920,99.864,%
Batch: 140 | Loss: 2.217 | Acc: 54.915,89.157,99.867,%
Batch: 160 | Loss: 2.228 | Acc: 54.707,89.174,99.859,%
Batch: 180 | Loss: 2.224 | Acc: 54.731,89.032,99.866,%
Batch: 200 | Loss: 2.221 | Acc: 54.789,89.097,99.876,%
Batch: 220 | Loss: 2.219 | Acc: 54.656,89.161,99.869,%
Batch: 240 | Loss: 2.221 | Acc: 54.736,89.150,99.867,%
Batch: 260 | Loss: 2.223 | Acc: 54.789,89.110,99.871,%
Batch: 280 | Loss: 2.224 | Acc: 54.807,89.040,99.875,%
Batch: 300 | Loss: 2.233 | Acc: 54.758,88.964,99.883,%
Batch: 320 | Loss: 2.235 | Acc: 54.702,88.992,99.888,%
Batch: 340 | Loss: 2.238 | Acc: 54.623,88.909,99.892,%
Batch: 360 | Loss: 2.243 | Acc: 54.601,88.796,99.890,%
Batch: 380 | Loss: 2.249 | Acc: 54.657,88.726,99.893,%
Batch: 0 | Loss: 4.010 | Acc: 55.469,71.875,77.344,%
Batch: 20 | Loss: 4.379 | Acc: 47.024,66.853,72.582,%
Batch: 40 | Loss: 4.371 | Acc: 47.142,66.730,72.142,%
Batch: 60 | Loss: 4.372 | Acc: 46.849,66.522,72.464,%
Train classifier parameters

Epoch: 242
Batch: 0 | Loss: 1.645 | Acc: 57.812,94.531,100.000,%
Batch: 20 | Loss: 2.317 | Acc: 55.357,88.430,99.926,%
Batch: 40 | Loss: 2.327 | Acc: 54.421,87.862,99.867,%
Batch: 60 | Loss: 2.289 | Acc: 54.598,88.717,99.910,%
Batch: 80 | Loss: 2.317 | Acc: 54.389,88.571,99.904,%
Batch: 100 | Loss: 2.331 | Acc: 54.223,88.212,99.892,%
Batch: 120 | Loss: 2.289 | Acc: 54.894,88.307,99.877,%
Batch: 140 | Loss: 2.303 | Acc: 54.809,88.392,99.873,%
Batch: 160 | Loss: 2.301 | Acc: 54.755,88.475,99.884,%
Batch: 180 | Loss: 2.315 | Acc: 54.644,88.199,99.888,%
Batch: 200 | Loss: 2.337 | Acc: 54.159,88.270,99.895,%
Batch: 220 | Loss: 2.338 | Acc: 54.231,88.228,99.894,%
Batch: 240 | Loss: 2.328 | Acc: 54.405,88.268,99.896,%
Batch: 260 | Loss: 2.317 | Acc: 54.412,88.404,99.898,%
Batch: 280 | Loss: 2.315 | Acc: 54.448,88.481,99.897,%
Batch: 300 | Loss: 2.314 | Acc: 54.516,88.491,99.896,%
Batch: 320 | Loss: 2.317 | Acc: 54.627,88.393,99.890,%
Batch: 340 | Loss: 2.329 | Acc: 54.348,88.325,99.890,%
Batch: 360 | Loss: 2.316 | Acc: 54.486,88.340,99.890,%
Batch: 380 | Loss: 2.309 | Acc: 54.546,88.367,99.891,%
Batch: 0 | Loss: 3.977 | Acc: 55.469,74.219,79.688,%
Batch: 20 | Loss: 4.375 | Acc: 47.284,67.262,72.656,%
Batch: 40 | Loss: 4.368 | Acc: 47.294,66.806,72.104,%
Batch: 60 | Loss: 4.370 | Acc: 46.875,66.534,72.272,%
Train classifier parameters

Epoch: 243
Batch: 0 | Loss: 2.799 | Acc: 45.312,89.062,100.000,%
Batch: 20 | Loss: 2.294 | Acc: 54.129,87.872,99.926,%
Batch: 40 | Loss: 2.205 | Acc: 55.202,88.624,99.905,%
Batch: 60 | Loss: 2.186 | Acc: 55.815,88.947,99.923,%
Batch: 80 | Loss: 2.198 | Acc: 55.691,88.513,99.913,%
Batch: 100 | Loss: 2.217 | Acc: 55.430,88.451,99.899,%
Batch: 120 | Loss: 2.221 | Acc: 55.617,88.359,99.897,%
Batch: 140 | Loss: 2.235 | Acc: 55.203,88.558,99.889,%
Batch: 160 | Loss: 2.238 | Acc: 55.158,88.631,99.879,%
Batch: 180 | Loss: 2.250 | Acc: 55.033,88.575,99.888,%
Batch: 200 | Loss: 2.233 | Acc: 55.072,88.627,99.883,%
Batch: 220 | Loss: 2.251 | Acc: 54.832,88.529,99.890,%
Batch: 240 | Loss: 2.242 | Acc: 54.898,88.602,99.883,%
Batch: 260 | Loss: 2.252 | Acc: 54.738,88.500,99.889,%
Batch: 280 | Loss: 2.253 | Acc: 54.699,88.495,99.892,%
Batch: 300 | Loss: 2.254 | Acc: 54.685,88.536,99.899,%
Batch: 320 | Loss: 2.259 | Acc: 54.663,88.493,99.900,%
Batch: 340 | Loss: 2.261 | Acc: 54.637,88.558,99.904,%
Batch: 360 | Loss: 2.262 | Acc: 54.675,88.515,99.900,%
Batch: 380 | Loss: 2.259 | Acc: 54.780,88.501,99.904,%
Batch: 0 | Loss: 4.023 | Acc: 55.469,72.656,78.125,%
Batch: 20 | Loss: 4.378 | Acc: 47.061,67.113,72.805,%
Batch: 40 | Loss: 4.372 | Acc: 47.218,66.864,72.275,%
Batch: 60 | Loss: 4.374 | Acc: 46.977,66.534,72.362,%
Train classifier parameters

Epoch: 244
Batch: 0 | Loss: 1.703 | Acc: 59.375,90.625,100.000,%
Batch: 20 | Loss: 2.274 | Acc: 53.720,88.690,99.963,%
Batch: 40 | Loss: 2.261 | Acc: 54.345,88.720,99.867,%
Batch: 60 | Loss: 2.208 | Acc: 54.534,88.819,99.910,%
Batch: 80 | Loss: 2.217 | Acc: 54.765,88.580,99.904,%
Batch: 100 | Loss: 2.221 | Acc: 54.865,88.714,99.869,%
Batch: 120 | Loss: 2.208 | Acc: 55.217,88.804,99.871,%
Batch: 140 | Loss: 2.218 | Acc: 55.114,88.896,99.856,%
Batch: 160 | Loss: 2.218 | Acc: 54.993,88.927,99.869,%
Batch: 180 | Loss: 2.227 | Acc: 54.830,88.916,99.879,%
Batch: 200 | Loss: 2.236 | Acc: 54.808,88.860,99.887,%
Batch: 220 | Loss: 2.252 | Acc: 54.755,88.649,99.887,%
Batch: 240 | Loss: 2.255 | Acc: 54.807,88.641,99.880,%
Batch: 260 | Loss: 2.259 | Acc: 54.726,88.679,99.886,%
Batch: 280 | Loss: 2.261 | Acc: 54.679,88.604,99.892,%
Batch: 300 | Loss: 2.257 | Acc: 54.695,88.665,99.894,%
Batch: 320 | Loss: 2.272 | Acc: 54.532,88.629,99.888,%
Batch: 340 | Loss: 2.265 | Acc: 54.669,88.641,99.885,%
Batch: 360 | Loss: 2.265 | Acc: 54.688,88.560,99.883,%
Batch: 380 | Loss: 2.265 | Acc: 54.653,88.495,99.887,%
Batch: 0 | Loss: 3.961 | Acc: 56.250,72.656,79.688,%
Batch: 20 | Loss: 4.379 | Acc: 47.061,67.001,72.731,%
Batch: 40 | Loss: 4.367 | Acc: 47.218,66.787,72.104,%
Batch: 60 | Loss: 4.369 | Acc: 46.939,66.598,72.323,%
Train classifier parameters

Epoch: 245
Batch: 0 | Loss: 2.265 | Acc: 57.812,93.750,100.000,%
Batch: 20 | Loss: 2.364 | Acc: 54.948,88.393,100.000,%
Batch: 40 | Loss: 2.343 | Acc: 53.849,88.643,99.962,%
Batch: 60 | Loss: 2.315 | Acc: 53.817,89.191,99.910,%
Batch: 80 | Loss: 2.334 | Acc: 53.993,88.764,99.923,%
Batch: 100 | Loss: 2.303 | Acc: 54.417,88.567,99.923,%
Batch: 120 | Loss: 2.284 | Acc: 54.636,88.701,99.890,%
Batch: 140 | Loss: 2.275 | Acc: 54.549,88.857,99.900,%
Batch: 160 | Loss: 2.283 | Acc: 54.479,88.791,99.898,%
Batch: 180 | Loss: 2.298 | Acc: 54.368,88.588,99.901,%
Batch: 200 | Loss: 2.301 | Acc: 54.334,88.561,99.899,%
Batch: 220 | Loss: 2.315 | Acc: 54.214,88.532,99.894,%
Batch: 240 | Loss: 2.315 | Acc: 54.117,88.534,99.896,%
Batch: 260 | Loss: 2.298 | Acc: 54.364,88.515,99.895,%
Batch: 280 | Loss: 2.277 | Acc: 54.543,88.668,99.897,%
Batch: 300 | Loss: 2.281 | Acc: 54.547,88.645,99.896,%
Batch: 320 | Loss: 2.289 | Acc: 54.459,88.593,99.898,%
Batch: 340 | Loss: 2.291 | Acc: 54.417,88.536,99.899,%
Batch: 360 | Loss: 2.289 | Acc: 54.540,88.489,99.903,%
Batch: 380 | Loss: 2.283 | Acc: 54.577,88.492,99.900,%
Batch: 0 | Loss: 3.992 | Acc: 56.250,72.656,78.125,%
Batch: 20 | Loss: 4.374 | Acc: 47.098,67.485,72.396,%
Batch: 40 | Loss: 4.366 | Acc: 47.161,66.902,72.142,%
Batch: 60 | Loss: 4.367 | Acc: 46.977,66.688,72.182,%
Train classifier parameters

Epoch: 246
Batch: 0 | Loss: 2.262 | Acc: 53.906,93.750,100.000,%
Batch: 20 | Loss: 2.352 | Acc: 54.204,87.612,99.851,%
Batch: 40 | Loss: 2.373 | Acc: 53.411,88.319,99.905,%
Batch: 60 | Loss: 2.309 | Acc: 55.033,88.320,99.910,%
Batch: 80 | Loss: 2.349 | Acc: 54.610,88.301,99.875,%
Batch: 100 | Loss: 2.319 | Acc: 54.602,88.266,99.876,%
Batch: 120 | Loss: 2.303 | Acc: 54.681,88.165,99.845,%
Batch: 140 | Loss: 2.292 | Acc: 54.732,88.154,99.856,%
Batch: 160 | Loss: 2.273 | Acc: 55.090,88.141,99.869,%
Batch: 180 | Loss: 2.293 | Acc: 54.752,87.949,99.879,%
Batch: 200 | Loss: 2.298 | Acc: 54.688,87.928,99.891,%
Batch: 220 | Loss: 2.299 | Acc: 54.610,88.055,99.901,%
Batch: 240 | Loss: 2.288 | Acc: 54.684,88.084,99.896,%
Batch: 260 | Loss: 2.282 | Acc: 54.801,88.203,99.901,%
Batch: 280 | Loss: 2.273 | Acc: 54.821,88.201,99.903,%
Batch: 300 | Loss: 2.283 | Acc: 54.765,88.131,99.907,%
Batch: 320 | Loss: 2.286 | Acc: 54.761,88.089,99.910,%
Batch: 340 | Loss: 2.279 | Acc: 54.894,88.158,99.913,%
Batch: 360 | Loss: 2.279 | Acc: 54.891,88.141,99.909,%
Batch: 380 | Loss: 2.268 | Acc: 55.005,88.191,99.902,%
Batch: 0 | Loss: 3.974 | Acc: 55.469,72.656,77.344,%
Batch: 20 | Loss: 4.375 | Acc: 46.987,67.039,72.470,%
Batch: 40 | Loss: 4.363 | Acc: 47.180,66.845,72.237,%
Batch: 60 | Loss: 4.363 | Acc: 46.926,66.624,72.477,%
Train classifier parameters

Epoch: 247
Batch: 0 | Loss: 1.673 | Acc: 60.938,91.406,100.000,%
Batch: 20 | Loss: 2.349 | Acc: 53.125,88.318,99.926,%
Batch: 40 | Loss: 2.256 | Acc: 55.240,88.796,99.829,%
Batch: 60 | Loss: 2.257 | Acc: 55.353,88.409,99.846,%
Batch: 80 | Loss: 2.259 | Acc: 55.247,88.474,99.865,%
Batch: 100 | Loss: 2.287 | Acc: 54.804,88.096,99.869,%
Batch: 120 | Loss: 2.279 | Acc: 54.888,88.243,99.871,%
Batch: 140 | Loss: 2.267 | Acc: 54.953,88.387,99.884,%
Batch: 160 | Loss: 2.259 | Acc: 55.129,88.320,99.884,%
Batch: 180 | Loss: 2.259 | Acc: 55.201,88.208,99.883,%
Batch: 200 | Loss: 2.265 | Acc: 54.983,88.165,99.883,%
Batch: 220 | Loss: 2.250 | Acc: 55.094,88.288,99.883,%
Batch: 240 | Loss: 2.249 | Acc: 55.112,88.366,99.877,%
Batch: 260 | Loss: 2.247 | Acc: 55.089,88.473,99.877,%
Batch: 280 | Loss: 2.248 | Acc: 55.066,88.495,99.883,%
Batch: 300 | Loss: 2.257 | Acc: 54.893,88.538,99.878,%
Batch: 320 | Loss: 2.254 | Acc: 54.960,88.517,99.883,%
Batch: 340 | Loss: 2.261 | Acc: 54.850,88.474,99.879,%
Batch: 360 | Loss: 2.260 | Acc: 54.917,88.470,99.881,%
Batch: 380 | Loss: 2.276 | Acc: 54.792,88.431,99.879,%
Batch: 0 | Loss: 3.986 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.378 | Acc: 47.173,67.150,72.321,%
Batch: 40 | Loss: 4.372 | Acc: 47.313,66.540,72.161,%
Batch: 60 | Loss: 4.373 | Acc: 46.926,66.355,72.362,%
Train classifier parameters

Epoch: 248
Batch: 0 | Loss: 2.076 | Acc: 54.688,89.844,99.219,%
Batch: 20 | Loss: 2.252 | Acc: 54.464,88.430,99.888,%
Batch: 40 | Loss: 2.324 | Acc: 53.830,88.014,99.867,%
Batch: 60 | Loss: 2.279 | Acc: 55.085,88.102,99.885,%
Batch: 80 | Loss: 2.297 | Acc: 55.083,87.731,99.904,%
Batch: 100 | Loss: 2.314 | Acc: 55.082,87.376,99.899,%
Batch: 120 | Loss: 2.337 | Acc: 54.759,87.339,99.884,%
Batch: 140 | Loss: 2.308 | Acc: 54.976,87.578,99.895,%
Batch: 160 | Loss: 2.310 | Acc: 54.828,87.573,99.898,%
Batch: 180 | Loss: 2.319 | Acc: 54.757,87.496,99.892,%
Batch: 200 | Loss: 2.330 | Acc: 54.435,87.620,99.899,%
Batch: 220 | Loss: 2.328 | Acc: 54.458,87.687,99.897,%
Batch: 240 | Loss: 2.323 | Acc: 54.454,87.844,99.890,%
Batch: 260 | Loss: 2.323 | Acc: 54.445,87.952,99.892,%
Batch: 280 | Loss: 2.301 | Acc: 54.635,88.114,99.897,%
Batch: 300 | Loss: 2.299 | Acc: 54.651,88.071,99.901,%
Batch: 320 | Loss: 2.299 | Acc: 54.666,88.089,99.900,%
Batch: 340 | Loss: 2.301 | Acc: 54.614,88.155,99.899,%
Batch: 360 | Loss: 2.292 | Acc: 54.625,88.095,99.900,%
Batch: 380 | Loss: 2.285 | Acc: 54.784,88.070,99.900,%
Batch: 0 | Loss: 3.994 | Acc: 55.469,72.656,78.125,%
Batch: 20 | Loss: 4.380 | Acc: 47.135,67.225,72.545,%
Batch: 40 | Loss: 4.373 | Acc: 47.104,66.768,72.046,%
Batch: 60 | Loss: 4.376 | Acc: 46.901,66.381,72.259,%
Train classifier parameters

Epoch: 249
Batch: 0 | Loss: 2.301 | Acc: 55.469,82.031,100.000,%
Batch: 20 | Loss: 2.404 | Acc: 54.390,87.165,99.926,%
Batch: 40 | Loss: 2.309 | Acc: 55.240,87.443,99.886,%
Batch: 60 | Loss: 2.290 | Acc: 55.059,88.281,99.885,%
Batch: 80 | Loss: 2.318 | Acc: 54.736,88.156,99.904,%
Batch: 100 | Loss: 2.295 | Acc: 55.121,88.134,99.915,%
Batch: 120 | Loss: 2.277 | Acc: 55.152,88.372,99.916,%
Batch: 140 | Loss: 2.275 | Acc: 55.014,88.442,99.917,%
Batch: 160 | Loss: 2.282 | Acc: 54.751,88.432,99.918,%
Batch: 180 | Loss: 2.281 | Acc: 54.696,88.402,99.922,%
Batch: 200 | Loss: 2.264 | Acc: 54.952,88.433,99.922,%
Batch: 220 | Loss: 2.268 | Acc: 55.009,88.359,99.919,%
Batch: 240 | Loss: 2.266 | Acc: 55.135,88.288,99.909,%
Batch: 260 | Loss: 2.274 | Acc: 54.942,88.197,99.913,%
Batch: 280 | Loss: 2.263 | Acc: 55.107,88.262,99.908,%
Batch: 300 | Loss: 2.265 | Acc: 55.040,88.268,99.901,%
Batch: 320 | Loss: 2.265 | Acc: 55.009,88.225,99.898,%
Batch: 340 | Loss: 2.262 | Acc: 55.015,88.284,99.895,%
Batch: 360 | Loss: 2.259 | Acc: 55.016,88.392,99.890,%
Batch: 380 | Loss: 2.264 | Acc: 54.985,88.365,99.889,%
Batch: 0 | Loss: 3.979 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.381 | Acc: 46.949,67.076,72.656,%
Batch: 40 | Loss: 4.372 | Acc: 47.085,66.711,72.066,%
Batch: 60 | Loss: 4.374 | Acc: 46.849,66.573,72.234,%
Train all parameters

Epoch: 250
Batch: 0 | Loss: 2.211 | Acc: 54.688,90.625,100.000,%
Batch: 20 | Loss: 2.257 | Acc: 54.167,88.988,99.851,%
Batch: 40 | Loss: 2.287 | Acc: 54.383,88.357,99.867,%
Batch: 60 | Loss: 2.281 | Acc: 54.572,88.384,99.885,%
Batch: 80 | Loss: 2.269 | Acc: 54.736,88.513,99.875,%
Batch: 100 | Loss: 2.271 | Acc: 54.695,88.637,99.876,%
Batch: 120 | Loss: 2.263 | Acc: 54.965,88.520,99.897,%
Batch: 140 | Loss: 2.248 | Acc: 55.153,88.647,99.884,%
Batch: 160 | Loss: 2.244 | Acc: 55.265,88.456,99.893,%
Batch: 180 | Loss: 2.241 | Acc: 55.249,88.463,99.888,%
Batch: 200 | Loss: 2.242 | Acc: 55.344,88.413,99.895,%
Batch: 220 | Loss: 2.254 | Acc: 55.193,88.225,99.894,%
Batch: 240 | Loss: 2.254 | Acc: 55.070,88.297,99.900,%
Batch: 260 | Loss: 2.253 | Acc: 54.975,88.353,99.904,%
Batch: 280 | Loss: 2.251 | Acc: 54.949,88.379,99.903,%
Batch: 300 | Loss: 2.252 | Acc: 54.877,88.346,99.904,%
Batch: 320 | Loss: 2.244 | Acc: 55.018,88.340,99.900,%
Batch: 340 | Loss: 2.245 | Acc: 55.107,88.254,99.895,%
Batch: 360 | Loss: 2.247 | Acc: 55.058,88.242,99.900,%
Batch: 380 | Loss: 2.256 | Acc: 54.948,88.214,99.897,%
Batch: 0 | Loss: 3.976 | Acc: 54.688,73.438,78.125,%
Batch: 20 | Loss: 4.385 | Acc: 46.987,66.964,72.247,%
Batch: 40 | Loss: 4.371 | Acc: 47.275,66.902,72.085,%
Batch: 60 | Loss: 4.373 | Acc: 46.837,66.688,72.234,%
Train all parameters

Epoch: 251
Batch: 0 | Loss: 2.166 | Acc: 51.562,92.188,100.000,%
Batch: 20 | Loss: 2.190 | Acc: 56.362,88.170,99.926,%
Batch: 40 | Loss: 2.217 | Acc: 55.716,88.796,99.924,%
Batch: 60 | Loss: 2.165 | Acc: 56.583,89.127,99.910,%
Batch: 80 | Loss: 2.181 | Acc: 55.903,89.014,99.923,%
Batch: 100 | Loss: 2.225 | Acc: 55.322,88.707,99.930,%
Batch: 120 | Loss: 2.214 | Acc: 55.365,88.733,99.923,%
Batch: 140 | Loss: 2.222 | Acc: 55.269,88.586,99.895,%
Batch: 160 | Loss: 2.226 | Acc: 55.202,88.485,99.898,%
Batch: 180 | Loss: 2.231 | Acc: 55.149,88.376,99.879,%
Batch: 200 | Loss: 2.233 | Acc: 55.162,88.266,99.880,%
Batch: 220 | Loss: 2.229 | Acc: 55.158,88.352,99.873,%
Batch: 240 | Loss: 2.240 | Acc: 54.976,88.314,99.877,%
Batch: 260 | Loss: 2.253 | Acc: 54.699,88.323,99.880,%
Batch: 280 | Loss: 2.259 | Acc: 54.582,88.242,99.872,%
Batch: 300 | Loss: 2.261 | Acc: 54.547,88.341,99.865,%
Batch: 320 | Loss: 2.256 | Acc: 54.529,88.418,99.864,%
Batch: 340 | Loss: 2.253 | Acc: 54.676,88.405,99.869,%
Batch: 360 | Loss: 2.256 | Acc: 54.586,88.411,99.866,%
Batch: 380 | Loss: 2.254 | Acc: 54.683,88.355,99.861,%
Batch: 0 | Loss: 3.960 | Acc: 53.906,72.656,77.344,%
Batch: 20 | Loss: 4.366 | Acc: 47.396,67.225,72.879,%
Batch: 40 | Loss: 4.362 | Acc: 47.370,66.806,72.218,%
Batch: 60 | Loss: 4.367 | Acc: 47.093,66.573,72.131,%
Train all parameters

Epoch: 252
Batch: 0 | Loss: 2.214 | Acc: 55.469,84.375,100.000,%
Batch: 20 | Loss: 2.187 | Acc: 56.957,87.872,99.888,%
Batch: 40 | Loss: 2.199 | Acc: 56.269,87.862,99.905,%
Batch: 60 | Loss: 2.233 | Acc: 55.699,87.769,99.923,%
Batch: 80 | Loss: 2.214 | Acc: 55.720,88.098,99.932,%
Batch: 100 | Loss: 2.214 | Acc: 55.608,88.266,99.915,%
Batch: 120 | Loss: 2.234 | Acc: 55.424,88.004,99.916,%
Batch: 140 | Loss: 2.252 | Acc: 55.314,87.993,99.911,%
Batch: 160 | Loss: 2.253 | Acc: 55.153,88.039,99.903,%
Batch: 180 | Loss: 2.258 | Acc: 55.059,87.975,99.901,%
Batch: 200 | Loss: 2.270 | Acc: 55.010,87.986,99.883,%
Batch: 220 | Loss: 2.248 | Acc: 55.239,88.221,99.883,%
Batch: 240 | Loss: 2.252 | Acc: 55.187,88.194,99.880,%
Batch: 260 | Loss: 2.248 | Acc: 55.169,88.248,99.886,%
Batch: 280 | Loss: 2.259 | Acc: 55.029,88.223,99.886,%
Batch: 300 | Loss: 2.257 | Acc: 55.137,88.211,99.891,%
Batch: 320 | Loss: 2.264 | Acc: 55.060,88.087,99.886,%
Batch: 340 | Loss: 2.264 | Acc: 55.102,88.052,99.883,%
Batch: 360 | Loss: 2.264 | Acc: 55.055,88.041,99.887,%
Batch: 380 | Loss: 2.270 | Acc: 54.958,88.047,99.883,%
Batch: 0 | Loss: 4.009 | Acc: 56.250,71.094,79.688,%
Batch: 20 | Loss: 4.363 | Acc: 47.321,67.150,72.693,%
Batch: 40 | Loss: 4.363 | Acc: 47.332,66.711,72.180,%
Batch: 60 | Loss: 4.369 | Acc: 46.888,66.560,72.285,%
Train all parameters

Epoch: 253
Batch: 0 | Loss: 2.115 | Acc: 57.812,84.375,100.000,%
Batch: 20 | Loss: 2.426 | Acc: 53.199,86.310,99.851,%
Batch: 40 | Loss: 2.297 | Acc: 54.421,88.072,99.905,%
Batch: 60 | Loss: 2.307 | Acc: 54.419,87.871,99.898,%
Batch: 80 | Loss: 2.269 | Acc: 54.803,88.030,99.865,%
Batch: 100 | Loss: 2.268 | Acc: 54.757,88.057,99.861,%
Batch: 120 | Loss: 2.281 | Acc: 54.707,88.023,99.884,%
Batch: 140 | Loss: 2.290 | Acc: 54.804,87.938,99.873,%
Batch: 160 | Loss: 2.280 | Acc: 54.925,88.053,99.874,%
Batch: 180 | Loss: 2.294 | Acc: 54.735,88.040,99.875,%
Batch: 200 | Loss: 2.286 | Acc: 54.956,88.056,99.876,%
Batch: 220 | Loss: 2.291 | Acc: 54.956,88.034,99.876,%
Batch: 240 | Loss: 2.287 | Acc: 54.914,88.168,99.883,%
Batch: 260 | Loss: 2.293 | Acc: 54.837,88.138,99.889,%
Batch: 280 | Loss: 2.306 | Acc: 54.713,88.012,99.894,%
Batch: 300 | Loss: 2.314 | Acc: 54.581,88.058,99.891,%
Batch: 320 | Loss: 2.318 | Acc: 54.537,88.052,99.888,%
Batch: 340 | Loss: 2.308 | Acc: 54.651,88.052,99.885,%
Batch: 360 | Loss: 2.306 | Acc: 54.666,87.991,99.881,%
Batch: 380 | Loss: 2.296 | Acc: 54.718,88.037,99.881,%
Batch: 0 | Loss: 4.012 | Acc: 54.688,70.312,75.781,%
Batch: 20 | Loss: 4.375 | Acc: 47.284,66.481,72.173,%
Batch: 40 | Loss: 4.369 | Acc: 47.466,66.368,71.704,%
Batch: 60 | Loss: 4.367 | Acc: 47.093,66.573,72.106,%
Train all parameters

Epoch: 254
Batch: 0 | Loss: 1.712 | Acc: 53.125,92.188,100.000,%
Batch: 20 | Loss: 2.017 | Acc: 58.147,89.360,99.814,%
Batch: 40 | Loss: 2.195 | Acc: 55.678,88.891,99.848,%
Batch: 60 | Loss: 2.170 | Acc: 55.776,89.178,99.859,%
Batch: 80 | Loss: 2.192 | Acc: 55.604,89.130,99.884,%
Batch: 100 | Loss: 2.183 | Acc: 55.523,89.264,99.869,%
Batch: 120 | Loss: 2.195 | Acc: 55.527,88.914,99.877,%
Batch: 140 | Loss: 2.198 | Acc: 55.436,88.863,99.850,%
Batch: 160 | Loss: 2.224 | Acc: 55.066,88.689,99.854,%
Batch: 180 | Loss: 2.227 | Acc: 55.041,88.592,99.862,%
Batch: 200 | Loss: 2.224 | Acc: 55.100,88.549,99.868,%
Batch: 220 | Loss: 2.208 | Acc: 55.402,88.561,99.866,%
Batch: 240 | Loss: 2.212 | Acc: 55.303,88.592,99.867,%
Batch: 260 | Loss: 2.205 | Acc: 55.460,88.536,99.871,%
Batch: 280 | Loss: 2.212 | Acc: 55.427,88.451,99.878,%
Batch: 300 | Loss: 2.203 | Acc: 55.445,88.598,99.878,%
Batch: 320 | Loss: 2.208 | Acc: 55.520,88.551,99.878,%
Batch: 340 | Loss: 2.213 | Acc: 55.423,88.524,99.881,%
Batch: 360 | Loss: 2.216 | Acc: 55.356,88.504,99.885,%
Batch: 380 | Loss: 2.221 | Acc: 55.403,88.400,99.889,%
Batch: 0 | Loss: 3.976 | Acc: 55.469,72.656,77.344,%
Batch: 20 | Loss: 4.367 | Acc: 47.359,67.001,72.247,%
Batch: 40 | Loss: 4.363 | Acc: 47.523,66.787,72.123,%
Batch: 60 | Loss: 4.366 | Acc: 47.182,66.624,72.221,%
Train all parameters

Epoch: 255
Batch: 0 | Loss: 2.988 | Acc: 41.406,85.156,100.000,%
Batch: 20 | Loss: 2.213 | Acc: 54.650,89.062,99.926,%
Batch: 40 | Loss: 2.235 | Acc: 55.088,88.091,99.943,%
Batch: 60 | Loss: 2.309 | Acc: 54.547,87.141,99.910,%
Batch: 80 | Loss: 2.265 | Acc: 55.266,87.876,99.904,%
Batch: 100 | Loss: 2.243 | Acc: 55.144,88.026,99.915,%
Batch: 120 | Loss: 2.276 | Acc: 54.604,88.049,99.910,%
Batch: 140 | Loss: 2.260 | Acc: 54.887,87.977,99.911,%
Batch: 160 | Loss: 2.262 | Acc: 55.017,87.709,99.908,%
Batch: 180 | Loss: 2.273 | Acc: 54.860,87.759,99.896,%
Batch: 200 | Loss: 2.259 | Acc: 55.107,87.725,99.891,%
Batch: 220 | Loss: 2.261 | Acc: 55.221,87.645,99.901,%
Batch: 240 | Loss: 2.260 | Acc: 55.222,87.740,99.893,%
Batch: 260 | Loss: 2.257 | Acc: 55.157,87.841,99.889,%
Batch: 280 | Loss: 2.254 | Acc: 55.132,87.759,99.892,%
Batch: 300 | Loss: 2.242 | Acc: 55.225,87.895,99.894,%
Batch: 320 | Loss: 2.244 | Acc: 55.335,87.860,99.893,%
Batch: 340 | Loss: 2.251 | Acc: 55.327,87.789,99.897,%
Batch: 360 | Loss: 2.252 | Acc: 55.354,87.801,99.900,%
Batch: 380 | Loss: 2.249 | Acc: 55.331,87.844,99.895,%
Batch: 0 | Loss: 3.970 | Acc: 55.469,71.875,78.125,%
Batch: 20 | Loss: 4.367 | Acc: 47.247,66.815,72.284,%
Batch: 40 | Loss: 4.357 | Acc: 47.523,66.635,71.970,%
Batch: 60 | Loss: 4.358 | Acc: 47.157,66.573,72.195,%
Train all parameters

Epoch: 256
Batch: 0 | Loss: 1.951 | Acc: 64.844,84.375,100.000,%
Batch: 20 | Loss: 2.144 | Acc: 57.440,89.583,99.740,%
Batch: 40 | Loss: 2.182 | Acc: 56.574,89.024,99.771,%
Batch: 60 | Loss: 2.225 | Acc: 56.186,88.947,99.834,%
Batch: 80 | Loss: 2.248 | Acc: 55.748,88.937,99.817,%
Batch: 100 | Loss: 2.256 | Acc: 55.322,88.753,99.838,%
Batch: 120 | Loss: 2.248 | Acc: 55.417,88.740,99.839,%
Batch: 140 | Loss: 2.240 | Acc: 55.297,88.730,99.834,%
Batch: 160 | Loss: 2.233 | Acc: 55.537,88.563,99.830,%
Batch: 180 | Loss: 2.240 | Acc: 55.611,88.368,99.836,%
Batch: 200 | Loss: 2.240 | Acc: 55.457,88.410,99.841,%
Batch: 220 | Loss: 2.244 | Acc: 55.493,88.288,99.844,%
Batch: 240 | Loss: 2.237 | Acc: 55.673,88.281,99.844,%
Batch: 260 | Loss: 2.250 | Acc: 55.529,88.093,99.847,%
Batch: 280 | Loss: 2.245 | Acc: 55.441,88.181,99.850,%
Batch: 300 | Loss: 2.245 | Acc: 55.334,88.227,99.857,%
Batch: 320 | Loss: 2.247 | Acc: 55.306,88.245,99.864,%
Batch: 340 | Loss: 2.251 | Acc: 55.253,88.144,99.858,%
Batch: 360 | Loss: 2.240 | Acc: 55.417,88.195,99.861,%
Batch: 380 | Loss: 2.233 | Acc: 55.477,88.216,99.865,%
Batch: 0 | Loss: 3.950 | Acc: 54.688,72.656,77.344,%
Batch: 20 | Loss: 4.369 | Acc: 47.359,66.927,72.731,%
Batch: 40 | Loss: 4.359 | Acc: 47.447,66.635,72.085,%
Batch: 60 | Loss: 4.362 | Acc: 47.144,66.688,72.157,%
Train all parameters

Epoch: 257
Batch: 0 | Loss: 1.711 | Acc: 60.938,93.750,99.219,%
Batch: 20 | Loss: 2.207 | Acc: 53.869,88.802,99.888,%
Batch: 40 | Loss: 2.234 | Acc: 54.459,88.567,99.924,%
Batch: 60 | Loss: 2.207 | Acc: 54.918,88.845,99.885,%
Batch: 80 | Loss: 2.226 | Acc: 54.823,88.744,99.865,%
Batch: 100 | Loss: 2.219 | Acc: 54.912,88.560,99.876,%
Batch: 120 | Loss: 2.245 | Acc: 54.649,88.494,99.884,%
Batch: 140 | Loss: 2.233 | Acc: 54.771,88.431,99.884,%
Batch: 160 | Loss: 2.225 | Acc: 54.901,88.529,99.879,%
Batch: 180 | Loss: 2.247 | Acc: 54.722,88.419,99.883,%
Batch: 200 | Loss: 2.231 | Acc: 54.948,88.410,99.880,%
Batch: 220 | Loss: 2.228 | Acc: 54.995,88.412,99.876,%
Batch: 240 | Loss: 2.215 | Acc: 55.158,88.450,99.874,%
Batch: 260 | Loss: 2.219 | Acc: 55.125,88.464,99.874,%
Batch: 280 | Loss: 2.223 | Acc: 55.260,88.401,99.883,%
Batch: 300 | Loss: 2.234 | Acc: 55.134,88.281,99.891,%
Batch: 320 | Loss: 2.228 | Acc: 55.147,88.364,99.895,%
Batch: 340 | Loss: 2.229 | Acc: 55.173,88.332,99.892,%
Batch: 360 | Loss: 2.233 | Acc: 55.166,88.338,99.896,%
Batch: 380 | Loss: 2.234 | Acc: 55.239,88.347,99.897,%
Batch: 0 | Loss: 3.990 | Acc: 53.906,72.656,78.906,%
Batch: 20 | Loss: 4.372 | Acc: 47.247,67.076,72.619,%
Batch: 40 | Loss: 4.367 | Acc: 47.409,66.730,72.180,%
Batch: 60 | Loss: 4.368 | Acc: 47.067,66.637,72.310,%
Train all parameters

Epoch: 258
Batch: 0 | Loss: 1.898 | Acc: 64.844,86.719,100.000,%
Batch: 20 | Loss: 2.208 | Acc: 54.725,89.137,99.851,%
Batch: 40 | Loss: 2.245 | Acc: 54.764,88.472,99.867,%
Batch: 60 | Loss: 2.251 | Acc: 54.150,88.640,99.885,%
Batch: 80 | Loss: 2.223 | Acc: 54.794,89.304,99.884,%
Batch: 100 | Loss: 2.231 | Acc: 54.610,89.039,99.892,%
Batch: 120 | Loss: 2.225 | Acc: 54.946,89.121,99.903,%
Batch: 140 | Loss: 2.256 | Acc: 54.688,88.869,99.889,%
Batch: 160 | Loss: 2.269 | Acc: 54.571,88.776,99.893,%
Batch: 180 | Loss: 2.249 | Acc: 54.882,88.734,99.896,%
Batch: 200 | Loss: 2.247 | Acc: 54.746,88.915,99.895,%
Batch: 220 | Loss: 2.240 | Acc: 54.938,88.812,99.894,%
Batch: 240 | Loss: 2.228 | Acc: 55.025,88.874,99.893,%
Batch: 260 | Loss: 2.230 | Acc: 55.014,88.670,99.889,%
Batch: 280 | Loss: 2.237 | Acc: 55.010,88.620,99.892,%
Batch: 300 | Loss: 2.230 | Acc: 55.116,88.639,99.891,%
Batch: 320 | Loss: 2.238 | Acc: 54.958,88.593,99.898,%
Batch: 340 | Loss: 2.239 | Acc: 54.937,88.611,99.901,%
Batch: 360 | Loss: 2.238 | Acc: 55.029,88.619,99.900,%
Batch: 380 | Loss: 2.238 | Acc: 54.917,88.691,99.900,%
Batch: 0 | Loss: 3.967 | Acc: 54.688,72.656,77.344,%
Batch: 20 | Loss: 4.383 | Acc: 46.875,66.853,72.433,%
Batch: 40 | Loss: 4.373 | Acc: 47.046,66.444,71.970,%
Batch: 60 | Loss: 4.373 | Acc: 46.734,66.381,72.285,%
Train all parameters

Epoch: 259
Batch: 0 | Loss: 1.773 | Acc: 57.031,89.844,99.219,%
Batch: 20 | Loss: 2.129 | Acc: 55.692,90.960,99.814,%
Batch: 40 | Loss: 2.194 | Acc: 55.793,89.482,99.867,%
Batch: 60 | Loss: 2.193 | Acc: 55.879,89.178,99.846,%
Batch: 80 | Loss: 2.215 | Acc: 55.633,89.024,99.846,%
Batch: 100 | Loss: 2.216 | Acc: 55.554,89.086,99.861,%
Batch: 120 | Loss: 2.213 | Acc: 55.475,89.056,99.871,%
Batch: 140 | Loss: 2.222 | Acc: 55.557,88.918,99.884,%
Batch: 160 | Loss: 2.223 | Acc: 55.444,88.936,99.859,%
Batch: 180 | Loss: 2.236 | Acc: 55.292,88.821,99.871,%
Batch: 200 | Loss: 2.229 | Acc: 55.344,88.907,99.868,%
Batch: 220 | Loss: 2.217 | Acc: 55.543,88.865,99.876,%
Batch: 240 | Loss: 2.212 | Acc: 55.423,88.939,99.874,%
Batch: 260 | Loss: 2.219 | Acc: 55.358,88.880,99.874,%
Batch: 280 | Loss: 2.213 | Acc: 55.408,88.923,99.875,%
Batch: 300 | Loss: 2.210 | Acc: 55.391,88.847,99.862,%
Batch: 320 | Loss: 2.211 | Acc: 55.388,88.822,99.859,%
Batch: 340 | Loss: 2.215 | Acc: 55.391,88.769,99.860,%
Batch: 360 | Loss: 2.216 | Acc: 55.343,88.755,99.866,%
Batch: 380 | Loss: 2.218 | Acc: 55.385,88.706,99.869,%
Batch: 0 | Loss: 3.977 | Acc: 55.469,71.875,79.688,%
Batch: 20 | Loss: 4.382 | Acc: 46.949,66.778,72.098,%
Batch: 40 | Loss: 4.370 | Acc: 47.161,66.635,71.799,%
Batch: 60 | Loss: 4.373 | Acc: 46.875,66.586,71.990,%
Train all parameters

Epoch: 260
Batch: 0 | Loss: 2.672 | Acc: 53.906,86.719,100.000,%
Batch: 20 | Loss: 2.190 | Acc: 55.246,88.988,100.000,%
Batch: 40 | Loss: 2.219 | Acc: 55.145,87.957,99.905,%
Batch: 60 | Loss: 2.208 | Acc: 55.379,88.230,99.885,%
Batch: 80 | Loss: 2.249 | Acc: 55.015,88.146,99.913,%
Batch: 100 | Loss: 2.230 | Acc: 55.422,87.949,99.907,%
Batch: 120 | Loss: 2.217 | Acc: 55.591,88.029,99.897,%
Batch: 140 | Loss: 2.223 | Acc: 55.895,88.032,99.889,%
Batch: 160 | Loss: 2.229 | Acc: 55.770,88.043,99.898,%
Batch: 180 | Loss: 2.223 | Acc: 55.874,87.953,99.896,%
Batch: 200 | Loss: 2.231 | Acc: 55.861,88.079,99.895,%
Batch: 220 | Loss: 2.233 | Acc: 55.638,88.200,99.897,%
Batch: 240 | Loss: 2.229 | Acc: 55.618,88.216,99.893,%
Batch: 260 | Loss: 2.227 | Acc: 55.574,88.413,99.895,%
Batch: 280 | Loss: 2.217 | Acc: 55.725,88.498,99.894,%
Batch: 300 | Loss: 2.214 | Acc: 55.759,88.479,99.901,%
Batch: 320 | Loss: 2.230 | Acc: 55.517,88.401,99.895,%
Batch: 340 | Loss: 2.238 | Acc: 55.482,88.350,99.892,%
Batch: 360 | Loss: 2.237 | Acc: 55.493,88.392,99.894,%
Batch: 380 | Loss: 2.235 | Acc: 55.485,88.421,99.895,%
Batch: 0 | Loss: 3.999 | Acc: 55.469,71.875,78.906,%
Batch: 20 | Loss: 4.380 | Acc: 47.024,66.853,72.545,%
Batch: 40 | Loss: 4.368 | Acc: 47.332,66.502,72.046,%
Batch: 60 | Loss: 4.369 | Acc: 47.041,66.457,72.259,%
Train all parameters

Epoch: 261
Batch: 0 | Loss: 1.700 | Acc: 64.844,91.406,99.219,%
Batch: 20 | Loss: 2.110 | Acc: 57.329,89.546,99.888,%
Batch: 40 | Loss: 2.125 | Acc: 56.688,89.177,99.924,%
Batch: 60 | Loss: 2.174 | Acc: 55.751,89.203,99.910,%
Batch: 80 | Loss: 2.207 | Acc: 55.488,88.609,99.923,%
Batch: 100 | Loss: 2.189 | Acc: 55.778,88.769,99.923,%
Batch: 120 | Loss: 2.181 | Acc: 55.804,88.675,99.910,%
Batch: 140 | Loss: 2.174 | Acc: 55.895,88.647,99.889,%
Batch: 160 | Loss: 2.156 | Acc: 55.920,88.810,99.893,%
Batch: 180 | Loss: 2.181 | Acc: 55.521,88.769,99.888,%
Batch: 200 | Loss: 2.183 | Acc: 55.698,88.752,99.895,%
Batch: 220 | Loss: 2.196 | Acc: 55.547,88.723,99.894,%
Batch: 240 | Loss: 2.202 | Acc: 55.339,88.800,99.900,%
Batch: 260 | Loss: 2.208 | Acc: 55.202,88.748,99.901,%
Batch: 280 | Loss: 2.214 | Acc: 55.207,88.768,99.903,%
Batch: 300 | Loss: 2.211 | Acc: 55.214,88.780,99.901,%
Batch: 320 | Loss: 2.218 | Acc: 55.065,88.863,99.903,%
Batch: 340 | Loss: 2.230 | Acc: 54.937,88.742,99.904,%
Batch: 360 | Loss: 2.241 | Acc: 54.869,88.673,99.900,%
Batch: 380 | Loss: 2.245 | Acc: 54.839,88.632,99.900,%
Batch: 0 | Loss: 3.976 | Acc: 54.688,73.438,76.562,%
Batch: 20 | Loss: 4.374 | Acc: 47.210,66.741,72.582,%
Batch: 40 | Loss: 4.368 | Acc: 47.237,66.482,72.085,%
Batch: 60 | Loss: 4.372 | Acc: 46.721,66.509,72.221,%
Train all parameters

Epoch: 262
Batch: 0 | Loss: 2.145 | Acc: 60.938,88.281,100.000,%
Batch: 20 | Loss: 2.196 | Acc: 55.469,89.397,99.963,%
Batch: 40 | Loss: 2.287 | Acc: 54.383,88.281,99.943,%
Batch: 60 | Loss: 2.315 | Acc: 54.457,87.910,99.910,%
Batch: 80 | Loss: 2.281 | Acc: 55.295,88.030,99.923,%
Batch: 100 | Loss: 2.266 | Acc: 55.213,88.134,99.915,%
Batch: 120 | Loss: 2.252 | Acc: 55.282,88.230,99.910,%
Batch: 140 | Loss: 2.255 | Acc: 55.136,88.193,99.911,%
Batch: 160 | Loss: 2.259 | Acc: 55.017,88.136,99.913,%
Batch: 180 | Loss: 2.262 | Acc: 54.886,88.229,99.918,%
Batch: 200 | Loss: 2.254 | Acc: 54.855,88.355,99.918,%
Batch: 220 | Loss: 2.251 | Acc: 55.030,88.348,99.912,%
Batch: 240 | Loss: 2.250 | Acc: 54.947,88.330,99.909,%
Batch: 260 | Loss: 2.243 | Acc: 55.047,88.275,99.913,%
Batch: 280 | Loss: 2.240 | Acc: 55.093,88.348,99.914,%
Batch: 300 | Loss: 2.237 | Acc: 55.209,88.302,99.914,%
Batch: 320 | Loss: 2.235 | Acc: 55.303,88.352,99.908,%
Batch: 340 | Loss: 2.239 | Acc: 55.329,88.332,99.911,%
Batch: 360 | Loss: 2.230 | Acc: 55.547,88.344,99.911,%
Batch: 380 | Loss: 2.235 | Acc: 55.528,88.294,99.908,%
Batch: 0 | Loss: 3.975 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.376 | Acc: 47.210,66.927,72.359,%
Batch: 40 | Loss: 4.365 | Acc: 47.485,66.578,71.970,%
Batch: 60 | Loss: 4.368 | Acc: 46.990,66.573,72.310,%
Train all parameters

Epoch: 263
Batch: 0 | Loss: 2.860 | Acc: 51.562,81.250,100.000,%
Batch: 20 | Loss: 2.125 | Acc: 55.915,89.435,99.926,%
Batch: 40 | Loss: 2.120 | Acc: 56.117,89.596,99.924,%
Batch: 60 | Loss: 2.182 | Acc: 55.725,88.640,99.923,%
Batch: 80 | Loss: 2.204 | Acc: 55.440,88.609,99.894,%
Batch: 100 | Loss: 2.203 | Acc: 55.523,88.420,99.892,%
Batch: 120 | Loss: 2.204 | Acc: 55.701,88.346,99.903,%
Batch: 140 | Loss: 2.214 | Acc: 55.685,88.375,99.911,%
Batch: 160 | Loss: 2.214 | Acc: 55.779,88.534,99.913,%
Batch: 180 | Loss: 2.214 | Acc: 55.732,88.592,99.909,%
Batch: 200 | Loss: 2.217 | Acc: 55.737,88.522,99.918,%
Batch: 220 | Loss: 2.202 | Acc: 55.762,88.550,99.915,%
Batch: 240 | Loss: 2.200 | Acc: 55.637,88.696,99.909,%
Batch: 260 | Loss: 2.204 | Acc: 55.493,88.688,99.907,%
Batch: 280 | Loss: 2.229 | Acc: 55.316,88.501,99.905,%
Batch: 300 | Loss: 2.219 | Acc: 55.482,88.600,99.909,%
Batch: 320 | Loss: 2.224 | Acc: 55.435,88.532,99.912,%
Batch: 340 | Loss: 2.218 | Acc: 55.464,88.604,99.915,%
Batch: 360 | Loss: 2.228 | Acc: 55.343,88.660,99.920,%
Batch: 380 | Loss: 2.230 | Acc: 55.387,88.704,99.918,%
Batch: 0 | Loss: 3.959 | Acc: 55.469,70.312,78.125,%
Batch: 20 | Loss: 4.378 | Acc: 47.024,66.406,72.210,%
Batch: 40 | Loss: 4.366 | Acc: 47.085,66.406,71.856,%
Batch: 60 | Loss: 4.368 | Acc: 46.760,66.457,72.157,%
Train all parameters

Epoch: 264
Batch: 0 | Loss: 2.395 | Acc: 46.094,93.750,100.000,%
Batch: 20 | Loss: 2.356 | Acc: 53.571,88.988,99.963,%
Batch: 40 | Loss: 2.288 | Acc: 54.402,88.529,99.981,%
Batch: 60 | Loss: 2.285 | Acc: 54.265,88.320,99.974,%
Batch: 80 | Loss: 2.286 | Acc: 54.215,88.281,99.981,%
Batch: 100 | Loss: 2.282 | Acc: 54.270,88.343,99.954,%
Batch: 120 | Loss: 2.300 | Acc: 54.132,88.262,99.935,%
Batch: 140 | Loss: 2.279 | Acc: 54.488,88.342,99.934,%
Batch: 160 | Loss: 2.274 | Acc: 54.498,88.446,99.918,%
Batch: 180 | Loss: 2.266 | Acc: 54.433,88.596,99.914,%
Batch: 200 | Loss: 2.256 | Acc: 54.660,88.724,99.899,%
Batch: 220 | Loss: 2.257 | Acc: 54.553,88.804,99.905,%
Batch: 240 | Loss: 2.239 | Acc: 54.814,88.855,99.909,%
Batch: 260 | Loss: 2.227 | Acc: 54.849,88.865,99.898,%
Batch: 280 | Loss: 2.225 | Acc: 55.046,88.826,99.897,%
Batch: 300 | Loss: 2.231 | Acc: 54.996,88.769,99.899,%
Batch: 320 | Loss: 2.224 | Acc: 55.084,88.787,99.898,%
Batch: 340 | Loss: 2.224 | Acc: 55.123,88.767,99.899,%
Batch: 360 | Loss: 2.222 | Acc: 55.296,88.649,99.903,%
Batch: 380 | Loss: 2.226 | Acc: 55.327,88.634,99.902,%
Batch: 0 | Loss: 3.945 | Acc: 54.688,71.875,79.688,%
Batch: 20 | Loss: 4.368 | Acc: 47.061,66.741,72.656,%
Batch: 40 | Loss: 4.357 | Acc: 47.351,66.482,72.237,%
Batch: 60 | Loss: 4.360 | Acc: 46.990,66.483,72.362,%
Train all parameters

Epoch: 265
Batch: 0 | Loss: 2.859 | Acc: 50.000,77.344,100.000,%
Batch: 20 | Loss: 2.442 | Acc: 52.455,86.979,99.963,%
Batch: 40 | Loss: 2.354 | Acc: 53.773,87.309,99.981,%
Batch: 60 | Loss: 2.333 | Acc: 53.778,88.089,99.962,%
Batch: 80 | Loss: 2.303 | Acc: 54.282,88.108,99.961,%
Batch: 100 | Loss: 2.282 | Acc: 54.688,88.119,99.946,%
Batch: 120 | Loss: 2.259 | Acc: 55.023,88.456,99.948,%
Batch: 140 | Loss: 2.243 | Acc: 55.170,88.436,99.956,%
Batch: 160 | Loss: 2.237 | Acc: 55.110,88.587,99.956,%
Batch: 180 | Loss: 2.256 | Acc: 54.791,88.605,99.961,%
Batch: 200 | Loss: 2.262 | Acc: 54.866,88.542,99.953,%
Batch: 220 | Loss: 2.251 | Acc: 55.037,88.606,99.951,%
Batch: 240 | Loss: 2.252 | Acc: 54.979,88.690,99.955,%
Batch: 260 | Loss: 2.241 | Acc: 55.107,88.823,99.952,%
Batch: 280 | Loss: 2.243 | Acc: 55.196,88.759,99.950,%
Batch: 300 | Loss: 2.242 | Acc: 55.222,88.777,99.951,%
Batch: 320 | Loss: 2.246 | Acc: 55.092,88.773,99.942,%
Batch: 340 | Loss: 2.260 | Acc: 54.958,88.719,99.938,%
Batch: 360 | Loss: 2.253 | Acc: 54.988,88.773,99.933,%
Batch: 380 | Loss: 2.258 | Acc: 54.878,88.833,99.932,%
Batch: 0 | Loss: 3.960 | Acc: 55.469,73.438,78.906,%
Batch: 20 | Loss: 4.375 | Acc: 47.247,66.927,72.433,%
Batch: 40 | Loss: 4.364 | Acc: 47.275,66.597,71.932,%
Batch: 60 | Loss: 4.367 | Acc: 46.875,66.509,72.208,%
Train all parameters

Epoch: 266
Batch: 0 | Loss: 2.408 | Acc: 55.469,78.125,100.000,%
Batch: 20 | Loss: 2.093 | Acc: 56.659,88.170,99.851,%
Batch: 40 | Loss: 2.123 | Acc: 55.850,89.082,99.867,%
Batch: 60 | Loss: 2.117 | Acc: 56.570,89.306,99.846,%
Batch: 80 | Loss: 2.131 | Acc: 56.559,88.725,99.836,%
Batch: 100 | Loss: 2.175 | Acc: 55.902,88.436,99.845,%
Batch: 120 | Loss: 2.163 | Acc: 55.940,88.669,99.851,%
Batch: 140 | Loss: 2.175 | Acc: 55.840,88.586,99.856,%
Batch: 160 | Loss: 2.181 | Acc: 55.784,88.587,99.859,%
Batch: 180 | Loss: 2.191 | Acc: 55.767,88.553,99.862,%
Batch: 200 | Loss: 2.182 | Acc: 55.857,88.647,99.872,%
Batch: 220 | Loss: 2.183 | Acc: 55.741,88.691,99.876,%
Batch: 240 | Loss: 2.168 | Acc: 55.887,88.926,99.880,%
Batch: 260 | Loss: 2.179 | Acc: 55.768,88.889,99.889,%
Batch: 280 | Loss: 2.202 | Acc: 55.383,88.846,99.889,%
Batch: 300 | Loss: 2.203 | Acc: 55.474,88.699,99.891,%
Batch: 320 | Loss: 2.204 | Acc: 55.469,88.666,99.893,%
Batch: 340 | Loss: 2.217 | Acc: 55.304,88.607,99.895,%
Batch: 360 | Loss: 2.218 | Acc: 55.371,88.632,99.890,%
Batch: 380 | Loss: 2.224 | Acc: 55.247,88.654,99.891,%
Batch: 0 | Loss: 3.976 | Acc: 55.469,71.875,78.906,%
Batch: 20 | Loss: 4.370 | Acc: 47.061,66.741,72.284,%
Batch: 40 | Loss: 4.360 | Acc: 47.161,66.578,71.970,%
Batch: 60 | Loss: 4.364 | Acc: 46.913,66.547,72.080,%
Train all parameters

Epoch: 267
Batch: 0 | Loss: 2.861 | Acc: 53.125,82.812,100.000,%
Batch: 20 | Loss: 2.321 | Acc: 55.543,87.612,99.888,%
Batch: 40 | Loss: 2.367 | Acc: 54.535,87.500,99.905,%
Batch: 60 | Loss: 2.342 | Acc: 54.892,87.372,99.898,%
Batch: 80 | Loss: 2.308 | Acc: 55.102,87.760,99.904,%
Batch: 100 | Loss: 2.278 | Acc: 55.213,88.219,99.907,%
Batch: 120 | Loss: 2.255 | Acc: 55.178,88.320,99.910,%
Batch: 140 | Loss: 2.261 | Acc: 55.258,88.364,99.895,%
Batch: 160 | Loss: 2.270 | Acc: 55.182,88.242,99.893,%
Batch: 180 | Loss: 2.263 | Acc: 55.369,88.303,99.901,%
Batch: 200 | Loss: 2.270 | Acc: 55.220,88.355,99.903,%
Batch: 220 | Loss: 2.250 | Acc: 55.306,88.366,99.905,%
Batch: 240 | Loss: 2.252 | Acc: 55.326,88.284,99.903,%
Batch: 260 | Loss: 2.255 | Acc: 55.292,88.251,99.904,%
Batch: 280 | Loss: 2.251 | Acc: 55.374,88.292,99.905,%
Batch: 300 | Loss: 2.247 | Acc: 55.438,88.398,99.907,%
Batch: 320 | Loss: 2.253 | Acc: 55.340,88.415,99.905,%
Batch: 340 | Loss: 2.246 | Acc: 55.453,88.520,99.908,%
Batch: 360 | Loss: 2.239 | Acc: 55.430,88.619,99.909,%
Batch: 380 | Loss: 2.245 | Acc: 55.440,88.568,99.914,%
Batch: 0 | Loss: 3.998 | Acc: 55.469,71.875,77.344,%
Batch: 20 | Loss: 4.378 | Acc: 47.098,66.667,72.321,%
Batch: 40 | Loss: 4.368 | Acc: 47.218,66.406,72.027,%
Batch: 60 | Loss: 4.369 | Acc: 46.952,66.509,72.246,%
Train all parameters

Epoch: 268
Batch: 0 | Loss: 2.601 | Acc: 46.094,90.625,99.219,%
Batch: 20 | Loss: 2.256 | Acc: 55.060,89.993,99.777,%
Batch: 40 | Loss: 2.212 | Acc: 55.393,89.234,99.809,%
Batch: 60 | Loss: 2.275 | Acc: 54.444,88.870,99.846,%
Batch: 80 | Loss: 2.287 | Acc: 54.311,88.976,99.855,%
Batch: 100 | Loss: 2.260 | Acc: 54.749,88.885,99.861,%
Batch: 120 | Loss: 2.240 | Acc: 54.926,88.662,99.871,%
Batch: 140 | Loss: 2.248 | Acc: 54.959,88.630,99.861,%
Batch: 160 | Loss: 2.244 | Acc: 55.003,88.766,99.854,%
Batch: 180 | Loss: 2.237 | Acc: 55.076,88.730,99.840,%
Batch: 200 | Loss: 2.259 | Acc: 54.722,88.507,99.848,%
Batch: 220 | Loss: 2.259 | Acc: 54.769,88.525,99.859,%
Batch: 240 | Loss: 2.265 | Acc: 54.807,88.534,99.864,%
Batch: 260 | Loss: 2.252 | Acc: 55.122,88.524,99.871,%
Batch: 280 | Loss: 2.245 | Acc: 55.185,88.579,99.875,%
Batch: 300 | Loss: 2.245 | Acc: 55.103,88.541,99.878,%
Batch: 320 | Loss: 2.238 | Acc: 55.165,88.644,99.886,%
Batch: 340 | Loss: 2.240 | Acc: 55.224,88.524,99.890,%
Batch: 360 | Loss: 2.240 | Acc: 55.300,88.502,99.890,%
Batch: 380 | Loss: 2.230 | Acc: 55.395,88.552,99.889,%
Batch: 0 | Loss: 3.956 | Acc: 55.469,72.656,78.125,%
Batch: 20 | Loss: 4.366 | Acc: 47.098,66.815,72.396,%
Batch: 40 | Loss: 4.359 | Acc: 47.275,66.635,71.932,%
Batch: 60 | Loss: 4.363 | Acc: 46.939,66.611,72.170,%
Train all parameters

Epoch: 269
Batch: 0 | Loss: 2.596 | Acc: 45.312,89.844,100.000,%
Batch: 20 | Loss: 2.137 | Acc: 54.650,89.732,99.814,%
Batch: 40 | Loss: 2.077 | Acc: 56.079,90.244,99.848,%
Batch: 60 | Loss: 2.089 | Acc: 56.135,89.754,99.859,%
Batch: 80 | Loss: 2.199 | Acc: 55.189,89.390,99.884,%
Batch: 100 | Loss: 2.226 | Acc: 55.105,89.240,99.884,%
Batch: 120 | Loss: 2.218 | Acc: 55.327,89.192,99.897,%
Batch: 140 | Loss: 2.205 | Acc: 55.458,89.195,99.900,%
Batch: 160 | Loss: 2.212 | Acc: 55.420,89.067,99.898,%
Batch: 180 | Loss: 2.225 | Acc: 55.348,88.890,99.888,%
Batch: 200 | Loss: 2.223 | Acc: 55.434,88.876,99.895,%
Batch: 220 | Loss: 2.215 | Acc: 55.653,88.879,99.901,%
Batch: 240 | Loss: 2.209 | Acc: 55.524,88.981,99.906,%
Batch: 260 | Loss: 2.206 | Acc: 55.505,88.958,99.907,%
Batch: 280 | Loss: 2.187 | Acc: 55.758,89.079,99.903,%
Batch: 300 | Loss: 2.194 | Acc: 55.614,89.109,99.904,%
Batch: 320 | Loss: 2.199 | Acc: 55.705,89.067,99.908,%
Batch: 340 | Loss: 2.210 | Acc: 55.627,88.930,99.906,%
Batch: 360 | Loss: 2.214 | Acc: 55.562,88.846,99.909,%
Batch: 380 | Loss: 2.225 | Acc: 55.475,88.730,99.912,%
Batch: 0 | Loss: 3.978 | Acc: 55.469,71.875,78.125,%
Batch: 20 | Loss: 4.372 | Acc: 47.061,66.704,72.545,%
Batch: 40 | Loss: 4.359 | Acc: 47.332,66.463,72.066,%
Batch: 60 | Loss: 4.362 | Acc: 47.054,66.547,72.246,%
Train all parameters

Epoch: 270
Batch: 0 | Loss: 2.188 | Acc: 49.219,94.531,99.219,%
Batch: 20 | Loss: 2.251 | Acc: 55.543,88.616,99.926,%
Batch: 40 | Loss: 2.246 | Acc: 55.526,88.929,99.905,%
Batch: 60 | Loss: 2.238 | Acc: 55.725,88.691,99.885,%
Batch: 80 | Loss: 2.202 | Acc: 55.932,89.188,99.865,%
Batch: 100 | Loss: 2.199 | Acc: 56.180,88.954,99.884,%
Batch: 120 | Loss: 2.231 | Acc: 55.598,88.630,99.897,%
Batch: 140 | Loss: 2.236 | Acc: 55.452,88.675,99.900,%
Batch: 160 | Loss: 2.238 | Acc: 55.512,88.645,99.903,%
Batch: 180 | Loss: 2.257 | Acc: 55.352,88.488,99.892,%
Batch: 200 | Loss: 2.258 | Acc: 55.407,88.394,99.899,%
Batch: 220 | Loss: 2.252 | Acc: 55.384,88.409,99.901,%
Batch: 240 | Loss: 2.236 | Acc: 55.521,88.537,99.893,%
Batch: 260 | Loss: 2.232 | Acc: 55.591,88.602,99.895,%
Batch: 280 | Loss: 2.246 | Acc: 55.344,88.559,99.903,%
Batch: 300 | Loss: 2.247 | Acc: 55.295,88.549,99.907,%
Batch: 320 | Loss: 2.232 | Acc: 55.503,88.634,99.903,%
Batch: 340 | Loss: 2.229 | Acc: 55.574,88.696,99.904,%
Batch: 360 | Loss: 2.234 | Acc: 55.410,88.677,99.903,%
Batch: 380 | Loss: 2.231 | Acc: 55.465,88.693,99.902,%
Batch: 0 | Loss: 3.947 | Acc: 55.469,71.875,78.906,%
Batch: 20 | Loss: 4.369 | Acc: 47.247,66.927,72.396,%
Batch: 40 | Loss: 4.361 | Acc: 47.389,66.635,71.875,%
Batch: 60 | Loss: 4.366 | Acc: 47.067,66.560,72.029,%
Train all parameters

Epoch: 271
Batch: 0 | Loss: 1.874 | Acc: 53.906,92.969,100.000,%
Batch: 20 | Loss: 2.168 | Acc: 56.027,88.765,99.888,%
Batch: 40 | Loss: 2.165 | Acc: 56.079,89.120,99.867,%
Batch: 60 | Loss: 2.195 | Acc: 56.084,88.883,99.872,%
Batch: 80 | Loss: 2.215 | Acc: 55.874,89.111,99.865,%
Batch: 100 | Loss: 2.212 | Acc: 55.747,89.155,99.884,%
Batch: 120 | Loss: 2.248 | Acc: 55.230,88.953,99.890,%
Batch: 140 | Loss: 2.254 | Acc: 55.247,88.747,99.895,%
Batch: 160 | Loss: 2.262 | Acc: 55.037,88.650,99.903,%
Batch: 180 | Loss: 2.271 | Acc: 54.826,88.493,99.909,%
Batch: 200 | Loss: 2.270 | Acc: 54.816,88.549,99.914,%
Batch: 220 | Loss: 2.275 | Acc: 54.772,88.486,99.919,%
Batch: 240 | Loss: 2.259 | Acc: 54.908,88.537,99.919,%
Batch: 260 | Loss: 2.247 | Acc: 55.071,88.611,99.919,%
Batch: 280 | Loss: 2.243 | Acc: 55.166,88.634,99.919,%
Batch: 300 | Loss: 2.259 | Acc: 54.929,88.562,99.922,%
Batch: 320 | Loss: 2.255 | Acc: 54.916,88.583,99.925,%
Batch: 340 | Loss: 2.249 | Acc: 54.905,88.602,99.922,%
Batch: 360 | Loss: 2.246 | Acc: 54.939,88.621,99.920,%
Batch: 380 | Loss: 2.243 | Acc: 54.966,88.659,99.924,%
Batch: 0 | Loss: 3.932 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.372 | Acc: 47.210,67.039,72.768,%
Batch: 40 | Loss: 4.359 | Acc: 47.332,66.711,72.275,%
Batch: 60 | Loss: 4.360 | Acc: 47.054,66.586,72.323,%
Train all parameters

Epoch: 272
Batch: 0 | Loss: 1.480 | Acc: 68.750,93.750,100.000,%
Batch: 20 | Loss: 2.014 | Acc: 55.990,90.737,99.926,%
Batch: 40 | Loss: 2.103 | Acc: 55.755,90.168,99.924,%
Batch: 60 | Loss: 2.139 | Acc: 55.213,90.599,99.923,%
Batch: 80 | Loss: 2.139 | Acc: 55.392,90.596,99.932,%
Batch: 100 | Loss: 2.166 | Acc: 55.082,90.486,99.915,%
Batch: 120 | Loss: 2.176 | Acc: 54.959,90.296,99.923,%
Batch: 140 | Loss: 2.178 | Acc: 54.937,90.115,99.922,%
Batch: 160 | Loss: 2.199 | Acc: 54.794,89.999,99.922,%
Batch: 180 | Loss: 2.205 | Acc: 54.705,89.831,99.922,%
Batch: 200 | Loss: 2.208 | Acc: 54.618,89.918,99.911,%
Batch: 220 | Loss: 2.214 | Acc: 54.638,89.844,99.912,%
Batch: 240 | Loss: 2.219 | Acc: 54.480,89.841,99.906,%
Batch: 260 | Loss: 2.231 | Acc: 54.532,89.625,99.904,%
Batch: 280 | Loss: 2.235 | Acc: 54.618,89.427,99.905,%
Batch: 300 | Loss: 2.234 | Acc: 54.791,89.294,99.901,%
Batch: 320 | Loss: 2.231 | Acc: 54.763,89.335,99.905,%
Batch: 340 | Loss: 2.227 | Acc: 54.852,89.294,99.899,%
Batch: 360 | Loss: 2.228 | Acc: 54.852,89.279,99.900,%
Batch: 380 | Loss: 2.233 | Acc: 54.860,89.140,99.895,%
Batch: 0 | Loss: 3.990 | Acc: 55.469,71.875,77.344,%
Batch: 20 | Loss: 4.375 | Acc: 47.098,66.815,72.619,%
Batch: 40 | Loss: 4.361 | Acc: 47.180,66.749,71.932,%
Batch: 60 | Loss: 4.361 | Acc: 46.965,66.739,72.234,%
Train all parameters

Epoch: 273
Batch: 0 | Loss: 1.615 | Acc: 62.500,94.531,100.000,%
Batch: 20 | Loss: 2.194 | Acc: 56.659,88.951,99.926,%
Batch: 40 | Loss: 2.240 | Acc: 55.278,88.700,99.867,%
Batch: 60 | Loss: 2.194 | Acc: 55.827,89.229,99.898,%
Batch: 80 | Loss: 2.219 | Acc: 55.276,89.226,99.884,%
Batch: 100 | Loss: 2.238 | Acc: 55.159,88.877,99.884,%
Batch: 120 | Loss: 2.244 | Acc: 54.817,88.895,99.890,%
Batch: 140 | Loss: 2.235 | Acc: 54.937,88.808,99.889,%
Batch: 160 | Loss: 2.234 | Acc: 55.085,88.524,99.884,%
Batch: 180 | Loss: 2.214 | Acc: 55.365,88.726,99.875,%
Batch: 200 | Loss: 2.206 | Acc: 55.430,88.919,99.876,%
Batch: 220 | Loss: 2.211 | Acc: 55.412,88.896,99.883,%
Batch: 240 | Loss: 2.224 | Acc: 55.187,88.751,99.883,%
Batch: 260 | Loss: 2.212 | Acc: 55.463,88.709,99.880,%
Batch: 280 | Loss: 2.195 | Acc: 55.599,88.860,99.889,%
Batch: 300 | Loss: 2.203 | Acc: 55.632,88.850,99.881,%
Batch: 320 | Loss: 2.211 | Acc: 55.525,88.863,99.886,%
Batch: 340 | Loss: 2.222 | Acc: 55.409,88.822,99.888,%
Batch: 360 | Loss: 2.231 | Acc: 55.317,88.747,99.894,%
Batch: 380 | Loss: 2.233 | Acc: 55.235,88.786,99.895,%
Batch: 0 | Loss: 3.964 | Acc: 55.469,71.875,78.906,%
Batch: 20 | Loss: 4.372 | Acc: 47.024,67.001,72.656,%
Batch: 40 | Loss: 4.361 | Acc: 47.237,66.825,72.161,%
Batch: 60 | Loss: 4.362 | Acc: 46.926,66.598,72.323,%
Train all parameters

Epoch: 274
Batch: 0 | Loss: 1.434 | Acc: 64.844,92.969,100.000,%
Batch: 20 | Loss: 2.128 | Acc: 56.771,89.732,99.888,%
Batch: 40 | Loss: 2.275 | Acc: 53.944,88.415,99.848,%
Batch: 60 | Loss: 2.304 | Acc: 53.778,88.256,99.846,%
Batch: 80 | Loss: 2.285 | Acc: 54.562,88.735,99.855,%
Batch: 100 | Loss: 2.278 | Acc: 54.726,88.660,99.876,%
Batch: 120 | Loss: 2.286 | Acc: 54.733,88.591,99.884,%
Batch: 140 | Loss: 2.267 | Acc: 54.920,88.569,99.895,%
Batch: 160 | Loss: 2.254 | Acc: 55.129,88.626,99.893,%
Batch: 180 | Loss: 2.256 | Acc: 55.141,88.497,99.896,%
Batch: 200 | Loss: 2.253 | Acc: 55.096,88.581,99.903,%
Batch: 220 | Loss: 2.248 | Acc: 55.126,88.674,99.901,%
Batch: 240 | Loss: 2.246 | Acc: 55.287,88.554,99.909,%
Batch: 260 | Loss: 2.251 | Acc: 55.181,88.512,99.910,%
Batch: 280 | Loss: 2.241 | Acc: 55.202,88.634,99.905,%
Batch: 300 | Loss: 2.239 | Acc: 55.284,88.608,99.907,%
Batch: 320 | Loss: 2.230 | Acc: 55.384,88.654,99.908,%
Batch: 340 | Loss: 2.225 | Acc: 55.356,88.657,99.908,%
Batch: 360 | Loss: 2.224 | Acc: 55.233,88.705,99.907,%
Batch: 380 | Loss: 2.227 | Acc: 55.227,88.695,99.908,%
Batch: 0 | Loss: 3.959 | Acc: 55.469,73.438,78.125,%
Batch: 20 | Loss: 4.369 | Acc: 47.135,67.039,72.470,%
Batch: 40 | Loss: 4.358 | Acc: 47.447,66.768,72.199,%
Batch: 60 | Loss: 4.358 | Acc: 47.029,66.739,72.374,%
Train all parameters

Epoch: 275
Batch: 0 | Loss: 2.374 | Acc: 45.312,90.625,100.000,%
Batch: 20 | Loss: 2.269 | Acc: 53.683,87.760,99.814,%
Batch: 40 | Loss: 2.274 | Acc: 53.239,87.843,99.867,%
Batch: 60 | Loss: 2.297 | Acc: 53.420,88.512,99.872,%
Batch: 80 | Loss: 2.216 | Acc: 54.716,88.976,99.884,%
Batch: 100 | Loss: 2.205 | Acc: 54.889,89.101,99.892,%
Batch: 120 | Loss: 2.201 | Acc: 55.068,88.946,99.903,%
Batch: 140 | Loss: 2.197 | Acc: 55.441,88.974,99.895,%
Batch: 160 | Loss: 2.194 | Acc: 55.673,88.936,99.879,%
Batch: 180 | Loss: 2.206 | Acc: 55.564,88.816,99.883,%
Batch: 200 | Loss: 2.215 | Acc: 55.445,88.891,99.880,%
Batch: 220 | Loss: 2.219 | Acc: 55.423,89.006,99.876,%
Batch: 240 | Loss: 2.220 | Acc: 55.427,88.943,99.877,%
Batch: 260 | Loss: 2.223 | Acc: 55.349,88.928,99.877,%
Batch: 280 | Loss: 2.222 | Acc: 55.399,88.912,99.880,%
Batch: 300 | Loss: 2.215 | Acc: 55.492,88.860,99.886,%
Batch: 320 | Loss: 2.215 | Acc: 55.384,89.004,99.890,%
Batch: 340 | Loss: 2.208 | Acc: 55.393,89.113,99.885,%
Batch: 360 | Loss: 2.216 | Acc: 55.402,89.008,99.887,%
Batch: 380 | Loss: 2.211 | Acc: 55.372,89.024,99.887,%
Batch: 0 | Loss: 4.014 | Acc: 56.250,71.094,78.125,%
Batch: 20 | Loss: 4.378 | Acc: 47.173,66.704,72.545,%
Batch: 40 | Loss: 4.371 | Acc: 47.142,66.540,71.818,%
Batch: 60 | Loss: 4.375 | Acc: 46.862,66.457,72.054,%
Train all parameters

Epoch: 276
Batch: 0 | Loss: 2.055 | Acc: 64.062,85.938,100.000,%
Batch: 20 | Loss: 2.138 | Acc: 56.808,90.253,99.888,%
Batch: 40 | Loss: 2.204 | Acc: 55.507,88.967,99.867,%
Batch: 60 | Loss: 2.211 | Acc: 55.815,88.934,99.872,%
Batch: 80 | Loss: 2.231 | Acc: 55.430,88.976,99.894,%
Batch: 100 | Loss: 2.193 | Acc: 55.507,89.619,99.892,%
Batch: 120 | Loss: 2.198 | Acc: 55.256,89.753,99.884,%
Batch: 140 | Loss: 2.204 | Acc: 55.070,89.628,99.900,%
Batch: 160 | Loss: 2.198 | Acc: 55.090,89.698,99.898,%
Batch: 180 | Loss: 2.200 | Acc: 55.197,89.546,99.901,%
Batch: 200 | Loss: 2.191 | Acc: 55.302,89.700,99.907,%
Batch: 220 | Loss: 2.196 | Acc: 55.211,89.543,99.908,%
Batch: 240 | Loss: 2.192 | Acc: 55.339,89.571,99.900,%
Batch: 260 | Loss: 2.190 | Acc: 55.403,89.535,99.901,%
Batch: 280 | Loss: 2.192 | Acc: 55.391,89.513,99.897,%
Batch: 300 | Loss: 2.195 | Acc: 55.383,89.512,99.886,%
Batch: 320 | Loss: 2.194 | Acc: 55.357,89.503,99.893,%
Batch: 340 | Loss: 2.198 | Acc: 55.320,89.482,99.888,%
Batch: 360 | Loss: 2.202 | Acc: 55.326,89.456,99.890,%
Batch: 380 | Loss: 2.208 | Acc: 55.309,89.331,99.893,%
Batch: 0 | Loss: 3.956 | Acc: 55.469,73.438,77.344,%
Batch: 20 | Loss: 4.363 | Acc: 47.247,67.188,72.731,%
Batch: 40 | Loss: 4.352 | Acc: 47.351,66.825,72.313,%
Batch: 60 | Loss: 4.354 | Acc: 47.093,66.611,72.413,%
Train all parameters

Epoch: 277
Batch: 0 | Loss: 1.769 | Acc: 59.375,89.844,100.000,%
Batch: 20 | Loss: 2.174 | Acc: 55.804,89.137,99.851,%
Batch: 40 | Loss: 2.290 | Acc: 54.573,88.396,99.905,%
Batch: 60 | Loss: 2.291 | Acc: 55.264,87.538,99.885,%
Batch: 80 | Loss: 2.254 | Acc: 55.565,88.137,99.894,%
Batch: 100 | Loss: 2.233 | Acc: 55.732,88.335,99.899,%
Batch: 120 | Loss: 2.203 | Acc: 55.966,88.514,99.910,%
Batch: 140 | Loss: 2.207 | Acc: 56.012,88.592,99.911,%
Batch: 160 | Loss: 2.215 | Acc: 56.007,88.611,99.908,%
Batch: 180 | Loss: 2.226 | Acc: 55.905,88.665,99.909,%
Batch: 200 | Loss: 2.212 | Acc: 55.927,88.740,99.911,%
Batch: 220 | Loss: 2.206 | Acc: 55.879,88.861,99.912,%
Batch: 240 | Loss: 2.209 | Acc: 55.696,88.793,99.906,%
Batch: 260 | Loss: 2.211 | Acc: 55.723,88.739,99.904,%
Batch: 280 | Loss: 2.215 | Acc: 55.769,88.734,99.900,%
Batch: 300 | Loss: 2.216 | Acc: 55.721,88.647,99.901,%
Batch: 320 | Loss: 2.218 | Acc: 55.719,88.588,99.908,%
Batch: 340 | Loss: 2.220 | Acc: 55.675,88.616,99.911,%
Batch: 360 | Loss: 2.228 | Acc: 55.620,88.591,99.913,%
Batch: 380 | Loss: 2.226 | Acc: 55.620,88.640,99.912,%
Batch: 0 | Loss: 3.930 | Acc: 55.469,73.438,80.469,%
Batch: 20 | Loss: 4.371 | Acc: 47.284,67.001,72.619,%
Batch: 40 | Loss: 4.362 | Acc: 47.466,66.730,72.161,%
Batch: 60 | Loss: 4.364 | Acc: 47.131,66.586,72.259,%
Train all parameters

Epoch: 278
Batch: 0 | Loss: 2.412 | Acc: 47.656,92.188,100.000,%
Batch: 20 | Loss: 2.112 | Acc: 55.134,90.923,99.963,%
Batch: 40 | Loss: 2.204 | Acc: 55.907,90.015,99.924,%
Batch: 60 | Loss: 2.250 | Acc: 55.456,89.152,99.898,%
Batch: 80 | Loss: 2.272 | Acc: 55.199,89.111,99.894,%
Batch: 100 | Loss: 2.284 | Acc: 54.865,89.248,99.869,%
Batch: 120 | Loss: 2.275 | Acc: 55.133,89.398,99.871,%
Batch: 140 | Loss: 2.274 | Acc: 55.147,89.306,99.884,%
Batch: 160 | Loss: 2.255 | Acc: 55.289,89.237,99.869,%
Batch: 180 | Loss: 2.251 | Acc: 55.257,89.352,99.875,%
Batch: 200 | Loss: 2.249 | Acc: 55.177,89.249,99.880,%
Batch: 220 | Loss: 2.249 | Acc: 55.147,89.239,99.883,%
Batch: 240 | Loss: 2.253 | Acc: 55.213,89.118,99.887,%
Batch: 260 | Loss: 2.256 | Acc: 55.056,89.230,99.889,%
Batch: 280 | Loss: 2.254 | Acc: 55.043,89.213,99.886,%
Batch: 300 | Loss: 2.252 | Acc: 55.066,89.169,99.886,%
Batch: 320 | Loss: 2.245 | Acc: 55.182,89.199,99.888,%
Batch: 340 | Loss: 2.240 | Acc: 55.276,89.111,99.885,%
Batch: 360 | Loss: 2.247 | Acc: 55.114,89.067,99.890,%
Batch: 380 | Loss: 2.252 | Acc: 55.014,89.110,99.889,%
Batch: 0 | Loss: 3.971 | Acc: 55.469,72.656,76.562,%
Batch: 20 | Loss: 4.370 | Acc: 47.284,66.927,72.247,%
Batch: 40 | Loss: 4.364 | Acc: 47.256,66.578,71.818,%
Batch: 60 | Loss: 4.366 | Acc: 46.965,66.534,71.939,%
Train all parameters

Epoch: 279
Batch: 0 | Loss: 2.276 | Acc: 53.906,92.188,100.000,%
Batch: 20 | Loss: 2.066 | Acc: 57.738,89.844,99.851,%
Batch: 40 | Loss: 2.159 | Acc: 56.707,89.291,99.886,%
Batch: 60 | Loss: 2.145 | Acc: 56.596,89.088,99.898,%
Batch: 80 | Loss: 2.161 | Acc: 56.404,88.792,99.884,%
Batch: 100 | Loss: 2.200 | Acc: 55.987,88.451,99.899,%
Batch: 120 | Loss: 2.185 | Acc: 56.192,88.707,99.903,%
Batch: 140 | Loss: 2.211 | Acc: 55.779,88.680,99.900,%
Batch: 160 | Loss: 2.222 | Acc: 55.527,88.708,99.898,%
Batch: 180 | Loss: 2.238 | Acc: 55.270,88.747,99.901,%
Batch: 200 | Loss: 2.235 | Acc: 55.410,88.666,99.895,%
Batch: 220 | Loss: 2.242 | Acc: 55.352,88.550,99.897,%
Batch: 240 | Loss: 2.242 | Acc: 55.359,88.424,99.896,%
Batch: 260 | Loss: 2.228 | Acc: 55.499,88.395,99.898,%
Batch: 280 | Loss: 2.236 | Acc: 55.344,88.443,99.903,%
Batch: 300 | Loss: 2.237 | Acc: 55.368,88.453,99.907,%
Batch: 320 | Loss: 2.239 | Acc: 55.274,88.515,99.905,%
Batch: 340 | Loss: 2.239 | Acc: 55.315,88.462,99.908,%
Batch: 360 | Loss: 2.228 | Acc: 55.480,88.545,99.907,%
Batch: 380 | Loss: 2.222 | Acc: 55.561,88.507,99.906,%
Batch: 0 | Loss: 3.975 | Acc: 55.469,71.875,78.906,%
Batch: 20 | Loss: 4.372 | Acc: 47.061,66.890,72.545,%
Batch: 40 | Loss: 4.360 | Acc: 47.275,66.616,72.066,%
Batch: 60 | Loss: 4.363 | Acc: 47.029,66.586,72.093,%
Train all parameters

Epoch: 280
Batch: 0 | Loss: 1.953 | Acc: 57.812,87.500,100.000,%
Batch: 20 | Loss: 2.267 | Acc: 53.646,88.914,99.926,%
Batch: 40 | Loss: 2.268 | Acc: 53.868,88.567,99.943,%
Batch: 60 | Loss: 2.285 | Acc: 54.188,88.371,99.910,%
Batch: 80 | Loss: 2.229 | Acc: 54.726,88.609,99.913,%
Batch: 100 | Loss: 2.230 | Acc: 55.020,88.683,99.899,%
Batch: 120 | Loss: 2.227 | Acc: 54.997,88.727,99.916,%
Batch: 140 | Loss: 2.236 | Acc: 54.909,88.808,99.922,%
Batch: 160 | Loss: 2.243 | Acc: 54.809,88.907,99.918,%
Batch: 180 | Loss: 2.233 | Acc: 54.864,89.101,99.914,%
Batch: 200 | Loss: 2.231 | Acc: 55.002,89.148,99.918,%
Batch: 220 | Loss: 2.213 | Acc: 55.165,89.193,99.915,%
Batch: 240 | Loss: 2.203 | Acc: 55.300,89.234,99.912,%
Batch: 260 | Loss: 2.213 | Acc: 55.181,89.254,99.913,%
Batch: 280 | Loss: 2.218 | Acc: 55.168,89.124,99.914,%
Batch: 300 | Loss: 2.211 | Acc: 55.279,89.203,99.914,%
Batch: 320 | Loss: 2.213 | Acc: 55.262,89.065,99.912,%
Batch: 340 | Loss: 2.218 | Acc: 55.233,88.982,99.913,%
Batch: 360 | Loss: 2.217 | Acc: 55.211,89.041,99.911,%
Batch: 380 | Loss: 2.217 | Acc: 55.282,89.044,99.916,%
Batch: 0 | Loss: 3.946 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.369 | Acc: 47.061,67.225,72.433,%
Batch: 40 | Loss: 4.359 | Acc: 47.370,66.806,72.008,%
Batch: 60 | Loss: 4.361 | Acc: 47.041,66.790,72.195,%
Train all parameters

Epoch: 281
Batch: 0 | Loss: 1.591 | Acc: 57.031,95.312,100.000,%
Batch: 20 | Loss: 2.053 | Acc: 55.915,90.476,99.851,%
Batch: 40 | Loss: 2.111 | Acc: 55.888,89.520,99.924,%
Batch: 60 | Loss: 2.146 | Acc: 56.237,89.331,99.910,%
Batch: 80 | Loss: 2.195 | Acc: 55.826,89.265,99.875,%
Batch: 100 | Loss: 2.179 | Acc: 55.925,89.658,99.853,%
Batch: 120 | Loss: 2.174 | Acc: 55.669,89.740,99.858,%
Batch: 140 | Loss: 2.188 | Acc: 55.657,89.500,99.861,%
Batch: 160 | Loss: 2.199 | Acc: 55.576,89.392,99.869,%
Batch: 180 | Loss: 2.227 | Acc: 55.348,89.283,99.875,%
Batch: 200 | Loss: 2.222 | Acc: 55.364,89.346,99.872,%
Batch: 220 | Loss: 2.214 | Acc: 55.515,89.313,99.873,%
Batch: 240 | Loss: 2.221 | Acc: 55.453,89.429,99.870,%
Batch: 260 | Loss: 2.220 | Acc: 55.594,89.338,99.874,%
Batch: 280 | Loss: 2.219 | Acc: 55.505,89.382,99.869,%
Batch: 300 | Loss: 2.235 | Acc: 55.329,89.281,99.870,%
Batch: 320 | Loss: 2.234 | Acc: 55.393,89.165,99.876,%
Batch: 340 | Loss: 2.235 | Acc: 55.304,89.154,99.879,%
Batch: 360 | Loss: 2.241 | Acc: 55.246,89.119,99.881,%
Batch: 380 | Loss: 2.248 | Acc: 55.182,89.132,99.875,%
Batch: 0 | Loss: 3.975 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.370 | Acc: 47.247,66.815,72.396,%
Batch: 40 | Loss: 4.356 | Acc: 47.409,66.692,72.161,%
Batch: 60 | Loss: 4.360 | Acc: 47.131,66.662,72.246,%
Train all parameters

Epoch: 282
Batch: 0 | Loss: 2.195 | Acc: 52.344,92.188,100.000,%
Batch: 20 | Loss: 2.267 | Acc: 54.874,89.695,100.000,%
Batch: 40 | Loss: 2.218 | Acc: 55.850,89.596,99.981,%
Batch: 60 | Loss: 2.279 | Acc: 54.803,89.191,99.987,%
Batch: 80 | Loss: 2.292 | Acc: 54.552,88.725,99.971,%
Batch: 100 | Loss: 2.249 | Acc: 55.144,88.784,99.969,%
Batch: 120 | Loss: 2.247 | Acc: 55.081,88.869,99.948,%
Batch: 140 | Loss: 2.208 | Acc: 55.391,88.996,99.950,%
Batch: 160 | Loss: 2.199 | Acc: 55.765,89.024,99.932,%
Batch: 180 | Loss: 2.201 | Acc: 55.883,88.903,99.927,%
Batch: 200 | Loss: 2.202 | Acc: 55.768,88.810,99.922,%
Batch: 220 | Loss: 2.215 | Acc: 55.603,88.762,99.929,%
Batch: 240 | Loss: 2.217 | Acc: 55.666,88.680,99.929,%
Batch: 260 | Loss: 2.224 | Acc: 55.472,88.518,99.925,%
Batch: 280 | Loss: 2.229 | Acc: 55.349,88.523,99.919,%
Batch: 300 | Loss: 2.236 | Acc: 55.144,88.473,99.917,%
Batch: 320 | Loss: 2.237 | Acc: 55.126,88.452,99.920,%
Batch: 340 | Loss: 2.235 | Acc: 55.157,88.446,99.920,%
Batch: 360 | Loss: 2.231 | Acc: 55.198,88.431,99.920,%
Batch: 380 | Loss: 2.233 | Acc: 55.085,88.544,99.922,%
Batch: 0 | Loss: 3.951 | Acc: 56.250,71.875,78.906,%
Batch: 20 | Loss: 4.373 | Acc: 47.321,66.853,72.619,%
Batch: 40 | Loss: 4.362 | Acc: 47.389,66.673,72.199,%
Batch: 60 | Loss: 4.365 | Acc: 47.080,66.662,72.285,%
Train all parameters

Epoch: 283
Batch: 0 | Loss: 1.919 | Acc: 52.344,92.969,100.000,%
Batch: 20 | Loss: 2.242 | Acc: 53.981,88.951,99.963,%
Batch: 40 | Loss: 2.219 | Acc: 55.945,88.548,99.924,%
Batch: 60 | Loss: 2.285 | Acc: 54.854,88.179,99.910,%
Batch: 80 | Loss: 2.291 | Acc: 54.668,88.262,99.875,%
Batch: 100 | Loss: 2.256 | Acc: 55.291,88.420,99.884,%
Batch: 120 | Loss: 2.231 | Acc: 55.404,88.688,99.903,%
Batch: 140 | Loss: 2.224 | Acc: 55.535,88.254,99.900,%
Batch: 160 | Loss: 2.214 | Acc: 55.503,88.223,99.913,%
Batch: 180 | Loss: 2.224 | Acc: 55.369,88.281,99.909,%
Batch: 200 | Loss: 2.223 | Acc: 55.399,88.363,99.911,%
Batch: 220 | Loss: 2.208 | Acc: 55.426,88.599,99.908,%
Batch: 240 | Loss: 2.205 | Acc: 55.469,88.732,99.909,%
Batch: 260 | Loss: 2.205 | Acc: 55.343,88.802,99.916,%
Batch: 280 | Loss: 2.206 | Acc: 55.360,88.776,99.911,%
Batch: 300 | Loss: 2.212 | Acc: 55.347,88.813,99.904,%
Batch: 320 | Loss: 2.208 | Acc: 55.381,88.882,99.900,%
Batch: 340 | Loss: 2.206 | Acc: 55.363,88.932,99.897,%
Batch: 360 | Loss: 2.211 | Acc: 55.352,88.920,99.900,%
Batch: 380 | Loss: 2.201 | Acc: 55.545,88.939,99.897,%
Batch: 0 | Loss: 3.974 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.370 | Acc: 47.284,67.001,72.396,%
Batch: 40 | Loss: 4.364 | Acc: 47.389,66.654,72.046,%
Batch: 60 | Loss: 4.366 | Acc: 46.965,66.534,72.144,%
Train all parameters

Epoch: 284
Batch: 0 | Loss: 1.702 | Acc: 63.281,89.844,100.000,%
Batch: 20 | Loss: 2.186 | Acc: 57.031,86.756,99.963,%
Batch: 40 | Loss: 2.298 | Acc: 54.707,87.652,99.981,%
Batch: 60 | Loss: 2.320 | Acc: 53.829,88.102,99.974,%
Batch: 80 | Loss: 2.319 | Acc: 54.022,88.368,99.971,%
Batch: 100 | Loss: 2.317 | Acc: 53.806,88.722,99.969,%
Batch: 120 | Loss: 2.316 | Acc: 53.816,88.830,99.955,%
Batch: 140 | Loss: 2.313 | Acc: 53.856,88.974,99.950,%
Batch: 160 | Loss: 2.301 | Acc: 54.197,88.873,99.947,%
Batch: 180 | Loss: 2.286 | Acc: 54.221,88.963,99.927,%
Batch: 200 | Loss: 2.269 | Acc: 54.493,88.942,99.922,%
Batch: 220 | Loss: 2.266 | Acc: 54.627,89.002,99.926,%
Batch: 240 | Loss: 2.260 | Acc: 54.564,89.043,99.929,%
Batch: 260 | Loss: 2.248 | Acc: 54.682,89.089,99.931,%
Batch: 280 | Loss: 2.251 | Acc: 54.637,89.076,99.936,%
Batch: 300 | Loss: 2.264 | Acc: 54.571,89.016,99.938,%
Batch: 320 | Loss: 2.253 | Acc: 54.743,89.021,99.927,%
Batch: 340 | Loss: 2.260 | Acc: 54.761,88.861,99.920,%
Batch: 360 | Loss: 2.249 | Acc: 54.900,88.913,99.911,%
Batch: 380 | Loss: 2.253 | Acc: 54.923,88.888,99.908,%
Batch: 0 | Loss: 3.983 | Acc: 55.469,71.875,77.344,%
Batch: 20 | Loss: 4.372 | Acc: 46.763,66.927,72.470,%
Batch: 40 | Loss: 4.359 | Acc: 47.142,66.616,72.104,%
Batch: 60 | Loss: 4.361 | Acc: 46.862,66.624,72.246,%
Train classifier parameters

Epoch: 285
Batch: 0 | Loss: 2.184 | Acc: 53.125,91.406,100.000,%
Batch: 20 | Loss: 2.430 | Acc: 53.051,88.021,99.963,%
Batch: 40 | Loss: 2.287 | Acc: 54.268,88.815,99.943,%
Batch: 60 | Loss: 2.282 | Acc: 54.547,88.896,99.923,%
Batch: 80 | Loss: 2.289 | Acc: 54.379,88.773,99.894,%
Batch: 100 | Loss: 2.264 | Acc: 54.533,88.892,99.915,%
Batch: 120 | Loss: 2.237 | Acc: 54.933,88.888,99.910,%
Batch: 140 | Loss: 2.234 | Acc: 54.970,88.846,99.906,%
Batch: 160 | Loss: 2.231 | Acc: 55.095,88.854,99.893,%
Batch: 180 | Loss: 2.253 | Acc: 54.726,88.804,99.901,%
Batch: 200 | Loss: 2.259 | Acc: 54.606,88.829,99.895,%
Batch: 220 | Loss: 2.257 | Acc: 54.818,88.780,99.890,%
Batch: 240 | Loss: 2.267 | Acc: 54.827,88.670,99.890,%
Batch: 260 | Loss: 2.278 | Acc: 54.762,88.578,99.895,%
Batch: 280 | Loss: 2.277 | Acc: 54.776,88.640,99.900,%
Batch: 300 | Loss: 2.267 | Acc: 54.880,88.704,99.904,%
Batch: 320 | Loss: 2.259 | Acc: 54.911,88.724,99.908,%
Batch: 340 | Loss: 2.256 | Acc: 54.923,88.717,99.913,%
Batch: 360 | Loss: 2.258 | Acc: 54.876,88.798,99.918,%
Batch: 380 | Loss: 2.251 | Acc: 55.014,88.755,99.918,%
Batch: 0 | Loss: 3.954 | Acc: 56.250,71.875,78.906,%
Batch: 20 | Loss: 4.370 | Acc: 47.284,66.853,72.582,%
Batch: 40 | Loss: 4.361 | Acc: 47.370,66.692,72.066,%
Batch: 60 | Loss: 4.364 | Acc: 46.913,66.560,72.182,%
Train classifier parameters

Epoch: 286
Batch: 0 | Loss: 1.923 | Acc: 62.500,98.438,100.000,%
Batch: 20 | Loss: 2.331 | Acc: 54.650,89.174,99.851,%
Batch: 40 | Loss: 2.289 | Acc: 55.526,89.310,99.867,%
Batch: 60 | Loss: 2.225 | Acc: 55.482,89.562,99.872,%
Batch: 80 | Loss: 2.251 | Acc: 55.382,89.313,99.884,%
Batch: 100 | Loss: 2.268 | Acc: 54.804,89.426,99.884,%
Batch: 120 | Loss: 2.274 | Acc: 54.849,89.321,99.897,%
Batch: 140 | Loss: 2.278 | Acc: 54.981,89.140,99.889,%
Batch: 160 | Loss: 2.280 | Acc: 55.066,88.980,99.893,%
Batch: 180 | Loss: 2.273 | Acc: 55.352,88.985,99.901,%
Batch: 200 | Loss: 2.262 | Acc: 55.251,89.066,99.895,%
Batch: 220 | Loss: 2.265 | Acc: 55.207,88.946,99.905,%
Batch: 240 | Loss: 2.262 | Acc: 55.196,88.855,99.900,%
Batch: 260 | Loss: 2.265 | Acc: 55.116,88.811,99.901,%
Batch: 280 | Loss: 2.250 | Acc: 55.335,88.818,99.905,%
Batch: 300 | Loss: 2.243 | Acc: 55.313,88.865,99.899,%
Batch: 320 | Loss: 2.240 | Acc: 55.252,88.934,99.900,%
Batch: 340 | Loss: 2.252 | Acc: 55.148,88.891,99.904,%
Batch: 360 | Loss: 2.248 | Acc: 55.164,88.853,99.905,%
Batch: 380 | Loss: 2.242 | Acc: 55.229,88.872,99.902,%
Batch: 0 | Loss: 3.964 | Acc: 55.469,71.875,79.688,%
Batch: 20 | Loss: 4.374 | Acc: 47.173,67.150,72.359,%
Batch: 40 | Loss: 4.360 | Acc: 47.370,66.616,71.856,%
Batch: 60 | Loss: 4.364 | Acc: 47.067,66.522,72.093,%
Train classifier parameters

Epoch: 287
Batch: 0 | Loss: 1.867 | Acc: 59.375,92.969,99.219,%
Batch: 20 | Loss: 2.084 | Acc: 57.999,88.914,99.814,%
Batch: 40 | Loss: 2.158 | Acc: 56.441,88.720,99.867,%
Batch: 60 | Loss: 2.204 | Acc: 55.802,88.730,99.859,%
Batch: 80 | Loss: 2.214 | Acc: 54.986,89.034,99.884,%
Batch: 100 | Loss: 2.174 | Acc: 55.469,89.202,99.899,%
Batch: 120 | Loss: 2.197 | Acc: 55.178,89.114,99.910,%
Batch: 140 | Loss: 2.227 | Acc: 54.726,89.251,99.900,%
Batch: 160 | Loss: 2.229 | Acc: 54.809,89.106,99.893,%
Batch: 180 | Loss: 2.233 | Acc: 55.011,89.058,99.896,%
Batch: 200 | Loss: 2.220 | Acc: 55.313,89.024,99.895,%
Batch: 220 | Loss: 2.216 | Acc: 55.313,89.169,99.897,%
Batch: 240 | Loss: 2.223 | Acc: 55.310,89.241,99.900,%
Batch: 260 | Loss: 2.215 | Acc: 55.307,89.359,99.901,%
Batch: 280 | Loss: 2.227 | Acc: 55.060,89.282,99.900,%
Batch: 300 | Loss: 2.232 | Acc: 54.939,89.223,99.907,%
Batch: 320 | Loss: 2.225 | Acc: 55.031,89.209,99.908,%
Batch: 340 | Loss: 2.228 | Acc: 54.926,89.255,99.911,%
Batch: 360 | Loss: 2.223 | Acc: 55.010,89.255,99.911,%
Batch: 380 | Loss: 2.230 | Acc: 54.948,89.214,99.914,%
Batch: 0 | Loss: 3.957 | Acc: 55.469,73.438,78.906,%
Batch: 20 | Loss: 4.375 | Acc: 47.247,67.001,72.507,%
Batch: 40 | Loss: 4.364 | Acc: 47.313,66.654,72.085,%
Batch: 60 | Loss: 4.366 | Acc: 46.939,66.534,72.285,%
Train classifier parameters

Epoch: 288
Batch: 0 | Loss: 2.659 | Acc: 53.125,89.844,100.000,%
Batch: 20 | Loss: 2.229 | Acc: 54.948,89.435,100.000,%
Batch: 40 | Loss: 2.247 | Acc: 55.069,88.853,99.981,%
Batch: 60 | Loss: 2.240 | Acc: 55.353,89.331,99.962,%
Batch: 80 | Loss: 2.268 | Acc: 54.736,89.226,99.942,%
Batch: 100 | Loss: 2.241 | Acc: 54.564,89.519,99.938,%
Batch: 120 | Loss: 2.217 | Acc: 54.636,89.463,99.935,%
Batch: 140 | Loss: 2.224 | Acc: 54.748,89.400,99.934,%
Batch: 160 | Loss: 2.216 | Acc: 54.794,89.281,99.932,%
Batch: 180 | Loss: 2.215 | Acc: 54.795,89.296,99.935,%
Batch: 200 | Loss: 2.204 | Acc: 55.002,89.288,99.942,%
Batch: 220 | Loss: 2.214 | Acc: 54.868,89.324,99.943,%
Batch: 240 | Loss: 2.213 | Acc: 55.021,89.348,99.935,%
Batch: 260 | Loss: 2.217 | Acc: 54.960,89.281,99.934,%
Batch: 280 | Loss: 2.222 | Acc: 54.888,89.302,99.933,%
Batch: 300 | Loss: 2.221 | Acc: 54.950,89.275,99.933,%
Batch: 320 | Loss: 2.222 | Acc: 55.072,89.228,99.929,%
Batch: 340 | Loss: 2.220 | Acc: 55.127,89.200,99.929,%
Batch: 360 | Loss: 2.219 | Acc: 55.157,89.177,99.929,%
Batch: 380 | Loss: 2.217 | Acc: 55.235,89.175,99.928,%
Batch: 0 | Loss: 3.946 | Acc: 54.688,72.656,79.688,%
Batch: 20 | Loss: 4.371 | Acc: 47.247,66.927,72.359,%
Batch: 40 | Loss: 4.358 | Acc: 47.542,66.635,71.970,%
Batch: 60 | Loss: 4.363 | Acc: 47.093,66.560,72.144,%
Train classifier parameters

Epoch: 289
Batch: 0 | Loss: 2.912 | Acc: 46.094,85.938,100.000,%
Batch: 20 | Loss: 2.290 | Acc: 53.720,89.137,99.888,%
Batch: 40 | Loss: 2.321 | Acc: 53.754,88.129,99.943,%
Batch: 60 | Loss: 2.275 | Acc: 54.918,88.256,99.949,%
Batch: 80 | Loss: 2.284 | Acc: 54.774,88.146,99.923,%
Batch: 100 | Loss: 2.242 | Acc: 55.183,88.637,99.930,%
Batch: 120 | Loss: 2.224 | Acc: 55.223,88.914,99.929,%
Batch: 140 | Loss: 2.213 | Acc: 55.336,89.013,99.911,%
Batch: 160 | Loss: 2.233 | Acc: 55.124,88.825,99.908,%
Batch: 180 | Loss: 2.254 | Acc: 54.770,88.691,99.909,%
Batch: 200 | Loss: 2.241 | Acc: 54.839,88.759,99.899,%
Batch: 220 | Loss: 2.247 | Acc: 54.719,88.804,99.905,%
Batch: 240 | Loss: 2.242 | Acc: 54.876,88.810,99.912,%
Batch: 260 | Loss: 2.227 | Acc: 55.020,89.000,99.901,%
Batch: 280 | Loss: 2.229 | Acc: 55.052,88.932,99.905,%
Batch: 300 | Loss: 2.228 | Acc: 55.082,88.819,99.899,%
Batch: 320 | Loss: 2.229 | Acc: 55.036,88.785,99.903,%
Batch: 340 | Loss: 2.226 | Acc: 55.020,88.898,99.895,%
Batch: 360 | Loss: 2.222 | Acc: 55.084,88.956,99.900,%
Batch: 380 | Loss: 2.217 | Acc: 55.176,88.950,99.893,%
Batch: 0 | Loss: 3.967 | Acc: 55.469,71.094,79.688,%
Batch: 20 | Loss: 4.377 | Acc: 46.987,66.741,72.693,%
Batch: 40 | Loss: 4.365 | Acc: 47.370,66.540,72.218,%
Batch: 60 | Loss: 4.368 | Acc: 47.106,66.457,72.362,%
Train classifier parameters

Epoch: 290
Batch: 0 | Loss: 1.681 | Acc: 65.625,89.844,100.000,%
Batch: 20 | Loss: 1.995 | Acc: 59.375,90.327,99.926,%
Batch: 40 | Loss: 2.081 | Acc: 57.622,90.072,99.943,%
Batch: 60 | Loss: 2.132 | Acc: 56.685,89.549,99.936,%
Batch: 80 | Loss: 2.116 | Acc: 56.665,89.632,99.942,%
Batch: 100 | Loss: 2.131 | Acc: 56.119,89.302,99.930,%
Batch: 120 | Loss: 2.161 | Acc: 55.959,88.862,99.942,%
Batch: 140 | Loss: 2.216 | Acc: 55.535,88.608,99.950,%
Batch: 160 | Loss: 2.210 | Acc: 55.483,88.810,99.947,%
Batch: 180 | Loss: 2.198 | Acc: 55.685,88.980,99.935,%
Batch: 200 | Loss: 2.213 | Acc: 55.496,88.911,99.922,%
Batch: 220 | Loss: 2.201 | Acc: 55.592,88.964,99.922,%
Batch: 240 | Loss: 2.204 | Acc: 55.488,89.004,99.919,%
Batch: 260 | Loss: 2.199 | Acc: 55.466,89.003,99.919,%
Batch: 280 | Loss: 2.220 | Acc: 55.263,88.837,99.919,%
Batch: 300 | Loss: 2.226 | Acc: 55.300,88.746,99.922,%
Batch: 320 | Loss: 2.224 | Acc: 55.313,88.775,99.925,%
Batch: 340 | Loss: 2.239 | Acc: 55.132,88.682,99.922,%
Batch: 360 | Loss: 2.243 | Acc: 55.103,88.580,99.920,%
Batch: 380 | Loss: 2.237 | Acc: 55.110,88.712,99.918,%
Batch: 0 | Loss: 3.960 | Acc: 55.469,71.875,78.125,%
Batch: 20 | Loss: 4.375 | Acc: 47.173,66.592,72.433,%
Batch: 40 | Loss: 4.362 | Acc: 47.237,66.463,71.932,%
Batch: 60 | Loss: 4.363 | Acc: 46.913,66.547,72.157,%
Train classifier parameters

Epoch: 291
Batch: 0 | Loss: 1.713 | Acc: 62.500,89.844,100.000,%
Batch: 20 | Loss: 2.202 | Acc: 55.246,89.062,99.851,%
Batch: 40 | Loss: 2.308 | Acc: 54.421,88.929,99.886,%
Batch: 60 | Loss: 2.273 | Acc: 54.611,89.114,99.910,%
Batch: 80 | Loss: 2.267 | Acc: 54.890,88.879,99.904,%
Batch: 100 | Loss: 2.279 | Acc: 54.780,88.877,99.915,%
Batch: 120 | Loss: 2.246 | Acc: 55.198,88.959,99.903,%
Batch: 140 | Loss: 2.252 | Acc: 55.242,88.680,99.889,%
Batch: 160 | Loss: 2.266 | Acc: 55.022,88.626,99.903,%
Batch: 180 | Loss: 2.256 | Acc: 55.201,88.704,99.901,%
Batch: 200 | Loss: 2.252 | Acc: 55.185,88.837,99.895,%
Batch: 220 | Loss: 2.243 | Acc: 55.225,88.928,99.901,%
Batch: 240 | Loss: 2.246 | Acc: 55.268,88.832,99.903,%
Batch: 260 | Loss: 2.242 | Acc: 55.361,88.742,99.910,%
Batch: 280 | Loss: 2.244 | Acc: 55.294,88.740,99.914,%
Batch: 300 | Loss: 2.236 | Acc: 55.308,88.741,99.912,%
Batch: 320 | Loss: 2.233 | Acc: 55.386,88.697,99.912,%
Batch: 340 | Loss: 2.236 | Acc: 55.311,88.744,99.913,%
Batch: 360 | Loss: 2.236 | Acc: 55.296,88.699,99.913,%
Batch: 380 | Loss: 2.242 | Acc: 55.221,88.685,99.912,%
Batch: 0 | Loss: 3.977 | Acc: 56.250,71.875,78.906,%
Batch: 20 | Loss: 4.372 | Acc: 46.987,66.853,72.284,%
Batch: 40 | Loss: 4.365 | Acc: 47.389,66.597,72.027,%
Batch: 60 | Loss: 4.368 | Acc: 46.952,66.547,72.259,%
Train classifier parameters

Epoch: 292
Batch: 0 | Loss: 1.684 | Acc: 63.281,90.625,100.000,%
Batch: 20 | Loss: 2.175 | Acc: 56.696,89.137,99.888,%
Batch: 40 | Loss: 2.144 | Acc: 57.012,89.310,99.924,%
Batch: 60 | Loss: 2.131 | Acc: 56.814,89.421,99.910,%
Batch: 80 | Loss: 2.152 | Acc: 56.559,89.535,99.904,%
Batch: 100 | Loss: 2.180 | Acc: 56.188,89.325,99.907,%
Batch: 120 | Loss: 2.205 | Acc: 55.759,89.418,99.910,%
Batch: 140 | Loss: 2.190 | Acc: 55.607,89.622,99.911,%
Batch: 160 | Loss: 2.194 | Acc: 55.726,89.456,99.913,%
Batch: 180 | Loss: 2.192 | Acc: 55.685,89.425,99.914,%
Batch: 200 | Loss: 2.190 | Acc: 55.791,89.366,99.911,%
Batch: 220 | Loss: 2.205 | Acc: 55.624,89.289,99.912,%
Batch: 240 | Loss: 2.206 | Acc: 55.624,89.263,99.916,%
Batch: 260 | Loss: 2.210 | Acc: 55.367,89.185,99.913,%
Batch: 280 | Loss: 2.215 | Acc: 55.349,89.085,99.917,%
Batch: 300 | Loss: 2.212 | Acc: 55.445,89.133,99.917,%
Batch: 320 | Loss: 2.222 | Acc: 55.294,89.109,99.910,%
Batch: 340 | Loss: 2.214 | Acc: 55.416,89.072,99.915,%
Batch: 360 | Loss: 2.210 | Acc: 55.408,89.101,99.916,%
Batch: 380 | Loss: 2.214 | Acc: 55.368,89.138,99.916,%
Batch: 0 | Loss: 3.960 | Acc: 56.250,71.875,77.344,%
Batch: 20 | Loss: 4.377 | Acc: 46.875,67.150,71.987,%
Batch: 40 | Loss: 4.361 | Acc: 47.294,66.768,71.799,%
Batch: 60 | Loss: 4.363 | Acc: 46.977,66.688,72.016,%
Train classifier parameters

Epoch: 293
Batch: 0 | Loss: 1.736 | Acc: 52.344,89.844,100.000,%
Batch: 20 | Loss: 2.205 | Acc: 54.874,88.988,99.963,%
Batch: 40 | Loss: 2.247 | Acc: 54.497,88.891,99.905,%
Batch: 60 | Loss: 2.268 | Acc: 54.355,88.934,99.910,%
Batch: 80 | Loss: 2.272 | Acc: 54.803,88.821,99.913,%
Batch: 100 | Loss: 2.251 | Acc: 55.067,88.560,99.923,%
Batch: 120 | Loss: 2.273 | Acc: 54.972,88.436,99.916,%
Batch: 140 | Loss: 2.281 | Acc: 54.981,88.414,99.922,%
Batch: 160 | Loss: 2.262 | Acc: 55.061,88.592,99.913,%
Batch: 180 | Loss: 2.264 | Acc: 55.037,88.769,99.914,%
Batch: 200 | Loss: 2.250 | Acc: 55.177,88.880,99.914,%
Batch: 220 | Loss: 2.248 | Acc: 55.144,88.964,99.919,%
Batch: 240 | Loss: 2.249 | Acc: 55.122,88.917,99.925,%
Batch: 260 | Loss: 2.241 | Acc: 55.292,88.922,99.925,%
Batch: 280 | Loss: 2.234 | Acc: 55.349,89.004,99.930,%
Batch: 300 | Loss: 2.217 | Acc: 55.523,89.070,99.933,%
Batch: 320 | Loss: 2.220 | Acc: 55.476,89.092,99.937,%
Batch: 340 | Loss: 2.223 | Acc: 55.515,88.978,99.934,%
Batch: 360 | Loss: 2.219 | Acc: 55.497,89.091,99.929,%
Batch: 380 | Loss: 2.233 | Acc: 55.307,89.048,99.928,%
Batch: 0 | Loss: 3.966 | Acc: 55.469,71.875,79.688,%
Batch: 20 | Loss: 4.378 | Acc: 47.173,66.964,72.507,%
Batch: 40 | Loss: 4.360 | Acc: 47.466,66.616,72.046,%
Batch: 60 | Loss: 4.363 | Acc: 47.106,66.586,72.182,%
Train classifier parameters

Epoch: 294
Batch: 0 | Loss: 1.851 | Acc: 59.375,90.625,100.000,%
Batch: 20 | Loss: 2.173 | Acc: 55.990,88.095,99.926,%
Batch: 40 | Loss: 2.230 | Acc: 55.564,87.995,99.924,%
Batch: 60 | Loss: 2.230 | Acc: 55.712,88.358,99.910,%
Batch: 80 | Loss: 2.223 | Acc: 55.681,88.773,99.913,%
Batch: 100 | Loss: 2.213 | Acc: 55.732,88.792,99.907,%
Batch: 120 | Loss: 2.190 | Acc: 55.966,88.972,99.890,%
Batch: 140 | Loss: 2.216 | Acc: 55.524,89.029,99.895,%
Batch: 160 | Loss: 2.209 | Acc: 55.449,89.033,99.888,%
Batch: 180 | Loss: 2.210 | Acc: 55.426,88.968,99.888,%
Batch: 200 | Loss: 2.225 | Acc: 55.131,88.829,99.895,%
Batch: 220 | Loss: 2.223 | Acc: 55.059,88.907,99.880,%
Batch: 240 | Loss: 2.224 | Acc: 55.070,88.881,99.877,%
Batch: 260 | Loss: 2.224 | Acc: 55.062,88.916,99.877,%
Batch: 280 | Loss: 2.224 | Acc: 55.007,88.949,99.886,%
Batch: 300 | Loss: 2.223 | Acc: 55.074,88.920,99.886,%
Batch: 320 | Loss: 2.233 | Acc: 55.070,88.860,99.886,%
Batch: 340 | Loss: 2.234 | Acc: 55.130,88.859,99.881,%
Batch: 360 | Loss: 2.243 | Acc: 55.066,88.844,99.885,%
Batch: 380 | Loss: 2.248 | Acc: 55.073,88.792,99.881,%
Batch: 0 | Loss: 3.968 | Acc: 55.469,72.656,78.125,%
Batch: 20 | Loss: 4.373 | Acc: 47.247,67.150,72.731,%
Batch: 40 | Loss: 4.363 | Acc: 47.351,66.730,72.237,%
Batch: 60 | Loss: 4.366 | Acc: 46.965,66.534,72.298,%
Train classifier parameters

Epoch: 295
Batch: 0 | Loss: 2.400 | Acc: 53.125,84.375,100.000,%
Batch: 20 | Loss: 2.233 | Acc: 54.464,89.993,99.926,%
Batch: 40 | Loss: 2.291 | Acc: 54.649,88.662,99.924,%
Batch: 60 | Loss: 2.265 | Acc: 54.572,88.973,99.936,%
Batch: 80 | Loss: 2.257 | Acc: 54.832,88.985,99.932,%
Batch: 100 | Loss: 2.260 | Acc: 54.517,88.838,99.930,%
Batch: 120 | Loss: 2.257 | Acc: 54.416,88.778,99.923,%
Batch: 140 | Loss: 2.244 | Acc: 54.621,88.924,99.922,%
Batch: 160 | Loss: 2.222 | Acc: 54.969,89.155,99.918,%
Batch: 180 | Loss: 2.234 | Acc: 54.964,89.019,99.922,%
Batch: 200 | Loss: 2.224 | Acc: 54.983,89.082,99.926,%
Batch: 220 | Loss: 2.222 | Acc: 55.080,89.154,99.933,%
Batch: 240 | Loss: 2.212 | Acc: 55.239,89.166,99.929,%
Batch: 260 | Loss: 2.202 | Acc: 55.340,89.245,99.916,%
Batch: 280 | Loss: 2.213 | Acc: 55.213,89.202,99.911,%
Batch: 300 | Loss: 2.219 | Acc: 55.126,89.140,99.912,%
Batch: 320 | Loss: 2.221 | Acc: 55.084,89.167,99.910,%
Batch: 340 | Loss: 2.204 | Acc: 55.324,89.248,99.911,%
Batch: 360 | Loss: 2.215 | Acc: 55.196,89.123,99.909,%
Batch: 380 | Loss: 2.218 | Acc: 55.167,89.116,99.912,%
Batch: 0 | Loss: 3.976 | Acc: 55.469,71.875,78.906,%
Batch: 20 | Loss: 4.372 | Acc: 47.024,67.001,72.693,%
Batch: 40 | Loss: 4.363 | Acc: 47.237,66.711,72.275,%
Batch: 60 | Loss: 4.364 | Acc: 47.016,66.662,72.310,%
Train classifier parameters

Epoch: 296
Batch: 0 | Loss: 1.875 | Acc: 59.375,85.156,100.000,%
Batch: 20 | Loss: 2.152 | Acc: 56.473,89.323,100.000,%
Batch: 40 | Loss: 2.144 | Acc: 57.031,89.405,99.943,%
Batch: 60 | Loss: 2.199 | Acc: 56.327,88.806,99.923,%
Batch: 80 | Loss: 2.187 | Acc: 55.864,89.014,99.923,%
Batch: 100 | Loss: 2.193 | Acc: 55.616,89.233,99.923,%
Batch: 120 | Loss: 2.192 | Acc: 55.404,89.282,99.929,%
Batch: 140 | Loss: 2.194 | Acc: 55.441,89.245,99.917,%
Batch: 160 | Loss: 2.194 | Acc: 55.328,89.383,99.918,%
Batch: 180 | Loss: 2.200 | Acc: 55.205,89.313,99.918,%
Batch: 200 | Loss: 2.214 | Acc: 55.068,89.187,99.922,%
Batch: 220 | Loss: 2.204 | Acc: 55.136,89.130,99.919,%
Batch: 240 | Loss: 2.214 | Acc: 55.138,89.121,99.922,%
Batch: 260 | Loss: 2.203 | Acc: 55.277,89.176,99.925,%
Batch: 280 | Loss: 2.196 | Acc: 55.430,89.174,99.911,%
Batch: 300 | Loss: 2.206 | Acc: 55.347,89.078,99.912,%
Batch: 320 | Loss: 2.207 | Acc: 55.376,89.065,99.912,%
Batch: 340 | Loss: 2.210 | Acc: 55.315,89.097,99.911,%
Batch: 360 | Loss: 2.215 | Acc: 55.222,89.069,99.911,%
Batch: 380 | Loss: 2.215 | Acc: 55.251,89.087,99.906,%
Batch: 0 | Loss: 3.993 | Acc: 55.469,72.656,78.906,%
Batch: 20 | Loss: 4.381 | Acc: 47.210,66.927,72.656,%
Batch: 40 | Loss: 4.370 | Acc: 47.294,66.730,71.951,%
Batch: 60 | Loss: 4.373 | Acc: 47.003,66.586,72.054,%
Train classifier parameters

Epoch: 297
Batch: 0 | Loss: 2.499 | Acc: 57.031,79.688,100.000,%
Batch: 20 | Loss: 2.377 | Acc: 52.790,88.690,99.888,%
Batch: 40 | Loss: 2.318 | Acc: 53.582,89.101,99.943,%
Batch: 60 | Loss: 2.264 | Acc: 54.329,88.986,99.949,%
Batch: 80 | Loss: 2.251 | Acc: 54.832,88.860,99.932,%
Batch: 100 | Loss: 2.236 | Acc: 55.028,88.885,99.938,%
Batch: 120 | Loss: 2.225 | Acc: 54.972,88.843,99.935,%
Batch: 140 | Loss: 2.219 | Acc: 54.987,89.018,99.928,%
Batch: 160 | Loss: 2.203 | Acc: 55.124,89.135,99.932,%
Batch: 180 | Loss: 2.193 | Acc: 55.270,89.227,99.927,%
Batch: 200 | Loss: 2.208 | Acc: 55.068,89.191,99.934,%
Batch: 220 | Loss: 2.208 | Acc: 55.260,89.101,99.933,%
Batch: 240 | Loss: 2.212 | Acc: 55.320,88.975,99.935,%
Batch: 260 | Loss: 2.221 | Acc: 55.253,89.039,99.940,%
Batch: 280 | Loss: 2.218 | Acc: 55.269,89.060,99.930,%
Batch: 300 | Loss: 2.212 | Acc: 55.393,89.086,99.933,%
Batch: 320 | Loss: 2.217 | Acc: 55.364,88.985,99.934,%
Batch: 340 | Loss: 2.212 | Acc: 55.434,89.008,99.929,%
Batch: 360 | Loss: 2.226 | Acc: 55.289,88.909,99.926,%
Batch: 380 | Loss: 2.230 | Acc: 55.214,88.872,99.922,%
Batch: 0 | Loss: 3.948 | Acc: 55.469,71.875,78.125,%
Batch: 20 | Loss: 4.378 | Acc: 47.135,67.076,72.321,%
Batch: 40 | Loss: 4.360 | Acc: 47.332,66.673,72.142,%
Batch: 60 | Loss: 4.361 | Acc: 46.990,66.586,72.208,%
Train classifier parameters

Epoch: 298
Batch: 0 | Loss: 1.797 | Acc: 63.281,95.312,100.000,%
Batch: 20 | Loss: 2.195 | Acc: 55.915,88.951,99.851,%
Batch: 40 | Loss: 2.167 | Acc: 56.021,89.367,99.886,%
Batch: 60 | Loss: 2.209 | Acc: 56.109,89.152,99.898,%
Batch: 80 | Loss: 2.211 | Acc: 56.144,88.966,99.884,%
Batch: 100 | Loss: 2.194 | Acc: 56.227,88.954,99.892,%
Batch: 120 | Loss: 2.224 | Acc: 56.121,88.623,99.903,%
Batch: 140 | Loss: 2.240 | Acc: 55.962,88.647,99.895,%
Batch: 160 | Loss: 2.238 | Acc: 55.944,88.631,99.898,%
Batch: 180 | Loss: 2.232 | Acc: 55.775,88.527,99.901,%
Batch: 200 | Loss: 2.217 | Acc: 55.881,88.662,99.907,%
Batch: 220 | Loss: 2.228 | Acc: 55.607,88.642,99.915,%
Batch: 240 | Loss: 2.220 | Acc: 55.686,88.725,99.912,%
Batch: 260 | Loss: 2.231 | Acc: 55.460,88.787,99.916,%
Batch: 280 | Loss: 2.228 | Acc: 55.560,88.723,99.919,%
Batch: 300 | Loss: 2.228 | Acc: 55.451,88.795,99.914,%
Batch: 320 | Loss: 2.227 | Acc: 55.474,88.756,99.915,%
Batch: 340 | Loss: 2.224 | Acc: 55.558,88.785,99.918,%
Batch: 360 | Loss: 2.231 | Acc: 55.482,88.742,99.916,%
Batch: 380 | Loss: 2.233 | Acc: 55.409,88.685,99.918,%
Batch: 0 | Loss: 3.989 | Acc: 55.469,72.656,77.344,%
Batch: 20 | Loss: 4.366 | Acc: 47.135,67.039,72.321,%
Batch: 40 | Loss: 4.355 | Acc: 47.313,66.559,71.989,%
Batch: 60 | Loss: 4.354 | Acc: 47.016,66.598,72.118,%
Train classifier parameters

Epoch: 299
Batch: 0 | Loss: 1.867 | Acc: 64.844,88.281,100.000,%
Batch: 20 | Loss: 2.099 | Acc: 55.766,90.253,99.851,%
Batch: 40 | Loss: 2.202 | Acc: 54.116,89.958,99.886,%
Batch: 60 | Loss: 2.203 | Acc: 54.342,89.703,99.910,%
Batch: 80 | Loss: 2.226 | Acc: 54.639,89.246,99.923,%
Batch: 100 | Loss: 2.227 | Acc: 54.602,89.279,99.907,%
Batch: 120 | Loss: 2.216 | Acc: 55.062,89.269,99.916,%
Batch: 140 | Loss: 2.232 | Acc: 54.826,89.107,99.922,%
Batch: 160 | Loss: 2.219 | Acc: 54.988,89.218,99.918,%
Batch: 180 | Loss: 2.221 | Acc: 55.041,89.313,99.914,%
Batch: 200 | Loss: 2.229 | Acc: 55.033,89.164,99.911,%
Batch: 220 | Loss: 2.234 | Acc: 55.133,88.949,99.915,%
Batch: 240 | Loss: 2.234 | Acc: 55.180,88.991,99.912,%
Batch: 260 | Loss: 2.248 | Acc: 55.011,88.886,99.919,%
Batch: 280 | Loss: 2.252 | Acc: 54.941,88.949,99.911,%
Batch: 300 | Loss: 2.246 | Acc: 55.020,88.977,99.914,%
Batch: 320 | Loss: 2.242 | Acc: 55.116,88.916,99.917,%
Batch: 340 | Loss: 2.243 | Acc: 55.111,88.948,99.920,%
Batch: 360 | Loss: 2.239 | Acc: 55.235,88.978,99.920,%
Batch: 380 | Loss: 2.238 | Acc: 55.282,88.958,99.920,%
Batch: 0 | Loss: 3.966 | Acc: 55.469,72.656,78.125,%
Batch: 20 | Loss: 4.373 | Acc: 47.396,66.927,72.396,%
Batch: 40 | Loss: 4.357 | Acc: 47.447,66.673,71.932,%
Batch: 60 | Loss: 4.359 | Acc: 47.080,66.662,72.106,%
