
Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=0 | lr=1.0e-02 | circles=5 
Training: Epoch=0 | Loss: nan |  Acc: 1.004,1.006,1.006,1.002,1.002,1.002,1.002,1.008,% 
Testing: Epoch=0 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=1 | lr=1.0e-02 | circles=5 
Training: Epoch=1 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=1 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=2 | lr=1.0e-02 | circles=5 
Training: Epoch=2 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=2 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=3 | lr=1.0e-02 | circles=5 
Training: Epoch=3 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=3 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=4 | lr=1.0e-02 | circles=5 
Training: Epoch=4 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=4 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=5 | lr=1.0e-02 | circles=5 
Training: Epoch=5 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=5 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=6 | lr=1.0e-02 | circles=5 
Training: Epoch=6 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=6 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=7 | lr=1.0e-02 | circles=5 
Training: Epoch=7 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=7 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=8 | lr=1.0e-02 | circles=5 
Training: Epoch=8 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=8 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=9 | lr=1.0e-02 | circles=5 
Training: Epoch=9 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=9 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=10 | lr=1.0e-02 | circles=5 
Training: Epoch=10 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=10 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=11 | lr=1.0e-02 | circles=5 
Training: Epoch=11 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=11 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=12 | lr=1.0e-02 | circles=5 
Training: Epoch=12 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=12 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=13 | lr=1.0e-02 | circles=5 
Training: Epoch=13 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=13 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=14 | lr=1.0e-02 | circles=5 
Training: Epoch=14 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=14 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=15 | lr=1.0e-02 | circles=5 
Training: Epoch=15 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=15 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=16 | lr=1.0e-02 | circles=5 
Training: Epoch=16 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=16 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=17 | lr=1.0e-02 | circles=5 
Training: Epoch=17 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=17 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=18 | lr=1.0e-02 | circles=5 
Training: Epoch=18 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=18 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=19 | lr=1.0e-02 | circles=5 
Training: Epoch=19 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=19 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=20 | lr=1.0e-02 | circles=5 
Training: Epoch=20 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=20 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=21 | lr=1.0e-02 | circles=5 
Training: Epoch=21 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=21 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=22 | lr=1.0e-02 | circles=5 
Training: Epoch=22 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 
Testing: Epoch=22 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 

Training Setting: backend=modelC | dataset=cifar100 | adaptive=1 | batch_size=128 | epoch=23 | lr=1.0e-02 | circles=5 
Training: Epoch=23 | Loss: nan |  Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,% 



==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=64, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=64, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=164, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=164, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (3): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (4): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (5): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (6): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (7): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Batch: 0 | Loss: 956.944 | Acc: 1.562,2.344,1.562,0.781,0.781,0.781,0.781,3.125,%
Batch: 20 | Loss: nan | Acc: 0.930,0.967,0.967,0.893,0.893,0.893,0.893,1.004,%
Batch: 40 | Loss: nan | Acc: 1.010,1.029,1.029,0.991,0.991,0.991,0.991,1.048,%
Batch: 60 | Loss: nan | Acc: 0.909,0.922,0.922,0.897,0.897,0.897,0.897,0.935,%
Batch: 80 | Loss: nan | Acc: 0.907,0.916,0.916,0.897,0.897,0.897,0.897,0.926,%
Batch: 100 | Loss: nan | Acc: 0.905,0.913,0.913,0.897,0.897,0.897,0.897,0.920,%
Batch: 120 | Loss: nan | Acc: 0.936,0.943,0.943,0.930,0.930,0.930,0.930,0.949,%
Batch: 140 | Loss: nan | Acc: 0.959,0.964,0.964,0.953,0.953,0.953,0.953,0.970,%
Batch: 160 | Loss: nan | Acc: 0.927,0.932,0.932,0.922,0.922,0.922,0.922,0.937,%
Batch: 180 | Loss: nan | Acc: 0.958,0.963,0.963,0.954,0.954,0.954,0.954,0.967,%
Batch: 200 | Loss: nan | Acc: 0.983,0.987,0.987,0.979,0.979,0.979,0.979,0.991,%
Batch: 220 | Loss: nan | Acc: 0.979,0.983,0.983,0.976,0.976,0.976,0.976,0.986,%
Batch: 240 | Loss: nan | Acc: 0.995,0.998,0.998,0.992,0.992,0.992,0.992,1.002,%
Batch: 260 | Loss: nan | Acc: 1.000,1.003,1.003,0.997,0.997,0.997,0.997,1.006,%
Batch: 280 | Loss: nan | Acc: 0.987,0.990,0.990,0.984,0.984,0.984,0.984,0.993,%
Batch: 300 | Loss: nan | Acc: 0.997,0.999,0.999,0.994,0.994,0.994,0.994,1.002,%
Batch: 320 | Loss: nan | Acc: 1.010,1.012,1.012,1.008,1.008,1.008,1.008,1.015,%
Batch: 340 | Loss: nan | Acc: 1.006,1.008,1.008,1.003,1.003,1.003,1.003,1.010,%
Batch: 360 | Loss: nan | Acc: 1.008,1.011,1.011,1.006,1.006,1.006,1.006,1.013,%
Batch: 380 | Loss: nan | Acc: 1.005,1.007,1.007,1.003,1.003,1.003,1.003,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 1
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.228,1.228,1.228,1.228,1.228,1.228,1.228,1.228,%
Batch: 40 | Loss: nan | Acc: 1.315,1.315,1.315,1.315,1.315,1.315,1.315,1.315,%
Batch: 60 | Loss: nan | Acc: 1.217,1.217,1.217,1.217,1.217,1.217,1.217,1.217,%
Batch: 80 | Loss: nan | Acc: 1.080,1.080,1.080,1.080,1.080,1.080,1.080,1.080,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.059,1.059,1.059,1.059,1.059,1.059,1.059,1.059,%
Batch: 140 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 160 | Loss: nan | Acc: 1.038,1.038,1.038,1.038,1.038,1.038,1.038,1.038,%
Batch: 180 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 200 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 220 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 240 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 260 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 280 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 300 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 360 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 2
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 80 | Loss: nan | Acc: 0.945,0.945,0.945,0.945,0.945,0.945,0.945,0.945,%
Batch: 100 | Loss: nan | Acc: 0.913,0.913,0.913,0.913,0.913,0.913,0.913,0.913,%
Batch: 120 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 180 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 200 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 320 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 340 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 3
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.451,1.451,1.451,1.451,1.451,1.451,1.451,1.451,%
Batch: 40 | Loss: nan | Acc: 1.353,1.353,1.353,1.353,1.353,1.353,1.353,1.353,%
Batch: 60 | Loss: nan | Acc: 1.153,1.153,1.153,1.153,1.153,1.153,1.153,1.153,%
Batch: 80 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 100 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 120 | Loss: nan | Acc: 0.910,0.910,0.910,0.910,0.910,0.910,0.910,0.910,%
Batch: 140 | Loss: nan | Acc: 0.903,0.903,0.903,0.903,0.903,0.903,0.903,0.903,%
Batch: 160 | Loss: nan | Acc: 0.927,0.927,0.927,0.927,0.927,0.927,0.927,0.927,%
Batch: 180 | Loss: nan | Acc: 0.924,0.924,0.924,0.924,0.924,0.924,0.924,0.924,%
Batch: 200 | Loss: nan | Acc: 0.921,0.921,0.921,0.921,0.921,0.921,0.921,0.921,%
Batch: 220 | Loss: nan | Acc: 0.919,0.919,0.919,0.919,0.919,0.919,0.919,0.919,%
Batch: 240 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 260 | Loss: nan | Acc: 0.958,0.958,0.958,0.958,0.958,0.958,0.958,0.958,%
Batch: 280 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 300 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 320 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 4
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.743,0.743,0.743,0.743,0.743,0.743,0.743,0.743,%
Batch: 60 | Loss: nan | Acc: 0.871,0.871,0.871,0.871,0.871,0.871,0.871,0.871,%
Batch: 80 | Loss: nan | Acc: 0.801,0.801,0.801,0.801,0.801,0.801,0.801,0.801,%
Batch: 100 | Loss: nan | Acc: 0.882,0.882,0.882,0.882,0.882,0.882,0.882,0.882,%
Batch: 120 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 140 | Loss: nan | Acc: 0.909,0.909,0.909,0.909,0.909,0.909,0.909,0.909,%
Batch: 160 | Loss: nan | Acc: 0.922,0.922,0.922,0.922,0.922,0.922,0.922,0.922,%
Batch: 180 | Loss: nan | Acc: 0.932,0.932,0.932,0.932,0.932,0.932,0.932,0.932,%
Batch: 200 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 220 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 240 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 260 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 280 | Loss: nan | Acc: 0.984,0.984,0.984,0.984,0.984,0.984,0.984,0.984,%
Batch: 300 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 320 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 340 | Loss: nan | Acc: 0.978,0.978,0.978,0.978,0.978,0.978,0.978,0.978,%
Batch: 360 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 5
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 100 | Loss: nan | Acc: 1.098,1.098,1.098,1.098,1.098,1.098,1.098,1.098,%
Batch: 120 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 180 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 200 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 220 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 240 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 260 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 280 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 6
Batch: 0 | Loss: nan | Acc: 3.906,3.906,3.906,3.906,3.906,3.906,3.906,3.906,%
Batch: 20 | Loss: nan | Acc: 1.339,1.339,1.339,1.339,1.339,1.339,1.339,1.339,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.178,1.178,1.178,1.178,1.178,1.178,1.178,1.178,%
Batch: 80 | Loss: nan | Acc: 1.080,1.080,1.080,1.080,1.080,1.080,1.080,1.080,%
Batch: 100 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 120 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 140 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 160 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 220 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 240 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 260 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 360 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 380 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 7
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 40 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 60 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 80 | Loss: nan | Acc: 1.071,1.071,1.071,1.071,1.071,1.071,1.071,1.071,%
Batch: 100 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 120 | Loss: nan | Acc: 1.072,1.072,1.072,1.072,1.072,1.072,1.072,1.072,%
Batch: 140 | Loss: nan | Acc: 1.097,1.097,1.097,1.097,1.097,1.097,1.097,1.097,%
Batch: 160 | Loss: nan | Acc: 1.072,1.072,1.072,1.072,1.072,1.072,1.072,1.072,%
Batch: 180 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 200 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 220 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 240 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 260 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 280 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 300 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 320 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 340 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 360 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 8
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 1.079,1.079,1.079,1.079,1.079,1.079,1.079,1.079,%
Batch: 40 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 60 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 80 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 100 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 120 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 140 | Loss: nan | Acc: 0.942,0.942,0.942,0.942,0.942,0.942,0.942,0.942,%
Batch: 160 | Loss: nan | Acc: 0.927,0.927,0.927,0.927,0.927,0.927,0.927,0.927,%
Batch: 180 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 200 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 220 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 240 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 260 | Loss: nan | Acc: 0.970,0.970,0.970,0.970,0.970,0.970,0.970,0.970,%
Batch: 280 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 300 | Loss: nan | Acc: 0.960,0.960,0.960,0.960,0.960,0.960,0.960,0.960,%
Batch: 320 | Loss: nan | Acc: 0.954,0.954,0.954,0.954,0.954,0.954,0.954,0.954,%
Batch: 340 | Loss: nan | Acc: 0.965,0.965,0.965,0.965,0.965,0.965,0.965,0.965,%
Batch: 360 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 380 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 9
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 1.200,1.200,1.200,1.200,1.200,1.200,1.200,1.200,%
Batch: 60 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 80 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 200 | Loss: nan | Acc: 1.014,1.014,1.014,1.014,1.014,1.014,1.014,1.014,%
Batch: 220 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 240 | Loss: nan | Acc: 1.024,1.024,1.024,1.024,1.024,1.024,1.024,1.024,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 320 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 10
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.670,0.670,0.670,0.670,0.670,0.670,0.670,0.670,%
Batch: 40 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 60 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 80 | Loss: nan | Acc: 0.839,0.839,0.839,0.839,0.839,0.839,0.839,0.839,%
Batch: 100 | Loss: nan | Acc: 0.859,0.859,0.859,0.859,0.859,0.859,0.859,0.859,%
Batch: 120 | Loss: nan | Acc: 0.943,0.943,0.943,0.943,0.943,0.943,0.943,0.943,%
Batch: 140 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 160 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 180 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 200 | Loss: nan | Acc: 0.999,0.999,0.999,0.999,0.999,0.999,0.999,0.999,%
Batch: 220 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 240 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 260 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 280 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 300 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 11
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 0.744,0.744,0.744,0.744,0.744,0.744,0.744,0.744,%
Batch: 40 | Loss: nan | Acc: 0.896,0.896,0.896,0.896,0.896,0.896,0.896,0.896,%
Batch: 60 | Loss: nan | Acc: 0.858,0.858,0.858,0.858,0.858,0.858,0.858,0.858,%
Batch: 80 | Loss: nan | Acc: 0.955,0.955,0.955,0.955,0.955,0.955,0.955,0.955,%
Batch: 100 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 120 | Loss: nan | Acc: 0.962,0.962,0.962,0.962,0.962,0.962,0.962,0.962,%
Batch: 140 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 160 | Loss: nan | Acc: 0.946,0.946,0.946,0.946,0.946,0.946,0.946,0.946,%
Batch: 180 | Loss: nan | Acc: 0.941,0.941,0.941,0.941,0.941,0.941,0.941,0.941,%
Batch: 200 | Loss: nan | Acc: 0.944,0.944,0.944,0.944,0.944,0.944,0.944,0.944,%
Batch: 220 | Loss: nan | Acc: 0.930,0.930,0.930,0.930,0.930,0.930,0.930,0.930,%
Batch: 240 | Loss: nan | Acc: 0.950,0.950,0.950,0.950,0.950,0.950,0.950,0.950,%
Batch: 260 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 280 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 340 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 360 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 12
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.302,1.302,1.302,1.302,1.302,1.302,1.302,1.302,%
Batch: 40 | Loss: nan | Acc: 1.124,1.124,1.124,1.124,1.124,1.124,1.124,1.124,%
Batch: 60 | Loss: nan | Acc: 1.076,1.076,1.076,1.076,1.076,1.076,1.076,1.076,%
Batch: 80 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 100 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 120 | Loss: nan | Acc: 1.065,1.065,1.065,1.065,1.065,1.065,1.065,1.065,%
Batch: 140 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 160 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 180 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 200 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 220 | Loss: nan | Acc: 1.011,1.011,1.011,1.011,1.011,1.011,1.011,1.011,%
Batch: 240 | Loss: nan | Acc: 0.989,0.989,0.989,0.989,0.989,0.989,0.989,0.989,%
Batch: 260 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 280 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 300 | Loss: nan | Acc: 0.973,0.973,0.973,0.973,0.973,0.973,0.973,0.973,%
Batch: 320 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 340 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 360 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 380 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 13
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.080,1.080,1.080,1.080,1.080,1.080,1.080,1.080,%
Batch: 100 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 120 | Loss: nan | Acc: 1.046,1.046,1.046,1.046,1.046,1.046,1.046,1.046,%
Batch: 140 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 160 | Loss: nan | Acc: 1.063,1.063,1.063,1.063,1.063,1.063,1.063,1.063,%
Batch: 180 | Loss: nan | Acc: 1.049,1.049,1.049,1.049,1.049,1.049,1.049,1.049,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 240 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 260 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 14
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 40 | Loss: nan | Acc: 0.819,0.819,0.819,0.819,0.819,0.819,0.819,0.819,%
Batch: 60 | Loss: nan | Acc: 0.858,0.858,0.858,0.858,0.858,0.858,0.858,0.858,%
Batch: 80 | Loss: nan | Acc: 0.849,0.849,0.849,0.849,0.849,0.849,0.849,0.849,%
Batch: 100 | Loss: nan | Acc: 0.944,0.944,0.944,0.944,0.944,0.944,0.944,0.944,%
Batch: 120 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 140 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 160 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 180 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.057,1.057,1.057,1.057,1.057,1.057,1.057,1.057,%
Batch: 240 | Loss: nan | Acc: 1.060,1.060,1.060,1.060,1.060,1.060,1.060,1.060,%
Batch: 260 | Loss: nan | Acc: 1.051,1.051,1.051,1.051,1.051,1.051,1.051,1.051,%
Batch: 280 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 300 | Loss: nan | Acc: 1.028,1.028,1.028,1.028,1.028,1.028,1.028,1.028,%
Batch: 320 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 340 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 15
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.877,0.877,0.877,0.877,0.877,0.877,0.877,0.877,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 0.993,0.993,0.993,0.993,0.993,0.993,0.993,0.993,%
Batch: 100 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 120 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.058,1.058,1.058,1.058,1.058,1.058,1.058,1.058,%
Batch: 180 | Loss: nan | Acc: 1.045,1.045,1.045,1.045,1.045,1.045,1.045,1.045,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.007,1.007,1.007,1.007,1.007,1.007,1.007,1.007,%
Batch: 240 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 300 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 320 | Loss: nan | Acc: 1.010,1.010,1.010,1.010,1.010,1.010,1.010,1.010,%
Batch: 340 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 360 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 380 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 16
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.265,1.265,1.265,1.265,1.265,1.265,1.265,1.265,%
Batch: 40 | Loss: nan | Acc: 1.105,1.105,1.105,1.105,1.105,1.105,1.105,1.105,%
Batch: 60 | Loss: nan | Acc: 1.101,1.101,1.101,1.101,1.101,1.101,1.101,1.101,%
Batch: 80 | Loss: nan | Acc: 1.100,1.100,1.100,1.100,1.100,1.100,1.100,1.100,%
Batch: 100 | Loss: nan | Acc: 1.106,1.106,1.106,1.106,1.106,1.106,1.106,1.106,%
Batch: 120 | Loss: nan | Acc: 1.143,1.143,1.143,1.143,1.143,1.143,1.143,1.143,%
Batch: 140 | Loss: nan | Acc: 1.075,1.075,1.075,1.075,1.075,1.075,1.075,1.075,%
Batch: 160 | Loss: nan | Acc: 1.097,1.097,1.097,1.097,1.097,1.097,1.097,1.097,%
Batch: 180 | Loss: nan | Acc: 1.053,1.053,1.053,1.053,1.053,1.053,1.053,1.053,%
Batch: 200 | Loss: nan | Acc: 1.061,1.061,1.061,1.061,1.061,1.061,1.061,1.061,%
Batch: 220 | Loss: nan | Acc: 1.043,1.043,1.043,1.043,1.043,1.043,1.043,1.043,%
Batch: 240 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 260 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 280 | Loss: nan | Acc: 1.020,1.020,1.020,1.020,1.020,1.020,1.020,1.020,%
Batch: 300 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 320 | Loss: nan | Acc: 1.032,1.032,1.032,1.032,1.032,1.032,1.032,1.032,%
Batch: 340 | Loss: nan | Acc: 1.017,1.017,1.017,1.017,1.017,1.017,1.017,1.017,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 17
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.744,0.744,0.744,0.744,0.744,0.744,0.744,0.744,%
Batch: 40 | Loss: nan | Acc: 0.915,0.915,0.915,0.915,0.915,0.915,0.915,0.915,%
Batch: 60 | Loss: nan | Acc: 0.884,0.884,0.884,0.884,0.884,0.884,0.884,0.884,%
Batch: 80 | Loss: nan | Acc: 0.907,0.907,0.907,0.907,0.907,0.907,0.907,0.907,%
Batch: 100 | Loss: nan | Acc: 0.959,0.959,0.959,0.959,0.959,0.959,0.959,0.959,%
Batch: 120 | Loss: nan | Acc: 0.949,0.949,0.949,0.949,0.949,0.949,0.949,0.949,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 0.956,0.956,0.956,0.956,0.956,0.956,0.956,0.956,%
Batch: 180 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 200 | Loss: nan | Acc: 0.983,0.983,0.983,0.983,0.983,0.983,0.983,0.983,%
Batch: 220 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 240 | Loss: nan | Acc: 0.982,0.982,0.982,0.982,0.982,0.982,0.982,0.982,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 300 | Loss: nan | Acc: 0.994,0.994,0.994,0.994,0.994,0.994,0.994,0.994,%
Batch: 320 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 340 | Loss: nan | Acc: 1.006,1.006,1.006,1.006,1.006,1.006,1.006,1.006,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 18
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.670,0.670,0.670,0.670,0.670,0.670,0.670,0.670,%
Batch: 40 | Loss: nan | Acc: 0.819,0.819,0.819,0.819,0.819,0.819,0.819,0.819,%
Batch: 60 | Loss: nan | Acc: 0.832,0.832,0.832,0.832,0.832,0.832,0.832,0.832,%
Batch: 80 | Loss: nan | Acc: 0.907,0.907,0.907,0.907,0.907,0.907,0.907,0.907,%
Batch: 100 | Loss: nan | Acc: 0.920,0.920,0.920,0.920,0.920,0.920,0.920,0.920,%
Batch: 120 | Loss: nan | Acc: 0.891,0.891,0.891,0.891,0.891,0.891,0.891,0.891,%
Batch: 140 | Loss: nan | Acc: 0.931,0.931,0.931,0.931,0.931,0.931,0.931,0.931,%
Batch: 160 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 180 | Loss: nan | Acc: 0.950,0.950,0.950,0.950,0.950,0.950,0.950,0.950,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 240 | Loss: nan | Acc: 0.966,0.966,0.966,0.966,0.966,0.966,0.966,0.966,%
Batch: 260 | Loss: nan | Acc: 0.988,0.988,0.988,0.988,0.988,0.988,0.988,0.988,%
Batch: 280 | Loss: nan | Acc: 0.951,0.951,0.951,0.951,0.951,0.951,0.951,0.951,%
Batch: 300 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 320 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 340 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 360 | Loss: nan | Acc: 1.002,1.002,1.002,1.002,1.002,1.002,1.002,1.002,%
Batch: 380 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 19
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 0.934,0.934,0.934,0.934,0.934,0.934,0.934,0.934,%
Batch: 60 | Loss: nan | Acc: 0.961,0.961,0.961,0.961,0.961,0.961,0.961,0.961,%
Batch: 80 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 100 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 120 | Loss: nan | Acc: 0.975,0.975,0.975,0.975,0.975,0.975,0.975,0.975,%
Batch: 140 | Loss: nan | Acc: 0.964,0.964,0.964,0.964,0.964,0.964,0.964,0.964,%
Batch: 160 | Loss: nan | Acc: 0.985,0.985,0.985,0.985,0.985,0.985,0.985,0.985,%
Batch: 180 | Loss: nan | Acc: 0.980,0.980,0.980,0.980,0.980,0.980,0.980,0.980,%
Batch: 200 | Loss: nan | Acc: 0.968,0.968,0.968,0.968,0.968,0.968,0.968,0.968,%
Batch: 220 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 240 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 260 | Loss: nan | Acc: 1.039,1.039,1.039,1.039,1.039,1.039,1.039,1.039,%
Batch: 280 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 300 | Loss: nan | Acc: 1.033,1.033,1.033,1.033,1.033,1.033,1.033,1.033,%
Batch: 320 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 340 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 360 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 20
Batch: 0 | Loss: nan | Acc: 0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,%
Batch: 20 | Loss: nan | Acc: 0.893,0.893,0.893,0.893,0.893,0.893,0.893,0.893,%
Batch: 40 | Loss: nan | Acc: 1.086,1.086,1.086,1.086,1.086,1.086,1.086,1.086,%
Batch: 60 | Loss: nan | Acc: 1.050,1.050,1.050,1.050,1.050,1.050,1.050,1.050,%
Batch: 80 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 100 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 120 | Loss: nan | Acc: 1.001,1.001,1.001,1.001,1.001,1.001,1.001,1.001,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 200 | Loss: nan | Acc: 1.030,1.030,1.030,1.030,1.030,1.030,1.030,1.030,%
Batch: 220 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 240 | Loss: nan | Acc: 1.005,1.005,1.005,1.005,1.005,1.005,1.005,1.005,%
Batch: 260 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%
Batch: 280 | Loss: nan | Acc: 0.995,0.995,0.995,0.995,0.995,0.995,0.995,0.995,%
Batch: 300 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.987,0.987,0.987,0.987,0.987,0.987,0.987,0.987,%
Batch: 360 | Loss: nan | Acc: 0.976,0.976,0.976,0.976,0.976,0.976,0.976,0.976,%
Batch: 380 | Loss: nan | Acc: 0.990,0.990,0.990,0.990,0.990,0.990,0.990,0.990,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 21
Batch: 0 | Loss: nan | Acc: 2.344,2.344,2.344,2.344,2.344,2.344,2.344,2.344,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 0.838,0.838,0.838,0.838,0.838,0.838,0.838,0.838,%
Batch: 60 | Loss: nan | Acc: 0.832,0.832,0.832,0.832,0.832,0.832,0.832,0.832,%
Batch: 80 | Loss: nan | Acc: 0.897,0.897,0.897,0.897,0.897,0.897,0.897,0.897,%
Batch: 100 | Loss: nan | Acc: 0.905,0.905,0.905,0.905,0.905,0.905,0.905,0.905,%
Batch: 120 | Loss: nan | Acc: 0.910,0.910,0.910,0.910,0.910,0.910,0.910,0.910,%
Batch: 140 | Loss: nan | Acc: 0.953,0.953,0.953,0.953,0.953,0.953,0.953,0.953,%
Batch: 160 | Loss: nan | Acc: 0.922,0.922,0.922,0.922,0.922,0.922,0.922,0.922,%
Batch: 180 | Loss: nan | Acc: 0.928,0.928,0.928,0.928,0.928,0.928,0.928,0.928,%
Batch: 200 | Loss: nan | Acc: 0.937,0.937,0.937,0.937,0.937,0.937,0.937,0.937,%
Batch: 220 | Loss: nan | Acc: 0.979,0.979,0.979,0.979,0.979,0.979,0.979,0.979,%
Batch: 240 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 260 | Loss: nan | Acc: 1.021,1.021,1.021,1.021,1.021,1.021,1.021,1.021,%
Batch: 280 | Loss: nan | Acc: 1.031,1.031,1.031,1.031,1.031,1.031,1.031,1.031,%
Batch: 300 | Loss: nan | Acc: 1.041,1.041,1.041,1.041,1.041,1.041,1.041,1.041,%
Batch: 320 | Loss: nan | Acc: 1.037,1.037,1.037,1.037,1.037,1.037,1.037,1.037,%
Batch: 340 | Loss: nan | Acc: 1.022,1.022,1.022,1.022,1.022,1.022,1.022,1.022,%
Batch: 360 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 380 | Loss: nan | Acc: 1.009,1.009,1.009,1.009,1.009,1.009,1.009,1.009,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 22
Batch: 0 | Loss: nan | Acc: 0.781,0.781,0.781,0.781,0.781,0.781,0.781,0.781,%
Batch: 20 | Loss: nan | Acc: 1.376,1.376,1.376,1.376,1.376,1.376,1.376,1.376,%
Batch: 40 | Loss: nan | Acc: 1.162,1.162,1.162,1.162,1.162,1.162,1.162,1.162,%
Batch: 60 | Loss: nan | Acc: 1.165,1.165,1.165,1.165,1.165,1.165,1.165,1.165,%
Batch: 80 | Loss: nan | Acc: 1.090,1.090,1.090,1.090,1.090,1.090,1.090,1.090,%
Batch: 100 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 120 | Loss: nan | Acc: 1.078,1.078,1.078,1.078,1.078,1.078,1.078,1.078,%
Batch: 140 | Loss: nan | Acc: 1.036,1.036,1.036,1.036,1.036,1.036,1.036,1.036,%
Batch: 160 | Loss: nan | Acc: 1.048,1.048,1.048,1.048,1.048,1.048,1.048,1.048,%
Batch: 180 | Loss: nan | Acc: 1.027,1.027,1.027,1.027,1.027,1.027,1.027,1.027,%
Batch: 200 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 220 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 240 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 260 | Loss: nan | Acc: 1.000,1.000,1.000,1.000,1.000,1.000,1.000,1.000,%
Batch: 280 | Loss: nan | Acc: 1.026,1.026,1.026,1.026,1.026,1.026,1.026,1.026,%
Batch: 300 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 320 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 340 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 360 | Loss: nan | Acc: 1.008,1.008,1.008,1.008,1.008,1.008,1.008,1.008,%
Batch: 380 | Loss: nan | Acc: 0.992,0.992,0.992,0.992,0.992,0.992,0.992,0.992,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Batch: 60 | Loss: nan | Acc: 1.012,1.012,1.012,1.012,1.012,1.012,1.012,1.012,%

Epoch: 23
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 0.967,0.967,0.967,0.967,0.967,0.967,0.967,0.967,%
Batch: 40 | Loss: nan | Acc: 0.972,0.972,0.972,0.972,0.972,0.972,0.972,0.972,%
Batch: 60 | Loss: nan | Acc: 1.025,1.025,1.025,1.025,1.025,1.025,1.025,1.025,%
Batch: 80 | Loss: nan | Acc: 1.013,1.013,1.013,1.013,1.013,1.013,1.013,1.013,%
Batch: 100 | Loss: nan | Acc: 1.029,1.029,1.029,1.029,1.029,1.029,1.029,1.029,%
Batch: 120 | Loss: nan | Acc: 1.040,1.040,1.040,1.040,1.040,1.040,1.040,1.040,%
Batch: 140 | Loss: nan | Acc: 1.047,1.047,1.047,1.047,1.047,1.047,1.047,1.047,%
Batch: 160 | Loss: nan | Acc: 1.019,1.019,1.019,1.019,1.019,1.019,1.019,1.019,%
Batch: 180 | Loss: nan | Acc: 0.997,0.997,0.997,0.997,0.997,0.997,0.997,0.997,%
Batch: 200 | Loss: nan | Acc: 0.991,0.991,0.991,0.991,0.991,0.991,0.991,0.991,%
Batch: 220 | Loss: nan | Acc: 1.004,1.004,1.004,1.004,1.004,1.004,1.004,1.004,%
Batch: 240 | Loss: nan | Acc: 0.998,0.998,0.998,0.998,0.998,0.998,0.998,0.998,%
Batch: 260 | Loss: nan | Acc: 1.018,1.018,1.018,1.018,1.018,1.018,1.018,1.018,%
Batch: 280 | Loss: nan | Acc: 1.023,1.023,1.023,1.023,1.023,1.023,1.023,1.023,%
Batch: 300 | Loss: nan | Acc: 1.015,1.015,1.015,1.015,1.015,1.015,1.015,1.015,%
Batch: 320 | Loss: nan | Acc: 1.003,1.003,1.003,1.003,1.003,1.003,1.003,1.003,%
Batch: 340 | Loss: nan | Acc: 0.981,0.981,0.981,0.981,0.981,0.981,0.981,0.981,%
Batch: 360 | Loss: nan | Acc: 0.974,0.974,0.974,0.974,0.974,0.974,0.974,0.974,%
Batch: 380 | Loss: nan | Acc: 0.986,0.986,0.986,0.986,0.986,0.986,0.986,0.986,%
Batch: 0 | Loss: nan | Acc: 1.562,1.562,1.562,1.562,1.562,1.562,1.562,1.562,%
Batch: 20 | Loss: nan | Acc: 1.042,1.042,1.042,1.042,1.042,1.042,1.042,1.042,%
Batch: 40 | Loss: nan | Acc: 1.067,1.067,1.067,1.067,1.067,1.067,1.067,1.067,%
Traceback (most recent call last):
  File "main.py", line 259, in <module>
    main_cifar(args)
  File "main.py", line 249, in main_cifar
    test(epoch)
  File "main.py", line 185, in test
    inputs, targets = inputs.cuda(), targets.cuda()
KeyboardInterrupt
