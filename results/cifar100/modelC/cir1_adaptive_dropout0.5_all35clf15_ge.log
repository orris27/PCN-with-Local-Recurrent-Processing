==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=128, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=128, bias=True)
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Train all parameters

Epoch: 0
Batch: 0 | Loss: 13.954 | Acc: 0.781,0.781,0.000,%
Batch: 20 | Loss: 13.662 | Acc: 1.302,2.046,3.720,%
Batch: 40 | Loss: 13.372 | Acc: 2.229,3.563,5.259,%
Batch: 60 | Loss: 13.169 | Acc: 3.176,4.290,6.122,%
Batch: 80 | Loss: 13.010 | Acc: 3.694,5.257,7.272,%
Batch: 100 | Loss: 12.894 | Acc: 3.960,5.886,7.990,%
Batch: 120 | Loss: 12.786 | Acc: 4.281,6.321,8.426,%
Batch: 140 | Loss: 12.706 | Acc: 4.671,6.682,8.849,%
Batch: 160 | Loss: 12.626 | Acc: 4.988,6.978,9.385,%
Batch: 180 | Loss: 12.553 | Acc: 5.266,7.312,9.630,%
Batch: 200 | Loss: 12.484 | Acc: 5.379,7.579,10.047,%
Batch: 220 | Loss: 12.426 | Acc: 5.621,7.770,10.337,%
Batch: 240 | Loss: 12.360 | Acc: 5.855,8.023,10.730,%
Batch: 260 | Loss: 12.301 | Acc: 6.031,8.324,11.018,%
Batch: 280 | Loss: 12.243 | Acc: 6.219,8.627,11.321,%
Batch: 300 | Loss: 12.199 | Acc: 6.338,8.809,11.573,%
Batch: 320 | Loss: 12.157 | Acc: 6.496,9.032,11.831,%
Batch: 340 | Loss: 12.112 | Acc: 6.601,9.244,12.124,%
Batch: 360 | Loss: 12.062 | Acc: 6.763,9.516,12.524,%
Batch: 380 | Loss: 12.016 | Acc: 6.894,9.685,12.793,%
Batch: 0 | Loss: 11.148 | Acc: 8.594,16.406,21.875,%
Batch: 20 | Loss: 11.269 | Acc: 8.929,13.728,18.304,%
Batch: 40 | Loss: 11.249 | Acc: 8.098,12.900,18.693,%
Batch: 60 | Loss: 11.237 | Acc: 8.325,12.923,18.558,%
Train all parameters

Epoch: 1
Batch: 0 | Loss: 11.053 | Acc: 6.250,13.281,21.875,%
Batch: 20 | Loss: 11.062 | Acc: 10.528,14.323,18.601,%
Batch: 40 | Loss: 11.012 | Acc: 10.614,14.482,19.607,%
Batch: 60 | Loss: 10.978 | Acc: 10.310,14.793,19.967,%
Batch: 80 | Loss: 10.959 | Acc: 10.523,14.554,19.686,%
Batch: 100 | Loss: 10.946 | Acc: 10.419,14.759,20.119,%
Batch: 120 | Loss: 10.938 | Acc: 10.382,14.611,20.138,%
Batch: 140 | Loss: 10.908 | Acc: 10.428,14.899,20.484,%
Batch: 160 | Loss: 10.883 | Acc: 10.462,14.984,20.744,%
Batch: 180 | Loss: 10.864 | Acc: 10.454,15.167,20.869,%
Batch: 200 | Loss: 10.843 | Acc: 10.510,15.365,21.222,%
Batch: 220 | Loss: 10.818 | Acc: 10.644,15.547,21.316,%
Batch: 240 | Loss: 10.789 | Acc: 10.655,15.713,21.441,%
Batch: 260 | Loss: 10.769 | Acc: 10.617,15.873,21.558,%
Batch: 280 | Loss: 10.757 | Acc: 10.607,15.914,21.658,%
Batch: 300 | Loss: 10.737 | Acc: 10.623,15.994,21.769,%
Batch: 320 | Loss: 10.702 | Acc: 10.694,16.129,22.053,%
Batch: 340 | Loss: 10.681 | Acc: 10.706,16.294,22.239,%
Batch: 360 | Loss: 10.662 | Acc: 10.760,16.408,22.414,%
Batch: 380 | Loss: 10.641 | Acc: 10.833,16.523,22.601,%
Batch: 0 | Loss: 10.445 | Acc: 13.281,22.656,25.000,%
Batch: 20 | Loss: 10.483 | Acc: 10.826,17.708,25.260,%
Batch: 40 | Loss: 10.473 | Acc: 9.966,17.378,25.038,%
Batch: 60 | Loss: 10.455 | Acc: 10.336,17.597,25.525,%
Train all parameters

Epoch: 2
Batch: 0 | Loss: 10.170 | Acc: 14.844,21.094,28.125,%
Batch: 20 | Loss: 10.159 | Acc: 11.979,19.792,26.414,%
Batch: 40 | Loss: 10.100 | Acc: 11.928,19.474,27.306,%
Batch: 60 | Loss: 10.105 | Acc: 11.475,19.326,27.305,%
Batch: 80 | Loss: 10.114 | Acc: 11.564,19.078,27.074,%
Batch: 100 | Loss: 10.085 | Acc: 11.920,19.570,27.444,%
Batch: 120 | Loss: 10.053 | Acc: 12.171,20.041,27.822,%
Batch: 140 | Loss: 10.015 | Acc: 12.367,20.135,28.097,%
Batch: 160 | Loss: 10.005 | Acc: 12.262,20.186,28.251,%
Batch: 180 | Loss: 9.987 | Acc: 12.358,20.347,28.367,%
Batch: 200 | Loss: 9.957 | Acc: 12.512,20.495,28.564,%
Batch: 220 | Loss: 9.949 | Acc: 12.567,20.599,28.602,%
Batch: 240 | Loss: 9.938 | Acc: 12.626,20.536,28.569,%
Batch: 260 | Loss: 9.919 | Acc: 12.745,20.767,28.840,%
Batch: 280 | Loss: 9.909 | Acc: 12.809,20.866,28.956,%
Batch: 300 | Loss: 9.903 | Acc: 12.819,20.871,29.039,%
Batch: 320 | Loss: 9.880 | Acc: 12.950,20.977,29.240,%
Batch: 340 | Loss: 9.863 | Acc: 13.013,21.126,29.474,%
Batch: 360 | Loss: 9.848 | Acc: 13.052,21.243,29.638,%
Batch: 380 | Loss: 9.829 | Acc: 13.127,21.391,29.784,%
Batch: 0 | Loss: 9.630 | Acc: 16.406,27.344,33.594,%
Batch: 20 | Loss: 9.748 | Acc: 13.579,21.689,32.664,%
Batch: 40 | Loss: 9.700 | Acc: 13.472,22.008,33.098,%
Batch: 60 | Loss: 9.698 | Acc: 13.294,21.862,32.556,%
Train all parameters

Epoch: 3
Batch: 0 | Loss: 9.533 | Acc: 10.156,21.094,30.469,%
Batch: 20 | Loss: 9.338 | Acc: 14.174,24.033,33.668,%
Batch: 40 | Loss: 9.430 | Acc: 14.482,23.761,33.670,%
Batch: 60 | Loss: 9.417 | Acc: 14.831,23.655,33.760,%
Batch: 80 | Loss: 9.422 | Acc: 14.390,23.196,33.816,%
Batch: 100 | Loss: 9.413 | Acc: 14.496,23.484,33.996,%
Batch: 120 | Loss: 9.395 | Acc: 14.715,23.450,34.065,%
Batch: 140 | Loss: 9.353 | Acc: 14.927,23.726,34.292,%
Batch: 160 | Loss: 9.348 | Acc: 14.912,23.777,34.322,%
Batch: 180 | Loss: 9.349 | Acc: 14.956,23.860,34.440,%
Batch: 200 | Loss: 9.345 | Acc: 14.980,23.951,34.542,%
Batch: 220 | Loss: 9.327 | Acc: 15.038,24.141,34.580,%
Batch: 240 | Loss: 9.320 | Acc: 15.148,24.147,34.540,%
Batch: 260 | Loss: 9.303 | Acc: 15.275,24.237,34.620,%
Batch: 280 | Loss: 9.303 | Acc: 15.316,24.285,34.639,%
Batch: 300 | Loss: 9.294 | Acc: 15.363,24.349,34.689,%
Batch: 320 | Loss: 9.282 | Acc: 15.416,24.491,34.699,%
Batch: 340 | Loss: 9.268 | Acc: 15.529,24.643,34.861,%
Batch: 360 | Loss: 9.251 | Acc: 15.655,24.788,34.981,%
Batch: 380 | Loss: 9.241 | Acc: 15.703,24.873,35.074,%
Batch: 0 | Loss: 9.059 | Acc: 15.625,29.688,39.062,%
Batch: 20 | Loss: 9.237 | Acc: 13.876,25.074,37.537,%
Batch: 40 | Loss: 9.213 | Acc: 14.005,24.962,37.348,%
Batch: 60 | Loss: 9.237 | Acc: 14.191,24.731,36.872,%
Train all parameters

Epoch: 4
Batch: 0 | Loss: 8.707 | Acc: 18.750,29.688,40.625,%
Batch: 20 | Loss: 8.973 | Acc: 16.071,26.488,37.537,%
Batch: 40 | Loss: 8.923 | Acc: 17.073,26.200,37.786,%
Batch: 60 | Loss: 8.899 | Acc: 17.495,26.755,37.935,%
Batch: 80 | Loss: 8.889 | Acc: 17.419,26.746,38.185,%
Batch: 100 | Loss: 8.868 | Acc: 17.435,26.903,38.451,%
Batch: 120 | Loss: 8.894 | Acc: 17.252,26.847,38.365,%
Batch: 140 | Loss: 8.869 | Acc: 17.359,27.211,38.575,%
Batch: 160 | Loss: 8.855 | Acc: 17.430,27.218,38.679,%
Batch: 180 | Loss: 8.843 | Acc: 17.485,27.318,38.821,%
Batch: 200 | Loss: 8.849 | Acc: 17.463,27.340,38.709,%
Batch: 220 | Loss: 8.827 | Acc: 17.629,27.531,38.903,%
Batch: 240 | Loss: 8.804 | Acc: 17.774,27.736,39.199,%
Batch: 260 | Loss: 8.801 | Acc: 17.762,27.745,39.170,%
Batch: 280 | Loss: 8.790 | Acc: 17.819,27.816,39.265,%
Batch: 300 | Loss: 8.788 | Acc: 17.844,27.806,39.296,%
Batch: 320 | Loss: 8.777 | Acc: 17.903,27.879,39.350,%
Batch: 340 | Loss: 8.770 | Acc: 17.889,27.974,39.429,%
Batch: 360 | Loss: 8.769 | Acc: 17.895,28.008,39.456,%
Batch: 380 | Loss: 8.769 | Acc: 17.858,27.969,39.425,%
Batch: 0 | Loss: 8.737 | Acc: 16.406,34.375,46.094,%
Batch: 20 | Loss: 8.841 | Acc: 16.406,28.906,40.588,%
Batch: 40 | Loss: 8.823 | Acc: 16.502,29.287,40.720,%
Batch: 60 | Loss: 8.830 | Acc: 16.650,28.740,40.138,%
Train all parameters

Epoch: 5
Batch: 0 | Loss: 8.009 | Acc: 19.531,28.125,43.750,%
Batch: 20 | Loss: 8.482 | Acc: 19.085,29.725,41.629,%
Batch: 40 | Loss: 8.464 | Acc: 18.121,29.840,42.873,%
Batch: 60 | Loss: 8.448 | Acc: 18.404,29.675,42.264,%
Batch: 80 | Loss: 8.487 | Acc: 18.316,29.668,42.033,%
Batch: 100 | Loss: 8.483 | Acc: 18.448,29.494,41.925,%
Batch: 120 | Loss: 8.419 | Acc: 18.873,29.888,42.355,%
Batch: 140 | Loss: 8.397 | Acc: 18.927,30.048,42.703,%
Batch: 160 | Loss: 8.401 | Acc: 19.031,29.979,42.576,%
Batch: 180 | Loss: 8.383 | Acc: 19.169,30.102,42.684,%
Batch: 200 | Loss: 8.379 | Acc: 19.166,30.146,42.798,%
Batch: 220 | Loss: 8.370 | Acc: 19.234,30.175,42.902,%
Batch: 240 | Loss: 8.364 | Acc: 19.298,30.245,43.021,%
Batch: 260 | Loss: 8.360 | Acc: 19.334,30.253,43.044,%
Batch: 280 | Loss: 8.341 | Acc: 19.542,30.274,43.149,%
Batch: 300 | Loss: 8.339 | Acc: 19.508,30.295,43.161,%
Batch: 320 | Loss: 8.324 | Acc: 19.670,30.440,43.409,%
Batch: 340 | Loss: 8.319 | Acc: 19.721,30.473,43.466,%
Batch: 360 | Loss: 8.318 | Acc: 19.700,30.499,43.555,%
Batch: 380 | Loss: 8.311 | Acc: 19.740,30.549,43.604,%
Batch: 0 | Loss: 8.105 | Acc: 21.094,35.156,51.562,%
Batch: 20 | Loss: 8.427 | Acc: 19.680,30.022,46.168,%
Batch: 40 | Loss: 8.435 | Acc: 18.921,29.726,45.655,%
Batch: 60 | Loss: 8.446 | Acc: 18.788,29.713,45.364,%
Train all parameters

Epoch: 6
Batch: 0 | Loss: 8.294 | Acc: 17.969,25.000,47.656,%
Batch: 20 | Loss: 8.004 | Acc: 20.573,31.659,46.019,%
Batch: 40 | Loss: 7.945 | Acc: 21.322,32.546,47.123,%
Batch: 60 | Loss: 8.006 | Acc: 21.119,31.929,46.363,%
Batch: 80 | Loss: 8.063 | Acc: 20.438,31.684,45.901,%
Batch: 100 | Loss: 8.036 | Acc: 20.529,31.931,46.078,%
Batch: 120 | Loss: 8.014 | Acc: 20.629,32.109,46.249,%
Batch: 140 | Loss: 8.010 | Acc: 20.700,32.203,46.326,%
Batch: 160 | Loss: 8.007 | Acc: 20.541,32.153,46.346,%
Batch: 180 | Loss: 8.003 | Acc: 20.520,32.286,46.495,%
Batch: 200 | Loss: 7.990 | Acc: 20.682,32.470,46.568,%
Batch: 220 | Loss: 7.989 | Acc: 20.776,32.505,46.529,%
Batch: 240 | Loss: 7.988 | Acc: 20.906,32.433,46.492,%
Batch: 260 | Loss: 7.980 | Acc: 21.025,32.585,46.552,%
Batch: 280 | Loss: 7.987 | Acc: 21.044,32.504,46.469,%
Batch: 300 | Loss: 7.983 | Acc: 21.057,32.675,46.548,%
Batch: 320 | Loss: 7.974 | Acc: 21.179,32.703,46.605,%
Batch: 340 | Loss: 7.972 | Acc: 21.195,32.650,46.570,%
Batch: 360 | Loss: 7.974 | Acc: 21.217,32.702,46.533,%
Batch: 380 | Loss: 7.976 | Acc: 21.219,32.722,46.586,%
Batch: 0 | Loss: 8.106 | Acc: 19.531,32.812,48.438,%
Batch: 20 | Loss: 8.231 | Acc: 19.531,32.664,46.168,%
Batch: 40 | Loss: 8.193 | Acc: 19.588,32.736,46.246,%
Batch: 60 | Loss: 8.201 | Acc: 19.531,32.415,46.644,%
Train all parameters

Epoch: 7
Batch: 0 | Loss: 7.890 | Acc: 22.656,32.812,50.781,%
Batch: 20 | Loss: 7.742 | Acc: 22.135,36.124,51.711,%
Batch: 40 | Loss: 7.669 | Acc: 22.294,36.261,51.886,%
Batch: 60 | Loss: 7.658 | Acc: 22.272,35.694,50.884,%
Batch: 80 | Loss: 7.637 | Acc: 22.348,35.822,50.810,%
Batch: 100 | Loss: 7.701 | Acc: 21.674,35.094,49.915,%
Batch: 120 | Loss: 7.719 | Acc: 21.630,34.969,49.651,%
Batch: 140 | Loss: 7.705 | Acc: 21.664,34.996,49.612,%
Batch: 160 | Loss: 7.703 | Acc: 21.715,35.020,49.816,%
Batch: 180 | Loss: 7.697 | Acc: 21.828,35.100,49.875,%
Batch: 200 | Loss: 7.677 | Acc: 22.011,35.292,49.969,%
Batch: 220 | Loss: 7.694 | Acc: 22.031,35.170,49.820,%
Batch: 240 | Loss: 7.704 | Acc: 22.050,35.117,49.763,%
Batch: 260 | Loss: 7.695 | Acc: 22.213,35.069,49.790,%
Batch: 280 | Loss: 7.693 | Acc: 22.225,34.964,49.683,%
Batch: 300 | Loss: 7.695 | Acc: 22.293,35.052,49.678,%
Batch: 320 | Loss: 7.696 | Acc: 22.286,35.125,49.594,%
Batch: 340 | Loss: 7.691 | Acc: 22.292,35.126,49.546,%
Batch: 360 | Loss: 7.689 | Acc: 22.386,35.210,49.580,%
Batch: 380 | Loss: 7.693 | Acc: 22.336,35.271,49.596,%
Batch: 0 | Loss: 7.639 | Acc: 24.219,38.281,58.594,%
Batch: 20 | Loss: 7.930 | Acc: 20.461,34.673,49.070,%
Batch: 40 | Loss: 7.934 | Acc: 19.989,34.223,48.399,%
Batch: 60 | Loss: 7.949 | Acc: 19.980,34.170,48.117,%
Train all parameters

Epoch: 8
Batch: 0 | Loss: 7.327 | Acc: 23.438,32.812,48.438,%
Batch: 20 | Loss: 7.654 | Acc: 22.098,34.859,50.930,%
Batch: 40 | Loss: 7.602 | Acc: 22.066,35.537,51.220,%
Batch: 60 | Loss: 7.573 | Acc: 22.503,36.002,51.486,%
Batch: 80 | Loss: 7.579 | Acc: 22.434,35.870,50.868,%
Batch: 100 | Loss: 7.507 | Acc: 22.826,36.409,51.508,%
Batch: 120 | Loss: 7.493 | Acc: 23.011,36.609,51.685,%
Batch: 140 | Loss: 7.480 | Acc: 22.983,36.558,51.729,%
Batch: 160 | Loss: 7.479 | Acc: 23.059,36.675,51.669,%
Batch: 180 | Loss: 7.476 | Acc: 23.058,36.749,51.558,%
Batch: 200 | Loss: 7.461 | Acc: 23.053,36.999,51.629,%
Batch: 220 | Loss: 7.460 | Acc: 23.119,36.959,51.711,%
Batch: 240 | Loss: 7.446 | Acc: 23.178,37.160,51.812,%
Batch: 260 | Loss: 7.434 | Acc: 23.414,37.216,51.889,%
Batch: 280 | Loss: 7.425 | Acc: 23.557,37.205,51.824,%
Batch: 300 | Loss: 7.436 | Acc: 23.580,37.248,51.757,%
Batch: 320 | Loss: 7.442 | Acc: 23.545,37.184,51.587,%
Batch: 340 | Loss: 7.437 | Acc: 23.584,37.211,51.549,%
Batch: 360 | Loss: 7.435 | Acc: 23.611,37.229,51.593,%
Batch: 380 | Loss: 7.422 | Acc: 23.698,37.274,51.546,%
Batch: 0 | Loss: 7.443 | Acc: 25.000,36.719,53.125,%
Batch: 20 | Loss: 7.788 | Acc: 20.871,35.156,50.260,%
Batch: 40 | Loss: 7.806 | Acc: 20.884,35.118,49.733,%
Batch: 60 | Loss: 7.813 | Acc: 20.966,35.195,49.936,%
Train all parameters

Epoch: 9
Batch: 0 | Loss: 6.392 | Acc: 32.812,45.312,60.156,%
Batch: 20 | Loss: 7.240 | Acc: 24.442,38.132,54.390,%
Batch: 40 | Loss: 7.116 | Acc: 25.133,38.472,55.145,%
Batch: 60 | Loss: 7.064 | Acc: 25.320,39.088,55.328,%
Batch: 80 | Loss: 7.098 | Acc: 25.193,39.236,55.247,%
Batch: 100 | Loss: 7.080 | Acc: 25.155,39.527,55.469,%
Batch: 120 | Loss: 7.101 | Acc: 25.207,39.766,55.294,%
Batch: 140 | Loss: 7.110 | Acc: 25.288,39.500,54.909,%
Batch: 160 | Loss: 7.119 | Acc: 25.146,39.378,54.852,%
Batch: 180 | Loss: 7.132 | Acc: 25.086,39.274,54.519,%
Batch: 200 | Loss: 7.137 | Acc: 25.070,39.199,54.357,%
Batch: 220 | Loss: 7.125 | Acc: 25.110,39.324,54.458,%
Batch: 240 | Loss: 7.125 | Acc: 25.272,39.406,54.470,%
Batch: 260 | Loss: 7.126 | Acc: 25.347,39.392,54.550,%
Batch: 280 | Loss: 7.116 | Acc: 25.392,39.463,54.546,%
Batch: 300 | Loss: 7.121 | Acc: 25.306,39.556,54.477,%
Batch: 320 | Loss: 7.123 | Acc: 25.316,39.608,54.532,%
Batch: 340 | Loss: 7.126 | Acc: 25.351,39.567,54.516,%
Batch: 360 | Loss: 7.133 | Acc: 25.249,39.547,54.452,%
Batch: 380 | Loss: 7.130 | Acc: 25.347,39.651,54.552,%
Batch: 0 | Loss: 7.322 | Acc: 27.344,39.844,55.469,%
Batch: 20 | Loss: 7.560 | Acc: 22.284,36.272,51.711,%
Batch: 40 | Loss: 7.573 | Acc: 22.123,36.604,51.791,%
Batch: 60 | Loss: 7.584 | Acc: 22.246,36.744,51.511,%
Train all parameters

Epoch: 10
Batch: 0 | Loss: 6.244 | Acc: 33.594,41.406,59.375,%
Batch: 20 | Loss: 6.888 | Acc: 25.632,39.472,56.027,%
Batch: 40 | Loss: 6.799 | Acc: 26.353,40.339,57.146,%
Batch: 60 | Loss: 6.804 | Acc: 26.780,40.702,57.339,%
Batch: 80 | Loss: 6.830 | Acc: 26.900,40.953,57.369,%
Batch: 100 | Loss: 6.859 | Acc: 26.624,41.066,57.333,%
Batch: 120 | Loss: 6.873 | Acc: 26.466,41.019,57.193,%
Batch: 140 | Loss: 6.893 | Acc: 26.291,41.046,57.015,%
Batch: 160 | Loss: 6.886 | Acc: 26.228,40.994,56.789,%
Batch: 180 | Loss: 6.872 | Acc: 26.532,41.065,56.716,%
Batch: 200 | Loss: 6.885 | Acc: 26.508,41.192,56.658,%
Batch: 220 | Loss: 6.910 | Acc: 26.361,41.077,56.543,%
Batch: 240 | Loss: 6.920 | Acc: 26.394,41.066,56.402,%
Batch: 260 | Loss: 6.903 | Acc: 26.476,41.215,56.394,%
Batch: 280 | Loss: 6.909 | Acc: 26.423,41.187,56.414,%
Batch: 300 | Loss: 6.900 | Acc: 26.472,41.282,56.419,%
Batch: 320 | Loss: 6.902 | Acc: 26.489,41.304,56.381,%
Batch: 340 | Loss: 6.905 | Acc: 26.489,41.340,56.284,%
Batch: 360 | Loss: 6.907 | Acc: 26.554,41.369,56.289,%
Batch: 380 | Loss: 6.909 | Acc: 26.526,41.394,56.266,%
Batch: 0 | Loss: 7.108 | Acc: 26.562,42.188,52.344,%
Batch: 20 | Loss: 7.287 | Acc: 24.256,40.179,53.125,%
Batch: 40 | Loss: 7.267 | Acc: 23.933,40.625,53.544,%
Batch: 60 | Loss: 7.287 | Acc: 24.283,40.126,53.189,%
Train all parameters

Epoch: 11
Batch: 0 | Loss: 6.472 | Acc: 27.344,40.625,59.375,%
Batch: 20 | Loss: 6.594 | Acc: 28.162,44.010,60.379,%
Batch: 40 | Loss: 6.595 | Acc: 27.744,43.502,60.156,%
Batch: 60 | Loss: 6.559 | Acc: 27.485,43.763,59.785,%
Batch: 80 | Loss: 6.581 | Acc: 27.556,43.808,59.452,%
Batch: 100 | Loss: 6.599 | Acc: 27.537,43.827,59.228,%
Batch: 120 | Loss: 6.620 | Acc: 27.389,43.505,58.775,%
Batch: 140 | Loss: 6.680 | Acc: 27.238,43.002,58.295,%
Batch: 160 | Loss: 6.677 | Acc: 27.232,43.129,58.176,%
Batch: 180 | Loss: 6.677 | Acc: 27.339,43.103,57.998,%
Batch: 200 | Loss: 6.691 | Acc: 27.149,43.194,57.914,%
Batch: 220 | Loss: 6.692 | Acc: 27.125,43.244,57.940,%
Batch: 240 | Loss: 6.688 | Acc: 27.133,43.166,57.936,%
Batch: 260 | Loss: 6.704 | Acc: 27.224,43.118,57.762,%
Batch: 280 | Loss: 6.716 | Acc: 27.082,43.138,57.749,%
Batch: 300 | Loss: 6.718 | Acc: 27.224,43.166,57.761,%
Batch: 320 | Loss: 6.717 | Acc: 27.283,43.144,57.739,%
Batch: 340 | Loss: 6.708 | Acc: 27.344,43.212,57.732,%
Batch: 360 | Loss: 6.704 | Acc: 27.394,43.196,57.797,%
Batch: 380 | Loss: 6.703 | Acc: 27.436,43.155,57.794,%
Batch: 0 | Loss: 6.835 | Acc: 31.250,46.875,57.812,%
Batch: 20 | Loss: 7.170 | Acc: 25.149,40.290,54.576,%
Batch: 40 | Loss: 7.150 | Acc: 24.219,40.339,54.897,%
Batch: 60 | Loss: 7.164 | Acc: 24.411,40.459,55.097,%
Train all parameters

Epoch: 12
Batch: 0 | Loss: 7.258 | Acc: 25.000,42.188,62.500,%
Batch: 20 | Loss: 6.557 | Acc: 27.902,43.043,62.909,%
Batch: 40 | Loss: 6.446 | Acc: 27.934,44.360,62.462,%
Batch: 60 | Loss: 6.489 | Acc: 27.690,43.724,61.270,%
Batch: 80 | Loss: 6.502 | Acc: 28.029,43.682,61.015,%
Batch: 100 | Loss: 6.493 | Acc: 28.117,44.090,61.208,%
Batch: 120 | Loss: 6.499 | Acc: 28.261,44.092,60.983,%
Batch: 140 | Loss: 6.508 | Acc: 28.258,44.105,60.960,%
Batch: 160 | Loss: 6.512 | Acc: 28.217,44.182,60.855,%
Batch: 180 | Loss: 6.522 | Acc: 28.138,44.130,60.739,%
Batch: 200 | Loss: 6.518 | Acc: 28.141,44.100,60.693,%
Batch: 220 | Loss: 6.526 | Acc: 28.008,44.040,60.580,%
Batch: 240 | Loss: 6.536 | Acc: 27.950,44.110,60.513,%
Batch: 260 | Loss: 6.532 | Acc: 28.014,44.097,60.497,%
Batch: 280 | Loss: 6.530 | Acc: 28.050,44.278,60.532,%
Batch: 300 | Loss: 6.533 | Acc: 28.055,44.256,60.460,%
Batch: 320 | Loss: 6.534 | Acc: 28.040,44.354,60.375,%
Batch: 340 | Loss: 6.525 | Acc: 28.146,44.396,60.443,%
Batch: 360 | Loss: 6.518 | Acc: 28.205,44.507,60.472,%
Batch: 380 | Loss: 6.517 | Acc: 28.256,44.464,60.433,%
Batch: 0 | Loss: 6.845 | Acc: 29.688,46.875,56.250,%
Batch: 20 | Loss: 7.277 | Acc: 23.438,40.253,54.241,%
Batch: 40 | Loss: 7.284 | Acc: 22.561,40.301,54.364,%
Batch: 60 | Loss: 7.286 | Acc: 22.567,40.190,54.252,%
Train all parameters

Epoch: 13
Batch: 0 | Loss: 6.237 | Acc: 29.688,38.281,55.469,%
Batch: 20 | Loss: 6.309 | Acc: 27.158,46.243,64.211,%
Batch: 40 | Loss: 6.335 | Acc: 27.820,46.208,63.110,%
Batch: 60 | Loss: 6.312 | Acc: 28.291,46.286,62.666,%
Batch: 80 | Loss: 6.337 | Acc: 28.231,45.920,61.970,%
Batch: 100 | Loss: 6.368 | Acc: 28.164,46.001,61.866,%
Batch: 120 | Loss: 6.367 | Acc: 28.299,46.023,62.009,%
Batch: 140 | Loss: 6.378 | Acc: 28.369,45.966,61.874,%
Batch: 160 | Loss: 6.375 | Acc: 28.348,46.055,61.889,%
Batch: 180 | Loss: 6.374 | Acc: 28.397,46.025,61.822,%
Batch: 200 | Loss: 6.377 | Acc: 28.479,46.063,61.835,%
Batch: 220 | Loss: 6.376 | Acc: 28.482,46.104,61.920,%
Batch: 240 | Loss: 6.369 | Acc: 28.572,46.223,61.994,%
Batch: 260 | Loss: 6.350 | Acc: 28.798,46.378,62.084,%
Batch: 280 | Loss: 6.337 | Acc: 28.998,46.380,61.986,%
Batch: 300 | Loss: 6.335 | Acc: 29.023,46.283,61.911,%
Batch: 320 | Loss: 6.340 | Acc: 29.052,46.181,61.855,%
Batch: 340 | Loss: 6.333 | Acc: 29.254,46.360,61.856,%
Batch: 360 | Loss: 6.333 | Acc: 29.244,46.293,61.747,%
Batch: 380 | Loss: 6.333 | Acc: 29.279,46.282,61.708,%
Batch: 0 | Loss: 6.766 | Acc: 28.125,42.969,55.469,%
Batch: 20 | Loss: 6.979 | Acc: 24.740,42.299,56.585,%
Batch: 40 | Loss: 6.967 | Acc: 24.543,42.702,56.841,%
Batch: 60 | Loss: 6.971 | Acc: 24.641,42.482,56.685,%
Train all parameters

Epoch: 14
Batch: 0 | Loss: 6.188 | Acc: 31.250,51.562,69.531,%
Batch: 20 | Loss: 6.261 | Acc: 29.427,47.470,64.472,%
Batch: 40 | Loss: 6.226 | Acc: 29.440,47.389,64.710,%
Batch: 60 | Loss: 6.191 | Acc: 29.380,47.451,64.600,%
Batch: 80 | Loss: 6.168 | Acc: 29.591,48.013,64.911,%
Batch: 100 | Loss: 6.170 | Acc: 29.757,48.082,65.014,%
Batch: 120 | Loss: 6.176 | Acc: 29.571,47.940,65.179,%
Batch: 140 | Loss: 6.180 | Acc: 29.660,47.983,64.705,%
Batch: 160 | Loss: 6.165 | Acc: 29.785,47.768,64.625,%
Batch: 180 | Loss: 6.176 | Acc: 29.679,47.540,64.162,%
Batch: 200 | Loss: 6.171 | Acc: 29.785,47.544,64.043,%
Batch: 220 | Loss: 6.169 | Acc: 29.808,47.490,64.055,%
Batch: 240 | Loss: 6.170 | Acc: 29.905,47.449,63.871,%
Batch: 260 | Loss: 6.164 | Acc: 30.035,47.474,63.955,%
Batch: 280 | Loss: 6.160 | Acc: 30.093,47.570,63.926,%
Batch: 300 | Loss: 6.166 | Acc: 29.947,47.433,63.826,%
Batch: 320 | Loss: 6.173 | Acc: 29.870,47.447,63.756,%
Batch: 340 | Loss: 6.174 | Acc: 29.944,47.450,63.698,%
Batch: 360 | Loss: 6.181 | Acc: 29.900,47.414,63.638,%
Batch: 380 | Loss: 6.179 | Acc: 29.993,47.441,63.661,%
Batch: 0 | Loss: 6.494 | Acc: 32.031,50.781,61.719,%
Batch: 20 | Loss: 6.835 | Acc: 25.893,43.527,56.473,%
Batch: 40 | Loss: 6.864 | Acc: 25.667,43.483,55.526,%
Batch: 60 | Loss: 6.868 | Acc: 25.679,43.673,56.122,%
Train all parameters

Epoch: 15
Batch: 0 | Loss: 6.297 | Acc: 24.219,46.094,71.094,%
Batch: 20 | Loss: 5.889 | Acc: 31.473,50.260,65.997,%
Batch: 40 | Loss: 5.917 | Acc: 31.402,50.629,67.530,%
Batch: 60 | Loss: 5.919 | Acc: 31.314,49.898,67.034,%
Batch: 80 | Loss: 5.955 | Acc: 30.903,49.325,66.570,%
Batch: 100 | Loss: 5.979 | Acc: 30.763,49.273,66.352,%
Batch: 120 | Loss: 5.989 | Acc: 30.695,49.057,65.987,%
Batch: 140 | Loss: 5.993 | Acc: 30.823,49.047,66.035,%
Batch: 160 | Loss: 6.007 | Acc: 30.954,48.787,65.780,%
Batch: 180 | Loss: 6.003 | Acc: 30.831,48.701,65.642,%
Batch: 200 | Loss: 6.013 | Acc: 30.718,48.721,65.497,%
Batch: 220 | Loss: 6.009 | Acc: 30.780,48.766,65.282,%
Batch: 240 | Loss: 6.011 | Acc: 30.855,48.658,65.239,%
Batch: 260 | Loss: 6.021 | Acc: 30.915,48.536,65.101,%
Batch: 280 | Loss: 6.028 | Acc: 30.750,48.499,65.102,%
Batch: 300 | Loss: 6.032 | Acc: 30.765,48.630,64.929,%
Batch: 320 | Loss: 6.027 | Acc: 30.788,48.520,64.956,%
Batch: 340 | Loss: 6.031 | Acc: 30.755,48.602,64.915,%
Batch: 360 | Loss: 6.023 | Acc: 30.889,48.697,64.870,%
Batch: 380 | Loss: 6.029 | Acc: 30.926,48.694,64.801,%
Batch: 0 | Loss: 6.889 | Acc: 25.781,48.438,60.156,%
Batch: 20 | Loss: 6.757 | Acc: 24.926,44.568,57.403,%
Batch: 40 | Loss: 6.783 | Acc: 25.229,44.703,56.879,%
Batch: 60 | Loss: 6.791 | Acc: 25.320,44.518,57.018,%
Train all parameters

Epoch: 16
Batch: 0 | Loss: 5.405 | Acc: 35.938,53.125,69.531,%
Batch: 20 | Loss: 5.911 | Acc: 31.064,48.810,67.746,%
Batch: 40 | Loss: 5.796 | Acc: 32.165,49.714,67.778,%
Batch: 60 | Loss: 5.860 | Acc: 31.468,50.346,67.905,%
Batch: 80 | Loss: 5.852 | Acc: 31.308,50.135,67.650,%
Batch: 100 | Loss: 5.859 | Acc: 31.490,49.884,67.087,%
Batch: 120 | Loss: 5.853 | Acc: 31.670,50.090,67.304,%
Batch: 140 | Loss: 5.856 | Acc: 31.643,49.994,67.287,%
Batch: 160 | Loss: 5.870 | Acc: 31.454,49.864,67.154,%
Batch: 180 | Loss: 5.864 | Acc: 31.492,49.750,67.222,%
Batch: 200 | Loss: 5.864 | Acc: 31.534,49.732,67.110,%
Batch: 220 | Loss: 5.857 | Acc: 31.635,49.788,67.188,%
Batch: 240 | Loss: 5.851 | Acc: 31.655,49.919,67.249,%
Batch: 260 | Loss: 5.859 | Acc: 31.630,49.922,67.158,%
Batch: 280 | Loss: 5.860 | Acc: 31.661,49.908,67.079,%
Batch: 300 | Loss: 5.861 | Acc: 31.655,49.961,66.985,%
Batch: 320 | Loss: 5.864 | Acc: 31.644,49.988,66.861,%
Batch: 340 | Loss: 5.866 | Acc: 31.584,50.046,66.807,%
Batch: 360 | Loss: 5.874 | Acc: 31.484,50.032,66.698,%
Batch: 380 | Loss: 5.868 | Acc: 31.537,49.992,66.601,%
Batch: 0 | Loss: 6.501 | Acc: 28.906,50.781,62.500,%
Batch: 20 | Loss: 6.562 | Acc: 28.423,46.949,59.077,%
Batch: 40 | Loss: 6.552 | Acc: 27.801,46.780,58.841,%
Batch: 60 | Loss: 6.589 | Acc: 27.754,46.491,58.350,%
Train all parameters

Epoch: 17
Batch: 0 | Loss: 5.620 | Acc: 39.062,51.562,70.312,%
Batch: 20 | Loss: 5.694 | Acc: 33.482,51.860,68.452,%
Batch: 40 | Loss: 5.658 | Acc: 33.194,51.429,69.055,%
Batch: 60 | Loss: 5.722 | Acc: 32.441,50.935,68.763,%
Batch: 80 | Loss: 5.670 | Acc: 32.147,50.993,68.673,%
Batch: 100 | Loss: 5.677 | Acc: 32.101,50.866,68.750,%
Batch: 120 | Loss: 5.694 | Acc: 31.947,50.988,68.653,%
Batch: 140 | Loss: 5.728 | Acc: 31.965,50.521,68.063,%
Batch: 160 | Loss: 5.706 | Acc: 32.274,50.713,68.207,%
Batch: 180 | Loss: 5.718 | Acc: 32.178,50.639,68.150,%
Batch: 200 | Loss: 5.725 | Acc: 32.140,50.696,68.155,%
Batch: 220 | Loss: 5.735 | Acc: 32.134,50.711,68.029,%
Batch: 240 | Loss: 5.738 | Acc: 32.184,50.694,68.014,%
Batch: 260 | Loss: 5.754 | Acc: 32.199,50.673,67.861,%
Batch: 280 | Loss: 5.757 | Acc: 32.220,50.673,67.807,%
Batch: 300 | Loss: 5.769 | Acc: 32.151,50.649,67.704,%
Batch: 320 | Loss: 5.776 | Acc: 32.090,50.655,67.665,%
Batch: 340 | Loss: 5.777 | Acc: 32.022,50.660,67.561,%
Batch: 360 | Loss: 5.773 | Acc: 32.139,50.690,67.542,%
Batch: 380 | Loss: 5.778 | Acc: 32.134,50.755,67.569,%
Batch: 0 | Loss: 6.526 | Acc: 28.906,50.781,61.719,%
Batch: 20 | Loss: 6.785 | Acc: 25.558,45.201,57.999,%
Batch: 40 | Loss: 6.754 | Acc: 25.800,44.550,57.793,%
Batch: 60 | Loss: 6.761 | Acc: 26.165,44.980,57.608,%
Train all parameters

Epoch: 18
Batch: 0 | Loss: 5.202 | Acc: 33.594,51.562,69.531,%
Batch: 20 | Loss: 5.538 | Acc: 31.287,52.158,71.168,%
Batch: 40 | Loss: 5.552 | Acc: 32.012,52.591,70.827,%
Batch: 60 | Loss: 5.594 | Acc: 31.916,52.177,70.492,%
Batch: 80 | Loss: 5.558 | Acc: 32.523,52.228,70.380,%
Batch: 100 | Loss: 5.587 | Acc: 32.704,51.895,70.119,%
Batch: 120 | Loss: 5.573 | Acc: 32.825,52.144,70.093,%
Batch: 140 | Loss: 5.610 | Acc: 32.624,52.017,69.686,%
Batch: 160 | Loss: 5.623 | Acc: 32.638,51.917,69.657,%
Batch: 180 | Loss: 5.617 | Acc: 32.778,52.024,69.747,%
Batch: 200 | Loss: 5.619 | Acc: 32.676,51.959,69.570,%
Batch: 220 | Loss: 5.631 | Acc: 32.689,51.951,69.439,%
Batch: 240 | Loss: 5.638 | Acc: 32.553,51.825,69.330,%
Batch: 260 | Loss: 5.644 | Acc: 32.660,51.841,69.256,%
Batch: 280 | Loss: 5.651 | Acc: 32.632,51.852,69.095,%
Batch: 300 | Loss: 5.638 | Acc: 32.683,51.970,69.082,%
Batch: 320 | Loss: 5.648 | Acc: 32.666,51.923,68.967,%
Batch: 340 | Loss: 5.644 | Acc: 32.712,51.943,68.993,%
Batch: 360 | Loss: 5.648 | Acc: 32.709,51.950,68.940,%
Batch: 380 | Loss: 5.646 | Acc: 32.730,51.969,68.891,%
Batch: 0 | Loss: 6.001 | Acc: 35.156,55.469,67.188,%
Batch: 20 | Loss: 6.284 | Acc: 28.906,48.996,61.793,%
Batch: 40 | Loss: 6.309 | Acc: 28.906,48.418,60.861,%
Batch: 60 | Loss: 6.323 | Acc: 29.073,48.450,60.400,%
Train all parameters

Epoch: 19
Batch: 0 | Loss: 5.136 | Acc: 34.375,53.906,75.000,%
Batch: 20 | Loss: 5.345 | Acc: 34.301,53.013,72.135,%
Batch: 40 | Loss: 5.395 | Acc: 33.365,53.678,72.828,%
Batch: 60 | Loss: 5.433 | Acc: 33.017,53.291,72.451,%
Batch: 80 | Loss: 5.389 | Acc: 33.517,53.964,72.521,%
Batch: 100 | Loss: 5.427 | Acc: 33.857,53.434,71.991,%
Batch: 120 | Loss: 5.416 | Acc: 34.026,53.493,71.952,%
Batch: 140 | Loss: 5.426 | Acc: 33.943,53.252,71.703,%
Batch: 160 | Loss: 5.451 | Acc: 33.734,53.125,71.594,%
Batch: 180 | Loss: 5.444 | Acc: 33.861,53.099,71.452,%
Batch: 200 | Loss: 5.448 | Acc: 33.912,53.074,71.269,%
Batch: 220 | Loss: 5.463 | Acc: 33.778,52.980,71.027,%
Batch: 240 | Loss: 5.468 | Acc: 33.723,53.073,70.993,%
Batch: 260 | Loss: 5.477 | Acc: 33.693,53.068,70.866,%
Batch: 280 | Loss: 5.479 | Acc: 33.602,52.972,70.763,%
Batch: 300 | Loss: 5.494 | Acc: 33.542,52.873,70.507,%
Batch: 320 | Loss: 5.498 | Acc: 33.494,52.831,70.327,%
Batch: 340 | Loss: 5.512 | Acc: 33.438,52.852,70.223,%
Batch: 360 | Loss: 5.516 | Acc: 33.392,52.803,70.074,%
Batch: 380 | Loss: 5.515 | Acc: 33.411,52.801,70.046,%
Batch: 0 | Loss: 5.957 | Acc: 35.938,53.125,64.844,%
Batch: 20 | Loss: 6.214 | Acc: 30.543,49.926,61.868,%
Batch: 40 | Loss: 6.225 | Acc: 30.145,49.066,61.071,%
Batch: 60 | Loss: 6.243 | Acc: 29.803,48.566,61.142,%
Train all parameters

Epoch: 20
Batch: 0 | Loss: 5.582 | Acc: 37.500,57.031,74.219,%
Batch: 20 | Loss: 5.480 | Acc: 32.329,56.027,73.512,%
Batch: 40 | Loss: 5.350 | Acc: 33.518,54.954,73.933,%
Batch: 60 | Loss: 5.339 | Acc: 33.914,54.483,73.796,%
Batch: 80 | Loss: 5.322 | Acc: 34.153,54.475,73.515,%
Batch: 100 | Loss: 5.310 | Acc: 34.383,54.664,73.275,%
Batch: 120 | Loss: 5.321 | Acc: 34.517,54.339,73.063,%
Batch: 140 | Loss: 5.328 | Acc: 34.425,54.433,73.039,%
Batch: 160 | Loss: 5.341 | Acc: 34.225,54.377,72.967,%
Batch: 180 | Loss: 5.347 | Acc: 34.077,54.536,72.820,%
Batch: 200 | Loss: 5.363 | Acc: 33.998,54.470,72.501,%
Batch: 220 | Loss: 5.372 | Acc: 33.930,54.500,72.359,%
Batch: 240 | Loss: 5.381 | Acc: 33.898,54.370,72.167,%
Batch: 260 | Loss: 5.387 | Acc: 33.866,54.215,72.031,%
Batch: 280 | Loss: 5.397 | Acc: 33.736,54.212,71.867,%
Batch: 300 | Loss: 5.398 | Acc: 33.846,54.197,71.688,%
Batch: 320 | Loss: 5.403 | Acc: 33.900,54.128,71.532,%
Batch: 340 | Loss: 5.414 | Acc: 33.837,54.092,71.408,%
Batch: 360 | Loss: 5.421 | Acc: 33.786,53.919,71.306,%
Batch: 380 | Loss: 5.423 | Acc: 33.709,53.906,71.243,%
Batch: 0 | Loss: 6.282 | Acc: 32.812,53.906,61.719,%
Batch: 20 | Loss: 6.258 | Acc: 30.841,50.223,61.533,%
Batch: 40 | Loss: 6.226 | Acc: 30.678,49.962,61.128,%
Batch: 60 | Loss: 6.256 | Acc: 30.571,49.232,60.771,%
Train all parameters

Epoch: 21
Batch: 0 | Loss: 5.039 | Acc: 43.750,56.250,74.219,%
Batch: 20 | Loss: 5.246 | Acc: 33.966,55.915,73.847,%
Batch: 40 | Loss: 5.314 | Acc: 33.670,54.878,73.457,%
Batch: 60 | Loss: 5.326 | Acc: 32.992,54.700,73.694,%
Batch: 80 | Loss: 5.331 | Acc: 32.861,54.890,73.081,%
Batch: 100 | Loss: 5.306 | Acc: 33.346,54.974,73.035,%
Batch: 120 | Loss: 5.320 | Acc: 33.219,54.804,73.031,%
Batch: 140 | Loss: 5.330 | Acc: 33.355,54.820,73.050,%
Batch: 160 | Loss: 5.311 | Acc: 33.540,54.804,73.156,%
Batch: 180 | Loss: 5.330 | Acc: 33.408,54.523,72.920,%
Batch: 200 | Loss: 5.327 | Acc: 33.361,54.614,72.928,%
Batch: 220 | Loss: 5.325 | Acc: 33.512,54.613,72.928,%
Batch: 240 | Loss: 5.341 | Acc: 33.399,54.328,72.886,%
Batch: 260 | Loss: 5.338 | Acc: 33.537,54.361,72.803,%
Batch: 280 | Loss: 5.347 | Acc: 33.560,54.351,72.609,%
Batch: 300 | Loss: 5.337 | Acc: 33.724,54.340,72.578,%
Batch: 320 | Loss: 5.339 | Acc: 33.805,54.364,72.430,%
Batch: 340 | Loss: 5.337 | Acc: 33.908,54.383,72.345,%
Batch: 360 | Loss: 5.333 | Acc: 34.048,54.356,72.275,%
Batch: 380 | Loss: 5.335 | Acc: 34.102,54.372,72.242,%
Batch: 0 | Loss: 5.759 | Acc: 35.156,54.688,71.094,%
Batch: 20 | Loss: 6.135 | Acc: 29.427,49.963,62.612,%
Batch: 40 | Loss: 6.143 | Acc: 29.516,49.619,62.633,%
Batch: 60 | Loss: 6.179 | Acc: 29.739,49.372,61.898,%
Train all parameters

Epoch: 22
Batch: 0 | Loss: 5.464 | Acc: 26.562,55.469,73.438,%
Batch: 20 | Loss: 5.333 | Acc: 32.701,55.804,73.549,%
Batch: 40 | Loss: 5.338 | Acc: 33.365,55.050,74.428,%
Batch: 60 | Loss: 5.237 | Acc: 33.863,56.212,75.538,%
Batch: 80 | Loss: 5.235 | Acc: 34.037,56.240,75.328,%
Batch: 100 | Loss: 5.269 | Acc: 33.687,55.910,74.698,%
Batch: 120 | Loss: 5.261 | Acc: 33.942,55.882,74.277,%
Batch: 140 | Loss: 5.247 | Acc: 34.104,56.017,74.318,%
Batch: 160 | Loss: 5.245 | Acc: 34.186,55.799,74.418,%
Batch: 180 | Loss: 5.260 | Acc: 34.159,55.754,74.102,%
Batch: 200 | Loss: 5.266 | Acc: 34.196,55.745,73.970,%
Batch: 220 | Loss: 5.263 | Acc: 34.272,55.614,73.773,%
Batch: 240 | Loss: 5.251 | Acc: 34.394,55.608,73.723,%
Batch: 260 | Loss: 5.260 | Acc: 34.318,55.439,73.426,%
Batch: 280 | Loss: 5.263 | Acc: 34.308,55.383,73.315,%
Batch: 300 | Loss: 5.259 | Acc: 34.424,55.404,73.300,%
Batch: 320 | Loss: 5.264 | Acc: 34.368,55.369,73.240,%
Batch: 340 | Loss: 5.265 | Acc: 34.361,55.411,73.142,%
Batch: 360 | Loss: 5.265 | Acc: 34.414,55.456,73.052,%
Batch: 380 | Loss: 5.267 | Acc: 34.420,55.420,73.038,%
Batch: 0 | Loss: 5.839 | Acc: 32.031,51.562,67.188,%
Batch: 20 | Loss: 6.178 | Acc: 29.167,50.409,62.351,%
Batch: 40 | Loss: 6.148 | Acc: 29.383,50.267,62.195,%
Batch: 60 | Loss: 6.179 | Acc: 29.303,50.282,61.898,%
Train all parameters

Epoch: 23
Batch: 0 | Loss: 4.810 | Acc: 38.281,57.031,72.656,%
Batch: 20 | Loss: 5.042 | Acc: 34.821,58.185,76.860,%
Batch: 40 | Loss: 5.017 | Acc: 35.537,58.041,76.524,%
Batch: 60 | Loss: 5.062 | Acc: 35.515,57.108,76.345,%
Batch: 80 | Loss: 5.084 | Acc: 35.397,56.703,76.013,%
Batch: 100 | Loss: 5.061 | Acc: 35.644,56.923,75.874,%
Batch: 120 | Loss: 5.101 | Acc: 35.266,56.573,75.458,%
Batch: 140 | Loss: 5.110 | Acc: 35.201,56.549,75.526,%
Batch: 160 | Loss: 5.126 | Acc: 35.093,56.507,75.466,%
Batch: 180 | Loss: 5.121 | Acc: 35.161,56.505,75.363,%
Batch: 200 | Loss: 5.124 | Acc: 35.226,56.382,75.330,%
Batch: 220 | Loss: 5.125 | Acc: 35.195,56.243,75.120,%
Batch: 240 | Loss: 5.134 | Acc: 35.241,56.266,75.094,%
Batch: 260 | Loss: 5.137 | Acc: 35.168,56.229,75.039,%
Batch: 280 | Loss: 5.146 | Acc: 35.162,56.189,74.922,%
Batch: 300 | Loss: 5.163 | Acc: 35.089,56.055,74.785,%
Batch: 320 | Loss: 5.168 | Acc: 35.064,55.953,74.674,%
Batch: 340 | Loss: 5.176 | Acc: 35.035,55.867,74.542,%
Batch: 360 | Loss: 5.170 | Acc: 35.226,55.899,74.450,%
Batch: 380 | Loss: 5.164 | Acc: 35.246,55.895,74.395,%
Batch: 0 | Loss: 5.892 | Acc: 32.031,53.125,67.969,%
Batch: 20 | Loss: 6.054 | Acc: 30.320,50.335,62.202,%
Batch: 40 | Loss: 6.078 | Acc: 30.602,49.924,62.195,%
Batch: 60 | Loss: 6.107 | Acc: 30.507,49.923,62.039,%
Train all parameters

Epoch: 24
Batch: 0 | Loss: 4.819 | Acc: 44.531,56.250,78.906,%
Batch: 20 | Loss: 4.987 | Acc: 36.161,57.924,76.562,%
Batch: 40 | Loss: 4.947 | Acc: 36.471,57.336,76.429,%
Batch: 60 | Loss: 5.001 | Acc: 35.938,57.569,76.601,%
Batch: 80 | Loss: 5.032 | Acc: 35.918,56.906,76.553,%
Batch: 100 | Loss: 5.028 | Acc: 35.999,57.024,76.238,%
Batch: 120 | Loss: 5.037 | Acc: 35.886,56.863,76.136,%
Batch: 140 | Loss: 5.049 | Acc: 35.877,56.544,76.014,%
Batch: 160 | Loss: 5.043 | Acc: 35.855,56.745,75.883,%
Batch: 180 | Loss: 5.041 | Acc: 35.894,56.569,75.622,%
Batch: 200 | Loss: 5.048 | Acc: 35.805,56.491,75.579,%
Batch: 220 | Loss: 5.050 | Acc: 35.849,56.345,75.467,%
Batch: 240 | Loss: 5.042 | Acc: 35.986,56.467,75.428,%
Batch: 260 | Loss: 5.058 | Acc: 35.926,56.412,75.186,%
Batch: 280 | Loss: 5.065 | Acc: 35.946,56.456,75.044,%
Batch: 300 | Loss: 5.074 | Acc: 35.932,56.367,74.909,%
Batch: 320 | Loss: 5.075 | Acc: 35.906,56.350,74.927,%
Batch: 340 | Loss: 5.086 | Acc: 35.830,56.339,74.796,%
Batch: 360 | Loss: 5.085 | Acc: 35.931,56.317,74.704,%
Batch: 380 | Loss: 5.092 | Acc: 35.890,56.279,74.598,%
Batch: 0 | Loss: 5.751 | Acc: 32.812,57.031,66.406,%
Batch: 20 | Loss: 5.986 | Acc: 33.185,52.530,63.542,%
Batch: 40 | Loss: 5.983 | Acc: 32.450,51.867,63.643,%
Batch: 60 | Loss: 6.004 | Acc: 32.415,51.960,63.179,%
Train all parameters

Epoch: 25
Batch: 0 | Loss: 4.833 | Acc: 34.375,54.688,79.688,%
Batch: 20 | Loss: 4.832 | Acc: 36.682,58.557,77.307,%
Batch: 40 | Loss: 4.913 | Acc: 36.242,58.022,77.287,%
Batch: 60 | Loss: 4.955 | Acc: 35.976,57.902,77.075,%
Batch: 80 | Loss: 4.951 | Acc: 36.323,57.388,76.861,%
Batch: 100 | Loss: 4.950 | Acc: 36.463,57.070,76.756,%
Batch: 120 | Loss: 4.969 | Acc: 36.222,57.135,76.672,%
Batch: 140 | Loss: 4.971 | Acc: 36.248,57.048,76.596,%
Batch: 160 | Loss: 4.949 | Acc: 36.350,57.114,76.781,%
Batch: 180 | Loss: 4.942 | Acc: 36.343,57.118,76.722,%
Batch: 200 | Loss: 4.953 | Acc: 36.217,57.109,76.648,%
Batch: 220 | Loss: 4.954 | Acc: 36.231,57.035,76.577,%
Batch: 240 | Loss: 4.978 | Acc: 36.106,56.872,76.284,%
Batch: 260 | Loss: 4.988 | Acc: 36.024,56.888,76.257,%
Batch: 280 | Loss: 4.975 | Acc: 36.282,56.984,76.248,%
Batch: 300 | Loss: 4.979 | Acc: 36.322,56.992,76.126,%
Batch: 320 | Loss: 4.988 | Acc: 36.286,57.012,76.161,%
Batch: 340 | Loss: 4.981 | Acc: 36.373,57.105,76.118,%
Batch: 360 | Loss: 4.982 | Acc: 36.318,57.116,76.071,%
Batch: 380 | Loss: 4.984 | Acc: 36.280,57.128,75.917,%
Batch: 0 | Loss: 5.763 | Acc: 32.812,52.344,71.094,%
Batch: 20 | Loss: 6.174 | Acc: 28.571,49.293,62.388,%
Batch: 40 | Loss: 6.201 | Acc: 28.316,49.352,62.005,%
Batch: 60 | Loss: 6.191 | Acc: 28.343,49.859,62.090,%
Train all parameters

Epoch: 26
Batch: 0 | Loss: 4.354 | Acc: 43.750,60.938,81.250,%
Batch: 20 | Loss: 4.851 | Acc: 36.979,59.003,78.385,%
Batch: 40 | Loss: 4.878 | Acc: 36.585,59.223,78.430,%
Batch: 60 | Loss: 4.916 | Acc: 36.386,58.619,78.560,%
Batch: 80 | Loss: 4.953 | Acc: 36.053,57.996,78.038,%
Batch: 100 | Loss: 4.951 | Acc: 35.922,58.253,77.839,%
Batch: 120 | Loss: 4.928 | Acc: 36.138,58.529,77.809,%
Batch: 140 | Loss: 4.911 | Acc: 36.431,58.361,77.770,%
Batch: 160 | Loss: 4.921 | Acc: 36.306,58.463,77.659,%
Batch: 180 | Loss: 4.911 | Acc: 36.468,58.412,77.516,%
Batch: 200 | Loss: 4.904 | Acc: 36.590,58.225,77.324,%
Batch: 220 | Loss: 4.909 | Acc: 36.588,58.244,77.199,%
Batch: 240 | Loss: 4.896 | Acc: 36.677,58.321,77.201,%
Batch: 260 | Loss: 4.894 | Acc: 36.767,58.396,77.092,%
Batch: 280 | Loss: 4.906 | Acc: 36.735,58.277,76.924,%
Batch: 300 | Loss: 4.905 | Acc: 36.776,58.246,76.879,%
Batch: 320 | Loss: 4.902 | Acc: 36.826,58.226,76.816,%
Batch: 340 | Loss: 4.907 | Acc: 36.799,58.213,76.702,%
Batch: 360 | Loss: 4.909 | Acc: 36.797,58.237,76.606,%
Batch: 380 | Loss: 4.911 | Acc: 36.780,58.204,76.532,%
Batch: 0 | Loss: 5.750 | Acc: 35.938,59.375,67.188,%
Batch: 20 | Loss: 6.053 | Acc: 32.701,51.711,61.198,%
Batch: 40 | Loss: 6.015 | Acc: 32.908,51.372,61.090,%
Batch: 60 | Loss: 6.031 | Acc: 32.787,51.345,60.899,%
Train all parameters

Epoch: 27
Batch: 0 | Loss: 5.817 | Acc: 30.469,55.469,76.562,%
Batch: 20 | Loss: 4.936 | Acc: 37.054,59.189,78.311,%
Batch: 40 | Loss: 4.952 | Acc: 37.252,58.155,78.258,%
Batch: 60 | Loss: 4.918 | Acc: 36.821,58.568,77.882,%
Batch: 80 | Loss: 4.919 | Acc: 36.458,58.420,77.691,%
Batch: 100 | Loss: 4.907 | Acc: 36.580,58.524,77.560,%
Batch: 120 | Loss: 4.904 | Acc: 36.532,58.574,77.479,%
Batch: 140 | Loss: 4.895 | Acc: 36.547,58.599,77.549,%
Batch: 160 | Loss: 4.897 | Acc: 36.544,58.395,77.538,%
Batch: 180 | Loss: 4.883 | Acc: 36.771,58.551,77.732,%
Batch: 200 | Loss: 4.885 | Acc: 36.703,58.532,77.647,%
Batch: 220 | Loss: 4.894 | Acc: 36.652,58.375,77.711,%
Batch: 240 | Loss: 4.902 | Acc: 36.586,58.325,77.652,%
Batch: 260 | Loss: 4.906 | Acc: 36.560,58.220,77.577,%
Batch: 280 | Loss: 4.904 | Acc: 36.591,58.171,77.497,%
Batch: 300 | Loss: 4.902 | Acc: 36.534,58.194,77.448,%
Batch: 320 | Loss: 4.891 | Acc: 36.651,58.277,77.446,%
Batch: 340 | Loss: 4.893 | Acc: 36.524,58.319,77.438,%
Batch: 360 | Loss: 4.895 | Acc: 36.528,58.271,77.357,%
Batch: 380 | Loss: 4.906 | Acc: 36.489,58.143,77.145,%
Batch: 0 | Loss: 5.587 | Acc: 34.375,56.250,68.750,%
Batch: 20 | Loss: 5.966 | Acc: 30.618,51.860,64.360,%
Batch: 40 | Loss: 6.028 | Acc: 30.354,51.010,62.938,%
Batch: 60 | Loss: 6.053 | Acc: 30.008,50.282,62.423,%
Train all parameters

Epoch: 28
Batch: 0 | Loss: 4.168 | Acc: 46.875,64.062,82.812,%
Batch: 20 | Loss: 4.600 | Acc: 37.909,60.379,80.506,%
Batch: 40 | Loss: 4.635 | Acc: 38.205,60.252,80.621,%
Batch: 60 | Loss: 4.621 | Acc: 38.128,60.553,80.366,%
Batch: 80 | Loss: 4.587 | Acc: 38.532,60.764,80.372,%
Batch: 100 | Loss: 4.621 | Acc: 38.351,60.210,80.260,%
Batch: 120 | Loss: 4.664 | Acc: 37.778,59.879,80.114,%
Batch: 140 | Loss: 4.676 | Acc: 37.644,59.885,79.965,%
Batch: 160 | Loss: 4.701 | Acc: 37.597,59.768,79.644,%
Batch: 180 | Loss: 4.734 | Acc: 37.366,59.582,79.463,%
Batch: 200 | Loss: 4.729 | Acc: 37.426,59.655,79.322,%
Batch: 220 | Loss: 4.744 | Acc: 37.373,59.470,79.065,%
Batch: 240 | Loss: 4.763 | Acc: 37.257,59.320,78.867,%
Batch: 260 | Loss: 4.766 | Acc: 37.323,59.267,78.724,%
Batch: 280 | Loss: 4.784 | Acc: 37.175,59.150,78.461,%
Batch: 300 | Loss: 4.787 | Acc: 37.222,59.167,78.408,%
Batch: 320 | Loss: 4.801 | Acc: 37.169,59.183,78.254,%
Batch: 340 | Loss: 4.805 | Acc: 37.035,59.157,78.244,%
Batch: 360 | Loss: 4.806 | Acc: 37.091,58.994,78.225,%
Batch: 380 | Loss: 4.814 | Acc: 37.094,58.992,78.148,%
Batch: 0 | Loss: 5.485 | Acc: 35.938,60.938,69.531,%
Batch: 20 | Loss: 5.941 | Acc: 32.068,52.827,63.467,%
Batch: 40 | Loss: 5.926 | Acc: 31.803,52.344,63.357,%
Batch: 60 | Loss: 5.933 | Acc: 31.634,52.318,63.307,%
Train all parameters

Epoch: 29
Batch: 0 | Loss: 4.147 | Acc: 39.844,60.156,74.219,%
Batch: 20 | Loss: 4.679 | Acc: 37.574,60.417,80.060,%
Batch: 40 | Loss: 4.652 | Acc: 38.357,60.728,80.945,%
Batch: 60 | Loss: 4.627 | Acc: 38.166,60.079,80.840,%
Batch: 80 | Loss: 4.590 | Acc: 38.127,60.475,81.211,%
Batch: 100 | Loss: 4.618 | Acc: 37.809,60.489,80.894,%
Batch: 120 | Loss: 4.635 | Acc: 37.687,60.356,80.798,%
Batch: 140 | Loss: 4.639 | Acc: 37.794,60.411,80.718,%
Batch: 160 | Loss: 4.654 | Acc: 37.582,60.200,80.420,%
Batch: 180 | Loss: 4.660 | Acc: 37.487,60.148,80.331,%
Batch: 200 | Loss: 4.688 | Acc: 37.403,59.845,80.057,%
Batch: 220 | Loss: 4.712 | Acc: 37.242,59.739,79.772,%
Batch: 240 | Loss: 4.725 | Acc: 37.257,59.647,79.597,%
Batch: 260 | Loss: 4.727 | Acc: 37.377,59.662,79.430,%
Batch: 280 | Loss: 4.735 | Acc: 37.478,59.614,79.257,%
Batch: 300 | Loss: 4.738 | Acc: 37.482,59.606,79.150,%
Batch: 320 | Loss: 4.742 | Acc: 37.420,59.516,78.979,%
Batch: 340 | Loss: 4.751 | Acc: 37.363,59.490,78.860,%
Batch: 360 | Loss: 4.759 | Acc: 37.312,59.427,78.707,%
Batch: 380 | Loss: 4.778 | Acc: 37.250,59.287,78.562,%
Batch: 0 | Loss: 5.856 | Acc: 36.719,59.375,63.281,%
Batch: 20 | Loss: 5.933 | Acc: 33.743,53.720,61.905,%
Batch: 40 | Loss: 5.929 | Acc: 33.327,53.182,61.776,%
Batch: 60 | Loss: 5.932 | Acc: 33.325,53.227,62.013,%
Train all parameters

Epoch: 30
Batch: 0 | Loss: 4.746 | Acc: 36.719,61.719,83.594,%
Batch: 20 | Loss: 4.554 | Acc: 39.955,60.751,80.878,%
Batch: 40 | Loss: 4.532 | Acc: 39.043,61.300,81.193,%
Batch: 60 | Loss: 4.588 | Acc: 38.371,60.566,80.776,%
Batch: 80 | Loss: 4.610 | Acc: 38.050,60.243,80.613,%
Batch: 100 | Loss: 4.597 | Acc: 38.204,60.118,80.523,%
Batch: 120 | Loss: 4.582 | Acc: 38.178,60.176,80.882,%
Batch: 140 | Loss: 4.580 | Acc: 38.503,60.090,80.857,%
Batch: 160 | Loss: 4.619 | Acc: 38.325,59.889,80.571,%
Batch: 180 | Loss: 4.635 | Acc: 38.169,59.858,80.365,%
Batch: 200 | Loss: 4.646 | Acc: 38.196,59.736,80.220,%
Batch: 220 | Loss: 4.662 | Acc: 38.094,59.587,79.977,%
Batch: 240 | Loss: 4.673 | Acc: 37.889,59.557,79.769,%
Batch: 260 | Loss: 4.692 | Acc: 37.751,59.447,79.535,%
Batch: 280 | Loss: 4.686 | Acc: 37.795,59.542,79.468,%
Batch: 300 | Loss: 4.687 | Acc: 37.804,59.590,79.482,%
Batch: 320 | Loss: 4.689 | Acc: 37.748,59.538,79.395,%
Batch: 340 | Loss: 4.700 | Acc: 37.626,59.366,79.326,%
Batch: 360 | Loss: 4.710 | Acc: 37.621,59.321,79.242,%
Batch: 380 | Loss: 4.712 | Acc: 37.619,59.277,79.128,%
Batch: 0 | Loss: 5.845 | Acc: 32.812,57.812,62.500,%
Batch: 20 | Loss: 5.895 | Acc: 31.696,52.604,63.132,%
Batch: 40 | Loss: 5.860 | Acc: 31.803,52.439,62.824,%
Batch: 60 | Loss: 5.875 | Acc: 32.006,52.510,62.743,%
Train all parameters

Epoch: 31
Batch: 0 | Loss: 4.197 | Acc: 42.969,67.188,83.594,%
Batch: 20 | Loss: 4.456 | Acc: 39.546,61.458,81.473,%
Batch: 40 | Loss: 4.484 | Acc: 39.196,61.300,81.974,%
Batch: 60 | Loss: 4.512 | Acc: 38.794,61.386,81.775,%
Batch: 80 | Loss: 4.525 | Acc: 38.735,60.976,81.472,%
Batch: 100 | Loss: 4.537 | Acc: 38.420,60.860,81.590,%
Batch: 120 | Loss: 4.536 | Acc: 38.565,61.009,81.599,%
Batch: 140 | Loss: 4.530 | Acc: 38.680,60.954,81.433,%
Batch: 160 | Loss: 4.535 | Acc: 38.684,60.675,81.381,%
Batch: 180 | Loss: 4.557 | Acc: 38.583,60.450,81.064,%
Batch: 200 | Loss: 4.566 | Acc: 38.511,60.366,80.947,%
Batch: 220 | Loss: 4.574 | Acc: 38.462,60.425,80.744,%
Batch: 240 | Loss: 4.589 | Acc: 38.398,60.422,80.595,%
Batch: 260 | Loss: 4.603 | Acc: 38.305,60.366,80.355,%
Batch: 280 | Loss: 4.615 | Acc: 38.315,60.276,80.230,%
Batch: 300 | Loss: 4.625 | Acc: 38.201,60.250,80.134,%
Batch: 320 | Loss: 4.643 | Acc: 38.113,60.173,79.955,%
Batch: 340 | Loss: 4.657 | Acc: 37.938,60.106,79.820,%
Batch: 360 | Loss: 4.667 | Acc: 37.864,59.985,79.692,%
Batch: 380 | Loss: 4.672 | Acc: 37.881,59.918,79.503,%
Batch: 0 | Loss: 5.317 | Acc: 38.281,62.500,67.969,%
Batch: 20 | Loss: 5.679 | Acc: 34.226,54.762,64.918,%
Batch: 40 | Loss: 5.711 | Acc: 34.032,54.745,64.653,%
Batch: 60 | Loss: 5.712 | Acc: 33.735,54.892,65.100,%
Train all parameters

Epoch: 32
Batch: 0 | Loss: 4.115 | Acc: 41.406,60.156,82.031,%
Batch: 20 | Loss: 4.537 | Acc: 37.835,61.458,82.180,%
Batch: 40 | Loss: 4.536 | Acc: 37.424,61.471,82.431,%
Batch: 60 | Loss: 4.566 | Acc: 37.731,61.078,82.377,%
Batch: 80 | Loss: 4.544 | Acc: 38.233,61.391,82.514,%
Batch: 100 | Loss: 4.543 | Acc: 38.103,61.773,82.395,%
Batch: 120 | Loss: 4.527 | Acc: 38.417,61.699,82.225,%
Batch: 140 | Loss: 4.528 | Acc: 38.287,61.608,82.369,%
Batch: 160 | Loss: 4.526 | Acc: 38.165,61.588,82.274,%
Batch: 180 | Loss: 4.540 | Acc: 38.130,61.287,82.131,%
Batch: 200 | Loss: 4.540 | Acc: 38.223,61.307,82.016,%
Batch: 220 | Loss: 4.548 | Acc: 38.200,61.135,81.844,%
Batch: 240 | Loss: 4.558 | Acc: 38.207,61.090,81.746,%
Batch: 260 | Loss: 4.562 | Acc: 38.329,60.976,81.552,%
Batch: 280 | Loss: 4.567 | Acc: 38.337,61.032,81.389,%
Batch: 300 | Loss: 4.588 | Acc: 38.185,60.839,81.172,%
Batch: 320 | Loss: 4.584 | Acc: 38.237,60.872,80.992,%
Batch: 340 | Loss: 4.600 | Acc: 38.087,60.745,80.817,%
Batch: 360 | Loss: 4.609 | Acc: 38.078,60.767,80.715,%
Batch: 380 | Loss: 4.612 | Acc: 38.031,60.769,80.633,%
Batch: 0 | Loss: 5.531 | Acc: 33.594,56.250,67.969,%
Batch: 20 | Loss: 5.674 | Acc: 33.929,54.315,66.109,%
Batch: 40 | Loss: 5.672 | Acc: 33.727,54.421,65.111,%
Batch: 60 | Loss: 5.699 | Acc: 33.722,54.098,64.575,%
Train all parameters

Epoch: 33
Batch: 0 | Loss: 3.789 | Acc: 41.406,71.094,88.281,%
Batch: 20 | Loss: 4.445 | Acc: 39.174,62.872,83.445,%
Batch: 40 | Loss: 4.423 | Acc: 38.758,63.319,83.880,%
Batch: 60 | Loss: 4.379 | Acc: 39.319,63.128,83.312,%
Batch: 80 | Loss: 4.384 | Acc: 39.111,62.346,83.121,%
Batch: 100 | Loss: 4.461 | Acc: 38.637,61.765,82.650,%
Batch: 120 | Loss: 4.478 | Acc: 38.811,61.796,82.619,%
Batch: 140 | Loss: 4.472 | Acc: 38.763,62.068,82.547,%
Batch: 160 | Loss: 4.485 | Acc: 38.640,61.835,82.080,%
Batch: 180 | Loss: 4.506 | Acc: 38.519,61.611,81.872,%
Batch: 200 | Loss: 4.526 | Acc: 38.437,61.307,81.635,%
Batch: 220 | Loss: 4.530 | Acc: 38.472,61.376,81.451,%
Batch: 240 | Loss: 4.524 | Acc: 38.589,61.450,81.331,%
Batch: 260 | Loss: 4.536 | Acc: 38.425,61.533,81.331,%
Batch: 280 | Loss: 4.537 | Acc: 38.426,61.530,81.192,%
Batch: 300 | Loss: 4.552 | Acc: 38.357,61.371,81.019,%
Batch: 320 | Loss: 4.562 | Acc: 38.415,61.315,80.929,%
Batch: 340 | Loss: 4.567 | Acc: 38.396,61.281,80.897,%
Batch: 360 | Loss: 4.571 | Acc: 38.374,61.273,80.813,%
Batch: 380 | Loss: 4.583 | Acc: 38.294,61.147,80.623,%
Batch: 0 | Loss: 5.708 | Acc: 32.812,57.812,67.969,%
Batch: 20 | Loss: 5.801 | Acc: 32.366,55.915,64.807,%
Batch: 40 | Loss: 5.851 | Acc: 32.393,54.364,63.548,%
Batch: 60 | Loss: 5.860 | Acc: 32.877,53.881,63.166,%
Train all parameters

Epoch: 34
Batch: 0 | Loss: 4.342 | Acc: 42.969,57.812,82.031,%
Batch: 20 | Loss: 4.449 | Acc: 40.104,62.202,82.254,%
Batch: 40 | Loss: 4.407 | Acc: 38.967,62.919,82.851,%
Batch: 60 | Loss: 4.466 | Acc: 38.665,62.026,82.633,%
Batch: 80 | Loss: 4.468 | Acc: 38.397,62.269,82.562,%
Batch: 100 | Loss: 4.467 | Acc: 38.281,62.082,82.503,%
Batch: 120 | Loss: 4.496 | Acc: 38.004,61.848,82.167,%
Batch: 140 | Loss: 4.521 | Acc: 37.910,61.503,82.064,%
Batch: 160 | Loss: 4.514 | Acc: 38.048,61.636,82.143,%
Batch: 180 | Loss: 4.518 | Acc: 38.147,61.697,82.092,%
Batch: 200 | Loss: 4.525 | Acc: 38.165,61.532,82.105,%
Batch: 220 | Loss: 4.515 | Acc: 38.249,61.627,82.035,%
Batch: 240 | Loss: 4.525 | Acc: 38.155,61.570,81.928,%
Batch: 260 | Loss: 4.534 | Acc: 38.123,61.410,81.855,%
Batch: 280 | Loss: 4.543 | Acc: 38.109,61.360,81.684,%
Batch: 300 | Loss: 4.552 | Acc: 38.068,61.270,81.546,%
Batch: 320 | Loss: 4.543 | Acc: 38.143,61.329,81.501,%
Batch: 340 | Loss: 4.539 | Acc: 38.235,61.419,81.351,%
Batch: 360 | Loss: 4.546 | Acc: 38.275,61.396,81.263,%
Batch: 380 | Loss: 4.547 | Acc: 38.357,61.288,81.213,%
Batch: 0 | Loss: 5.360 | Acc: 31.250,57.812,70.312,%
Batch: 20 | Loss: 5.833 | Acc: 32.068,52.865,63.430,%
Batch: 40 | Loss: 5.870 | Acc: 32.165,52.649,63.167,%
Batch: 60 | Loss: 5.876 | Acc: 31.903,52.818,62.948,%
Train classifier parameters

Epoch: 35
Batch: 0 | Loss: 3.340 | Acc: 55.469,68.750,83.594,%
Batch: 20 | Loss: 4.837 | Acc: 36.310,60.305,78.013,%
Batch: 40 | Loss: 4.951 | Acc: 35.423,59.261,77.172,%
Batch: 60 | Loss: 5.007 | Acc: 35.246,59.413,77.075,%
Batch: 80 | Loss: 5.028 | Acc: 35.147,58.883,76.919,%
Batch: 100 | Loss: 5.020 | Acc: 35.110,58.841,76.872,%
Batch: 120 | Loss: 5.034 | Acc: 35.072,58.897,77.079,%
Batch: 140 | Loss: 4.999 | Acc: 35.500,59.292,77.261,%
Batch: 160 | Loss: 5.003 | Acc: 35.447,59.239,77.363,%
Batch: 180 | Loss: 4.985 | Acc: 35.704,59.323,77.434,%
Batch: 200 | Loss: 4.975 | Acc: 35.568,59.348,77.394,%
Batch: 220 | Loss: 4.965 | Acc: 35.556,59.368,77.598,%
Batch: 240 | Loss: 4.969 | Acc: 35.617,59.352,77.593,%
Batch: 260 | Loss: 4.949 | Acc: 35.860,59.492,77.838,%
Batch: 280 | Loss: 4.931 | Acc: 35.985,59.483,77.950,%
Batch: 300 | Loss: 4.930 | Acc: 35.950,59.380,77.974,%
Batch: 320 | Loss: 4.925 | Acc: 36.023,59.297,78.011,%
Batch: 340 | Loss: 4.915 | Acc: 36.148,59.304,78.072,%
Batch: 360 | Loss: 4.910 | Acc: 36.072,59.356,78.153,%
Batch: 380 | Loss: 4.908 | Acc: 36.040,59.328,78.176,%
Batch: 0 | Loss: 5.603 | Acc: 35.938,57.812,70.312,%
Batch: 20 | Loss: 5.806 | Acc: 32.403,55.469,65.662,%
Batch: 40 | Loss: 5.834 | Acc: 32.355,54.649,64.977,%
Batch: 60 | Loss: 5.853 | Acc: 32.531,54.355,64.472,%
Train classifier parameters

Epoch: 36
Batch: 0 | Loss: 5.117 | Acc: 32.812,54.688,74.219,%
Batch: 20 | Loss: 4.909 | Acc: 37.128,58.259,79.204,%
Batch: 40 | Loss: 4.807 | Acc: 37.119,59.546,79.802,%
Batch: 60 | Loss: 4.823 | Acc: 36.962,59.682,79.508,%
Batch: 80 | Loss: 4.799 | Acc: 36.728,59.973,79.958,%
Batch: 100 | Loss: 4.735 | Acc: 37.283,60.094,79.920,%
Batch: 120 | Loss: 4.711 | Acc: 37.416,60.137,80.010,%
Batch: 140 | Loss: 4.716 | Acc: 37.206,60.012,80.020,%
Batch: 160 | Loss: 4.710 | Acc: 37.311,59.821,80.047,%
Batch: 180 | Loss: 4.739 | Acc: 36.956,59.815,80.041,%
Batch: 200 | Loss: 4.740 | Acc: 36.952,59.810,80.045,%
Batch: 220 | Loss: 4.743 | Acc: 36.899,59.757,80.133,%
Batch: 240 | Loss: 4.746 | Acc: 36.826,59.767,80.135,%
Batch: 260 | Loss: 4.754 | Acc: 36.713,59.857,80.038,%
Batch: 280 | Loss: 4.749 | Acc: 36.716,59.914,80.127,%
Batch: 300 | Loss: 4.758 | Acc: 36.669,59.816,80.116,%
Batch: 320 | Loss: 4.758 | Acc: 36.692,59.898,80.233,%
Batch: 340 | Loss: 4.754 | Acc: 36.742,59.943,80.235,%
Batch: 360 | Loss: 4.757 | Acc: 36.779,59.955,80.103,%
Batch: 380 | Loss: 4.748 | Acc: 36.864,60.072,80.145,%
Batch: 0 | Loss: 5.655 | Acc: 34.375,57.812,71.875,%
Batch: 20 | Loss: 5.736 | Acc: 33.259,55.729,65.662,%
Batch: 40 | Loss: 5.758 | Acc: 33.155,55.069,65.149,%
Batch: 60 | Loss: 5.770 | Acc: 33.414,54.752,64.626,%
Train classifier parameters

Epoch: 37
Batch: 0 | Loss: 4.835 | Acc: 31.250,64.844,75.000,%
Batch: 20 | Loss: 4.649 | Acc: 36.979,60.379,81.287,%
Batch: 40 | Loss: 4.561 | Acc: 37.767,60.957,81.307,%
Batch: 60 | Loss: 4.578 | Acc: 37.910,60.592,81.314,%
Batch: 80 | Loss: 4.592 | Acc: 37.915,60.388,80.845,%
Batch: 100 | Loss: 4.580 | Acc: 37.848,60.899,81.002,%
Batch: 120 | Loss: 4.594 | Acc: 37.984,61.060,80.927,%
Batch: 140 | Loss: 4.622 | Acc: 37.849,60.954,80.840,%
Batch: 160 | Loss: 4.628 | Acc: 37.786,61.059,80.896,%
Batch: 180 | Loss: 4.628 | Acc: 37.716,60.994,80.831,%
Batch: 200 | Loss: 4.637 | Acc: 37.554,60.953,80.865,%
Batch: 220 | Loss: 4.638 | Acc: 37.496,60.923,80.808,%
Batch: 240 | Loss: 4.644 | Acc: 37.519,60.882,80.744,%
Batch: 260 | Loss: 4.654 | Acc: 37.533,60.842,80.696,%
Batch: 280 | Loss: 4.650 | Acc: 37.617,60.818,80.716,%
Batch: 300 | Loss: 4.653 | Acc: 37.508,60.800,80.671,%
Batch: 320 | Loss: 4.653 | Acc: 37.549,60.721,80.600,%
Batch: 340 | Loss: 4.657 | Acc: 37.445,60.663,80.583,%
Batch: 360 | Loss: 4.663 | Acc: 37.524,60.671,80.551,%
Batch: 380 | Loss: 4.664 | Acc: 37.598,60.665,80.604,%
Batch: 0 | Loss: 5.498 | Acc: 35.938,60.156,70.312,%
Batch: 20 | Loss: 5.672 | Acc: 34.338,55.506,66.295,%
Batch: 40 | Loss: 5.701 | Acc: 33.765,55.126,65.720,%
Batch: 60 | Loss: 5.725 | Acc: 33.940,54.944,65.318,%
Train classifier parameters

Epoch: 38
Batch: 0 | Loss: 4.732 | Acc: 33.594,61.719,75.781,%
Batch: 20 | Loss: 4.679 | Acc: 38.690,61.905,80.692,%
Batch: 40 | Loss: 4.724 | Acc: 37.805,60.709,80.831,%
Batch: 60 | Loss: 4.667 | Acc: 38.320,60.643,81.019,%
Batch: 80 | Loss: 4.691 | Acc: 38.127,60.600,81.038,%
Batch: 100 | Loss: 4.664 | Acc: 38.026,60.938,81.188,%
Batch: 120 | Loss: 4.653 | Acc: 38.042,61.189,81.114,%
Batch: 140 | Loss: 4.627 | Acc: 38.121,61.248,81.195,%
Batch: 160 | Loss: 4.629 | Acc: 38.107,61.059,81.158,%
Batch: 180 | Loss: 4.631 | Acc: 38.087,61.119,81.211,%
Batch: 200 | Loss: 4.627 | Acc: 38.021,61.202,81.258,%
Batch: 220 | Loss: 4.616 | Acc: 38.133,61.146,81.169,%
Batch: 240 | Loss: 4.617 | Acc: 38.152,61.093,81.266,%
Batch: 260 | Loss: 4.615 | Acc: 38.156,61.096,81.244,%
Batch: 280 | Loss: 4.617 | Acc: 38.123,61.071,81.172,%
Batch: 300 | Loss: 4.624 | Acc: 38.042,61.067,81.074,%
Batch: 320 | Loss: 4.619 | Acc: 38.040,61.030,81.123,%
Batch: 340 | Loss: 4.621 | Acc: 37.967,61.102,81.140,%
Batch: 360 | Loss: 4.620 | Acc: 37.961,61.152,81.142,%
Batch: 380 | Loss: 4.618 | Acc: 37.959,61.204,81.070,%
Batch: 0 | Loss: 5.635 | Acc: 37.500,56.250,67.969,%
Batch: 20 | Loss: 5.669 | Acc: 34.710,55.990,66.518,%
Batch: 40 | Loss: 5.686 | Acc: 33.937,55.278,66.101,%
Batch: 60 | Loss: 5.704 | Acc: 34.068,55.289,65.868,%
Train classifier parameters

Epoch: 39
Batch: 0 | Loss: 3.529 | Acc: 48.438,74.219,81.250,%
Batch: 20 | Loss: 4.419 | Acc: 40.513,62.649,81.957,%
Batch: 40 | Loss: 4.470 | Acc: 39.520,62.367,82.069,%
Batch: 60 | Loss: 4.503 | Acc: 38.845,62.052,82.031,%
Batch: 80 | Loss: 4.520 | Acc: 38.339,61.825,81.935,%
Batch: 100 | Loss: 4.522 | Acc: 38.475,61.765,81.915,%
Batch: 120 | Loss: 4.509 | Acc: 38.617,62.009,82.173,%
Batch: 140 | Loss: 4.521 | Acc: 38.520,61.935,82.181,%
Batch: 160 | Loss: 4.544 | Acc: 38.388,61.748,82.119,%
Batch: 180 | Loss: 4.561 | Acc: 38.139,61.840,82.131,%
Batch: 200 | Loss: 4.572 | Acc: 38.075,61.715,82.012,%
Batch: 220 | Loss: 4.576 | Acc: 37.942,61.620,81.908,%
Batch: 240 | Loss: 4.575 | Acc: 37.973,61.537,81.950,%
Batch: 260 | Loss: 4.580 | Acc: 37.910,61.378,81.873,%
Batch: 280 | Loss: 4.580 | Acc: 37.956,61.241,81.809,%
Batch: 300 | Loss: 4.587 | Acc: 37.889,61.119,81.689,%
Batch: 320 | Loss: 4.584 | Acc: 37.919,61.140,81.688,%
Batch: 340 | Loss: 4.587 | Acc: 37.915,61.160,81.694,%
Batch: 360 | Loss: 4.591 | Acc: 37.974,61.178,81.622,%
Batch: 380 | Loss: 4.599 | Acc: 37.933,61.132,81.582,%
Batch: 0 | Loss: 5.546 | Acc: 35.938,58.594,70.312,%
Batch: 20 | Loss: 5.631 | Acc: 34.635,55.915,65.513,%
Batch: 40 | Loss: 5.649 | Acc: 34.165,55.640,65.168,%
Batch: 60 | Loss: 5.658 | Acc: 34.362,55.699,65.151,%
Train classifier parameters

Epoch: 40
Batch: 0 | Loss: 4.103 | Acc: 46.875,58.594,81.250,%
Batch: 20 | Loss: 4.593 | Acc: 37.946,60.714,81.138,%
Batch: 40 | Loss: 4.527 | Acc: 38.586,61.833,81.803,%
Batch: 60 | Loss: 4.511 | Acc: 39.536,61.808,81.698,%
Batch: 80 | Loss: 4.514 | Acc: 38.976,62.037,81.838,%
Batch: 100 | Loss: 4.525 | Acc: 38.916,61.997,82.016,%
Batch: 120 | Loss: 4.518 | Acc: 38.914,61.822,81.760,%
Batch: 140 | Loss: 4.516 | Acc: 38.802,61.619,81.538,%
Batch: 160 | Loss: 4.535 | Acc: 38.626,61.588,81.599,%
Batch: 180 | Loss: 4.536 | Acc: 38.631,61.723,81.759,%
Batch: 200 | Loss: 4.547 | Acc: 38.573,61.754,81.755,%
Batch: 220 | Loss: 4.547 | Acc: 38.681,61.687,81.635,%
Batch: 240 | Loss: 4.546 | Acc: 38.742,61.647,81.626,%
Batch: 260 | Loss: 4.554 | Acc: 38.718,61.533,81.588,%
Batch: 280 | Loss: 4.556 | Acc: 38.657,61.530,81.648,%
Batch: 300 | Loss: 4.546 | Acc: 38.691,61.540,81.650,%
Batch: 320 | Loss: 4.549 | Acc: 38.649,61.541,81.686,%
Batch: 340 | Loss: 4.546 | Acc: 38.710,61.623,81.729,%
Batch: 360 | Loss: 4.543 | Acc: 38.723,61.691,81.668,%
Batch: 380 | Loss: 4.548 | Acc: 38.638,61.639,81.617,%
Batch: 0 | Loss: 5.428 | Acc: 37.500,61.719,70.312,%
Batch: 20 | Loss: 5.626 | Acc: 34.821,57.068,66.592,%
Batch: 40 | Loss: 5.645 | Acc: 34.432,56.002,65.816,%
Batch: 60 | Loss: 5.645 | Acc: 34.413,55.853,65.548,%
Train classifier parameters

Epoch: 41
Batch: 0 | Loss: 3.778 | Acc: 53.125,66.406,89.062,%
Batch: 20 | Loss: 4.541 | Acc: 38.058,62.016,81.250,%
Batch: 40 | Loss: 4.540 | Acc: 38.434,61.300,82.069,%
Batch: 60 | Loss: 4.575 | Acc: 37.795,61.463,82.031,%
Batch: 80 | Loss: 4.592 | Acc: 37.519,61.497,81.935,%
Batch: 100 | Loss: 4.565 | Acc: 37.624,61.572,82.101,%
Batch: 120 | Loss: 4.552 | Acc: 37.933,61.777,82.212,%
Batch: 140 | Loss: 4.551 | Acc: 37.871,61.979,82.258,%
Batch: 160 | Loss: 4.528 | Acc: 38.068,62.058,82.332,%
Batch: 180 | Loss: 4.530 | Acc: 38.208,62.137,82.385,%
Batch: 200 | Loss: 4.529 | Acc: 38.219,62.220,82.424,%
Batch: 220 | Loss: 4.528 | Acc: 38.207,62.221,82.307,%
Batch: 240 | Loss: 4.527 | Acc: 38.242,62.156,82.291,%
Batch: 260 | Loss: 4.542 | Acc: 38.162,61.976,82.130,%
Batch: 280 | Loss: 4.549 | Acc: 38.187,61.902,82.115,%
Batch: 300 | Loss: 4.541 | Acc: 38.216,61.989,82.094,%
Batch: 320 | Loss: 4.541 | Acc: 38.315,61.887,82.077,%
Batch: 340 | Loss: 4.534 | Acc: 38.508,61.895,82.050,%
Batch: 360 | Loss: 4.528 | Acc: 38.565,61.894,81.997,%
Batch: 380 | Loss: 4.535 | Acc: 38.597,61.922,81.966,%
Batch: 0 | Loss: 5.497 | Acc: 38.281,62.500,68.750,%
Batch: 20 | Loss: 5.613 | Acc: 35.082,57.292,66.295,%
Batch: 40 | Loss: 5.626 | Acc: 34.546,56.612,65.777,%
Batch: 60 | Loss: 5.641 | Acc: 34.529,56.173,65.433,%
Train classifier parameters

Epoch: 42
Batch: 0 | Loss: 4.794 | Acc: 36.719,61.719,82.812,%
Batch: 20 | Loss: 4.498 | Acc: 39.249,61.124,82.106,%
Batch: 40 | Loss: 4.443 | Acc: 39.177,62.443,82.946,%
Batch: 60 | Loss: 4.459 | Acc: 38.960,62.500,82.941,%
Batch: 80 | Loss: 4.466 | Acc: 38.927,62.404,82.687,%
Batch: 100 | Loss: 4.440 | Acc: 39.062,62.794,82.766,%
Batch: 120 | Loss: 4.462 | Acc: 38.869,62.597,82.393,%
Batch: 140 | Loss: 4.486 | Acc: 38.702,62.411,82.342,%
Batch: 160 | Loss: 4.484 | Acc: 38.883,62.369,82.196,%
Batch: 180 | Loss: 4.479 | Acc: 39.015,62.418,82.213,%
Batch: 200 | Loss: 4.490 | Acc: 38.930,62.380,82.194,%
Batch: 220 | Loss: 4.490 | Acc: 38.956,62.313,82.282,%
Batch: 240 | Loss: 4.487 | Acc: 39.040,62.202,82.381,%
Batch: 260 | Loss: 4.488 | Acc: 38.985,62.102,82.334,%
Batch: 280 | Loss: 4.485 | Acc: 39.071,62.000,82.326,%
Batch: 300 | Loss: 4.499 | Acc: 38.878,61.911,82.273,%
Batch: 320 | Loss: 4.495 | Acc: 38.878,61.918,82.258,%
Batch: 340 | Loss: 4.502 | Acc: 38.838,61.822,82.123,%
Batch: 360 | Loss: 4.508 | Acc: 38.783,61.751,82.079,%
Batch: 380 | Loss: 4.502 | Acc: 38.915,61.846,82.093,%
Batch: 0 | Loss: 5.523 | Acc: 38.281,60.156,70.312,%
Batch: 20 | Loss: 5.599 | Acc: 34.859,57.106,65.885,%
Batch: 40 | Loss: 5.608 | Acc: 34.775,56.479,65.758,%
Batch: 60 | Loss: 5.628 | Acc: 34.657,55.904,65.164,%
Train classifier parameters

Epoch: 43
Batch: 0 | Loss: 4.673 | Acc: 31.250,61.719,89.844,%
Batch: 20 | Loss: 4.402 | Acc: 40.290,62.388,83.668,%
Batch: 40 | Loss: 4.487 | Acc: 39.539,62.710,83.327,%
Batch: 60 | Loss: 4.417 | Acc: 39.331,63.076,83.581,%
Batch: 80 | Loss: 4.420 | Acc: 39.130,63.059,83.661,%
Batch: 100 | Loss: 4.411 | Acc: 39.372,63.204,83.462,%
Batch: 120 | Loss: 4.431 | Acc: 39.385,63.075,83.219,%
Batch: 140 | Loss: 4.442 | Acc: 39.395,62.766,82.979,%
Batch: 160 | Loss: 4.452 | Acc: 39.247,62.612,82.851,%
Batch: 180 | Loss: 4.461 | Acc: 39.101,62.422,82.748,%
Batch: 200 | Loss: 4.452 | Acc: 39.179,62.438,82.824,%
Batch: 220 | Loss: 4.464 | Acc: 39.017,62.327,82.752,%
Batch: 240 | Loss: 4.462 | Acc: 39.121,62.322,82.803,%
Batch: 260 | Loss: 4.471 | Acc: 39.086,62.320,82.759,%
Batch: 280 | Loss: 4.477 | Acc: 39.043,62.255,82.693,%
Batch: 300 | Loss: 4.474 | Acc: 39.070,62.077,82.714,%
Batch: 320 | Loss: 4.476 | Acc: 39.026,62.111,82.703,%
Batch: 340 | Loss: 4.473 | Acc: 39.122,62.111,82.673,%
Batch: 360 | Loss: 4.464 | Acc: 39.177,62.158,82.680,%
Batch: 380 | Loss: 4.473 | Acc: 39.140,62.047,82.632,%
Batch: 0 | Loss: 5.490 | Acc: 38.281,61.719,68.750,%
Batch: 20 | Loss: 5.566 | Acc: 35.193,57.068,66.815,%
Batch: 40 | Loss: 5.588 | Acc: 34.680,55.812,65.854,%
Batch: 60 | Loss: 5.597 | Acc: 34.670,55.751,65.894,%
Train classifier parameters

Epoch: 44
Batch: 0 | Loss: 3.622 | Acc: 50.781,66.406,83.594,%
Batch: 20 | Loss: 4.444 | Acc: 40.327,62.760,83.073,%
Batch: 40 | Loss: 4.419 | Acc: 39.787,62.881,83.594,%
Batch: 60 | Loss: 4.432 | Acc: 39.818,62.769,83.056,%
Batch: 80 | Loss: 4.431 | Acc: 39.554,62.760,82.890,%
Batch: 100 | Loss: 4.443 | Acc: 39.604,62.438,82.812,%
Batch: 120 | Loss: 4.459 | Acc: 39.534,62.293,82.890,%
Batch: 140 | Loss: 4.473 | Acc: 39.417,62.057,82.619,%
Batch: 160 | Loss: 4.468 | Acc: 39.669,62.155,82.594,%
Batch: 180 | Loss: 4.473 | Acc: 39.602,62.198,82.610,%
Batch: 200 | Loss: 4.466 | Acc: 39.611,62.302,82.673,%
Batch: 220 | Loss: 4.476 | Acc: 39.557,62.295,82.604,%
Batch: 240 | Loss: 4.478 | Acc: 39.435,62.293,82.621,%
Batch: 260 | Loss: 4.482 | Acc: 39.299,62.278,82.630,%
Batch: 280 | Loss: 4.480 | Acc: 39.271,62.308,82.621,%
Batch: 300 | Loss: 4.492 | Acc: 39.122,62.285,82.485,%
Batch: 320 | Loss: 4.487 | Acc: 39.174,62.310,82.477,%
Batch: 340 | Loss: 4.482 | Acc: 39.099,62.374,82.492,%
Batch: 360 | Loss: 4.485 | Acc: 39.101,62.314,82.451,%
Batch: 380 | Loss: 4.492 | Acc: 39.030,62.326,82.435,%
Batch: 0 | Loss: 5.415 | Acc: 37.500,59.375,70.312,%
Batch: 20 | Loss: 5.578 | Acc: 35.379,57.143,66.220,%
Batch: 40 | Loss: 5.591 | Acc: 34.813,55.926,65.568,%
Batch: 60 | Loss: 5.592 | Acc: 35.003,55.840,65.587,%
Train classifier parameters

Epoch: 45
Batch: 0 | Loss: 3.888 | Acc: 44.531,66.406,85.938,%
Batch: 20 | Loss: 4.290 | Acc: 40.439,64.583,83.185,%
Batch: 40 | Loss: 4.352 | Acc: 39.367,63.091,83.289,%
Batch: 60 | Loss: 4.381 | Acc: 39.421,62.910,83.325,%
Batch: 80 | Loss: 4.405 | Acc: 39.226,63.059,83.208,%
Batch: 100 | Loss: 4.404 | Acc: 38.985,63.026,83.246,%
Batch: 120 | Loss: 4.399 | Acc: 39.198,63.126,83.135,%
Batch: 140 | Loss: 4.424 | Acc: 38.918,62.910,83.073,%
Batch: 160 | Loss: 4.436 | Acc: 39.077,62.874,82.900,%
Batch: 180 | Loss: 4.448 | Acc: 38.933,62.711,82.834,%
Batch: 200 | Loss: 4.450 | Acc: 38.775,62.644,82.793,%
Batch: 220 | Loss: 4.449 | Acc: 38.815,62.443,82.781,%
Batch: 240 | Loss: 4.455 | Acc: 38.790,62.422,82.793,%
Batch: 260 | Loss: 4.453 | Acc: 38.889,62.467,82.759,%
Batch: 280 | Loss: 4.461 | Acc: 38.809,62.464,82.749,%
Batch: 300 | Loss: 4.457 | Acc: 38.738,62.484,82.771,%
Batch: 320 | Loss: 4.456 | Acc: 38.846,62.519,82.893,%
Batch: 340 | Loss: 4.461 | Acc: 38.840,62.372,82.863,%
Batch: 360 | Loss: 4.467 | Acc: 38.866,62.377,82.812,%
Batch: 380 | Loss: 4.464 | Acc: 38.997,62.389,82.763,%
Batch: 0 | Loss: 5.458 | Acc: 35.938,60.156,71.094,%
Batch: 20 | Loss: 5.567 | Acc: 35.305,56.176,66.518,%
Batch: 40 | Loss: 5.573 | Acc: 35.023,55.545,66.063,%
Batch: 60 | Loss: 5.581 | Acc: 35.015,55.482,65.817,%
Train classifier parameters

Epoch: 46
Batch: 0 | Loss: 4.342 | Acc: 35.938,71.875,82.031,%
Batch: 20 | Loss: 4.354 | Acc: 38.914,64.695,83.259,%
Batch: 40 | Loss: 4.355 | Acc: 39.386,63.700,83.670,%
Batch: 60 | Loss: 4.358 | Acc: 39.857,63.614,83.325,%
Batch: 80 | Loss: 4.373 | Acc: 39.323,63.484,83.295,%
Batch: 100 | Loss: 4.363 | Acc: 39.650,63.598,83.199,%
Batch: 120 | Loss: 4.376 | Acc: 39.366,63.385,83.245,%
Batch: 140 | Loss: 4.386 | Acc: 39.256,63.298,83.206,%
Batch: 160 | Loss: 4.406 | Acc: 39.155,63.005,83.109,%
Batch: 180 | Loss: 4.397 | Acc: 39.317,63.173,82.981,%
Batch: 200 | Loss: 4.392 | Acc: 39.389,63.250,83.034,%
Batch: 220 | Loss: 4.413 | Acc: 39.278,62.942,83.018,%
Batch: 240 | Loss: 4.436 | Acc: 38.998,62.672,83.010,%
Batch: 260 | Loss: 4.439 | Acc: 39.012,62.518,82.872,%
Batch: 280 | Loss: 4.423 | Acc: 39.160,62.519,82.826,%
Batch: 300 | Loss: 4.415 | Acc: 39.345,62.622,82.802,%
Batch: 320 | Loss: 4.423 | Acc: 39.386,62.551,82.725,%
Batch: 340 | Loss: 4.419 | Acc: 39.273,62.576,82.808,%
Batch: 360 | Loss: 4.424 | Acc: 39.166,62.574,82.704,%
Batch: 380 | Loss: 4.431 | Acc: 39.183,62.471,82.724,%
Batch: 0 | Loss: 5.452 | Acc: 33.594,64.844,71.094,%
Batch: 20 | Loss: 5.574 | Acc: 34.821,57.403,66.071,%
Batch: 40 | Loss: 5.581 | Acc: 35.004,56.612,65.796,%
Batch: 60 | Loss: 5.591 | Acc: 35.067,56.481,65.523,%
Train classifier parameters

Epoch: 47
Batch: 0 | Loss: 4.703 | Acc: 34.375,60.938,78.125,%
Batch: 20 | Loss: 4.277 | Acc: 40.588,64.249,84.301,%
Batch: 40 | Loss: 4.344 | Acc: 39.672,63.681,84.013,%
Batch: 60 | Loss: 4.414 | Acc: 39.216,63.179,83.350,%
Batch: 80 | Loss: 4.431 | Acc: 38.956,63.088,83.266,%
Batch: 100 | Loss: 4.437 | Acc: 38.885,63.041,83.060,%
Batch: 120 | Loss: 4.425 | Acc: 38.991,62.920,83.181,%
Batch: 140 | Loss: 4.418 | Acc: 39.301,62.832,83.078,%
Batch: 160 | Loss: 4.437 | Acc: 39.344,62.689,82.968,%
Batch: 180 | Loss: 4.439 | Acc: 39.214,62.591,82.895,%
Batch: 200 | Loss: 4.436 | Acc: 39.249,62.578,82.816,%
Batch: 220 | Loss: 4.443 | Acc: 39.179,62.528,82.763,%
Batch: 240 | Loss: 4.438 | Acc: 39.257,62.568,82.832,%
Batch: 260 | Loss: 4.440 | Acc: 39.257,62.545,82.839,%
Batch: 280 | Loss: 4.443 | Acc: 39.124,62.478,82.871,%
Batch: 300 | Loss: 4.445 | Acc: 39.070,62.526,82.763,%
Batch: 320 | Loss: 4.447 | Acc: 39.106,62.571,82.771,%
Batch: 340 | Loss: 4.457 | Acc: 39.040,62.498,82.760,%
Batch: 360 | Loss: 4.465 | Acc: 38.987,62.481,82.702,%
Batch: 380 | Loss: 4.463 | Acc: 39.015,62.518,82.704,%
Batch: 0 | Loss: 5.472 | Acc: 39.062,60.156,67.969,%
Batch: 20 | Loss: 5.564 | Acc: 35.305,56.845,66.704,%
Batch: 40 | Loss: 5.567 | Acc: 35.385,56.174,66.254,%
Batch: 60 | Loss: 5.581 | Acc: 35.182,56.160,65.817,%
Train classifier parameters

Epoch: 48
Batch: 0 | Loss: 3.543 | Acc: 46.094,71.875,82.812,%
Batch: 20 | Loss: 4.208 | Acc: 41.629,63.244,82.515,%
Batch: 40 | Loss: 4.272 | Acc: 41.330,63.110,83.117,%
Batch: 60 | Loss: 4.317 | Acc: 40.433,62.897,83.543,%
Batch: 80 | Loss: 4.314 | Acc: 40.123,63.098,83.362,%
Batch: 100 | Loss: 4.312 | Acc: 40.138,63.235,83.532,%
Batch: 120 | Loss: 4.293 | Acc: 40.367,63.243,83.452,%
Batch: 140 | Loss: 4.309 | Acc: 40.126,63.209,83.516,%
Batch: 160 | Loss: 4.326 | Acc: 40.086,62.966,83.361,%
Batch: 180 | Loss: 4.348 | Acc: 39.749,62.880,83.348,%
Batch: 200 | Loss: 4.364 | Acc: 39.556,62.749,83.088,%
Batch: 220 | Loss: 4.373 | Acc: 39.582,62.747,83.010,%
Batch: 240 | Loss: 4.371 | Acc: 39.507,62.827,83.104,%
Batch: 260 | Loss: 4.391 | Acc: 39.308,62.674,83.103,%
Batch: 280 | Loss: 4.388 | Acc: 39.377,62.720,83.113,%
Batch: 300 | Loss: 4.389 | Acc: 39.465,62.617,83.072,%
Batch: 320 | Loss: 4.390 | Acc: 39.549,62.651,83.078,%
Batch: 340 | Loss: 4.395 | Acc: 39.507,62.720,82.955,%
Batch: 360 | Loss: 4.407 | Acc: 39.404,62.732,82.886,%
Batch: 380 | Loss: 4.409 | Acc: 39.469,62.744,82.806,%
Batch: 0 | Loss: 5.368 | Acc: 39.062,61.719,71.094,%
Batch: 20 | Loss: 5.552 | Acc: 35.193,56.548,66.109,%
Batch: 40 | Loss: 5.560 | Acc: 35.213,56.479,65.816,%
Batch: 60 | Loss: 5.563 | Acc: 35.143,56.429,65.753,%
Train classifier parameters

Epoch: 49
Batch: 0 | Loss: 4.114 | Acc: 42.969,57.031,81.250,%
Batch: 20 | Loss: 4.514 | Acc: 37.277,61.049,83.557,%
Batch: 40 | Loss: 4.502 | Acc: 37.481,61.357,83.327,%
Batch: 60 | Loss: 4.461 | Acc: 38.409,62.193,83.107,%
Batch: 80 | Loss: 4.431 | Acc: 38.976,62.760,83.353,%
Batch: 100 | Loss: 4.412 | Acc: 39.194,62.763,83.439,%
Batch: 120 | Loss: 4.422 | Acc: 39.159,62.874,83.368,%
Batch: 140 | Loss: 4.441 | Acc: 38.990,62.644,83.344,%
Batch: 160 | Loss: 4.446 | Acc: 39.062,62.777,83.206,%
Batch: 180 | Loss: 4.440 | Acc: 39.235,62.841,83.253,%
Batch: 200 | Loss: 4.426 | Acc: 39.370,63.075,83.155,%
Batch: 220 | Loss: 4.421 | Acc: 39.377,62.984,83.092,%
Batch: 240 | Loss: 4.422 | Acc: 39.299,62.899,83.137,%
Batch: 260 | Loss: 4.418 | Acc: 39.344,62.865,83.088,%
Batch: 280 | Loss: 4.427 | Acc: 39.327,62.789,83.096,%
Batch: 300 | Loss: 4.420 | Acc: 39.431,62.850,83.127,%
Batch: 320 | Loss: 4.426 | Acc: 39.364,62.724,83.061,%
Batch: 340 | Loss: 4.421 | Acc: 39.477,62.747,82.991,%
Batch: 360 | Loss: 4.418 | Acc: 39.454,62.736,83.072,%
Batch: 380 | Loss: 4.417 | Acc: 39.512,62.719,83.054,%
Batch: 0 | Loss: 5.393 | Acc: 37.500,62.500,68.750,%
Batch: 20 | Loss: 5.535 | Acc: 36.012,57.552,66.481,%
Batch: 40 | Loss: 5.542 | Acc: 35.442,56.898,66.082,%
Batch: 60 | Loss: 5.551 | Acc: 35.195,56.839,65.971,%
Train all parameters

Epoch: 50
Batch: 0 | Loss: 4.184 | Acc: 36.719,64.844,89.844,%
Batch: 20 | Loss: 5.258 | Acc: 35.156,56.287,72.470,%
Batch: 40 | Loss: 5.922 | Acc: 33.175,52.515,62.309,%
Batch: 60 | Loss: 6.133 | Acc: 32.787,51.217,58.133,%
Batch: 80 | Loss: 6.198 | Acc: 32.832,50.839,56.674,%
Batch: 100 | Loss: 6.302 | Acc: 32.426,49.768,55.654,%
Batch: 120 | Loss: 6.273 | Acc: 32.619,49.955,55.882,%
Batch: 140 | Loss: 6.218 | Acc: 32.885,50.316,56.521,%
Batch: 160 | Loss: 6.189 | Acc: 32.900,50.417,56.755,%
Batch: 180 | Loss: 6.152 | Acc: 33.106,50.514,57.057,%
Batch: 200 | Loss: 6.136 | Acc: 33.046,50.498,57.257,%
Batch: 220 | Loss: 6.103 | Acc: 33.124,50.728,57.554,%
Batch: 240 | Loss: 6.073 | Acc: 33.373,50.898,57.838,%
Batch: 260 | Loss: 6.038 | Acc: 33.588,51.164,58.148,%
Batch: 280 | Loss: 6.009 | Acc: 33.716,51.332,58.458,%
Batch: 300 | Loss: 5.984 | Acc: 33.814,51.511,58.739,%
Batch: 320 | Loss: 5.971 | Acc: 33.842,51.621,58.934,%
Batch: 340 | Loss: 5.958 | Acc: 33.935,51.727,59.125,%
Batch: 360 | Loss: 5.930 | Acc: 34.122,51.987,59.440,%
Batch: 380 | Loss: 5.911 | Acc: 34.242,52.137,59.664,%
Batch: 0 | Loss: 6.270 | Acc: 33.594,50.781,61.719,%
Batch: 20 | Loss: 6.571 | Acc: 27.679,47.842,56.250,%
Batch: 40 | Loss: 6.531 | Acc: 27.877,48.171,55.907,%
Batch: 60 | Loss: 6.591 | Acc: 27.792,47.106,55.174,%
Train all parameters

Epoch: 51
Batch: 0 | Loss: 4.975 | Acc: 37.500,52.344,64.844,%
Batch: 20 | Loss: 5.174 | Acc: 36.384,57.701,69.196,%
Batch: 40 | Loss: 5.209 | Acc: 36.319,57.889,69.264,%
Batch: 60 | Loss: 5.200 | Acc: 36.527,57.672,69.506,%
Batch: 80 | Loss: 5.212 | Acc: 36.516,57.542,69.348,%
Batch: 100 | Loss: 5.198 | Acc: 36.688,57.704,69.191,%
Batch: 120 | Loss: 5.201 | Acc: 36.667,57.522,68.982,%
Batch: 140 | Loss: 5.218 | Acc: 36.686,57.369,69.055,%
Batch: 160 | Loss: 5.207 | Acc: 36.835,57.483,69.196,%
Batch: 180 | Loss: 5.191 | Acc: 36.909,57.528,69.302,%
Batch: 200 | Loss: 5.214 | Acc: 36.649,57.369,69.158,%
Batch: 220 | Loss: 5.204 | Acc: 36.789,57.360,69.171,%
Batch: 240 | Loss: 5.201 | Acc: 36.835,57.329,69.158,%
Batch: 260 | Loss: 5.210 | Acc: 36.764,57.322,69.079,%
Batch: 280 | Loss: 5.206 | Acc: 36.760,57.407,69.078,%
Batch: 300 | Loss: 5.203 | Acc: 36.784,57.421,69.025,%
Batch: 320 | Loss: 5.202 | Acc: 36.799,57.452,69.086,%
Batch: 340 | Loss: 5.202 | Acc: 36.836,57.515,69.153,%
Batch: 360 | Loss: 5.198 | Acc: 36.807,57.572,69.194,%
Batch: 380 | Loss: 5.198 | Acc: 36.844,57.491,69.156,%
Batch: 0 | Loss: 5.785 | Acc: 40.625,57.812,63.281,%
Batch: 20 | Loss: 6.031 | Acc: 31.473,52.493,61.682,%
Batch: 40 | Loss: 6.023 | Acc: 31.707,52.706,61.319,%
Batch: 60 | Loss: 6.031 | Acc: 31.468,52.088,61.014,%
Train all parameters

Epoch: 52
Batch: 0 | Loss: 4.725 | Acc: 38.281,65.625,78.125,%
Batch: 20 | Loss: 4.886 | Acc: 37.612,61.198,73.177,%
Batch: 40 | Loss: 4.893 | Acc: 37.348,60.499,73.971,%
Batch: 60 | Loss: 4.851 | Acc: 37.846,60.694,74.052,%
Batch: 80 | Loss: 4.820 | Acc: 38.156,60.658,74.151,%
Batch: 100 | Loss: 4.850 | Acc: 38.011,60.350,73.994,%
Batch: 120 | Loss: 4.830 | Acc: 38.133,60.479,74.128,%
Batch: 140 | Loss: 4.853 | Acc: 38.154,60.356,73.814,%
Batch: 160 | Loss: 4.868 | Acc: 38.199,60.205,73.680,%
Batch: 180 | Loss: 4.868 | Acc: 38.217,60.217,73.662,%
Batch: 200 | Loss: 4.878 | Acc: 38.192,60.075,73.523,%
Batch: 220 | Loss: 4.881 | Acc: 38.189,59.990,73.349,%
Batch: 240 | Loss: 4.883 | Acc: 38.148,60.004,73.224,%
Batch: 260 | Loss: 4.899 | Acc: 38.063,59.938,73.048,%
Batch: 280 | Loss: 4.898 | Acc: 38.078,59.864,72.945,%
Batch: 300 | Loss: 4.902 | Acc: 38.151,59.762,72.913,%
Batch: 320 | Loss: 4.897 | Acc: 38.206,59.757,72.939,%
Batch: 340 | Loss: 4.896 | Acc: 38.233,59.760,72.885,%
Batch: 360 | Loss: 4.909 | Acc: 38.117,59.669,72.799,%
Batch: 380 | Loss: 4.918 | Acc: 38.070,59.674,72.749,%
Batch: 0 | Loss: 6.116 | Acc: 29.688,52.344,60.156,%
Batch: 20 | Loss: 6.250 | Acc: 29.167,50.260,60.528,%
Batch: 40 | Loss: 6.245 | Acc: 29.192,49.657,60.690,%
Batch: 60 | Loss: 6.277 | Acc: 28.881,49.834,60.105,%
Train all parameters

Epoch: 53
Batch: 0 | Loss: 4.093 | Acc: 49.219,67.969,79.688,%
Batch: 20 | Loss: 4.569 | Acc: 40.290,63.690,76.860,%
Batch: 40 | Loss: 4.646 | Acc: 39.482,61.947,76.848,%
Batch: 60 | Loss: 4.677 | Acc: 39.216,61.475,76.883,%
Batch: 80 | Loss: 4.682 | Acc: 39.323,61.497,76.804,%
Batch: 100 | Loss: 4.677 | Acc: 39.248,61.440,76.818,%
Batch: 120 | Loss: 4.696 | Acc: 39.159,61.170,76.537,%
Batch: 140 | Loss: 4.725 | Acc: 38.797,60.782,76.169,%
Batch: 160 | Loss: 4.708 | Acc: 38.927,60.840,76.310,%
Batch: 180 | Loss: 4.698 | Acc: 39.011,61.002,76.234,%
Batch: 200 | Loss: 4.704 | Acc: 38.989,60.918,76.011,%
Batch: 220 | Loss: 4.707 | Acc: 38.868,60.994,75.873,%
Batch: 240 | Loss: 4.722 | Acc: 38.923,60.886,75.752,%
Batch: 260 | Loss: 4.737 | Acc: 38.928,60.725,75.551,%
Batch: 280 | Loss: 4.749 | Acc: 38.818,60.712,75.417,%
Batch: 300 | Loss: 4.750 | Acc: 38.821,60.668,75.389,%
Batch: 320 | Loss: 4.752 | Acc: 38.843,60.631,75.316,%
Batch: 340 | Loss: 4.753 | Acc: 38.810,60.578,75.270,%
Batch: 360 | Loss: 4.758 | Acc: 38.760,60.576,75.245,%
Batch: 380 | Loss: 4.759 | Acc: 38.780,60.505,75.180,%
Batch: 0 | Loss: 5.633 | Acc: 32.031,54.688,64.844,%
Batch: 20 | Loss: 5.764 | Acc: 33.854,55.208,64.807,%
Batch: 40 | Loss: 5.738 | Acc: 33.479,55.221,63.967,%
Batch: 60 | Loss: 5.783 | Acc: 33.427,54.495,63.512,%
Train all parameters

Epoch: 54
Batch: 0 | Loss: 4.236 | Acc: 46.875,64.062,77.344,%
Batch: 20 | Loss: 4.469 | Acc: 40.848,62.760,78.199,%
Batch: 40 | Loss: 4.569 | Acc: 39.787,61.547,77.877,%
Batch: 60 | Loss: 4.544 | Acc: 39.716,61.527,78.291,%
Batch: 80 | Loss: 4.555 | Acc: 39.728,61.497,78.221,%
Batch: 100 | Loss: 4.536 | Acc: 39.875,61.494,78.195,%
Batch: 120 | Loss: 4.585 | Acc: 39.566,61.228,77.925,%
Batch: 140 | Loss: 4.571 | Acc: 39.678,61.192,77.953,%
Batch: 160 | Loss: 4.555 | Acc: 39.829,61.544,78.101,%
Batch: 180 | Loss: 4.553 | Acc: 39.921,61.408,78.034,%
Batch: 200 | Loss: 4.562 | Acc: 40.003,61.295,77.783,%
Batch: 220 | Loss: 4.592 | Acc: 39.777,61.210,77.566,%
Batch: 240 | Loss: 4.601 | Acc: 39.857,61.294,77.477,%
Batch: 260 | Loss: 4.610 | Acc: 39.859,61.207,77.311,%
Batch: 280 | Loss: 4.617 | Acc: 39.760,61.193,77.199,%
Batch: 300 | Loss: 4.626 | Acc: 39.717,61.158,77.100,%
Batch: 320 | Loss: 4.630 | Acc: 39.759,61.215,76.986,%
Batch: 340 | Loss: 4.639 | Acc: 39.635,61.226,76.902,%
Batch: 360 | Loss: 4.649 | Acc: 39.556,61.176,76.855,%
Batch: 380 | Loss: 4.642 | Acc: 39.620,61.313,76.936,%
Batch: 0 | Loss: 5.487 | Acc: 36.719,59.375,71.094,%
Batch: 20 | Loss: 5.752 | Acc: 34.375,54.204,64.658,%
Batch: 40 | Loss: 5.759 | Acc: 34.546,53.639,63.739,%
Batch: 60 | Loss: 5.757 | Acc: 34.388,53.432,63.755,%
Train all parameters

Epoch: 55
Batch: 0 | Loss: 3.796 | Acc: 46.094,75.000,86.719,%
Batch: 20 | Loss: 4.510 | Acc: 40.625,61.458,78.460,%
Batch: 40 | Loss: 4.431 | Acc: 40.396,62.976,79.345,%
Batch: 60 | Loss: 4.423 | Acc: 40.830,63.294,79.483,%
Batch: 80 | Loss: 4.462 | Acc: 40.480,63.030,79.485,%
Batch: 100 | Loss: 4.462 | Acc: 40.741,63.057,79.425,%
Batch: 120 | Loss: 4.471 | Acc: 40.418,62.926,79.352,%
Batch: 140 | Loss: 4.474 | Acc: 40.409,62.999,79.211,%
Batch: 160 | Loss: 4.495 | Acc: 40.295,62.612,79.003,%
Batch: 180 | Loss: 4.497 | Acc: 40.172,62.582,78.906,%
Batch: 200 | Loss: 4.493 | Acc: 40.260,62.535,78.844,%
Batch: 220 | Loss: 4.512 | Acc: 40.035,62.313,78.737,%
Batch: 240 | Loss: 4.521 | Acc: 40.006,62.237,78.634,%
Batch: 260 | Loss: 4.528 | Acc: 39.940,62.296,78.550,%
Batch: 280 | Loss: 4.530 | Acc: 39.944,62.214,78.386,%
Batch: 300 | Loss: 4.530 | Acc: 39.942,62.253,78.265,%
Batch: 320 | Loss: 4.535 | Acc: 39.897,62.266,78.266,%
Batch: 340 | Loss: 4.537 | Acc: 39.915,62.308,78.169,%
Batch: 360 | Loss: 4.546 | Acc: 39.889,62.245,78.058,%
Batch: 380 | Loss: 4.547 | Acc: 39.909,62.166,78.031,%
Batch: 0 | Loss: 5.447 | Acc: 33.594,62.500,64.844,%
Batch: 20 | Loss: 5.543 | Acc: 35.789,56.622,65.216,%
Batch: 40 | Loss: 5.569 | Acc: 35.347,56.174,64.977,%
Batch: 60 | Loss: 5.604 | Acc: 35.387,55.879,64.293,%
Train all parameters

Epoch: 56
Batch: 0 | Loss: 4.080 | Acc: 39.844,60.938,83.594,%
Batch: 20 | Loss: 4.411 | Acc: 39.583,63.467,81.324,%
Batch: 40 | Loss: 4.403 | Acc: 40.549,63.091,81.479,%
Batch: 60 | Loss: 4.406 | Acc: 40.689,62.782,81.186,%
Batch: 80 | Loss: 4.400 | Acc: 40.953,62.905,80.999,%
Batch: 100 | Loss: 4.414 | Acc: 40.625,62.531,80.817,%
Batch: 120 | Loss: 4.434 | Acc: 40.438,62.403,80.430,%
Batch: 140 | Loss: 4.424 | Acc: 40.470,62.395,80.397,%
Batch: 160 | Loss: 4.433 | Acc: 40.523,62.354,80.352,%
Batch: 180 | Loss: 4.437 | Acc: 40.629,62.362,80.193,%
Batch: 200 | Loss: 4.430 | Acc: 40.555,62.481,80.076,%
Batch: 220 | Loss: 4.438 | Acc: 40.657,62.472,79.935,%
Batch: 240 | Loss: 4.450 | Acc: 40.619,62.396,79.756,%
Batch: 260 | Loss: 4.454 | Acc: 40.511,62.518,79.637,%
Batch: 280 | Loss: 4.465 | Acc: 40.391,62.364,79.496,%
Batch: 300 | Loss: 4.478 | Acc: 40.212,62.274,79.366,%
Batch: 320 | Loss: 4.479 | Acc: 40.272,62.405,79.347,%
Batch: 340 | Loss: 4.486 | Acc: 40.178,62.317,79.177,%
Batch: 360 | Loss: 4.494 | Acc: 40.056,62.292,79.069,%
Batch: 380 | Loss: 4.502 | Acc: 40.004,62.260,78.970,%
Batch: 0 | Loss: 6.081 | Acc: 32.031,57.812,65.625,%
Batch: 20 | Loss: 6.102 | Acc: 29.055,51.265,61.682,%
Batch: 40 | Loss: 6.082 | Acc: 29.116,50.724,61.966,%
Batch: 60 | Loss: 6.092 | Acc: 29.073,50.692,61.719,%
Train all parameters

Epoch: 57
Batch: 0 | Loss: 4.372 | Acc: 36.719,66.406,82.812,%
Batch: 20 | Loss: 4.326 | Acc: 40.774,64.918,83.631,%
Batch: 40 | Loss: 4.273 | Acc: 41.349,64.539,83.975,%
Batch: 60 | Loss: 4.272 | Acc: 41.534,64.575,83.607,%
Batch: 80 | Loss: 4.295 | Acc: 41.426,63.947,83.063,%
Batch: 100 | Loss: 4.291 | Acc: 41.337,63.800,82.789,%
Batch: 120 | Loss: 4.292 | Acc: 41.187,63.656,82.638,%
Batch: 140 | Loss: 4.316 | Acc: 40.841,63.531,82.308,%
Batch: 160 | Loss: 4.336 | Acc: 40.790,63.339,81.818,%
Batch: 180 | Loss: 4.346 | Acc: 40.919,63.320,81.474,%
Batch: 200 | Loss: 4.359 | Acc: 40.800,63.246,81.137,%
Batch: 220 | Loss: 4.366 | Acc: 40.756,63.189,80.971,%
Batch: 240 | Loss: 4.380 | Acc: 40.667,63.158,80.790,%
Batch: 260 | Loss: 4.387 | Acc: 40.619,63.114,80.744,%
Batch: 280 | Loss: 4.393 | Acc: 40.494,63.151,80.641,%
Batch: 300 | Loss: 4.398 | Acc: 40.441,63.237,80.552,%
Batch: 320 | Loss: 4.408 | Acc: 40.399,63.116,80.398,%
Batch: 340 | Loss: 4.401 | Acc: 40.510,63.215,80.281,%
Batch: 360 | Loss: 4.398 | Acc: 40.547,63.197,80.211,%
Batch: 380 | Loss: 4.397 | Acc: 40.600,63.156,80.135,%
Batch: 0 | Loss: 5.296 | Acc: 39.062,64.062,69.531,%
Batch: 20 | Loss: 5.522 | Acc: 34.970,56.882,65.997,%
Batch: 40 | Loss: 5.499 | Acc: 34.604,56.841,66.025,%
Batch: 60 | Loss: 5.550 | Acc: 34.196,56.404,65.369,%
Train all parameters

Epoch: 58
Batch: 0 | Loss: 4.099 | Acc: 45.312,67.969,84.375,%
Batch: 20 | Loss: 4.451 | Acc: 40.365,63.170,82.068,%
Batch: 40 | Loss: 4.281 | Acc: 41.216,64.825,83.155,%
Batch: 60 | Loss: 4.252 | Acc: 41.355,65.241,83.312,%
Batch: 80 | Loss: 4.270 | Acc: 41.049,64.419,83.362,%
Batch: 100 | Loss: 4.272 | Acc: 41.012,64.527,83.184,%
Batch: 120 | Loss: 4.276 | Acc: 40.864,64.437,83.038,%
Batch: 140 | Loss: 4.287 | Acc: 40.891,64.245,82.907,%
Batch: 160 | Loss: 4.282 | Acc: 40.921,64.339,82.822,%
Batch: 180 | Loss: 4.292 | Acc: 40.893,64.347,82.674,%
Batch: 200 | Loss: 4.310 | Acc: 40.839,64.175,82.408,%
Batch: 220 | Loss: 4.323 | Acc: 40.756,63.999,82.268,%
Batch: 240 | Loss: 4.329 | Acc: 40.722,63.962,82.232,%
Batch: 260 | Loss: 4.335 | Acc: 40.742,63.883,82.031,%
Batch: 280 | Loss: 4.351 | Acc: 40.686,63.715,81.806,%
Batch: 300 | Loss: 4.353 | Acc: 40.737,63.660,81.657,%
Batch: 320 | Loss: 4.355 | Acc: 40.730,63.603,81.503,%
Batch: 340 | Loss: 4.361 | Acc: 40.703,63.636,81.429,%
Batch: 360 | Loss: 4.370 | Acc: 40.597,63.589,81.352,%
Batch: 380 | Loss: 4.366 | Acc: 40.596,63.572,81.256,%
Batch: 0 | Loss: 5.255 | Acc: 35.156,57.812,68.750,%
Batch: 20 | Loss: 5.507 | Acc: 34.710,56.362,65.811,%
Batch: 40 | Loss: 5.522 | Acc: 34.794,55.907,65.130,%
Batch: 60 | Loss: 5.545 | Acc: 34.285,55.674,64.831,%
Train all parameters

Epoch: 59
Batch: 0 | Loss: 3.823 | Acc: 34.375,65.625,86.719,%
Batch: 20 | Loss: 4.098 | Acc: 42.299,65.402,83.222,%
Batch: 40 | Loss: 4.172 | Acc: 41.063,64.710,83.632,%
Batch: 60 | Loss: 4.124 | Acc: 41.624,64.306,83.965,%
Batch: 80 | Loss: 4.151 | Acc: 41.464,64.554,83.738,%
Batch: 100 | Loss: 4.227 | Acc: 41.004,64.194,83.130,%
Batch: 120 | Loss: 4.246 | Acc: 40.741,63.869,83.000,%
Batch: 140 | Loss: 4.261 | Acc: 40.775,63.902,82.668,%
Batch: 160 | Loss: 4.262 | Acc: 40.902,63.970,82.677,%
Batch: 180 | Loss: 4.260 | Acc: 40.940,64.153,82.605,%
Batch: 200 | Loss: 4.250 | Acc: 40.971,64.335,82.599,%
Batch: 220 | Loss: 4.246 | Acc: 41.028,64.257,82.572,%
Batch: 240 | Loss: 4.249 | Acc: 41.066,64.202,82.469,%
Batch: 260 | Loss: 4.263 | Acc: 40.990,64.068,82.268,%
Batch: 280 | Loss: 4.283 | Acc: 40.839,63.865,82.081,%
Batch: 300 | Loss: 4.292 | Acc: 40.817,63.857,81.946,%
Batch: 320 | Loss: 4.299 | Acc: 40.864,63.773,81.805,%
Batch: 340 | Loss: 4.296 | Acc: 40.946,63.739,81.690,%
Batch: 360 | Loss: 4.295 | Acc: 40.999,63.738,81.644,%
Batch: 380 | Loss: 4.297 | Acc: 40.972,63.741,81.515,%
Batch: 0 | Loss: 5.343 | Acc: 34.375,60.938,70.312,%
Batch: 20 | Loss: 5.743 | Acc: 33.222,54.278,64.658,%
Batch: 40 | Loss: 5.761 | Acc: 33.556,54.002,63.224,%
Batch: 60 | Loss: 5.782 | Acc: 33.645,53.509,62.423,%
Train all parameters

Epoch: 60
Batch: 0 | Loss: 3.844 | Acc: 45.312,68.750,84.375,%
Batch: 20 | Loss: 4.094 | Acc: 42.485,66.183,85.603,%
Batch: 40 | Loss: 4.090 | Acc: 42.035,65.530,84.909,%
Batch: 60 | Loss: 4.149 | Acc: 41.662,64.857,84.465,%
Batch: 80 | Loss: 4.168 | Acc: 41.927,64.361,84.105,%
Batch: 100 | Loss: 4.231 | Acc: 41.499,64.016,83.601,%
Batch: 120 | Loss: 4.229 | Acc: 41.458,64.050,83.381,%
Batch: 140 | Loss: 4.218 | Acc: 41.439,64.074,83.206,%
Batch: 160 | Loss: 4.223 | Acc: 41.498,64.067,83.050,%
Batch: 180 | Loss: 4.233 | Acc: 41.285,63.972,82.912,%
Batch: 200 | Loss: 4.241 | Acc: 41.239,63.899,82.855,%
Batch: 220 | Loss: 4.254 | Acc: 41.229,63.964,82.767,%
Batch: 240 | Loss: 4.244 | Acc: 41.455,64.043,82.761,%
Batch: 260 | Loss: 4.254 | Acc: 41.218,64.104,82.738,%
Batch: 280 | Loss: 4.258 | Acc: 41.192,64.024,82.646,%
Batch: 300 | Loss: 4.255 | Acc: 41.289,64.078,82.556,%
Batch: 320 | Loss: 4.259 | Acc: 41.238,64.128,82.443,%
Batch: 340 | Loss: 4.262 | Acc: 41.182,64.127,82.393,%
Batch: 360 | Loss: 4.269 | Acc: 41.157,64.024,82.284,%
Batch: 380 | Loss: 4.281 | Acc: 41.154,63.970,82.124,%
Batch: 0 | Loss: 5.410 | Acc: 37.500,54.688,70.312,%
Batch: 20 | Loss: 5.751 | Acc: 33.780,54.539,65.067,%
Batch: 40 | Loss: 5.780 | Acc: 33.441,53.887,64.024,%
Batch: 60 | Loss: 5.799 | Acc: 33.376,53.753,63.332,%
Train all parameters

Epoch: 61
Batch: 0 | Loss: 3.916 | Acc: 42.969,67.188,82.812,%
Batch: 20 | Loss: 3.996 | Acc: 42.634,65.551,84.710,%
Batch: 40 | Loss: 4.063 | Acc: 42.226,64.977,84.851,%
Batch: 60 | Loss: 4.127 | Acc: 41.329,65.510,84.567,%
Batch: 80 | Loss: 4.098 | Acc: 41.628,65.702,84.578,%
Batch: 100 | Loss: 4.118 | Acc: 41.337,65.780,84.545,%
Batch: 120 | Loss: 4.117 | Acc: 41.484,65.573,84.582,%
Batch: 140 | Loss: 4.131 | Acc: 41.412,65.232,84.514,%
Batch: 160 | Loss: 4.153 | Acc: 41.382,65.077,84.453,%
Batch: 180 | Loss: 4.158 | Acc: 41.346,64.999,84.181,%
Batch: 200 | Loss: 4.157 | Acc: 41.363,65.038,83.901,%
Batch: 220 | Loss: 4.161 | Acc: 41.283,64.946,83.813,%
Batch: 240 | Loss: 4.172 | Acc: 41.332,64.821,83.542,%
Batch: 260 | Loss: 4.184 | Acc: 41.328,64.604,83.390,%
Batch: 280 | Loss: 4.195 | Acc: 41.289,64.607,83.360,%
Batch: 300 | Loss: 4.202 | Acc: 41.201,64.473,83.298,%
Batch: 320 | Loss: 4.201 | Acc: 41.321,64.513,83.255,%
Batch: 340 | Loss: 4.201 | Acc: 41.315,64.457,83.129,%
Batch: 360 | Loss: 4.211 | Acc: 41.296,64.400,83.051,%
Batch: 380 | Loss: 4.223 | Acc: 41.275,64.309,82.954,%
Batch: 0 | Loss: 5.014 | Acc: 36.719,64.062,71.875,%
Batch: 20 | Loss: 5.388 | Acc: 35.454,57.292,67.188,%
Batch: 40 | Loss: 5.430 | Acc: 35.271,56.879,66.425,%
Batch: 60 | Loss: 5.441 | Acc: 35.067,56.711,66.253,%
Train all parameters

Epoch: 62
Batch: 0 | Loss: 3.444 | Acc: 44.531,67.188,84.375,%
Batch: 20 | Loss: 4.144 | Acc: 40.290,63.244,84.859,%
Batch: 40 | Loss: 4.037 | Acc: 41.673,65.263,86.128,%
Batch: 60 | Loss: 4.092 | Acc: 40.996,65.279,86.194,%
Batch: 80 | Loss: 4.062 | Acc: 41.406,65.586,86.198,%
Batch: 100 | Loss: 4.091 | Acc: 41.391,65.300,85.945,%
Batch: 120 | Loss: 4.096 | Acc: 41.439,65.296,85.763,%
Batch: 140 | Loss: 4.089 | Acc: 41.550,65.354,85.611,%
Batch: 160 | Loss: 4.103 | Acc: 41.528,65.402,85.287,%
Batch: 180 | Loss: 4.116 | Acc: 41.480,65.375,85.070,%
Batch: 200 | Loss: 4.143 | Acc: 41.208,65.166,84.768,%
Batch: 220 | Loss: 4.144 | Acc: 41.219,65.201,84.615,%
Batch: 240 | Loss: 4.156 | Acc: 41.286,65.106,84.365,%
Batch: 260 | Loss: 4.157 | Acc: 41.322,65.020,84.151,%
Batch: 280 | Loss: 4.165 | Acc: 41.373,64.966,83.994,%
Batch: 300 | Loss: 4.169 | Acc: 41.334,64.937,83.890,%
Batch: 320 | Loss: 4.173 | Acc: 41.338,64.895,83.842,%
Batch: 340 | Loss: 4.170 | Acc: 41.475,64.908,83.791,%
Batch: 360 | Loss: 4.185 | Acc: 41.517,64.720,83.576,%
Batch: 380 | Loss: 4.189 | Acc: 41.513,64.665,83.549,%
Batch: 0 | Loss: 5.108 | Acc: 39.062,63.281,67.188,%
Batch: 20 | Loss: 5.425 | Acc: 37.202,58.519,66.071,%
Batch: 40 | Loss: 5.417 | Acc: 36.566,58.537,66.292,%
Batch: 60 | Loss: 5.442 | Acc: 36.411,57.864,65.843,%
Train all parameters

Epoch: 63
Batch: 0 | Loss: 4.122 | Acc: 38.281,58.594,80.469,%
Batch: 20 | Loss: 4.014 | Acc: 42.225,66.220,85.603,%
Batch: 40 | Loss: 4.092 | Acc: 41.540,65.835,85.766,%
Batch: 60 | Loss: 4.048 | Acc: 41.765,65.971,85.733,%
Batch: 80 | Loss: 4.061 | Acc: 42.130,65.654,85.436,%
Batch: 100 | Loss: 4.085 | Acc: 41.932,65.439,85.257,%
Batch: 120 | Loss: 4.111 | Acc: 41.800,65.309,84.930,%
Batch: 140 | Loss: 4.131 | Acc: 41.700,65.065,84.763,%
Batch: 160 | Loss: 4.121 | Acc: 41.717,65.174,84.773,%
Batch: 180 | Loss: 4.125 | Acc: 41.812,65.137,84.586,%
Batch: 200 | Loss: 4.120 | Acc: 41.834,65.310,84.433,%
Batch: 220 | Loss: 4.133 | Acc: 41.830,65.328,84.276,%
Batch: 240 | Loss: 4.145 | Acc: 41.779,65.259,84.129,%
Batch: 260 | Loss: 4.159 | Acc: 41.739,65.209,84.115,%
Batch: 280 | Loss: 4.152 | Acc: 41.704,65.186,84.122,%
Batch: 300 | Loss: 4.143 | Acc: 41.835,65.186,83.965,%
Batch: 320 | Loss: 4.147 | Acc: 41.713,65.211,83.881,%
Batch: 340 | Loss: 4.146 | Acc: 41.793,65.103,83.823,%
Batch: 360 | Loss: 4.152 | Acc: 41.817,65.123,83.693,%
Batch: 380 | Loss: 4.156 | Acc: 41.755,65.090,83.618,%
Batch: 0 | Loss: 5.173 | Acc: 38.281,65.625,69.531,%
Batch: 20 | Loss: 5.388 | Acc: 36.347,58.445,65.885,%
Batch: 40 | Loss: 5.399 | Acc: 36.814,57.908,66.082,%
Batch: 60 | Loss: 5.428 | Acc: 36.322,57.556,65.612,%
Train all parameters

Epoch: 64
Batch: 0 | Loss: 4.476 | Acc: 42.188,59.375,80.469,%
Batch: 20 | Loss: 4.120 | Acc: 43.118,64.732,85.565,%
Batch: 40 | Loss: 4.078 | Acc: 43.236,65.206,85.918,%
Batch: 60 | Loss: 4.053 | Acc: 42.841,65.663,85.720,%
Batch: 80 | Loss: 4.026 | Acc: 42.930,65.712,85.764,%
Batch: 100 | Loss: 4.020 | Acc: 42.953,65.873,85.698,%
Batch: 120 | Loss: 4.024 | Acc: 42.943,66.006,85.770,%
Batch: 140 | Loss: 4.049 | Acc: 42.603,65.830,85.395,%
Batch: 160 | Loss: 4.054 | Acc: 42.639,65.688,85.224,%
Batch: 180 | Loss: 4.069 | Acc: 42.382,65.599,85.005,%
Batch: 200 | Loss: 4.097 | Acc: 42.071,65.574,84.841,%
Batch: 220 | Loss: 4.094 | Acc: 42.180,65.547,84.693,%
Batch: 240 | Loss: 4.109 | Acc: 42.188,65.434,84.485,%
Batch: 260 | Loss: 4.110 | Acc: 42.020,65.368,84.483,%
Batch: 280 | Loss: 4.117 | Acc: 42.023,65.230,84.269,%
Batch: 300 | Loss: 4.130 | Acc: 41.936,65.134,84.167,%
Batch: 320 | Loss: 4.130 | Acc: 41.983,65.000,84.100,%
Batch: 340 | Loss: 4.129 | Acc: 41.979,65.052,83.979,%
Batch: 360 | Loss: 4.134 | Acc: 42.008,65.000,83.858,%
Batch: 380 | Loss: 4.142 | Acc: 41.939,64.875,83.733,%
Batch: 0 | Loss: 5.021 | Acc: 36.719,65.625,69.531,%
Batch: 20 | Loss: 5.287 | Acc: 37.388,59.338,66.853,%
Batch: 40 | Loss: 5.294 | Acc: 37.614,59.299,66.502,%
Batch: 60 | Loss: 5.329 | Acc: 37.385,58.914,65.984,%
Train all parameters

Epoch: 65
Batch: 0 | Loss: 3.864 | Acc: 44.531,71.875,88.281,%
Batch: 20 | Loss: 3.936 | Acc: 42.485,68.601,86.235,%
Batch: 40 | Loss: 3.987 | Acc: 41.768,66.959,86.071,%
Batch: 60 | Loss: 3.986 | Acc: 41.598,66.560,86.130,%
Batch: 80 | Loss: 3.993 | Acc: 41.937,66.667,86.082,%
Batch: 100 | Loss: 3.993 | Acc: 41.909,66.484,86.177,%
Batch: 120 | Loss: 4.003 | Acc: 41.916,66.316,85.976,%
Batch: 140 | Loss: 4.026 | Acc: 41.750,66.223,85.827,%
Batch: 160 | Loss: 4.028 | Acc: 41.867,66.139,85.651,%
Batch: 180 | Loss: 4.039 | Acc: 41.765,66.057,85.458,%
Batch: 200 | Loss: 4.048 | Acc: 41.888,65.948,85.300,%
Batch: 220 | Loss: 4.051 | Acc: 41.834,65.841,85.259,%
Batch: 240 | Loss: 4.059 | Acc: 41.854,65.787,85.010,%
Batch: 260 | Loss: 4.076 | Acc: 41.816,65.667,84.926,%
Batch: 280 | Loss: 4.079 | Acc: 41.926,65.619,84.895,%
Batch: 300 | Loss: 4.090 | Acc: 41.868,65.628,84.744,%
Batch: 320 | Loss: 4.097 | Acc: 41.861,65.603,84.660,%
Batch: 340 | Loss: 4.100 | Acc: 41.917,65.598,84.613,%
Batch: 360 | Loss: 4.096 | Acc: 41.975,65.593,84.511,%
Batch: 380 | Loss: 4.105 | Acc: 41.900,65.576,84.363,%
Batch: 0 | Loss: 5.251 | Acc: 35.156,65.625,67.969,%
Batch: 20 | Loss: 5.550 | Acc: 35.751,56.808,65.588,%
Batch: 40 | Loss: 5.515 | Acc: 36.052,56.421,64.863,%
Batch: 60 | Loss: 5.517 | Acc: 36.040,56.173,64.524,%
Train all parameters

Epoch: 66
Batch: 0 | Loss: 3.769 | Acc: 41.406,62.500,82.812,%
Batch: 20 | Loss: 4.098 | Acc: 41.629,65.365,84.784,%
Batch: 40 | Loss: 3.984 | Acc: 43.159,66.197,85.785,%
Batch: 60 | Loss: 3.985 | Acc: 42.687,66.304,86.104,%
Batch: 80 | Loss: 4.019 | Acc: 42.486,66.146,86.101,%
Batch: 100 | Loss: 4.017 | Acc: 42.087,66.244,86.131,%
Batch: 120 | Loss: 4.018 | Acc: 42.045,66.225,86.034,%
Batch: 140 | Loss: 4.025 | Acc: 42.188,66.029,85.932,%
Batch: 160 | Loss: 4.017 | Acc: 42.086,66.164,85.962,%
Batch: 180 | Loss: 4.023 | Acc: 42.067,66.316,85.916,%
Batch: 200 | Loss: 4.024 | Acc: 42.129,66.103,85.619,%
Batch: 220 | Loss: 4.041 | Acc: 42.096,65.940,85.425,%
Batch: 240 | Loss: 4.039 | Acc: 42.162,66.033,85.257,%
Batch: 260 | Loss: 4.047 | Acc: 42.122,65.987,85.159,%
Batch: 280 | Loss: 4.056 | Acc: 42.149,65.953,85.103,%
Batch: 300 | Loss: 4.058 | Acc: 42.099,65.962,85.006,%
Batch: 320 | Loss: 4.074 | Acc: 42.063,65.895,84.879,%
Batch: 340 | Loss: 4.076 | Acc: 42.084,65.790,84.748,%
Batch: 360 | Loss: 4.079 | Acc: 42.088,65.759,84.682,%
Batch: 380 | Loss: 4.087 | Acc: 42.017,65.728,84.576,%
Batch: 0 | Loss: 5.095 | Acc: 35.938,66.406,70.312,%
Batch: 20 | Loss: 5.359 | Acc: 36.607,59.189,67.188,%
Batch: 40 | Loss: 5.385 | Acc: 36.509,58.232,66.692,%
Batch: 60 | Loss: 5.393 | Acc: 36.450,58.094,66.368,%
Train all parameters

Epoch: 67
Batch: 0 | Loss: 4.445 | Acc: 44.531,66.406,87.500,%
Batch: 20 | Loss: 4.096 | Acc: 42.708,66.406,87.091,%
Batch: 40 | Loss: 4.024 | Acc: 42.797,66.578,86.681,%
Batch: 60 | Loss: 3.970 | Acc: 42.828,66.688,86.437,%
Batch: 80 | Loss: 3.986 | Acc: 42.506,66.715,86.584,%
Batch: 100 | Loss: 4.006 | Acc: 42.172,66.708,86.440,%
Batch: 120 | Loss: 4.016 | Acc: 42.123,66.671,86.235,%
Batch: 140 | Loss: 4.022 | Acc: 42.127,66.595,86.015,%
Batch: 160 | Loss: 4.028 | Acc: 42.061,66.416,86.161,%
Batch: 180 | Loss: 4.024 | Acc: 42.175,66.562,86.063,%
Batch: 200 | Loss: 4.033 | Acc: 42.191,66.426,86.011,%
Batch: 220 | Loss: 4.035 | Acc: 42.343,66.307,85.846,%
Batch: 240 | Loss: 4.038 | Acc: 42.431,66.199,85.762,%
Batch: 260 | Loss: 4.040 | Acc: 42.439,66.164,85.674,%
Batch: 280 | Loss: 4.038 | Acc: 42.491,66.231,85.607,%
Batch: 300 | Loss: 4.047 | Acc: 42.421,66.113,85.468,%
Batch: 320 | Loss: 4.051 | Acc: 42.402,65.985,85.373,%
Batch: 340 | Loss: 4.054 | Acc: 42.412,65.907,85.209,%
Batch: 360 | Loss: 4.061 | Acc: 42.441,65.848,85.102,%
Batch: 380 | Loss: 4.059 | Acc: 42.479,65.859,85.068,%
Batch: 0 | Loss: 5.185 | Acc: 38.281,60.938,70.312,%
Batch: 20 | Loss: 5.350 | Acc: 36.607,57.478,67.150,%
Batch: 40 | Loss: 5.383 | Acc: 36.700,57.279,66.044,%
Batch: 60 | Loss: 5.386 | Acc: 36.501,57.313,65.727,%
Train all parameters

Epoch: 68
Batch: 0 | Loss: 4.040 | Acc: 42.188,70.312,89.844,%
Batch: 20 | Loss: 3.739 | Acc: 45.015,67.411,88.207,%
Batch: 40 | Loss: 3.840 | Acc: 43.178,67.550,87.633,%
Batch: 60 | Loss: 3.895 | Acc: 42.802,67.456,87.679,%
Batch: 80 | Loss: 3.915 | Acc: 42.921,67.197,87.404,%
Batch: 100 | Loss: 3.918 | Acc: 42.845,66.994,87.299,%
Batch: 120 | Loss: 3.943 | Acc: 42.788,66.742,86.867,%
Batch: 140 | Loss: 3.949 | Acc: 42.858,66.645,86.763,%
Batch: 160 | Loss: 3.958 | Acc: 42.872,66.586,86.661,%
Batch: 180 | Loss: 3.951 | Acc: 42.913,66.769,86.645,%
Batch: 200 | Loss: 3.944 | Acc: 43.078,66.849,86.521,%
Batch: 220 | Loss: 3.960 | Acc: 42.940,66.629,86.411,%
Batch: 240 | Loss: 3.978 | Acc: 42.703,66.374,86.307,%
Batch: 260 | Loss: 3.978 | Acc: 42.654,66.287,86.207,%
Batch: 280 | Loss: 3.985 | Acc: 42.652,66.253,86.149,%
Batch: 300 | Loss: 3.990 | Acc: 42.673,66.243,85.971,%
Batch: 320 | Loss: 4.001 | Acc: 42.579,66.153,85.889,%
Batch: 340 | Loss: 4.006 | Acc: 42.559,66.218,85.871,%
Batch: 360 | Loss: 4.009 | Acc: 42.592,66.138,85.855,%
Batch: 380 | Loss: 4.015 | Acc: 42.499,66.103,85.804,%
Batch: 0 | Loss: 4.876 | Acc: 36.719,68.750,73.438,%
Batch: 20 | Loss: 5.245 | Acc: 38.765,59.710,68.192,%
Batch: 40 | Loss: 5.263 | Acc: 38.205,58.918,67.378,%
Batch: 60 | Loss: 5.287 | Acc: 37.935,58.709,66.919,%
Train all parameters

Epoch: 69
Batch: 0 | Loss: 3.861 | Acc: 42.188,63.281,86.719,%
Batch: 20 | Loss: 3.891 | Acc: 42.336,67.299,88.170,%
Batch: 40 | Loss: 3.924 | Acc: 42.207,67.035,87.900,%
Batch: 60 | Loss: 3.939 | Acc: 42.098,66.816,87.654,%
Batch: 80 | Loss: 3.957 | Acc: 41.956,66.946,87.587,%
Batch: 100 | Loss: 3.974 | Acc: 42.002,66.932,87.446,%
Batch: 120 | Loss: 3.955 | Acc: 42.323,66.865,87.358,%
Batch: 140 | Loss: 3.944 | Acc: 42.509,66.739,87.267,%
Batch: 160 | Loss: 3.948 | Acc: 42.537,66.678,87.092,%
Batch: 180 | Loss: 3.965 | Acc: 42.572,66.501,86.904,%
Batch: 200 | Loss: 3.975 | Acc: 42.685,66.511,86.629,%
Batch: 220 | Loss: 3.984 | Acc: 42.750,66.374,86.595,%
Batch: 240 | Loss: 3.996 | Acc: 42.687,66.377,86.453,%
Batch: 260 | Loss: 4.002 | Acc: 42.598,66.442,86.351,%
Batch: 280 | Loss: 4.008 | Acc: 42.724,66.428,86.210,%
Batch: 300 | Loss: 4.016 | Acc: 42.566,66.393,86.158,%
Batch: 320 | Loss: 4.005 | Acc: 42.701,66.479,85.998,%
Batch: 340 | Loss: 4.006 | Acc: 42.760,66.441,85.926,%
Batch: 360 | Loss: 4.012 | Acc: 42.763,66.369,85.866,%
Batch: 380 | Loss: 4.016 | Acc: 42.733,66.285,85.700,%
Batch: 0 | Loss: 4.954 | Acc: 35.156,64.844,72.656,%
Batch: 20 | Loss: 5.219 | Acc: 36.793,60.826,68.080,%
Batch: 40 | Loss: 5.226 | Acc: 38.186,59.775,67.188,%
Batch: 60 | Loss: 5.248 | Acc: 37.948,59.273,67.098,%
Train all parameters

Epoch: 70
Batch: 0 | Loss: 3.806 | Acc: 32.031,75.000,91.406,%
Batch: 20 | Loss: 3.834 | Acc: 44.531,68.527,88.356,%
Batch: 40 | Loss: 3.822 | Acc: 43.807,67.759,88.453,%
Batch: 60 | Loss: 3.794 | Acc: 43.955,67.713,88.576,%
Batch: 80 | Loss: 3.808 | Acc: 43.769,67.641,88.050,%
Batch: 100 | Loss: 3.819 | Acc: 43.711,67.458,88.049,%
Batch: 120 | Loss: 3.845 | Acc: 43.298,67.381,87.784,%
Batch: 140 | Loss: 3.875 | Acc: 43.146,67.243,87.633,%
Batch: 160 | Loss: 3.887 | Acc: 43.250,67.188,87.413,%
Batch: 180 | Loss: 3.881 | Acc: 43.241,67.088,87.224,%
Batch: 200 | Loss: 3.899 | Acc: 43.194,66.892,86.929,%
Batch: 220 | Loss: 3.898 | Acc: 43.244,66.926,86.811,%
Batch: 240 | Loss: 3.908 | Acc: 43.163,66.854,86.664,%
Batch: 260 | Loss: 3.922 | Acc: 43.053,66.792,86.467,%
Batch: 280 | Loss: 3.932 | Acc: 43.055,66.723,86.382,%
Batch: 300 | Loss: 3.940 | Acc: 42.997,66.796,86.329,%
Batch: 320 | Loss: 3.943 | Acc: 43.000,66.669,86.283,%
Batch: 340 | Loss: 3.951 | Acc: 43.021,66.569,86.169,%
Batch: 360 | Loss: 3.959 | Acc: 42.945,66.469,86.126,%
Batch: 380 | Loss: 3.961 | Acc: 42.940,66.429,86.071,%
Batch: 0 | Loss: 5.292 | Acc: 35.938,60.156,68.750,%
Batch: 20 | Loss: 5.531 | Acc: 34.115,57.106,66.295,%
Batch: 40 | Loss: 5.517 | Acc: 34.775,56.593,65.835,%
Batch: 60 | Loss: 5.511 | Acc: 34.644,56.609,65.446,%
Train all parameters

Epoch: 71
Batch: 0 | Loss: 2.837 | Acc: 50.781,75.000,91.406,%
Batch: 20 | Loss: 3.758 | Acc: 42.894,69.345,89.435,%
Batch: 40 | Loss: 3.803 | Acc: 43.350,68.540,89.043,%
Batch: 60 | Loss: 3.806 | Acc: 43.584,68.020,88.806,%
Batch: 80 | Loss: 3.830 | Acc: 43.596,67.776,88.715,%
Batch: 100 | Loss: 3.864 | Acc: 43.239,67.381,88.490,%
Batch: 120 | Loss: 3.876 | Acc: 43.046,67.375,88.475,%
Batch: 140 | Loss: 3.892 | Acc: 42.875,67.409,88.193,%
Batch: 160 | Loss: 3.907 | Acc: 42.974,67.105,87.878,%
Batch: 180 | Loss: 3.920 | Acc: 42.913,66.920,87.517,%
Batch: 200 | Loss: 3.924 | Acc: 42.930,66.981,87.337,%
Batch: 220 | Loss: 3.920 | Acc: 42.891,67.149,87.168,%
Batch: 240 | Loss: 3.912 | Acc: 43.063,67.249,87.124,%
Batch: 260 | Loss: 3.909 | Acc: 43.166,67.185,86.973,%
Batch: 280 | Loss: 3.914 | Acc: 43.144,67.135,86.872,%
Batch: 300 | Loss: 3.920 | Acc: 43.231,67.081,86.693,%
Batch: 320 | Loss: 3.922 | Acc: 43.207,67.068,86.475,%
Batch: 340 | Loss: 3.934 | Acc: 43.180,66.963,86.254,%
Batch: 360 | Loss: 3.940 | Acc: 43.207,66.904,86.106,%
Batch: 380 | Loss: 3.954 | Acc: 43.082,66.745,85.931,%
Batch: 0 | Loss: 5.967 | Acc: 33.594,54.688,62.500,%
Batch: 20 | Loss: 5.872 | Acc: 32.887,53.051,62.314,%
Batch: 40 | Loss: 5.821 | Acc: 32.393,53.811,62.138,%
Batch: 60 | Loss: 5.822 | Acc: 32.787,53.599,62.039,%
Train all parameters

Epoch: 72
Batch: 0 | Loss: 4.131 | Acc: 48.438,62.500,86.719,%
Batch: 20 | Loss: 3.926 | Acc: 42.262,68.118,87.165,%
Batch: 40 | Loss: 3.881 | Acc: 43.216,68.350,87.881,%
Batch: 60 | Loss: 3.846 | Acc: 43.455,68.443,87.935,%
Batch: 80 | Loss: 3.877 | Acc: 43.229,68.142,87.703,%
Batch: 100 | Loss: 3.877 | Acc: 43.309,67.775,87.662,%
Batch: 120 | Loss: 3.883 | Acc: 43.453,67.446,87.590,%
Batch: 140 | Loss: 3.904 | Acc: 43.196,67.426,87.428,%
Batch: 160 | Loss: 3.924 | Acc: 42.954,67.154,87.393,%
Batch: 180 | Loss: 3.932 | Acc: 43.072,67.101,87.319,%
Batch: 200 | Loss: 3.929 | Acc: 43.128,67.024,87.135,%
Batch: 220 | Loss: 3.935 | Acc: 43.220,66.965,86.956,%
Batch: 240 | Loss: 3.941 | Acc: 43.254,67.045,86.897,%
Batch: 260 | Loss: 3.938 | Acc: 43.316,67.056,86.791,%
Batch: 280 | Loss: 3.945 | Acc: 43.227,66.873,86.688,%
Batch: 300 | Loss: 3.955 | Acc: 43.080,66.715,86.677,%
Batch: 320 | Loss: 3.956 | Acc: 43.037,66.749,86.621,%
Batch: 340 | Loss: 3.959 | Acc: 43.056,66.667,86.508,%
Batch: 360 | Loss: 3.965 | Acc: 43.042,66.664,86.403,%
Batch: 380 | Loss: 3.964 | Acc: 43.125,66.706,86.350,%
Batch: 0 | Loss: 5.169 | Acc: 36.719,60.156,70.312,%
Batch: 20 | Loss: 5.327 | Acc: 36.979,57.961,67.150,%
Batch: 40 | Loss: 5.364 | Acc: 36.376,57.584,65.949,%
Batch: 60 | Loss: 5.374 | Acc: 36.322,57.211,65.753,%
Train all parameters

Epoch: 73
Batch: 0 | Loss: 3.847 | Acc: 38.281,64.844,84.375,%
Batch: 20 | Loss: 3.842 | Acc: 43.192,68.192,89.137,%
Batch: 40 | Loss: 3.800 | Acc: 43.121,67.645,89.082,%
Batch: 60 | Loss: 3.793 | Acc: 43.071,67.520,89.306,%
Batch: 80 | Loss: 3.810 | Acc: 43.181,67.429,89.111,%
Batch: 100 | Loss: 3.831 | Acc: 43.154,67.427,88.861,%
Batch: 120 | Loss: 3.825 | Acc: 43.188,67.814,88.817,%
Batch: 140 | Loss: 3.846 | Acc: 43.091,67.786,88.525,%
Batch: 160 | Loss: 3.831 | Acc: 43.265,67.978,88.354,%
Batch: 180 | Loss: 3.816 | Acc: 43.448,68.055,88.286,%
Batch: 200 | Loss: 3.837 | Acc: 43.361,67.903,88.141,%
Batch: 220 | Loss: 3.843 | Acc: 43.209,67.852,88.080,%
Batch: 240 | Loss: 3.862 | Acc: 43.176,67.748,87.902,%
Batch: 260 | Loss: 3.868 | Acc: 43.148,67.639,87.778,%
Batch: 280 | Loss: 3.884 | Acc: 43.038,67.474,87.525,%
Batch: 300 | Loss: 3.893 | Acc: 43.054,67.380,87.220,%
Batch: 320 | Loss: 3.897 | Acc: 43.129,67.185,87.011,%
Batch: 340 | Loss: 3.909 | Acc: 43.134,67.089,86.950,%
Batch: 360 | Loss: 3.919 | Acc: 43.070,66.969,86.838,%
Batch: 380 | Loss: 3.921 | Acc: 43.104,67.009,86.731,%
Batch: 0 | Loss: 5.083 | Acc: 39.844,60.938,71.094,%
Batch: 20 | Loss: 5.244 | Acc: 38.579,59.189,67.113,%
Batch: 40 | Loss: 5.235 | Acc: 38.891,59.051,66.692,%
Batch: 60 | Loss: 5.275 | Acc: 38.268,58.658,66.291,%
Train all parameters

Epoch: 74
Batch: 0 | Loss: 3.891 | Acc: 46.094,65.625,87.500,%
Batch: 20 | Loss: 3.928 | Acc: 43.378,66.964,87.984,%
Batch: 40 | Loss: 3.868 | Acc: 43.483,67.454,88.091,%
Batch: 60 | Loss: 3.858 | Acc: 43.532,67.456,88.115,%
Batch: 80 | Loss: 3.836 | Acc: 44.010,67.342,88.214,%
Batch: 100 | Loss: 3.838 | Acc: 43.912,67.683,88.382,%
Batch: 120 | Loss: 3.825 | Acc: 44.157,67.717,88.178,%
Batch: 140 | Loss: 3.834 | Acc: 43.938,67.642,88.193,%
Batch: 160 | Loss: 3.839 | Acc: 43.852,67.799,88.359,%
Batch: 180 | Loss: 3.838 | Acc: 43.828,67.770,88.221,%
Batch: 200 | Loss: 3.843 | Acc: 43.824,67.736,88.036,%
Batch: 220 | Loss: 3.849 | Acc: 43.828,67.615,88.027,%
Batch: 240 | Loss: 3.848 | Acc: 43.808,67.677,87.912,%
Batch: 260 | Loss: 3.865 | Acc: 43.651,67.535,87.662,%
Batch: 280 | Loss: 3.874 | Acc: 43.516,67.388,87.539,%
Batch: 300 | Loss: 3.880 | Acc: 43.464,67.333,87.378,%
Batch: 320 | Loss: 3.893 | Acc: 43.502,67.214,87.206,%
Batch: 340 | Loss: 3.895 | Acc: 43.509,67.185,87.117,%
Batch: 360 | Loss: 3.894 | Acc: 43.531,67.196,87.084,%
Batch: 380 | Loss: 3.894 | Acc: 43.502,67.142,87.016,%
Batch: 0 | Loss: 5.084 | Acc: 39.844,67.188,69.531,%
Batch: 20 | Loss: 5.485 | Acc: 34.933,57.701,66.704,%
Batch: 40 | Loss: 5.522 | Acc: 34.832,57.374,65.720,%
Batch: 60 | Loss: 5.535 | Acc: 34.682,57.006,65.100,%
Train all parameters

Epoch: 75
Batch: 0 | Loss: 3.572 | Acc: 43.750,69.531,88.281,%
Batch: 20 | Loss: 3.842 | Acc: 42.560,68.787,89.137,%
Batch: 40 | Loss: 3.720 | Acc: 43.883,68.769,89.005,%
Batch: 60 | Loss: 3.775 | Acc: 43.161,68.430,89.229,%
Batch: 80 | Loss: 3.793 | Acc: 43.393,68.123,88.918,%
Batch: 100 | Loss: 3.802 | Acc: 43.472,67.899,88.854,%
Batch: 120 | Loss: 3.803 | Acc: 43.447,67.762,88.817,%
Batch: 140 | Loss: 3.833 | Acc: 43.190,67.631,88.614,%
Batch: 160 | Loss: 3.832 | Acc: 43.134,67.639,88.602,%
Batch: 180 | Loss: 3.846 | Acc: 43.172,67.593,88.475,%
Batch: 200 | Loss: 3.857 | Acc: 43.050,67.549,88.246,%
Batch: 220 | Loss: 3.854 | Acc: 43.149,67.598,88.101,%
Batch: 240 | Loss: 3.870 | Acc: 43.089,67.505,87.879,%
Batch: 260 | Loss: 3.881 | Acc: 43.151,67.367,87.730,%
Batch: 280 | Loss: 3.887 | Acc: 43.133,67.249,87.617,%
Batch: 300 | Loss: 3.888 | Acc: 43.163,67.309,87.567,%
Batch: 320 | Loss: 3.887 | Acc: 43.110,67.346,87.476,%
Batch: 340 | Loss: 3.884 | Acc: 43.143,67.352,87.369,%
Batch: 360 | Loss: 3.890 | Acc: 43.157,67.222,87.262,%
Batch: 380 | Loss: 3.902 | Acc: 43.098,67.108,87.172,%
Batch: 0 | Loss: 5.249 | Acc: 39.844,62.500,65.625,%
Batch: 20 | Loss: 5.340 | Acc: 38.393,59.673,66.704,%
Batch: 40 | Loss: 5.327 | Acc: 38.434,59.432,66.368,%
Batch: 60 | Loss: 5.333 | Acc: 38.204,59.298,66.714,%
Train all parameters

Epoch: 76
Batch: 0 | Loss: 3.841 | Acc: 45.312,67.969,94.531,%
Batch: 20 | Loss: 3.813 | Acc: 43.713,68.304,88.914,%
Batch: 40 | Loss: 3.813 | Acc: 42.912,68.178,88.681,%
Batch: 60 | Loss: 3.793 | Acc: 42.969,68.507,88.896,%
Batch: 80 | Loss: 3.824 | Acc: 42.708,68.084,89.101,%
Batch: 100 | Loss: 3.817 | Acc: 43.123,68.093,89.140,%
Batch: 120 | Loss: 3.819 | Acc: 43.162,68.227,89.224,%
Batch: 140 | Loss: 3.840 | Acc: 43.152,67.847,88.918,%
Batch: 160 | Loss: 3.848 | Acc: 43.158,67.770,88.912,%
Batch: 180 | Loss: 3.836 | Acc: 43.262,67.610,88.898,%
Batch: 200 | Loss: 3.840 | Acc: 43.221,67.530,88.701,%
Batch: 220 | Loss: 3.840 | Acc: 43.361,67.481,88.663,%
Batch: 240 | Loss: 3.841 | Acc: 43.374,67.401,88.528,%
Batch: 260 | Loss: 3.843 | Acc: 43.310,67.442,88.434,%
Batch: 280 | Loss: 3.859 | Acc: 43.149,67.321,88.323,%
Batch: 300 | Loss: 3.861 | Acc: 43.171,67.304,88.258,%
Batch: 320 | Loss: 3.866 | Acc: 43.125,67.334,88.101,%
Batch: 340 | Loss: 3.869 | Acc: 43.214,67.272,87.970,%
Batch: 360 | Loss: 3.876 | Acc: 43.103,67.198,87.848,%
Batch: 380 | Loss: 3.889 | Acc: 42.948,67.124,87.705,%
Batch: 0 | Loss: 5.147 | Acc: 40.625,63.281,67.969,%
Batch: 20 | Loss: 5.324 | Acc: 37.872,59.859,66.555,%
Batch: 40 | Loss: 5.342 | Acc: 37.348,59.642,66.311,%
Batch: 60 | Loss: 5.361 | Acc: 37.269,59.144,65.689,%
Train all parameters

Epoch: 77
Batch: 0 | Loss: 3.986 | Acc: 42.188,68.750,83.594,%
Batch: 20 | Loss: 3.893 | Acc: 42.188,67.820,87.798,%
Batch: 40 | Loss: 3.861 | Acc: 43.350,67.588,87.938,%
Batch: 60 | Loss: 3.866 | Acc: 43.110,67.789,88.486,%
Batch: 80 | Loss: 3.797 | Acc: 44.020,68.345,88.638,%
Batch: 100 | Loss: 3.802 | Acc: 43.912,68.077,88.629,%
Batch: 120 | Loss: 3.782 | Acc: 44.041,68.240,88.798,%
Batch: 140 | Loss: 3.790 | Acc: 43.767,68.273,88.785,%
Batch: 160 | Loss: 3.783 | Acc: 43.828,68.270,88.655,%
Batch: 180 | Loss: 3.804 | Acc: 43.715,67.913,88.480,%
Batch: 200 | Loss: 3.806 | Acc: 43.711,67.778,88.487,%
Batch: 220 | Loss: 3.821 | Acc: 43.605,67.668,88.380,%
Batch: 240 | Loss: 3.825 | Acc: 43.601,67.683,88.288,%
Batch: 260 | Loss: 3.821 | Acc: 43.732,67.711,88.147,%
Batch: 280 | Loss: 3.832 | Acc: 43.642,67.685,88.000,%
Batch: 300 | Loss: 3.836 | Acc: 43.729,67.634,87.796,%
Batch: 320 | Loss: 3.838 | Acc: 43.636,67.682,87.702,%
Batch: 340 | Loss: 3.858 | Acc: 43.487,67.685,87.493,%
Batch: 360 | Loss: 3.860 | Acc: 43.495,67.644,87.396,%
Batch: 380 | Loss: 3.864 | Acc: 43.537,67.559,87.205,%
Batch: 0 | Loss: 5.410 | Acc: 37.500,57.031,65.625,%
Batch: 20 | Loss: 5.361 | Acc: 36.086,58.110,66.071,%
Batch: 40 | Loss: 5.409 | Acc: 35.957,57.698,65.473,%
Batch: 60 | Loss: 5.435 | Acc: 36.027,57.070,64.946,%
Train all parameters

Epoch: 78
Batch: 0 | Loss: 4.099 | Acc: 37.500,64.844,85.156,%
Batch: 20 | Loss: 3.823 | Acc: 42.262,68.229,88.988,%
Batch: 40 | Loss: 3.768 | Acc: 42.683,68.331,89.272,%
Batch: 60 | Loss: 3.762 | Acc: 43.046,68.519,89.203,%
Batch: 80 | Loss: 3.806 | Acc: 42.679,68.557,88.879,%
Batch: 100 | Loss: 3.808 | Acc: 42.768,68.657,88.738,%
Batch: 120 | Loss: 3.820 | Acc: 42.936,68.466,88.720,%
Batch: 140 | Loss: 3.798 | Acc: 43.207,68.534,88.652,%
Batch: 160 | Loss: 3.816 | Acc: 43.372,68.435,88.354,%
Batch: 180 | Loss: 3.826 | Acc: 43.422,68.210,88.165,%
Batch: 200 | Loss: 3.830 | Acc: 43.528,68.252,88.095,%
Batch: 220 | Loss: 3.832 | Acc: 43.556,68.269,88.073,%
Batch: 240 | Loss: 3.837 | Acc: 43.471,68.264,87.944,%
Batch: 260 | Loss: 3.840 | Acc: 43.427,68.136,87.967,%
Batch: 280 | Loss: 3.838 | Acc: 43.408,67.997,87.889,%
Batch: 300 | Loss: 3.846 | Acc: 43.506,67.826,87.622,%
Batch: 320 | Loss: 3.858 | Acc: 43.526,67.684,87.417,%
Batch: 340 | Loss: 3.860 | Acc: 43.519,67.692,87.365,%
Batch: 360 | Loss: 3.865 | Acc: 43.456,67.579,87.292,%
Batch: 380 | Loss: 3.869 | Acc: 43.494,67.544,87.303,%
Batch: 0 | Loss: 5.340 | Acc: 38.281,56.250,68.750,%
Batch: 20 | Loss: 5.348 | Acc: 38.132,58.631,67.001,%
Batch: 40 | Loss: 5.344 | Acc: 38.357,58.232,66.578,%
Batch: 60 | Loss: 5.353 | Acc: 37.948,58.376,66.445,%
Train all parameters

Epoch: 79
Batch: 0 | Loss: 3.465 | Acc: 54.688,67.188,89.062,%
Batch: 20 | Loss: 3.871 | Acc: 43.341,67.336,89.955,%
Batch: 40 | Loss: 3.754 | Acc: 43.998,68.617,90.053,%
Batch: 60 | Loss: 3.775 | Acc: 43.622,68.276,89.869,%
Batch: 80 | Loss: 3.752 | Acc: 44.155,68.499,89.824,%
Batch: 100 | Loss: 3.751 | Acc: 44.206,68.735,89.588,%
Batch: 120 | Loss: 3.759 | Acc: 43.924,68.653,89.198,%
Batch: 140 | Loss: 3.774 | Acc: 43.900,68.606,89.096,%
Batch: 160 | Loss: 3.779 | Acc: 43.784,68.590,88.737,%
Batch: 180 | Loss: 3.788 | Acc: 43.728,68.409,88.419,%
Batch: 200 | Loss: 3.823 | Acc: 43.455,68.144,88.227,%
Batch: 220 | Loss: 3.823 | Acc: 43.450,68.068,88.115,%
Batch: 240 | Loss: 3.835 | Acc: 43.520,67.985,88.022,%
Batch: 260 | Loss: 3.840 | Acc: 43.388,67.957,87.925,%
Batch: 280 | Loss: 3.837 | Acc: 43.583,67.788,87.747,%
Batch: 300 | Loss: 3.851 | Acc: 43.470,67.779,87.661,%
Batch: 320 | Loss: 3.857 | Acc: 43.502,67.682,87.463,%
Batch: 340 | Loss: 3.857 | Acc: 43.530,67.648,87.392,%
Batch: 360 | Loss: 3.862 | Acc: 43.464,67.594,87.329,%
Batch: 380 | Loss: 3.861 | Acc: 43.492,67.548,87.270,%
Batch: 0 | Loss: 4.922 | Acc: 39.062,62.500,77.344,%
Batch: 20 | Loss: 5.333 | Acc: 36.198,58.780,67.262,%
Batch: 40 | Loss: 5.374 | Acc: 36.357,57.774,65.873,%
Batch: 60 | Loss: 5.377 | Acc: 36.885,57.774,65.651,%
Train all parameters

Epoch: 80
Batch: 0 | Loss: 3.968 | Acc: 39.062,67.969,90.625,%
Batch: 20 | Loss: 3.719 | Acc: 44.308,70.647,89.881,%
Batch: 40 | Loss: 3.664 | Acc: 44.550,70.027,89.558,%
Batch: 60 | Loss: 3.681 | Acc: 44.736,69.249,89.024,%
Batch: 80 | Loss: 3.717 | Acc: 44.541,68.856,89.043,%
Batch: 100 | Loss: 3.724 | Acc: 44.230,68.912,89.202,%
Batch: 120 | Loss: 3.734 | Acc: 44.273,68.756,89.172,%
Batch: 140 | Loss: 3.745 | Acc: 44.376,68.717,89.024,%
Batch: 160 | Loss: 3.738 | Acc: 44.342,68.624,89.111,%
Batch: 180 | Loss: 3.753 | Acc: 44.069,68.504,89.132,%
Batch: 200 | Loss: 3.761 | Acc: 44.053,68.381,88.868,%
Batch: 220 | Loss: 3.783 | Acc: 43.990,68.167,88.819,%
Batch: 240 | Loss: 3.785 | Acc: 43.922,68.264,88.780,%
Batch: 260 | Loss: 3.785 | Acc: 44.064,68.325,88.670,%
Batch: 280 | Loss: 3.790 | Acc: 44.034,68.302,88.570,%
Batch: 300 | Loss: 3.796 | Acc: 43.952,68.179,88.450,%
Batch: 320 | Loss: 3.795 | Acc: 43.933,68.139,88.296,%
Batch: 340 | Loss: 3.804 | Acc: 43.945,68.076,88.139,%
Batch: 360 | Loss: 3.810 | Acc: 43.975,68.075,88.073,%
Batch: 380 | Loss: 3.817 | Acc: 43.971,68.047,87.994,%
Batch: 0 | Loss: 4.846 | Acc: 38.281,67.188,74.219,%
Batch: 20 | Loss: 5.449 | Acc: 35.082,57.812,66.853,%
Batch: 40 | Loss: 5.475 | Acc: 35.175,57.622,66.463,%
Batch: 60 | Loss: 5.501 | Acc: 35.003,57.236,65.561,%
Train all parameters

Epoch: 81
Batch: 0 | Loss: 4.390 | Acc: 42.188,65.625,89.844,%
Batch: 20 | Loss: 3.613 | Acc: 44.308,70.499,90.551,%
Batch: 40 | Loss: 3.674 | Acc: 43.197,69.627,90.015,%
Batch: 60 | Loss: 3.749 | Acc: 42.546,68.801,89.895,%
Batch: 80 | Loss: 3.733 | Acc: 43.142,68.943,89.651,%
Batch: 100 | Loss: 3.744 | Acc: 43.193,68.827,89.387,%
Batch: 120 | Loss: 3.758 | Acc: 43.279,68.685,89.327,%
Batch: 140 | Loss: 3.764 | Acc: 43.434,68.584,89.123,%
Batch: 160 | Loss: 3.784 | Acc: 43.372,68.556,89.029,%
Batch: 180 | Loss: 3.785 | Acc: 43.474,68.431,88.903,%
Batch: 200 | Loss: 3.792 | Acc: 43.474,68.532,88.744,%
Batch: 220 | Loss: 3.797 | Acc: 43.538,68.237,88.716,%
Batch: 240 | Loss: 3.808 | Acc: 43.513,68.244,88.557,%
Batch: 260 | Loss: 3.805 | Acc: 43.591,68.208,88.479,%
Batch: 280 | Loss: 3.800 | Acc: 43.717,68.275,88.440,%
Batch: 300 | Loss: 3.820 | Acc: 43.646,68.127,88.222,%
Batch: 320 | Loss: 3.819 | Acc: 43.550,68.071,88.108,%
Batch: 340 | Loss: 3.828 | Acc: 43.516,68.001,87.986,%
Batch: 360 | Loss: 3.825 | Acc: 43.534,68.116,87.974,%
Batch: 380 | Loss: 3.829 | Acc: 43.539,68.014,87.879,%
Batch: 0 | Loss: 4.927 | Acc: 43.750,68.750,73.438,%
Batch: 20 | Loss: 5.348 | Acc: 36.979,60.565,66.778,%
Batch: 40 | Loss: 5.358 | Acc: 37.671,59.642,66.292,%
Batch: 60 | Loss: 5.370 | Acc: 37.334,58.799,65.676,%
Train all parameters

Epoch: 82
Batch: 0 | Loss: 3.039 | Acc: 52.344,75.781,90.625,%
Batch: 20 | Loss: 3.688 | Acc: 46.243,69.940,88.653,%
Batch: 40 | Loss: 3.725 | Acc: 45.046,69.436,89.177,%
Batch: 60 | Loss: 3.692 | Acc: 45.236,68.891,88.870,%
Batch: 80 | Loss: 3.725 | Acc: 44.792,68.673,88.513,%
Batch: 100 | Loss: 3.717 | Acc: 45.135,68.603,88.591,%
Batch: 120 | Loss: 3.717 | Acc: 44.906,68.524,88.746,%
Batch: 140 | Loss: 3.728 | Acc: 44.614,68.334,88.774,%
Batch: 160 | Loss: 3.725 | Acc: 44.784,68.328,88.917,%
Batch: 180 | Loss: 3.727 | Acc: 44.881,68.215,88.791,%
Batch: 200 | Loss: 3.727 | Acc: 44.772,68.113,88.794,%
Batch: 220 | Loss: 3.731 | Acc: 44.772,68.188,88.564,%
Batch: 240 | Loss: 3.748 | Acc: 44.680,68.212,88.498,%
Batch: 260 | Loss: 3.754 | Acc: 44.537,68.077,88.353,%
Batch: 280 | Loss: 3.768 | Acc: 44.492,67.972,88.173,%
Batch: 300 | Loss: 3.775 | Acc: 44.461,67.971,88.076,%
Batch: 320 | Loss: 3.787 | Acc: 44.285,67.859,87.950,%
Batch: 340 | Loss: 3.790 | Acc: 44.233,67.806,87.857,%
Batch: 360 | Loss: 3.792 | Acc: 44.213,67.787,87.799,%
Batch: 380 | Loss: 3.795 | Acc: 44.199,67.758,87.750,%
Batch: 0 | Loss: 5.134 | Acc: 36.719,60.156,69.531,%
Batch: 20 | Loss: 5.410 | Acc: 35.975,57.775,68.080,%
Batch: 40 | Loss: 5.440 | Acc: 35.785,57.622,66.845,%
Batch: 60 | Loss: 5.470 | Acc: 35.745,57.480,66.176,%
Train all parameters

Epoch: 83
Batch: 0 | Loss: 4.183 | Acc: 35.938,66.406,90.625,%
Batch: 20 | Loss: 3.764 | Acc: 42.076,69.494,88.728,%
Batch: 40 | Loss: 3.729 | Acc: 43.064,68.902,89.139,%
Batch: 60 | Loss: 3.748 | Acc: 43.276,68.353,88.973,%
Batch: 80 | Loss: 3.749 | Acc: 43.326,68.441,88.879,%
Batch: 100 | Loss: 3.727 | Acc: 43.773,68.642,89.055,%
Batch: 120 | Loss: 3.730 | Acc: 43.718,68.795,89.037,%
Batch: 140 | Loss: 3.728 | Acc: 43.772,68.595,89.002,%
Batch: 160 | Loss: 3.730 | Acc: 43.842,68.653,89.029,%
Batch: 180 | Loss: 3.738 | Acc: 43.646,68.582,89.145,%
Batch: 200 | Loss: 3.744 | Acc: 43.715,68.497,89.016,%
Batch: 220 | Loss: 3.746 | Acc: 43.845,68.343,88.921,%
Batch: 240 | Loss: 3.770 | Acc: 43.643,68.244,88.797,%
Batch: 260 | Loss: 3.783 | Acc: 43.585,68.196,88.730,%
Batch: 280 | Loss: 3.788 | Acc: 43.522,68.108,88.684,%
Batch: 300 | Loss: 3.782 | Acc: 43.711,68.052,88.580,%
Batch: 320 | Loss: 3.785 | Acc: 43.679,68.032,88.444,%
Batch: 340 | Loss: 3.787 | Acc: 43.629,68.054,88.405,%
Batch: 360 | Loss: 3.791 | Acc: 43.689,68.047,88.338,%
Batch: 380 | Loss: 3.791 | Acc: 43.795,68.018,88.269,%
Batch: 0 | Loss: 5.240 | Acc: 40.625,57.812,68.750,%
Batch: 20 | Loss: 5.306 | Acc: 37.463,60.603,67.597,%
Batch: 40 | Loss: 5.310 | Acc: 37.386,59.585,66.921,%
Batch: 60 | Loss: 5.321 | Acc: 36.860,59.144,66.624,%
Train all parameters

Epoch: 84
Batch: 0 | Loss: 3.743 | Acc: 45.312,67.969,87.500,%
Batch: 20 | Loss: 3.706 | Acc: 44.643,69.494,90.365,%
Batch: 40 | Loss: 3.699 | Acc: 44.360,68.902,90.339,%
Batch: 60 | Loss: 3.679 | Acc: 44.570,69.198,90.382,%
Batch: 80 | Loss: 3.688 | Acc: 44.425,68.972,90.258,%
Batch: 100 | Loss: 3.653 | Acc: 44.748,68.951,90.122,%
Batch: 120 | Loss: 3.676 | Acc: 44.654,68.537,89.766,%
Batch: 140 | Loss: 3.724 | Acc: 44.204,68.052,89.484,%
Batch: 160 | Loss: 3.718 | Acc: 44.478,68.158,89.402,%
Batch: 180 | Loss: 3.735 | Acc: 44.359,67.990,89.166,%
Batch: 200 | Loss: 3.741 | Acc: 44.496,67.848,88.989,%
Batch: 220 | Loss: 3.732 | Acc: 44.570,67.866,88.946,%
Batch: 240 | Loss: 3.735 | Acc: 44.570,67.904,88.816,%
Batch: 260 | Loss: 3.730 | Acc: 44.537,67.954,88.811,%
Batch: 280 | Loss: 3.737 | Acc: 44.573,67.980,88.670,%
Batch: 300 | Loss: 3.742 | Acc: 44.521,67.922,88.598,%
Batch: 320 | Loss: 3.758 | Acc: 44.290,67.913,88.476,%
Batch: 340 | Loss: 3.770 | Acc: 44.240,67.875,88.302,%
Batch: 360 | Loss: 3.771 | Acc: 44.306,67.889,88.143,%
Batch: 380 | Loss: 3.780 | Acc: 44.330,67.774,88.043,%
Batch: 0 | Loss: 4.928 | Acc: 40.625,60.938,65.625,%
Batch: 20 | Loss: 5.195 | Acc: 38.058,59.821,68.824,%
Batch: 40 | Loss: 5.189 | Acc: 38.567,59.718,67.988,%
Batch: 60 | Loss: 5.211 | Acc: 38.512,59.426,67.610,%
Train classifier parameters

Epoch: 85
Batch: 0 | Loss: 4.057 | Acc: 43.750,63.281,90.625,%
Batch: 20 | Loss: 4.272 | Acc: 39.323,65.327,85.379,%
Batch: 40 | Loss: 4.492 | Acc: 37.919,63.834,83.346,%
Batch: 60 | Loss: 4.520 | Acc: 37.769,63.332,82.428,%
Batch: 80 | Loss: 4.552 | Acc: 37.355,62.905,82.002,%
Batch: 100 | Loss: 4.508 | Acc: 37.670,63.567,82.232,%
Batch: 120 | Loss: 4.464 | Acc: 38.281,63.772,82.102,%
Batch: 140 | Loss: 4.466 | Acc: 38.531,63.736,81.970,%
Batch: 160 | Loss: 4.478 | Acc: 38.373,63.529,81.944,%
Batch: 180 | Loss: 4.459 | Acc: 38.454,63.640,82.044,%
Batch: 200 | Loss: 4.454 | Acc: 38.732,63.697,82.163,%
Batch: 220 | Loss: 4.442 | Acc: 38.804,63.751,82.303,%
Batch: 240 | Loss: 4.426 | Acc: 38.819,63.819,82.475,%
Batch: 260 | Loss: 4.416 | Acc: 38.922,63.931,82.639,%
Batch: 280 | Loss: 4.416 | Acc: 38.862,64.074,82.632,%
Batch: 300 | Loss: 4.422 | Acc: 38.790,64.021,82.646,%
Batch: 320 | Loss: 4.430 | Acc: 38.756,63.989,82.662,%
Batch: 340 | Loss: 4.433 | Acc: 38.751,63.859,82.636,%
Batch: 360 | Loss: 4.429 | Acc: 38.781,63.837,82.657,%
Batch: 380 | Loss: 4.422 | Acc: 38.849,63.880,82.665,%
Batch: 0 | Loss: 5.236 | Acc: 41.406,63.281,61.719,%
Batch: 20 | Loss: 5.537 | Acc: 35.751,58.891,67.039,%
Batch: 40 | Loss: 5.531 | Acc: 35.423,58.460,66.616,%
Batch: 60 | Loss: 5.571 | Acc: 35.400,57.992,65.817,%
Train classifier parameters

Epoch: 86
Batch: 0 | Loss: 4.689 | Acc: 39.062,68.750,81.250,%
Batch: 20 | Loss: 4.140 | Acc: 41.741,65.662,85.305,%
Batch: 40 | Loss: 4.282 | Acc: 39.806,65.377,84.489,%
Batch: 60 | Loss: 4.260 | Acc: 39.677,65.856,84.324,%
Batch: 80 | Loss: 4.262 | Acc: 39.516,65.683,84.317,%
Batch: 100 | Loss: 4.310 | Acc: 39.302,65.323,84.213,%
Batch: 120 | Loss: 4.298 | Acc: 39.792,65.373,84.259,%
Batch: 140 | Loss: 4.296 | Acc: 39.689,65.348,84.264,%
Batch: 160 | Loss: 4.280 | Acc: 39.917,65.387,84.278,%
Batch: 180 | Loss: 4.285 | Acc: 39.874,65.327,84.293,%
Batch: 200 | Loss: 4.268 | Acc: 40.089,65.291,84.278,%
Batch: 220 | Loss: 4.267 | Acc: 40.102,65.307,84.403,%
Batch: 240 | Loss: 4.265 | Acc: 40.058,65.317,84.420,%
Batch: 260 | Loss: 4.266 | Acc: 40.140,65.284,84.315,%
Batch: 280 | Loss: 4.281 | Acc: 40.061,65.116,84.322,%
Batch: 300 | Loss: 4.279 | Acc: 39.994,65.207,84.367,%
Batch: 320 | Loss: 4.276 | Acc: 40.014,65.240,84.343,%
Batch: 340 | Loss: 4.264 | Acc: 40.061,65.359,84.419,%
Batch: 360 | Loss: 4.260 | Acc: 40.173,65.283,84.420,%
Batch: 380 | Loss: 4.257 | Acc: 40.145,65.369,84.471,%
Batch: 0 | Loss: 5.148 | Acc: 39.844,61.719,67.969,%
Batch: 20 | Loss: 5.435 | Acc: 36.905,58.519,66.890,%
Batch: 40 | Loss: 5.451 | Acc: 36.585,58.460,66.502,%
Batch: 60 | Loss: 5.503 | Acc: 36.501,58.005,65.689,%
Train classifier parameters

Epoch: 87
Batch: 0 | Loss: 3.976 | Acc: 42.969,74.219,86.719,%
Batch: 20 | Loss: 4.133 | Acc: 40.811,65.960,85.603,%
Batch: 40 | Loss: 4.203 | Acc: 40.111,64.710,84.947,%
Batch: 60 | Loss: 4.191 | Acc: 40.689,64.703,85.041,%
Batch: 80 | Loss: 4.175 | Acc: 40.779,65.095,85.436,%
Batch: 100 | Loss: 4.162 | Acc: 41.259,65.323,85.365,%
Batch: 120 | Loss: 4.136 | Acc: 41.522,65.812,85.550,%
Batch: 140 | Loss: 4.116 | Acc: 41.439,65.769,85.594,%
Batch: 160 | Loss: 4.129 | Acc: 41.460,65.805,85.477,%
Batch: 180 | Loss: 4.141 | Acc: 41.294,65.664,85.402,%
Batch: 200 | Loss: 4.155 | Acc: 41.247,65.621,85.382,%
Batch: 220 | Loss: 4.154 | Acc: 41.304,65.618,85.301,%
Batch: 240 | Loss: 4.150 | Acc: 41.406,65.619,85.257,%
Batch: 260 | Loss: 4.153 | Acc: 41.454,65.607,85.231,%
Batch: 280 | Loss: 4.149 | Acc: 41.479,65.669,85.270,%
Batch: 300 | Loss: 4.152 | Acc: 41.448,65.654,85.234,%
Batch: 320 | Loss: 4.144 | Acc: 41.516,65.705,85.293,%
Batch: 340 | Loss: 4.141 | Acc: 41.619,65.620,85.262,%
Batch: 360 | Loss: 4.142 | Acc: 41.569,65.590,85.260,%
Batch: 380 | Loss: 4.147 | Acc: 41.628,65.625,85.238,%
Batch: 0 | Loss: 5.000 | Acc: 39.844,62.500,65.625,%
Batch: 20 | Loss: 5.396 | Acc: 36.756,58.631,67.671,%
Batch: 40 | Loss: 5.409 | Acc: 36.947,58.594,67.550,%
Batch: 60 | Loss: 5.449 | Acc: 36.642,58.248,66.457,%
Train classifier parameters

Epoch: 88
Batch: 0 | Loss: 3.392 | Acc: 46.875,73.438,86.719,%
Batch: 20 | Loss: 4.041 | Acc: 42.113,66.257,86.310,%
Batch: 40 | Loss: 4.036 | Acc: 42.397,66.730,85.995,%
Batch: 60 | Loss: 4.086 | Acc: 41.432,66.522,85.976,%
Batch: 80 | Loss: 4.105 | Acc: 41.204,66.358,85.928,%
Batch: 100 | Loss: 4.105 | Acc: 40.888,66.507,85.852,%
Batch: 120 | Loss: 4.103 | Acc: 40.896,66.748,85.938,%
Batch: 140 | Loss: 4.116 | Acc: 40.930,66.816,85.904,%
Batch: 160 | Loss: 4.124 | Acc: 41.052,66.571,85.850,%
Batch: 180 | Loss: 4.135 | Acc: 40.992,66.372,85.799,%
Batch: 200 | Loss: 4.153 | Acc: 40.823,66.181,85.747,%
Batch: 220 | Loss: 4.159 | Acc: 40.837,66.063,85.648,%
Batch: 240 | Loss: 4.154 | Acc: 40.862,65.920,85.668,%
Batch: 260 | Loss: 4.154 | Acc: 41.026,65.849,85.593,%
Batch: 280 | Loss: 4.147 | Acc: 41.134,65.909,85.646,%
Batch: 300 | Loss: 4.143 | Acc: 41.040,65.960,85.623,%
Batch: 320 | Loss: 4.133 | Acc: 41.250,66.017,85.628,%
Batch: 340 | Loss: 4.128 | Acc: 41.223,65.987,85.628,%
Batch: 360 | Loss: 4.136 | Acc: 41.127,65.926,85.615,%
Batch: 380 | Loss: 4.132 | Acc: 41.150,65.900,85.609,%
Batch: 0 | Loss: 5.025 | Acc: 42.188,61.719,65.625,%
Batch: 20 | Loss: 5.394 | Acc: 37.240,59.710,67.262,%
Batch: 40 | Loss: 5.399 | Acc: 37.005,59.146,67.226,%
Batch: 60 | Loss: 5.442 | Acc: 36.962,58.811,66.368,%
Train classifier parameters

Epoch: 89
Batch: 0 | Loss: 3.502 | Acc: 44.531,71.094,87.500,%
Batch: 20 | Loss: 4.047 | Acc: 42.113,66.518,87.798,%
Batch: 40 | Loss: 4.000 | Acc: 42.759,66.692,87.043,%
Batch: 60 | Loss: 4.040 | Acc: 42.316,66.176,86.514,%
Batch: 80 | Loss: 4.065 | Acc: 41.802,66.522,86.236,%
Batch: 100 | Loss: 4.059 | Acc: 41.507,66.576,86.479,%
Batch: 120 | Loss: 4.104 | Acc: 41.368,66.284,86.344,%
Batch: 140 | Loss: 4.089 | Acc: 41.517,66.318,86.331,%
Batch: 160 | Loss: 4.074 | Acc: 41.663,66.241,86.360,%
Batch: 180 | Loss: 4.075 | Acc: 41.626,66.126,86.373,%
Batch: 200 | Loss: 4.083 | Acc: 41.639,66.142,86.280,%
Batch: 220 | Loss: 4.089 | Acc: 41.647,65.996,86.249,%
Batch: 240 | Loss: 4.079 | Acc: 41.721,65.995,86.220,%
Batch: 260 | Loss: 4.084 | Acc: 41.685,66.047,86.177,%
Batch: 280 | Loss: 4.088 | Acc: 41.520,66.117,86.207,%
Batch: 300 | Loss: 4.088 | Acc: 41.453,66.160,86.153,%
Batch: 320 | Loss: 4.075 | Acc: 41.618,66.212,86.166,%
Batch: 340 | Loss: 4.072 | Acc: 41.700,66.189,86.203,%
Batch: 360 | Loss: 4.073 | Acc: 41.692,66.164,86.171,%
Batch: 380 | Loss: 4.076 | Acc: 41.603,66.160,86.157,%
Batch: 0 | Loss: 5.082 | Acc: 39.062,64.062,68.750,%
Batch: 20 | Loss: 5.354 | Acc: 37.016,59.561,68.862,%
Batch: 40 | Loss: 5.376 | Acc: 36.986,59.032,67.816,%
Batch: 60 | Loss: 5.420 | Acc: 36.783,58.786,66.880,%
Train classifier parameters

Epoch: 90
Batch: 0 | Loss: 3.557 | Acc: 56.250,67.969,81.250,%
Batch: 20 | Loss: 4.073 | Acc: 42.188,67.001,86.496,%
Batch: 40 | Loss: 4.080 | Acc: 41.635,66.616,86.890,%
Batch: 60 | Loss: 4.103 | Acc: 41.906,66.983,86.770,%
Batch: 80 | Loss: 4.123 | Acc: 41.889,66.580,86.497,%
Batch: 100 | Loss: 4.118 | Acc: 41.530,66.368,86.448,%
Batch: 120 | Loss: 4.104 | Acc: 41.581,66.439,86.312,%
Batch: 140 | Loss: 4.109 | Acc: 41.739,66.318,86.281,%
Batch: 160 | Loss: 4.088 | Acc: 42.086,66.406,86.301,%
Batch: 180 | Loss: 4.097 | Acc: 41.890,66.514,86.278,%
Batch: 200 | Loss: 4.102 | Acc: 41.861,66.332,86.334,%
Batch: 220 | Loss: 4.096 | Acc: 41.944,66.321,86.280,%
Batch: 240 | Loss: 4.086 | Acc: 41.863,66.435,86.268,%
Batch: 260 | Loss: 4.100 | Acc: 41.646,66.466,86.309,%
Batch: 280 | Loss: 4.101 | Acc: 41.743,66.498,86.249,%
Batch: 300 | Loss: 4.098 | Acc: 41.689,66.437,86.270,%
Batch: 320 | Loss: 4.095 | Acc: 41.689,66.457,86.298,%
Batch: 340 | Loss: 4.096 | Acc: 41.553,66.425,86.240,%
Batch: 360 | Loss: 4.097 | Acc: 41.547,66.467,86.273,%
Batch: 380 | Loss: 4.102 | Acc: 41.511,66.486,86.186,%
Batch: 0 | Loss: 5.009 | Acc: 42.188,62.500,67.188,%
Batch: 20 | Loss: 5.361 | Acc: 37.202,59.449,68.415,%
Batch: 40 | Loss: 5.362 | Acc: 37.119,59.394,67.835,%
Batch: 60 | Loss: 5.407 | Acc: 37.231,59.004,66.714,%
Train classifier parameters

Epoch: 91
Batch: 0 | Loss: 5.016 | Acc: 36.719,65.625,83.594,%
Batch: 20 | Loss: 3.994 | Acc: 42.188,67.485,86.644,%
Batch: 40 | Loss: 3.935 | Acc: 42.721,67.816,86.738,%
Batch: 60 | Loss: 4.019 | Acc: 41.714,67.034,86.898,%
Batch: 80 | Loss: 4.006 | Acc: 41.995,66.879,86.786,%
Batch: 100 | Loss: 3.991 | Acc: 41.979,67.195,86.850,%
Batch: 120 | Loss: 3.998 | Acc: 41.826,67.278,86.667,%
Batch: 140 | Loss: 4.008 | Acc: 41.656,67.165,86.824,%
Batch: 160 | Loss: 3.997 | Acc: 41.882,67.197,86.792,%
Batch: 180 | Loss: 4.010 | Acc: 41.713,66.937,86.693,%
Batch: 200 | Loss: 4.029 | Acc: 41.581,66.947,86.812,%
Batch: 220 | Loss: 4.026 | Acc: 41.544,66.937,86.694,%
Batch: 240 | Loss: 4.034 | Acc: 41.442,66.818,86.638,%
Batch: 260 | Loss: 4.035 | Acc: 41.394,66.727,86.524,%
Batch: 280 | Loss: 4.040 | Acc: 41.465,66.681,86.471,%
Batch: 300 | Loss: 4.049 | Acc: 41.463,66.536,86.425,%
Batch: 320 | Loss: 4.048 | Acc: 41.491,66.530,86.458,%
Batch: 340 | Loss: 4.054 | Acc: 41.619,66.521,86.494,%
Batch: 360 | Loss: 4.056 | Acc: 41.642,66.532,86.453,%
Batch: 380 | Loss: 4.057 | Acc: 41.663,66.544,86.401,%
Batch: 0 | Loss: 5.041 | Acc: 42.188,63.281,64.062,%
Batch: 20 | Loss: 5.344 | Acc: 37.388,59.970,68.229,%
Batch: 40 | Loss: 5.341 | Acc: 37.386,60.061,68.007,%
Batch: 60 | Loss: 5.385 | Acc: 37.257,59.196,66.995,%
Train classifier parameters

Epoch: 92
Batch: 0 | Loss: 3.417 | Acc: 51.562,66.406,88.281,%
Batch: 20 | Loss: 4.001 | Acc: 43.266,67.485,86.942,%
Batch: 40 | Loss: 3.963 | Acc: 43.521,67.969,87.157,%
Batch: 60 | Loss: 3.939 | Acc: 43.609,67.200,86.911,%
Batch: 80 | Loss: 3.987 | Acc: 43.065,66.927,87.018,%
Batch: 100 | Loss: 3.986 | Acc: 42.783,66.816,86.688,%
Batch: 120 | Loss: 3.969 | Acc: 42.782,66.923,86.699,%
Batch: 140 | Loss: 3.987 | Acc: 42.426,66.927,86.780,%
Batch: 160 | Loss: 3.993 | Acc: 42.289,66.843,86.816,%
Batch: 180 | Loss: 3.998 | Acc: 42.205,66.816,86.736,%
Batch: 200 | Loss: 3.989 | Acc: 42.137,66.880,86.742,%
Batch: 220 | Loss: 3.995 | Acc: 42.230,66.802,86.609,%
Batch: 240 | Loss: 4.007 | Acc: 42.197,66.769,86.450,%
Batch: 260 | Loss: 4.024 | Acc: 42.110,66.571,86.351,%
Batch: 280 | Loss: 4.030 | Acc: 42.068,66.559,86.291,%
Batch: 300 | Loss: 4.032 | Acc: 42.084,66.539,86.254,%
Batch: 320 | Loss: 4.035 | Acc: 42.000,66.496,86.215,%
Batch: 340 | Loss: 4.038 | Acc: 41.972,66.569,86.238,%
Batch: 360 | Loss: 4.034 | Acc: 41.926,66.569,86.258,%
Batch: 380 | Loss: 4.029 | Acc: 41.960,66.634,86.339,%
Batch: 0 | Loss: 5.024 | Acc: 42.188,63.281,66.406,%
Batch: 20 | Loss: 5.314 | Acc: 37.388,59.189,68.118,%
Batch: 40 | Loss: 5.318 | Acc: 37.424,59.527,67.797,%
Batch: 60 | Loss: 5.369 | Acc: 37.218,59.068,66.842,%
Train classifier parameters

Epoch: 93
Batch: 0 | Loss: 3.295 | Acc: 53.906,74.219,92.188,%
Batch: 20 | Loss: 4.020 | Acc: 42.150,67.857,86.868,%
Batch: 40 | Loss: 3.988 | Acc: 42.302,67.397,87.462,%
Batch: 60 | Loss: 3.952 | Acc: 42.239,67.072,87.269,%
Batch: 80 | Loss: 3.924 | Acc: 42.795,67.139,87.510,%
Batch: 100 | Loss: 3.897 | Acc: 42.984,67.211,87.392,%
Batch: 120 | Loss: 3.925 | Acc: 42.743,67.020,87.377,%
Batch: 140 | Loss: 3.943 | Acc: 42.647,66.944,87.223,%
Batch: 160 | Loss: 3.944 | Acc: 42.498,66.989,87.078,%
Batch: 180 | Loss: 3.941 | Acc: 42.490,66.911,86.870,%
Batch: 200 | Loss: 3.949 | Acc: 42.413,66.985,86.983,%
Batch: 220 | Loss: 3.962 | Acc: 42.283,66.887,86.789,%
Batch: 240 | Loss: 3.970 | Acc: 42.175,66.941,86.790,%
Batch: 260 | Loss: 3.969 | Acc: 42.182,66.882,86.821,%
Batch: 280 | Loss: 3.970 | Acc: 42.182,66.979,86.791,%
Batch: 300 | Loss: 3.972 | Acc: 42.221,66.998,86.765,%
Batch: 320 | Loss: 3.973 | Acc: 42.175,66.942,86.775,%
Batch: 340 | Loss: 3.991 | Acc: 42.096,66.851,86.732,%
Batch: 360 | Loss: 4.001 | Acc: 42.006,66.882,86.693,%
Batch: 380 | Loss: 4.010 | Acc: 41.878,66.833,86.657,%
Batch: 0 | Loss: 4.994 | Acc: 42.969,61.719,70.312,%
Batch: 20 | Loss: 5.300 | Acc: 38.058,60.045,68.676,%
Batch: 40 | Loss: 5.337 | Acc: 37.481,59.470,68.064,%
Batch: 60 | Loss: 5.384 | Acc: 37.551,58.952,67.226,%
Train classifier parameters

Epoch: 94
Batch: 0 | Loss: 3.864 | Acc: 42.188,67.969,82.031,%
Batch: 20 | Loss: 3.862 | Acc: 43.118,67.969,87.946,%
Batch: 40 | Loss: 3.917 | Acc: 42.969,67.302,87.995,%
Batch: 60 | Loss: 3.965 | Acc: 42.239,66.803,87.474,%
Batch: 80 | Loss: 3.996 | Acc: 42.207,67.033,87.432,%
Batch: 100 | Loss: 4.012 | Acc: 41.894,67.017,87.129,%
Batch: 120 | Loss: 3.978 | Acc: 42.110,67.297,87.048,%
Batch: 140 | Loss: 3.963 | Acc: 42.293,67.088,86.724,%
Batch: 160 | Loss: 3.982 | Acc: 42.042,67.018,86.714,%
Batch: 180 | Loss: 3.991 | Acc: 42.049,66.890,86.650,%
Batch: 200 | Loss: 3.993 | Acc: 41.962,66.830,86.637,%
Batch: 220 | Loss: 3.989 | Acc: 42.085,66.859,86.641,%
Batch: 240 | Loss: 3.989 | Acc: 42.090,66.880,86.741,%
Batch: 260 | Loss: 3.984 | Acc: 42.125,67.008,86.821,%
Batch: 280 | Loss: 3.991 | Acc: 42.126,66.990,86.799,%
Batch: 300 | Loss: 4.000 | Acc: 42.076,66.855,86.698,%
Batch: 320 | Loss: 3.991 | Acc: 42.248,66.922,86.719,%
Batch: 340 | Loss: 3.988 | Acc: 42.231,67.009,86.712,%
Batch: 360 | Loss: 3.982 | Acc: 42.380,67.034,86.678,%
Batch: 380 | Loss: 3.989 | Acc: 42.325,66.966,86.641,%
Batch: 0 | Loss: 4.967 | Acc: 42.188,64.062,67.188,%
Batch: 20 | Loss: 5.303 | Acc: 37.463,59.859,68.006,%
Batch: 40 | Loss: 5.319 | Acc: 38.053,59.356,67.835,%
Batch: 60 | Loss: 5.360 | Acc: 38.012,59.016,67.175,%
Train classifier parameters

Epoch: 95
Batch: 0 | Loss: 4.715 | Acc: 35.938,64.844,89.062,%
Batch: 20 | Loss: 3.918 | Acc: 43.415,66.257,86.310,%
Batch: 40 | Loss: 3.943 | Acc: 42.816,65.606,87.062,%
Batch: 60 | Loss: 3.935 | Acc: 42.866,66.483,87.039,%
Batch: 80 | Loss: 3.926 | Acc: 43.181,66.937,87.172,%
Batch: 100 | Loss: 3.971 | Acc: 43.069,66.816,86.843,%
Batch: 120 | Loss: 3.965 | Acc: 43.020,66.942,86.841,%
Batch: 140 | Loss: 3.990 | Acc: 42.847,66.927,86.846,%
Batch: 160 | Loss: 3.985 | Acc: 42.862,66.964,86.927,%
Batch: 180 | Loss: 3.992 | Acc: 42.744,67.028,86.973,%
Batch: 200 | Loss: 3.977 | Acc: 43.004,67.090,86.859,%
Batch: 220 | Loss: 3.969 | Acc: 42.951,67.014,86.853,%
Batch: 240 | Loss: 3.961 | Acc: 42.797,67.019,86.929,%
Batch: 260 | Loss: 3.955 | Acc: 42.681,67.122,86.949,%
Batch: 280 | Loss: 3.979 | Acc: 42.493,66.957,86.911,%
Batch: 300 | Loss: 3.986 | Acc: 42.426,66.941,86.807,%
Batch: 320 | Loss: 3.979 | Acc: 42.516,66.993,86.758,%
Batch: 340 | Loss: 3.984 | Acc: 42.474,66.906,86.721,%
Batch: 360 | Loss: 3.982 | Acc: 42.506,66.906,86.812,%
Batch: 380 | Loss: 3.989 | Acc: 42.419,66.948,86.752,%
Batch: 0 | Loss: 4.943 | Acc: 42.188,64.062,66.406,%
Batch: 20 | Loss: 5.278 | Acc: 37.351,59.784,68.564,%
Batch: 40 | Loss: 5.312 | Acc: 37.443,59.756,67.969,%
Batch: 60 | Loss: 5.359 | Acc: 37.436,59.490,67.149,%
Train classifier parameters

Epoch: 96
Batch: 0 | Loss: 3.597 | Acc: 43.750,64.844,89.062,%
Batch: 20 | Loss: 4.074 | Acc: 40.216,66.964,88.095,%
Batch: 40 | Loss: 3.984 | Acc: 41.406,67.797,87.786,%
Batch: 60 | Loss: 3.943 | Acc: 41.457,68.571,88.166,%
Batch: 80 | Loss: 3.970 | Acc: 42.120,67.959,87.799,%
Batch: 100 | Loss: 3.962 | Acc: 42.257,67.899,87.539,%
Batch: 120 | Loss: 3.939 | Acc: 42.549,67.911,87.623,%
Batch: 140 | Loss: 3.939 | Acc: 42.797,67.775,87.533,%
Batch: 160 | Loss: 3.952 | Acc: 42.648,67.474,87.485,%
Batch: 180 | Loss: 3.983 | Acc: 42.377,67.382,87.254,%
Batch: 200 | Loss: 3.989 | Acc: 42.374,67.114,87.135,%
Batch: 220 | Loss: 4.004 | Acc: 42.350,66.873,86.899,%
Batch: 240 | Loss: 4.020 | Acc: 42.155,66.795,86.936,%
Batch: 260 | Loss: 4.022 | Acc: 42.047,66.700,86.892,%
Batch: 280 | Loss: 4.020 | Acc: 42.074,66.709,86.891,%
Batch: 300 | Loss: 4.022 | Acc: 42.055,66.783,86.846,%
Batch: 320 | Loss: 4.022 | Acc: 42.022,66.810,86.811,%
Batch: 340 | Loss: 4.021 | Acc: 41.940,66.910,86.856,%
Batch: 360 | Loss: 4.009 | Acc: 41.986,66.975,86.862,%
Batch: 380 | Loss: 4.010 | Acc: 41.991,66.925,86.823,%
Batch: 0 | Loss: 5.013 | Acc: 42.969,61.719,67.969,%
Batch: 20 | Loss: 5.302 | Acc: 37.500,59.561,68.899,%
Batch: 40 | Loss: 5.326 | Acc: 38.091,59.318,68.255,%
Batch: 60 | Loss: 5.368 | Acc: 37.679,59.068,67.226,%
Train classifier parameters

Epoch: 97
Batch: 0 | Loss: 3.327 | Acc: 49.219,74.219,89.844,%
Batch: 20 | Loss: 3.753 | Acc: 44.345,69.531,88.132,%
Batch: 40 | Loss: 3.800 | Acc: 43.921,68.655,88.338,%
Batch: 60 | Loss: 3.858 | Acc: 43.545,68.353,88.089,%
Batch: 80 | Loss: 3.853 | Acc: 43.383,68.287,88.059,%
Batch: 100 | Loss: 3.888 | Acc: 43.007,68.309,88.088,%
Batch: 120 | Loss: 3.904 | Acc: 42.846,68.027,88.004,%
Batch: 140 | Loss: 3.893 | Acc: 43.057,68.002,87.849,%
Batch: 160 | Loss: 3.898 | Acc: 43.042,67.891,87.612,%
Batch: 180 | Loss: 3.903 | Acc: 42.999,67.714,87.642,%
Batch: 200 | Loss: 3.936 | Acc: 42.774,67.568,87.543,%
Batch: 220 | Loss: 3.942 | Acc: 42.682,67.682,87.486,%
Batch: 240 | Loss: 3.940 | Acc: 42.768,67.651,87.403,%
Batch: 260 | Loss: 3.949 | Acc: 42.729,67.568,87.422,%
Batch: 280 | Loss: 3.961 | Acc: 42.643,67.449,87.419,%
Batch: 300 | Loss: 3.972 | Acc: 42.553,67.364,87.396,%
Batch: 320 | Loss: 3.977 | Acc: 42.543,67.312,87.354,%
Batch: 340 | Loss: 3.975 | Acc: 42.616,67.336,87.346,%
Batch: 360 | Loss: 3.977 | Acc: 42.551,67.339,87.286,%
Batch: 380 | Loss: 3.979 | Acc: 42.503,67.319,87.276,%
Batch: 0 | Loss: 4.967 | Acc: 46.094,58.594,68.750,%
Batch: 20 | Loss: 5.305 | Acc: 38.244,58.668,68.378,%
Batch: 40 | Loss: 5.328 | Acc: 38.110,58.784,67.835,%
Batch: 60 | Loss: 5.366 | Acc: 37.833,58.414,67.111,%
Train classifier parameters

Epoch: 98
Batch: 0 | Loss: 3.204 | Acc: 51.562,71.094,90.625,%
Batch: 20 | Loss: 3.841 | Acc: 43.936,67.894,87.835,%
Batch: 40 | Loss: 3.878 | Acc: 43.083,67.740,87.862,%
Batch: 60 | Loss: 3.928 | Acc: 43.097,67.405,87.462,%
Batch: 80 | Loss: 3.932 | Acc: 43.017,67.448,87.760,%
Batch: 100 | Loss: 3.942 | Acc: 43.015,67.358,87.755,%
Batch: 120 | Loss: 3.959 | Acc: 42.736,67.116,87.610,%
Batch: 140 | Loss: 3.949 | Acc: 42.675,67.398,87.616,%
Batch: 160 | Loss: 3.957 | Acc: 42.425,67.285,87.578,%
Batch: 180 | Loss: 3.972 | Acc: 42.162,67.377,87.642,%
Batch: 200 | Loss: 3.967 | Acc: 42.261,67.452,87.671,%
Batch: 220 | Loss: 3.972 | Acc: 42.315,67.354,87.507,%
Batch: 240 | Loss: 3.963 | Acc: 42.392,67.356,87.435,%
Batch: 260 | Loss: 3.957 | Acc: 42.595,67.364,87.401,%
Batch: 280 | Loss: 3.969 | Acc: 42.516,67.193,87.291,%
Batch: 300 | Loss: 3.970 | Acc: 42.444,67.297,87.230,%
Batch: 320 | Loss: 3.986 | Acc: 42.336,67.195,87.164,%
Batch: 340 | Loss: 3.980 | Acc: 42.391,67.149,87.106,%
Batch: 360 | Loss: 3.980 | Acc: 42.374,67.209,87.143,%
Batch: 380 | Loss: 3.980 | Acc: 42.347,67.216,87.217,%
Batch: 0 | Loss: 4.965 | Acc: 42.188,64.062,66.406,%
Batch: 20 | Loss: 5.277 | Acc: 37.872,59.449,68.713,%
Batch: 40 | Loss: 5.300 | Acc: 37.767,59.356,67.950,%
Batch: 60 | Loss: 5.342 | Acc: 37.654,58.837,67.123,%
Train classifier parameters

Epoch: 99
Batch: 0 | Loss: 4.347 | Acc: 36.719,64.844,89.844,%
Batch: 20 | Loss: 3.827 | Acc: 42.039,68.266,87.723,%
Batch: 40 | Loss: 3.905 | Acc: 42.378,67.702,87.538,%
Batch: 60 | Loss: 3.867 | Acc: 42.879,67.764,87.487,%
Batch: 80 | Loss: 3.939 | Acc: 42.371,67.323,87.201,%
Batch: 100 | Loss: 3.980 | Acc: 41.855,66.894,87.113,%
Batch: 120 | Loss: 3.992 | Acc: 41.768,66.897,87.158,%
Batch: 140 | Loss: 3.992 | Acc: 41.861,67.021,87.234,%
Batch: 160 | Loss: 3.977 | Acc: 42.149,67.018,87.165,%
Batch: 180 | Loss: 3.991 | Acc: 42.175,67.105,87.060,%
Batch: 200 | Loss: 3.983 | Acc: 42.230,67.094,87.107,%
Batch: 220 | Loss: 3.993 | Acc: 42.180,67.032,87.058,%
Batch: 240 | Loss: 3.992 | Acc: 42.181,66.999,87.017,%
Batch: 260 | Loss: 3.988 | Acc: 42.217,67.077,87.189,%
Batch: 280 | Loss: 3.989 | Acc: 42.132,67.224,87.191,%
Batch: 300 | Loss: 3.990 | Acc: 42.091,67.208,87.152,%
Batch: 320 | Loss: 4.001 | Acc: 42.078,67.068,87.079,%
Batch: 340 | Loss: 3.987 | Acc: 42.233,67.126,87.076,%
Batch: 360 | Loss: 3.991 | Acc: 42.235,67.177,87.059,%
Batch: 380 | Loss: 3.986 | Acc: 42.222,67.208,87.115,%
Batch: 0 | Loss: 4.925 | Acc: 46.875,64.844,65.625,%
Batch: 20 | Loss: 5.267 | Acc: 37.946,59.673,68.304,%
Batch: 40 | Loss: 5.295 | Acc: 38.053,59.451,68.007,%
Batch: 60 | Loss: 5.335 | Acc: 38.089,58.978,67.149,%
Train all parameters

Epoch: 100
Batch: 0 | Loss: 4.002 | Acc: 36.719,67.969,89.062,%
Batch: 20 | Loss: 5.004 | Acc: 37.500,59.040,74.814,%
Batch: 40 | Loss: 5.814 | Acc: 34.546,53.620,63.262,%
Batch: 60 | Loss: 6.123 | Acc: 33.863,51.562,58.235,%
Batch: 80 | Loss: 6.192 | Acc: 33.854,51.235,56.867,%
Batch: 100 | Loss: 6.177 | Acc: 33.718,51.199,56.467,%
Batch: 120 | Loss: 6.135 | Acc: 33.742,51.472,56.896,%
Batch: 140 | Loss: 6.080 | Acc: 33.849,51.768,57.364,%
Batch: 160 | Loss: 6.045 | Acc: 34.157,51.985,57.754,%
Batch: 180 | Loss: 5.999 | Acc: 34.366,52.296,58.205,%
Batch: 200 | Loss: 5.939 | Acc: 34.651,52.771,58.823,%
Batch: 220 | Loss: 5.866 | Acc: 35.132,53.213,59.488,%
Batch: 240 | Loss: 5.813 | Acc: 35.244,53.559,60.091,%
Batch: 260 | Loss: 5.771 | Acc: 35.462,53.784,60.372,%
Batch: 280 | Loss: 5.730 | Acc: 35.659,54.084,60.710,%
Batch: 300 | Loss: 5.690 | Acc: 35.880,54.386,61.127,%
Batch: 320 | Loss: 5.653 | Acc: 36.081,54.644,61.556,%
Batch: 340 | Loss: 5.614 | Acc: 36.199,54.850,61.952,%
Batch: 360 | Loss: 5.581 | Acc: 36.461,55.097,62.277,%
Batch: 380 | Loss: 5.560 | Acc: 36.530,55.268,62.547,%
Batch: 0 | Loss: 5.747 | Acc: 34.375,59.375,60.938,%
Batch: 20 | Loss: 6.153 | Acc: 31.399,52.865,58.036,%
Batch: 40 | Loss: 6.145 | Acc: 31.059,52.934,57.660,%
Batch: 60 | Loss: 6.200 | Acc: 30.558,52.177,57.454,%
Train all parameters

Epoch: 101
Batch: 0 | Loss: 4.420 | Acc: 46.094,65.625,78.125,%
Batch: 20 | Loss: 4.665 | Acc: 39.509,60.379,73.177,%
Batch: 40 | Loss: 4.655 | Acc: 40.434,61.128,73.304,%
Batch: 60 | Loss: 4.683 | Acc: 40.254,61.347,73.207,%
Batch: 80 | Loss: 4.722 | Acc: 39.882,61.275,72.888,%
Batch: 100 | Loss: 4.748 | Acc: 40.099,60.961,72.734,%
Batch: 120 | Loss: 4.727 | Acc: 40.509,61.086,72.927,%
Batch: 140 | Loss: 4.695 | Acc: 40.824,61.181,72.933,%
Batch: 160 | Loss: 4.693 | Acc: 40.659,61.253,73.074,%
Batch: 180 | Loss: 4.697 | Acc: 40.634,61.278,73.092,%
Batch: 200 | Loss: 4.700 | Acc: 40.711,61.427,73.006,%
Batch: 220 | Loss: 4.710 | Acc: 40.540,61.270,72.928,%
Batch: 240 | Loss: 4.694 | Acc: 40.687,61.404,72.971,%
Batch: 260 | Loss: 4.709 | Acc: 40.607,61.318,72.791,%
Batch: 280 | Loss: 4.721 | Acc: 40.539,61.274,72.653,%
Batch: 300 | Loss: 4.720 | Acc: 40.519,61.301,72.659,%
Batch: 320 | Loss: 4.733 | Acc: 40.479,61.266,72.649,%
Batch: 340 | Loss: 4.721 | Acc: 40.593,61.302,72.654,%
Batch: 360 | Loss: 4.719 | Acc: 40.567,61.368,72.700,%
Batch: 380 | Loss: 4.712 | Acc: 40.537,61.380,72.683,%
Batch: 0 | Loss: 5.416 | Acc: 39.062,59.375,62.500,%
Batch: 20 | Loss: 5.828 | Acc: 33.668,53.534,63.021,%
Batch: 40 | Loss: 5.866 | Acc: 33.727,53.049,61.986,%
Batch: 60 | Loss: 5.879 | Acc: 33.466,52.638,61.757,%
Train all parameters

Epoch: 102
Batch: 0 | Loss: 4.031 | Acc: 44.531,67.188,81.250,%
Batch: 20 | Loss: 4.435 | Acc: 40.662,63.876,79.204,%
Batch: 40 | Loss: 4.409 | Acc: 41.120,64.501,78.639,%
Batch: 60 | Loss: 4.410 | Acc: 41.406,64.600,78.663,%
Batch: 80 | Loss: 4.398 | Acc: 41.532,64.767,78.675,%
Batch: 100 | Loss: 4.417 | Acc: 41.631,64.751,78.458,%
Batch: 120 | Loss: 4.441 | Acc: 41.665,64.437,78.054,%
Batch: 140 | Loss: 4.448 | Acc: 41.783,64.273,77.909,%
Batch: 160 | Loss: 4.463 | Acc: 41.605,64.014,77.683,%
Batch: 180 | Loss: 4.457 | Acc: 41.644,63.942,77.806,%
Batch: 200 | Loss: 4.458 | Acc: 41.604,63.903,77.732,%
Batch: 220 | Loss: 4.466 | Acc: 41.435,63.822,77.687,%
Batch: 240 | Loss: 4.484 | Acc: 41.312,63.774,77.535,%
Batch: 260 | Loss: 4.465 | Acc: 41.433,63.757,77.481,%
Batch: 280 | Loss: 4.464 | Acc: 41.395,63.809,77.516,%
Batch: 300 | Loss: 4.463 | Acc: 41.404,63.847,77.458,%
Batch: 320 | Loss: 4.464 | Acc: 41.358,63.846,77.353,%
Batch: 340 | Loss: 4.462 | Acc: 41.404,63.893,77.225,%
Batch: 360 | Loss: 4.460 | Acc: 41.393,63.857,77.104,%
Batch: 380 | Loss: 4.455 | Acc: 41.462,63.958,77.159,%
Batch: 0 | Loss: 5.410 | Acc: 40.625,58.594,67.188,%
Batch: 20 | Loss: 5.659 | Acc: 34.747,55.618,62.984,%
Batch: 40 | Loss: 5.697 | Acc: 35.023,54.897,62.214,%
Batch: 60 | Loss: 5.739 | Acc: 34.618,54.457,61.603,%
Train all parameters

Epoch: 103
Batch: 0 | Loss: 5.111 | Acc: 39.062,63.281,78.906,%
Batch: 20 | Loss: 4.329 | Acc: 41.964,65.402,80.878,%
Batch: 40 | Loss: 4.270 | Acc: 42.321,65.739,80.907,%
Batch: 60 | Loss: 4.287 | Acc: 42.303,65.894,80.469,%
Batch: 80 | Loss: 4.290 | Acc: 42.130,65.374,80.469,%
Batch: 100 | Loss: 4.302 | Acc: 41.762,65.107,80.391,%
Batch: 120 | Loss: 4.295 | Acc: 42.065,65.141,80.365,%
Batch: 140 | Loss: 4.278 | Acc: 42.171,65.148,80.513,%
Batch: 160 | Loss: 4.278 | Acc: 42.047,64.936,80.328,%
Batch: 180 | Loss: 4.263 | Acc: 42.088,65.090,80.421,%
Batch: 200 | Loss: 4.266 | Acc: 42.141,65.112,80.333,%
Batch: 220 | Loss: 4.280 | Acc: 42.050,64.989,80.144,%
Batch: 240 | Loss: 4.288 | Acc: 42.136,64.977,79.976,%
Batch: 260 | Loss: 4.290 | Acc: 42.032,64.865,79.930,%
Batch: 280 | Loss: 4.285 | Acc: 42.115,64.833,79.827,%
Batch: 300 | Loss: 4.276 | Acc: 42.172,64.955,79.755,%
Batch: 320 | Loss: 4.290 | Acc: 42.095,64.868,79.607,%
Batch: 340 | Loss: 4.290 | Acc: 42.123,64.851,79.470,%
Batch: 360 | Loss: 4.299 | Acc: 42.136,64.766,79.341,%
Batch: 380 | Loss: 4.305 | Acc: 42.130,64.676,79.236,%
Batch: 0 | Loss: 5.316 | Acc: 35.938,57.031,62.500,%
Batch: 20 | Loss: 5.628 | Acc: 34.673,55.171,62.388,%
Batch: 40 | Loss: 5.700 | Acc: 35.023,54.878,61.833,%
Batch: 60 | Loss: 5.745 | Acc: 35.323,54.457,61.437,%
Train all parameters

Epoch: 104
Batch: 0 | Loss: 3.701 | Acc: 54.688,64.844,85.938,%
Batch: 20 | Loss: 4.163 | Acc: 42.932,66.629,82.031,%
Batch: 40 | Loss: 4.122 | Acc: 43.064,66.768,82.698,%
Batch: 60 | Loss: 4.085 | Acc: 43.430,66.355,82.595,%
Batch: 80 | Loss: 4.083 | Acc: 43.846,66.213,81.983,%
Batch: 100 | Loss: 4.106 | Acc: 43.626,65.903,81.877,%
Batch: 120 | Loss: 4.126 | Acc: 43.524,65.799,81.805,%
Batch: 140 | Loss: 4.121 | Acc: 43.578,65.791,81.799,%
Batch: 160 | Loss: 4.142 | Acc: 43.405,65.766,81.561,%
Batch: 180 | Loss: 4.148 | Acc: 43.292,65.750,81.518,%
Batch: 200 | Loss: 4.152 | Acc: 43.206,65.668,81.534,%
Batch: 220 | Loss: 4.168 | Acc: 43.001,65.526,81.367,%
Batch: 240 | Loss: 4.173 | Acc: 42.891,65.602,81.386,%
Batch: 260 | Loss: 4.165 | Acc: 42.864,65.718,81.463,%
Batch: 280 | Loss: 4.160 | Acc: 42.938,65.820,81.470,%
Batch: 300 | Loss: 4.172 | Acc: 42.826,65.734,81.312,%
Batch: 320 | Loss: 4.173 | Acc: 42.837,65.754,81.211,%
Batch: 340 | Loss: 4.182 | Acc: 42.827,65.724,81.195,%
Batch: 360 | Loss: 4.187 | Acc: 42.839,65.690,81.120,%
Batch: 380 | Loss: 4.190 | Acc: 42.854,65.656,81.090,%
Batch: 0 | Loss: 4.932 | Acc: 40.625,60.156,70.312,%
Batch: 20 | Loss: 5.337 | Acc: 36.868,59.449,67.671,%
Batch: 40 | Loss: 5.313 | Acc: 38.034,58.708,66.864,%
Batch: 60 | Loss: 5.320 | Acc: 37.628,58.427,66.547,%
Train all parameters

Epoch: 105
Batch: 0 | Loss: 4.099 | Acc: 47.656,66.406,88.281,%
Batch: 20 | Loss: 3.901 | Acc: 44.494,67.783,85.454,%
Batch: 40 | Loss: 3.912 | Acc: 44.322,67.721,84.889,%
Batch: 60 | Loss: 4.007 | Acc: 43.302,67.188,84.388,%
Batch: 80 | Loss: 4.032 | Acc: 42.930,66.869,83.738,%
Batch: 100 | Loss: 4.035 | Acc: 42.984,66.971,83.586,%
Batch: 120 | Loss: 4.022 | Acc: 43.259,67.097,83.710,%
Batch: 140 | Loss: 4.046 | Acc: 42.969,67.010,83.483,%
Batch: 160 | Loss: 4.031 | Acc: 43.250,66.925,83.492,%
Batch: 180 | Loss: 4.047 | Acc: 43.223,66.717,83.447,%
Batch: 200 | Loss: 4.050 | Acc: 43.206,66.729,83.364,%
Batch: 220 | Loss: 4.057 | Acc: 43.241,66.544,83.124,%
Batch: 240 | Loss: 4.052 | Acc: 43.387,66.546,83.072,%
Batch: 260 | Loss: 4.056 | Acc: 43.337,66.475,82.965,%
Batch: 280 | Loss: 4.062 | Acc: 43.169,66.417,82.860,%
Batch: 300 | Loss: 4.077 | Acc: 43.047,66.360,82.659,%
Batch: 320 | Loss: 4.070 | Acc: 43.139,66.474,82.618,%
Batch: 340 | Loss: 4.083 | Acc: 43.028,66.393,82.434,%
Batch: 360 | Loss: 4.083 | Acc: 42.934,66.478,82.416,%
Batch: 380 | Loss: 4.092 | Acc: 42.997,66.365,82.267,%
Batch: 0 | Loss: 4.948 | Acc: 39.844,63.281,67.188,%
Batch: 20 | Loss: 5.227 | Acc: 37.798,59.189,66.406,%
Batch: 40 | Loss: 5.213 | Acc: 37.957,58.937,66.044,%
Batch: 60 | Loss: 5.250 | Acc: 37.666,58.312,65.894,%
Train all parameters

Epoch: 106
Batch: 0 | Loss: 4.541 | Acc: 37.500,66.406,89.062,%
Batch: 20 | Loss: 3.920 | Acc: 43.452,68.638,86.012,%
Batch: 40 | Loss: 3.919 | Acc: 43.617,68.331,85.004,%
Batch: 60 | Loss: 3.912 | Acc: 43.955,67.969,85.195,%
Batch: 80 | Loss: 3.917 | Acc: 44.203,67.641,85.031,%
Batch: 100 | Loss: 3.918 | Acc: 43.967,67.706,85.079,%
Batch: 120 | Loss: 3.908 | Acc: 43.944,67.575,84.956,%
Batch: 140 | Loss: 3.928 | Acc: 43.750,67.586,84.813,%
Batch: 160 | Loss: 3.942 | Acc: 43.634,67.474,84.584,%
Batch: 180 | Loss: 3.951 | Acc: 43.439,67.576,84.643,%
Batch: 200 | Loss: 3.970 | Acc: 43.319,67.514,84.562,%
Batch: 220 | Loss: 3.977 | Acc: 43.322,67.495,84.474,%
Batch: 240 | Loss: 3.973 | Acc: 43.504,67.437,84.420,%
Batch: 260 | Loss: 3.978 | Acc: 43.520,67.328,84.219,%
Batch: 280 | Loss: 3.992 | Acc: 43.453,67.132,84.033,%
Batch: 300 | Loss: 4.007 | Acc: 43.348,67.066,83.980,%
Batch: 320 | Loss: 4.013 | Acc: 43.285,66.944,83.922,%
Batch: 340 | Loss: 4.013 | Acc: 43.383,67.013,83.933,%
Batch: 360 | Loss: 4.016 | Acc: 43.298,66.913,83.834,%
Batch: 380 | Loss: 4.026 | Acc: 43.278,66.823,83.694,%
Batch: 0 | Loss: 4.897 | Acc: 43.750,64.062,72.656,%
Batch: 20 | Loss: 5.318 | Acc: 37.277,59.115,67.336,%
Batch: 40 | Loss: 5.339 | Acc: 37.481,58.422,65.968,%
Batch: 60 | Loss: 5.367 | Acc: 37.244,57.787,65.548,%
Train all parameters

Epoch: 107
Batch: 0 | Loss: 3.897 | Acc: 34.375,68.750,93.750,%
Batch: 20 | Loss: 3.942 | Acc: 43.043,67.932,87.240,%
Batch: 40 | Loss: 3.812 | Acc: 44.322,68.579,86.662,%
Batch: 60 | Loss: 3.807 | Acc: 43.814,68.840,87.141,%
Batch: 80 | Loss: 3.833 | Acc: 43.480,68.731,87.172,%
Batch: 100 | Loss: 3.839 | Acc: 43.510,68.386,86.866,%
Batch: 120 | Loss: 3.867 | Acc: 43.472,68.246,86.512,%
Batch: 140 | Loss: 3.887 | Acc: 43.373,68.273,86.226,%
Batch: 160 | Loss: 3.915 | Acc: 43.148,68.177,86.025,%
Batch: 180 | Loss: 3.922 | Acc: 43.280,68.003,85.868,%
Batch: 200 | Loss: 3.902 | Acc: 43.579,68.163,85.790,%
Batch: 220 | Loss: 3.923 | Acc: 43.549,67.986,85.436,%
Batch: 240 | Loss: 3.932 | Acc: 43.543,67.897,85.318,%
Batch: 260 | Loss: 3.939 | Acc: 43.546,67.696,85.156,%
Batch: 280 | Loss: 3.951 | Acc: 43.466,67.577,85.101,%
Batch: 300 | Loss: 3.955 | Acc: 43.394,67.572,84.998,%
Batch: 320 | Loss: 3.957 | Acc: 43.436,67.557,84.869,%
Batch: 340 | Loss: 3.962 | Acc: 43.383,67.538,84.778,%
Batch: 360 | Loss: 3.977 | Acc: 43.261,67.477,84.659,%
Batch: 380 | Loss: 3.974 | Acc: 43.391,67.456,84.525,%
Batch: 0 | Loss: 4.884 | Acc: 42.188,63.281,73.438,%
Batch: 20 | Loss: 5.369 | Acc: 36.644,57.552,66.034,%
Batch: 40 | Loss: 5.425 | Acc: 36.871,56.784,64.882,%
Batch: 60 | Loss: 5.434 | Acc: 36.860,56.647,64.575,%
Train all parameters

Epoch: 108
Batch: 0 | Loss: 4.197 | Acc: 44.531,69.531,88.281,%
Batch: 20 | Loss: 3.810 | Acc: 44.940,69.531,87.054,%
Batch: 40 | Loss: 3.826 | Acc: 44.512,69.036,86.833,%
Batch: 60 | Loss: 3.847 | Acc: 43.545,68.558,86.757,%
Batch: 80 | Loss: 3.797 | Acc: 43.885,68.576,86.863,%
Batch: 100 | Loss: 3.800 | Acc: 43.665,68.502,86.525,%
Batch: 120 | Loss: 3.813 | Acc: 43.634,68.453,86.538,%
Batch: 140 | Loss: 3.838 | Acc: 43.645,68.323,86.242,%
Batch: 160 | Loss: 3.846 | Acc: 43.634,68.347,86.117,%
Batch: 180 | Loss: 3.862 | Acc: 43.551,68.128,86.054,%
Batch: 200 | Loss: 3.873 | Acc: 43.622,68.105,85.871,%
Batch: 220 | Loss: 3.870 | Acc: 43.686,68.146,85.754,%
Batch: 240 | Loss: 3.870 | Acc: 43.821,68.060,85.574,%
Batch: 260 | Loss: 3.858 | Acc: 44.007,68.020,85.653,%
Batch: 280 | Loss: 3.869 | Acc: 43.989,67.944,85.573,%
Batch: 300 | Loss: 3.878 | Acc: 43.903,67.777,85.473,%
Batch: 320 | Loss: 3.873 | Acc: 44.100,67.840,85.402,%
Batch: 340 | Loss: 3.883 | Acc: 44.091,67.749,85.216,%
Batch: 360 | Loss: 3.887 | Acc: 44.174,67.672,85.111,%
Batch: 380 | Loss: 3.890 | Acc: 44.203,67.708,85.019,%
Batch: 0 | Loss: 5.195 | Acc: 39.062,61.719,67.969,%
Batch: 20 | Loss: 5.247 | Acc: 38.839,58.557,67.671,%
Batch: 40 | Loss: 5.283 | Acc: 39.348,58.289,66.292,%
Batch: 60 | Loss: 5.312 | Acc: 38.870,58.133,65.920,%
Train all parameters

Epoch: 109
Batch: 0 | Loss: 3.570 | Acc: 44.531,68.750,88.281,%
Batch: 20 | Loss: 3.762 | Acc: 44.308,69.234,86.384,%
Batch: 40 | Loss: 3.774 | Acc: 44.512,69.284,87.024,%
Batch: 60 | Loss: 3.813 | Acc: 43.929,68.916,87.065,%
Batch: 80 | Loss: 3.767 | Acc: 44.531,69.290,87.105,%
Batch: 100 | Loss: 3.740 | Acc: 44.964,69.175,87.237,%
Batch: 120 | Loss: 3.753 | Acc: 44.854,69.144,87.261,%
Batch: 140 | Loss: 3.750 | Acc: 45.069,68.999,87.217,%
Batch: 160 | Loss: 3.765 | Acc: 44.924,68.910,87.049,%
Batch: 180 | Loss: 3.777 | Acc: 44.877,68.711,86.952,%
Batch: 200 | Loss: 3.788 | Acc: 44.846,68.750,86.851,%
Batch: 220 | Loss: 3.790 | Acc: 44.807,68.782,86.768,%
Batch: 240 | Loss: 3.794 | Acc: 44.813,68.737,86.725,%
Batch: 260 | Loss: 3.801 | Acc: 44.702,68.603,86.620,%
Batch: 280 | Loss: 3.815 | Acc: 44.715,68.402,86.402,%
Batch: 300 | Loss: 3.819 | Acc: 44.635,68.332,86.350,%
Batch: 320 | Loss: 3.826 | Acc: 44.651,68.222,86.210,%
Batch: 340 | Loss: 3.835 | Acc: 44.625,68.184,86.057,%
Batch: 360 | Loss: 3.854 | Acc: 44.432,68.101,85.983,%
Batch: 380 | Loss: 3.864 | Acc: 44.345,68.077,85.880,%
Batch: 0 | Loss: 5.065 | Acc: 36.719,58.594,70.312,%
Batch: 20 | Loss: 5.582 | Acc: 33.631,55.618,65.774,%
Batch: 40 | Loss: 5.615 | Acc: 33.460,55.297,64.672,%
Batch: 60 | Loss: 5.635 | Acc: 33.184,54.892,64.037,%
Train all parameters

Epoch: 110
Batch: 0 | Loss: 4.070 | Acc: 39.844,71.875,88.281,%
Batch: 20 | Loss: 3.718 | Acc: 45.312,70.573,87.946,%
Batch: 40 | Loss: 3.729 | Acc: 45.522,69.855,87.976,%
Batch: 60 | Loss: 3.722 | Acc: 45.697,69.557,88.179,%
Batch: 80 | Loss: 3.735 | Acc: 45.361,69.406,87.934,%
Batch: 100 | Loss: 3.734 | Acc: 44.848,69.268,88.142,%
Batch: 120 | Loss: 3.756 | Acc: 44.673,68.963,87.958,%
Batch: 140 | Loss: 3.787 | Acc: 44.310,68.977,87.844,%
Batch: 160 | Loss: 3.792 | Acc: 44.070,68.905,87.791,%
Batch: 180 | Loss: 3.782 | Acc: 44.337,68.905,87.824,%
Batch: 200 | Loss: 3.789 | Acc: 44.302,68.824,87.659,%
Batch: 220 | Loss: 3.805 | Acc: 44.096,68.584,87.429,%
Batch: 240 | Loss: 3.812 | Acc: 44.145,68.390,87.412,%
Batch: 260 | Loss: 3.816 | Acc: 44.241,68.325,87.252,%
Batch: 280 | Loss: 3.827 | Acc: 44.184,68.280,87.061,%
Batch: 300 | Loss: 3.833 | Acc: 44.119,68.345,86.971,%
Batch: 320 | Loss: 3.838 | Acc: 44.169,68.271,86.838,%
Batch: 340 | Loss: 3.845 | Acc: 44.126,68.129,86.673,%
Batch: 360 | Loss: 3.850 | Acc: 44.107,68.161,86.600,%
Batch: 380 | Loss: 3.840 | Acc: 44.277,68.209,86.555,%
Batch: 0 | Loss: 4.952 | Acc: 40.625,68.750,70.312,%
Batch: 20 | Loss: 5.333 | Acc: 37.091,59.933,67.225,%
Batch: 40 | Loss: 5.338 | Acc: 37.348,59.032,66.730,%
Batch: 60 | Loss: 5.354 | Acc: 37.013,58.927,66.073,%
Train all parameters

Epoch: 111
Batch: 0 | Loss: 3.562 | Acc: 41.406,71.094,81.250,%
Batch: 20 | Loss: 3.766 | Acc: 43.973,69.978,87.723,%
Batch: 40 | Loss: 3.713 | Acc: 44.360,70.293,88.643,%
Batch: 60 | Loss: 3.737 | Acc: 44.544,69.544,88.422,%
Batch: 80 | Loss: 3.750 | Acc: 44.280,69.223,88.117,%
Batch: 100 | Loss: 3.716 | Acc: 44.400,69.346,88.297,%
Batch: 120 | Loss: 3.754 | Acc: 44.254,69.131,88.113,%
Batch: 140 | Loss: 3.767 | Acc: 44.304,68.689,88.132,%
Batch: 160 | Loss: 3.759 | Acc: 44.327,68.803,88.019,%
Batch: 180 | Loss: 3.784 | Acc: 44.160,68.651,87.966,%
Batch: 200 | Loss: 3.813 | Acc: 44.022,68.389,87.655,%
Batch: 220 | Loss: 3.823 | Acc: 43.990,68.336,87.553,%
Batch: 240 | Loss: 3.819 | Acc: 43.967,68.325,87.400,%
Batch: 260 | Loss: 3.823 | Acc: 43.977,68.220,87.216,%
Batch: 280 | Loss: 3.828 | Acc: 44.098,68.049,87.022,%
Batch: 300 | Loss: 3.832 | Acc: 44.043,67.964,86.838,%
Batch: 320 | Loss: 3.828 | Acc: 44.115,67.959,86.797,%
Batch: 340 | Loss: 3.832 | Acc: 44.098,68.008,86.680,%
Batch: 360 | Loss: 3.837 | Acc: 44.085,67.999,86.530,%
Batch: 380 | Loss: 3.838 | Acc: 44.086,67.997,86.442,%
Batch: 0 | Loss: 4.995 | Acc: 39.844,64.062,71.094,%
Batch: 20 | Loss: 5.280 | Acc: 37.054,60.417,68.452,%
Batch: 40 | Loss: 5.274 | Acc: 36.795,59.870,67.721,%
Batch: 60 | Loss: 5.308 | Acc: 36.373,59.529,66.919,%
Train all parameters

Epoch: 112
Batch: 0 | Loss: 3.269 | Acc: 47.656,73.438,90.625,%
Batch: 20 | Loss: 3.570 | Acc: 46.280,71.466,88.690,%
Batch: 40 | Loss: 3.650 | Acc: 45.598,70.941,88.739,%
Batch: 60 | Loss: 3.658 | Acc: 45.312,70.940,88.986,%
Batch: 80 | Loss: 3.632 | Acc: 45.361,70.370,88.667,%
Batch: 100 | Loss: 3.655 | Acc: 45.104,70.305,88.823,%
Batch: 120 | Loss: 3.675 | Acc: 45.003,70.106,88.759,%
Batch: 140 | Loss: 3.663 | Acc: 45.152,70.002,88.747,%
Batch: 160 | Loss: 3.655 | Acc: 45.371,69.682,88.577,%
Batch: 180 | Loss: 3.667 | Acc: 45.243,69.557,88.424,%
Batch: 200 | Loss: 3.678 | Acc: 45.118,69.473,88.262,%
Batch: 220 | Loss: 3.699 | Acc: 45.086,69.340,88.020,%
Batch: 240 | Loss: 3.719 | Acc: 44.917,69.201,87.912,%
Batch: 260 | Loss: 3.732 | Acc: 44.753,69.043,87.751,%
Batch: 280 | Loss: 3.735 | Acc: 44.862,68.956,87.667,%
Batch: 300 | Loss: 3.748 | Acc: 44.806,68.786,87.477,%
Batch: 320 | Loss: 3.770 | Acc: 44.706,68.636,87.257,%
Batch: 340 | Loss: 3.780 | Acc: 44.614,68.601,87.026,%
Batch: 360 | Loss: 3.793 | Acc: 44.503,68.456,86.905,%
Batch: 380 | Loss: 3.790 | Acc: 44.552,68.529,86.862,%
Batch: 0 | Loss: 4.807 | Acc: 45.312,66.406,71.094,%
Batch: 20 | Loss: 5.163 | Acc: 39.100,61.868,68.118,%
Batch: 40 | Loss: 5.214 | Acc: 38.739,61.147,67.359,%
Batch: 60 | Loss: 5.244 | Acc: 38.384,60.835,66.432,%
Train all parameters

Epoch: 113
Batch: 0 | Loss: 3.752 | Acc: 40.625,73.438,91.406,%
Batch: 20 | Loss: 3.850 | Acc: 42.783,69.234,88.690,%
Batch: 40 | Loss: 3.806 | Acc: 43.083,69.417,88.815,%
Batch: 60 | Loss: 3.786 | Acc: 43.455,69.570,88.832,%
Batch: 80 | Loss: 3.780 | Acc: 43.972,68.933,88.339,%
Batch: 100 | Loss: 3.748 | Acc: 44.175,69.214,88.250,%
Batch: 120 | Loss: 3.759 | Acc: 43.963,69.396,88.397,%
Batch: 140 | Loss: 3.729 | Acc: 44.177,69.293,88.392,%
Batch: 160 | Loss: 3.730 | Acc: 44.187,69.211,88.422,%
Batch: 180 | Loss: 3.716 | Acc: 44.441,69.311,88.419,%
Batch: 200 | Loss: 3.722 | Acc: 44.364,69.294,88.355,%
Batch: 220 | Loss: 3.743 | Acc: 44.245,69.160,88.073,%
Batch: 240 | Loss: 3.732 | Acc: 44.340,69.239,88.142,%
Batch: 260 | Loss: 3.743 | Acc: 44.268,69.079,87.937,%
Batch: 280 | Loss: 3.747 | Acc: 44.242,69.120,87.839,%
Batch: 300 | Loss: 3.751 | Acc: 44.425,69.048,87.736,%
Batch: 320 | Loss: 3.754 | Acc: 44.531,68.903,87.585,%
Batch: 340 | Loss: 3.765 | Acc: 44.458,68.860,87.381,%
Batch: 360 | Loss: 3.772 | Acc: 44.451,68.718,87.197,%
Batch: 380 | Loss: 3.772 | Acc: 44.492,68.652,87.115,%
Batch: 0 | Loss: 4.684 | Acc: 38.281,67.188,67.969,%
Batch: 20 | Loss: 5.252 | Acc: 36.905,60.193,66.629,%
Batch: 40 | Loss: 5.267 | Acc: 37.176,59.585,65.816,%
Batch: 60 | Loss: 5.296 | Acc: 37.077,59.209,65.215,%
Train all parameters

Epoch: 114
Batch: 0 | Loss: 3.132 | Acc: 50.781,71.094,88.281,%
Batch: 20 | Loss: 3.548 | Acc: 46.912,70.871,88.914,%
Batch: 40 | Loss: 3.618 | Acc: 45.655,70.560,89.062,%
Batch: 60 | Loss: 3.661 | Acc: 45.274,70.236,89.395,%
Batch: 80 | Loss: 3.674 | Acc: 44.898,69.686,89.275,%
Batch: 100 | Loss: 3.621 | Acc: 45.746,69.926,89.209,%
Batch: 120 | Loss: 3.647 | Acc: 45.597,69.706,89.069,%
Batch: 140 | Loss: 3.670 | Acc: 45.274,69.465,88.802,%
Batch: 160 | Loss: 3.701 | Acc: 44.832,69.226,88.519,%
Batch: 180 | Loss: 3.703 | Acc: 44.877,69.169,88.523,%
Batch: 200 | Loss: 3.705 | Acc: 44.866,69.038,88.456,%
Batch: 220 | Loss: 3.707 | Acc: 44.754,69.093,88.394,%
Batch: 240 | Loss: 3.711 | Acc: 44.719,69.068,88.353,%
Batch: 260 | Loss: 3.717 | Acc: 44.741,69.007,88.284,%
Batch: 280 | Loss: 3.723 | Acc: 44.704,69.006,88.217,%
Batch: 300 | Loss: 3.720 | Acc: 44.754,69.085,88.172,%
Batch: 320 | Loss: 3.738 | Acc: 44.541,68.962,87.984,%
Batch: 340 | Loss: 3.750 | Acc: 44.534,68.892,87.908,%
Batch: 360 | Loss: 3.754 | Acc: 44.518,68.830,87.879,%
Batch: 380 | Loss: 3.759 | Acc: 44.498,68.805,87.801,%
Batch: 0 | Loss: 4.818 | Acc: 42.188,65.625,68.750,%
Batch: 20 | Loss: 5.261 | Acc: 37.909,60.528,67.262,%
Batch: 40 | Loss: 5.279 | Acc: 37.957,59.623,66.864,%
Batch: 60 | Loss: 5.310 | Acc: 37.769,59.516,66.060,%
Train all parameters

Epoch: 115
Batch: 0 | Loss: 4.385 | Acc: 39.844,64.844,89.062,%
Batch: 20 | Loss: 3.718 | Acc: 46.168,69.903,88.579,%
Batch: 40 | Loss: 3.620 | Acc: 45.598,70.255,89.196,%
Batch: 60 | Loss: 3.639 | Acc: 45.300,69.800,89.395,%
Batch: 80 | Loss: 3.647 | Acc: 45.284,69.579,89.169,%
Batch: 100 | Loss: 3.639 | Acc: 45.444,69.616,89.070,%
Batch: 120 | Loss: 3.650 | Acc: 45.396,69.512,89.069,%
Batch: 140 | Loss: 3.628 | Acc: 45.445,69.681,89.146,%
Batch: 160 | Loss: 3.634 | Acc: 45.400,69.720,89.126,%
Batch: 180 | Loss: 3.648 | Acc: 45.187,69.661,89.127,%
Batch: 200 | Loss: 3.646 | Acc: 45.068,69.636,89.144,%
Batch: 220 | Loss: 3.663 | Acc: 44.934,69.542,88.985,%
Batch: 240 | Loss: 3.671 | Acc: 44.846,69.599,88.894,%
Batch: 260 | Loss: 3.675 | Acc: 44.923,69.615,88.739,%
Batch: 280 | Loss: 3.687 | Acc: 44.973,69.376,88.540,%
Batch: 300 | Loss: 3.693 | Acc: 45.056,69.368,88.351,%
Batch: 320 | Loss: 3.702 | Acc: 44.952,69.210,88.281,%
Batch: 340 | Loss: 3.714 | Acc: 44.905,69.146,88.139,%
Batch: 360 | Loss: 3.721 | Acc: 44.884,68.958,88.026,%
Batch: 380 | Loss: 3.723 | Acc: 44.913,69.000,87.892,%
Batch: 0 | Loss: 4.828 | Acc: 46.875,65.625,72.656,%
Batch: 20 | Loss: 5.245 | Acc: 39.658,59.673,66.964,%
Batch: 40 | Loss: 5.253 | Acc: 39.444,59.318,66.768,%
Batch: 60 | Loss: 5.240 | Acc: 39.191,59.413,66.790,%
Train all parameters

Epoch: 116
Batch: 0 | Loss: 3.998 | Acc: 46.094,71.875,89.844,%
Batch: 20 | Loss: 3.747 | Acc: 43.601,70.201,89.062,%
Batch: 40 | Loss: 3.669 | Acc: 44.131,70.293,88.739,%
Batch: 60 | Loss: 3.697 | Acc: 43.763,70.517,88.998,%
Batch: 80 | Loss: 3.656 | Acc: 44.165,70.534,89.024,%
Batch: 100 | Loss: 3.664 | Acc: 44.230,70.042,89.225,%
Batch: 120 | Loss: 3.641 | Acc: 44.815,70.074,89.282,%
Batch: 140 | Loss: 3.640 | Acc: 44.864,70.196,89.312,%
Batch: 160 | Loss: 3.654 | Acc: 44.706,69.852,89.232,%
Batch: 180 | Loss: 3.659 | Acc: 44.648,69.782,89.140,%
Batch: 200 | Loss: 3.656 | Acc: 44.722,69.757,89.090,%
Batch: 220 | Loss: 3.665 | Acc: 44.793,69.648,89.027,%
Batch: 240 | Loss: 3.678 | Acc: 44.752,69.479,88.865,%
Batch: 260 | Loss: 3.680 | Acc: 44.777,69.435,88.733,%
Batch: 280 | Loss: 3.683 | Acc: 44.718,69.364,88.612,%
Batch: 300 | Loss: 3.694 | Acc: 44.583,69.313,88.541,%
Batch: 320 | Loss: 3.693 | Acc: 44.619,69.276,88.483,%
Batch: 340 | Loss: 3.703 | Acc: 44.673,69.204,88.284,%
Batch: 360 | Loss: 3.713 | Acc: 44.696,69.163,88.073,%
Batch: 380 | Loss: 3.719 | Acc: 44.792,69.103,87.927,%
Batch: 0 | Loss: 4.986 | Acc: 41.406,65.625,72.656,%
Batch: 20 | Loss: 5.276 | Acc: 37.872,60.751,67.671,%
Batch: 40 | Loss: 5.254 | Acc: 38.472,60.461,67.149,%
Batch: 60 | Loss: 5.288 | Acc: 37.910,59.823,66.650,%
Train all parameters

Epoch: 117
Batch: 0 | Loss: 3.350 | Acc: 53.125,71.875,84.375,%
Batch: 20 | Loss: 3.648 | Acc: 46.317,69.159,88.728,%
Batch: 40 | Loss: 3.616 | Acc: 45.579,70.560,89.710,%
Batch: 60 | Loss: 3.619 | Acc: 45.940,70.261,89.331,%
Batch: 80 | Loss: 3.614 | Acc: 45.833,70.004,89.390,%
Batch: 100 | Loss: 3.602 | Acc: 46.016,69.895,89.194,%
Batch: 120 | Loss: 3.611 | Acc: 45.874,69.867,89.256,%
Batch: 140 | Loss: 3.616 | Acc: 45.628,69.825,89.151,%
Batch: 160 | Loss: 3.633 | Acc: 45.264,69.648,89.111,%
Batch: 180 | Loss: 3.644 | Acc: 45.343,69.618,89.080,%
Batch: 200 | Loss: 3.654 | Acc: 45.301,69.586,89.035,%
Batch: 220 | Loss: 3.670 | Acc: 45.118,69.595,88.886,%
Batch: 240 | Loss: 3.661 | Acc: 45.300,69.697,88.884,%
Batch: 260 | Loss: 3.686 | Acc: 45.112,69.361,88.709,%
Batch: 280 | Loss: 3.690 | Acc: 45.090,69.273,88.634,%
Batch: 300 | Loss: 3.695 | Acc: 45.014,69.225,88.575,%
Batch: 320 | Loss: 3.700 | Acc: 45.030,69.091,88.522,%
Batch: 340 | Loss: 3.704 | Acc: 44.969,69.002,88.444,%
Batch: 360 | Loss: 3.708 | Acc: 44.994,69.025,88.407,%
Batch: 380 | Loss: 3.699 | Acc: 45.085,69.160,88.449,%
Batch: 0 | Loss: 5.038 | Acc: 35.938,62.500,75.000,%
Batch: 20 | Loss: 5.246 | Acc: 36.719,59.412,68.638,%
Batch: 40 | Loss: 5.282 | Acc: 37.081,58.994,67.492,%
Batch: 60 | Loss: 5.300 | Acc: 37.052,58.799,66.880,%
Train all parameters

Epoch: 118
Batch: 0 | Loss: 4.499 | Acc: 36.719,62.500,85.156,%
Batch: 20 | Loss: 3.599 | Acc: 45.610,69.903,89.435,%
Batch: 40 | Loss: 3.566 | Acc: 45.446,70.351,90.091,%
Batch: 60 | Loss: 3.570 | Acc: 45.774,70.031,89.985,%
Batch: 80 | Loss: 3.585 | Acc: 45.862,70.110,89.969,%
Batch: 100 | Loss: 3.596 | Acc: 45.784,70.235,89.929,%
Batch: 120 | Loss: 3.623 | Acc: 45.706,69.828,89.508,%
Batch: 140 | Loss: 3.624 | Acc: 45.772,69.731,89.467,%
Batch: 160 | Loss: 3.649 | Acc: 45.633,69.643,89.223,%
Batch: 180 | Loss: 3.659 | Acc: 45.382,69.484,89.239,%
Batch: 200 | Loss: 3.648 | Acc: 45.464,69.625,89.218,%
Batch: 220 | Loss: 3.658 | Acc: 45.465,69.651,89.052,%
Batch: 240 | Loss: 3.668 | Acc: 45.332,69.651,88.900,%
Batch: 260 | Loss: 3.668 | Acc: 45.420,69.603,88.844,%
Batch: 280 | Loss: 3.683 | Acc: 45.257,69.517,88.684,%
Batch: 300 | Loss: 3.678 | Acc: 45.323,69.526,88.577,%
Batch: 320 | Loss: 3.689 | Acc: 45.254,69.397,88.422,%
Batch: 340 | Loss: 3.697 | Acc: 45.127,69.282,88.352,%
Batch: 360 | Loss: 3.696 | Acc: 45.172,69.178,88.329,%
Batch: 380 | Loss: 3.702 | Acc: 45.163,69.150,88.322,%
Batch: 0 | Loss: 5.040 | Acc: 38.281,58.594,63.281,%
Batch: 20 | Loss: 5.233 | Acc: 37.240,60.007,66.369,%
Batch: 40 | Loss: 5.231 | Acc: 37.767,60.194,65.892,%
Batch: 60 | Loss: 5.257 | Acc: 37.718,59.529,65.779,%
Train all parameters

Epoch: 119
Batch: 0 | Loss: 3.902 | Acc: 50.000,74.219,91.406,%
Batch: 20 | Loss: 3.648 | Acc: 45.387,71.652,89.881,%
Batch: 40 | Loss: 3.636 | Acc: 45.732,70.808,89.710,%
Batch: 60 | Loss: 3.642 | Acc: 45.364,70.069,89.831,%
Batch: 80 | Loss: 3.629 | Acc: 45.100,70.071,89.824,%
Batch: 100 | Loss: 3.622 | Acc: 45.498,70.305,89.751,%
Batch: 120 | Loss: 3.630 | Acc: 45.442,70.080,89.527,%
Batch: 140 | Loss: 3.630 | Acc: 45.639,70.240,89.378,%
Batch: 160 | Loss: 3.648 | Acc: 45.395,70.099,89.305,%
Batch: 180 | Loss: 3.667 | Acc: 45.218,69.993,89.252,%
Batch: 200 | Loss: 3.686 | Acc: 45.095,69.729,89.132,%
Batch: 220 | Loss: 3.681 | Acc: 45.125,69.772,89.130,%
Batch: 240 | Loss: 3.668 | Acc: 45.277,69.713,89.192,%
Batch: 260 | Loss: 3.681 | Acc: 45.217,69.753,88.991,%
Batch: 280 | Loss: 3.685 | Acc: 45.251,69.681,89.018,%
Batch: 300 | Loss: 3.689 | Acc: 45.211,69.692,88.946,%
Batch: 320 | Loss: 3.697 | Acc: 45.198,69.675,88.880,%
Batch: 340 | Loss: 3.696 | Acc: 45.209,69.685,88.831,%
Batch: 360 | Loss: 3.701 | Acc: 45.122,69.577,88.740,%
Batch: 380 | Loss: 3.704 | Acc: 45.245,69.683,88.587,%
Batch: 0 | Loss: 5.017 | Acc: 44.531,61.719,71.875,%
Batch: 20 | Loss: 5.357 | Acc: 39.025,58.408,65.997,%
Batch: 40 | Loss: 5.358 | Acc: 38.929,58.232,65.320,%
Batch: 60 | Loss: 5.351 | Acc: 38.781,58.235,65.254,%
Train all parameters

Epoch: 120
Batch: 0 | Loss: 4.004 | Acc: 37.500,68.750,90.625,%
Batch: 20 | Loss: 3.609 | Acc: 45.275,70.871,89.472,%
Batch: 40 | Loss: 3.602 | Acc: 45.293,70.503,89.748,%
Batch: 60 | Loss: 3.642 | Acc: 44.839,70.210,89.498,%
Batch: 80 | Loss: 3.618 | Acc: 45.091,70.293,89.506,%
Batch: 100 | Loss: 3.619 | Acc: 45.212,70.220,89.503,%
Batch: 120 | Loss: 3.622 | Acc: 45.345,69.957,89.618,%
Batch: 140 | Loss: 3.645 | Acc: 45.069,69.603,89.461,%
Batch: 160 | Loss: 3.648 | Acc: 45.080,69.478,89.295,%
Batch: 180 | Loss: 3.648 | Acc: 45.166,69.415,89.287,%
Batch: 200 | Loss: 3.651 | Acc: 45.037,69.586,89.132,%
Batch: 220 | Loss: 3.638 | Acc: 45.270,69.680,89.098,%
Batch: 240 | Loss: 3.639 | Acc: 45.270,69.710,89.101,%
Batch: 260 | Loss: 3.646 | Acc: 45.280,69.699,88.949,%
Batch: 280 | Loss: 3.644 | Acc: 45.312,69.734,88.893,%
Batch: 300 | Loss: 3.658 | Acc: 45.268,69.666,88.748,%
Batch: 320 | Loss: 3.663 | Acc: 45.317,69.607,88.673,%
Batch: 340 | Loss: 3.658 | Acc: 45.457,69.627,88.597,%
Batch: 360 | Loss: 3.657 | Acc: 45.490,69.674,88.560,%
Batch: 380 | Loss: 3.667 | Acc: 45.388,69.576,88.521,%
Batch: 0 | Loss: 4.987 | Acc: 35.938,61.719,66.406,%
Batch: 20 | Loss: 5.250 | Acc: 37.388,59.003,67.932,%
Batch: 40 | Loss: 5.251 | Acc: 37.995,59.375,67.397,%
Batch: 60 | Loss: 5.284 | Acc: 37.590,58.760,66.957,%
Train all parameters

Epoch: 121
Batch: 0 | Loss: 3.277 | Acc: 50.781,71.094,90.625,%
Batch: 20 | Loss: 3.447 | Acc: 46.205,72.173,90.365,%
Batch: 40 | Loss: 3.501 | Acc: 45.789,70.484,90.816,%
Batch: 60 | Loss: 3.511 | Acc: 46.017,70.722,90.856,%
Batch: 80 | Loss: 3.530 | Acc: 45.824,70.505,91.001,%
Batch: 100 | Loss: 3.492 | Acc: 46.295,70.630,90.718,%
Batch: 120 | Loss: 3.501 | Acc: 46.404,70.810,90.638,%
Batch: 140 | Loss: 3.520 | Acc: 45.972,70.418,90.398,%
Batch: 160 | Loss: 3.519 | Acc: 46.069,70.575,90.300,%
Batch: 180 | Loss: 3.535 | Acc: 46.007,70.576,90.107,%
Batch: 200 | Loss: 3.546 | Acc: 45.899,70.425,90.232,%
Batch: 220 | Loss: 3.550 | Acc: 45.956,70.461,90.102,%
Batch: 240 | Loss: 3.565 | Acc: 45.789,70.300,89.999,%
Batch: 260 | Loss: 3.578 | Acc: 45.741,70.193,89.901,%
Batch: 280 | Loss: 3.595 | Acc: 45.513,70.026,89.769,%
Batch: 300 | Loss: 3.609 | Acc: 45.531,69.897,89.613,%
Batch: 320 | Loss: 3.617 | Acc: 45.585,69.767,89.447,%
Batch: 340 | Loss: 3.625 | Acc: 45.535,69.760,89.376,%
Batch: 360 | Loss: 3.628 | Acc: 45.609,69.748,89.296,%
Batch: 380 | Loss: 3.631 | Acc: 45.554,69.757,89.186,%
Batch: 0 | Loss: 4.931 | Acc: 36.719,66.406,73.438,%
Batch: 20 | Loss: 5.490 | Acc: 35.119,57.478,65.960,%
Batch: 40 | Loss: 5.477 | Acc: 35.861,57.127,65.358,%
Batch: 60 | Loss: 5.503 | Acc: 35.822,57.044,64.959,%
Train all parameters

Epoch: 122
Batch: 0 | Loss: 3.203 | Acc: 50.000,71.875,89.062,%
Batch: 20 | Loss: 3.455 | Acc: 48.251,70.424,89.621,%
Batch: 40 | Loss: 3.520 | Acc: 47.542,70.598,89.844,%
Batch: 60 | Loss: 3.514 | Acc: 47.067,71.657,90.151,%
Batch: 80 | Loss: 3.553 | Acc: 46.441,71.074,90.114,%
Batch: 100 | Loss: 3.561 | Acc: 46.171,71.171,90.176,%
Batch: 120 | Loss: 3.552 | Acc: 45.990,71.139,90.360,%
Batch: 140 | Loss: 3.541 | Acc: 46.061,71.005,90.348,%
Batch: 160 | Loss: 3.559 | Acc: 46.084,70.861,90.314,%
Batch: 180 | Loss: 3.564 | Acc: 46.055,70.748,90.241,%
Batch: 200 | Loss: 3.576 | Acc: 45.969,70.596,90.174,%
Batch: 220 | Loss: 3.592 | Acc: 45.850,70.493,90.066,%
Batch: 240 | Loss: 3.598 | Acc: 45.815,70.445,89.889,%
Batch: 260 | Loss: 3.606 | Acc: 45.797,70.435,89.823,%
Batch: 280 | Loss: 3.612 | Acc: 45.771,70.290,89.677,%
Batch: 300 | Loss: 3.610 | Acc: 45.691,70.344,89.569,%
Batch: 320 | Loss: 3.612 | Acc: 45.685,70.325,89.493,%
Batch: 340 | Loss: 3.623 | Acc: 45.624,70.090,89.365,%
Batch: 360 | Loss: 3.634 | Acc: 45.572,69.975,89.223,%
Batch: 380 | Loss: 3.642 | Acc: 45.501,69.892,89.118,%
Batch: 0 | Loss: 4.715 | Acc: 41.406,64.062,76.562,%
Batch: 20 | Loss: 5.553 | Acc: 34.970,55.990,65.699,%
Batch: 40 | Loss: 5.573 | Acc: 35.347,55.964,65.358,%
Batch: 60 | Loss: 5.582 | Acc: 35.515,56.212,64.946,%
Train all parameters

Epoch: 123
Batch: 0 | Loss: 3.127 | Acc: 48.438,74.219,90.625,%
Batch: 20 | Loss: 3.572 | Acc: 45.536,70.201,91.183,%
Batch: 40 | Loss: 3.540 | Acc: 44.893,71.113,91.616,%
Batch: 60 | Loss: 3.530 | Acc: 45.159,70.569,91.099,%
Batch: 80 | Loss: 3.516 | Acc: 45.380,70.997,91.127,%
Batch: 100 | Loss: 3.529 | Acc: 45.104,70.908,91.050,%
Batch: 120 | Loss: 3.523 | Acc: 45.551,70.952,90.954,%
Batch: 140 | Loss: 3.527 | Acc: 45.700,70.883,90.769,%
Batch: 160 | Loss: 3.553 | Acc: 45.720,70.545,90.601,%
Batch: 180 | Loss: 3.561 | Acc: 45.641,70.433,90.534,%
Batch: 200 | Loss: 3.573 | Acc: 45.561,70.375,90.384,%
Batch: 220 | Loss: 3.592 | Acc: 45.390,70.252,90.314,%
Batch: 240 | Loss: 3.592 | Acc: 45.403,70.163,90.106,%
Batch: 260 | Loss: 3.617 | Acc: 45.193,70.085,89.928,%
Batch: 280 | Loss: 3.617 | Acc: 45.190,70.068,89.810,%
Batch: 300 | Loss: 3.621 | Acc: 45.235,70.001,89.659,%
Batch: 320 | Loss: 3.613 | Acc: 45.366,70.086,89.617,%
Batch: 340 | Loss: 3.622 | Acc: 45.400,69.971,89.509,%
Batch: 360 | Loss: 3.629 | Acc: 45.414,69.869,89.361,%
Batch: 380 | Loss: 3.640 | Acc: 45.360,69.751,89.218,%
Batch: 0 | Loss: 4.958 | Acc: 33.594,64.062,67.188,%
Batch: 20 | Loss: 5.440 | Acc: 36.012,57.924,64.918,%
Batch: 40 | Loss: 5.438 | Acc: 36.204,58.079,65.111,%
Batch: 60 | Loss: 5.448 | Acc: 35.861,57.787,64.780,%
Train all parameters

Epoch: 124
Batch: 0 | Loss: 3.188 | Acc: 49.219,73.438,89.062,%
Batch: 20 | Loss: 3.585 | Acc: 44.792,70.647,89.695,%
Batch: 40 | Loss: 3.561 | Acc: 44.722,70.846,89.939,%
Batch: 60 | Loss: 3.544 | Acc: 45.018,70.991,90.087,%
Batch: 80 | Loss: 3.549 | Acc: 45.110,71.258,89.815,%
Batch: 100 | Loss: 3.571 | Acc: 45.065,70.846,89.658,%
Batch: 120 | Loss: 3.580 | Acc: 45.209,70.629,89.728,%
Batch: 140 | Loss: 3.585 | Acc: 45.357,70.423,89.589,%
Batch: 160 | Loss: 3.604 | Acc: 45.172,70.211,89.528,%
Batch: 180 | Loss: 3.599 | Acc: 45.312,70.338,89.593,%
Batch: 200 | Loss: 3.606 | Acc: 45.250,70.328,89.393,%
Batch: 220 | Loss: 3.618 | Acc: 45.256,70.107,89.317,%
Batch: 240 | Loss: 3.619 | Acc: 45.364,70.180,89.348,%
Batch: 260 | Loss: 3.626 | Acc: 45.321,70.166,89.350,%
Batch: 280 | Loss: 3.621 | Acc: 45.343,70.165,89.329,%
Batch: 300 | Loss: 3.627 | Acc: 45.331,70.100,89.226,%
Batch: 320 | Loss: 3.626 | Acc: 45.544,69.991,89.123,%
Batch: 340 | Loss: 3.625 | Acc: 45.539,70.028,89.056,%
Batch: 360 | Loss: 3.631 | Acc: 45.483,69.929,88.980,%
Batch: 380 | Loss: 3.645 | Acc: 45.388,69.810,88.818,%
Batch: 0 | Loss: 4.874 | Acc: 39.062,66.406,70.312,%
Batch: 20 | Loss: 5.170 | Acc: 38.318,60.379,67.411,%
Batch: 40 | Loss: 5.191 | Acc: 38.700,59.966,66.749,%
Batch: 60 | Loss: 5.221 | Acc: 38.589,59.516,66.419,%
Train all parameters

Epoch: 125
Batch: 0 | Loss: 4.153 | Acc: 34.375,64.844,90.625,%
Batch: 20 | Loss: 3.638 | Acc: 45.424,69.829,90.625,%
Batch: 40 | Loss: 3.596 | Acc: 45.427,70.732,90.263,%
Batch: 60 | Loss: 3.531 | Acc: 45.850,71.145,90.177,%
Batch: 80 | Loss: 3.571 | Acc: 45.515,70.988,90.239,%
Batch: 100 | Loss: 3.549 | Acc: 45.854,71.032,90.463,%
Batch: 120 | Loss: 3.555 | Acc: 45.881,70.622,90.470,%
Batch: 140 | Loss: 3.544 | Acc: 45.955,70.617,90.398,%
Batch: 160 | Loss: 3.527 | Acc: 46.113,70.453,90.426,%
Batch: 180 | Loss: 3.544 | Acc: 45.740,70.610,90.392,%
Batch: 200 | Loss: 3.548 | Acc: 45.756,70.581,90.267,%
Batch: 220 | Loss: 3.553 | Acc: 45.846,70.436,90.151,%
Batch: 240 | Loss: 3.559 | Acc: 45.821,70.423,90.084,%
Batch: 260 | Loss: 3.561 | Acc: 45.747,70.405,90.065,%
Batch: 280 | Loss: 3.571 | Acc: 45.746,70.388,89.974,%
Batch: 300 | Loss: 3.577 | Acc: 45.756,70.320,89.844,%
Batch: 320 | Loss: 3.583 | Acc: 45.743,70.174,89.617,%
Batch: 340 | Loss: 3.594 | Acc: 45.709,70.079,89.534,%
Batch: 360 | Loss: 3.603 | Acc: 45.624,70.033,89.400,%
Batch: 380 | Loss: 3.616 | Acc: 45.563,69.913,89.235,%
Batch: 0 | Loss: 4.974 | Acc: 39.062,67.969,72.656,%
Batch: 20 | Loss: 5.134 | Acc: 38.951,60.863,67.820,%
Batch: 40 | Loss: 5.167 | Acc: 39.367,60.747,66.845,%
Batch: 60 | Loss: 5.177 | Acc: 39.434,60.720,66.342,%
Train all parameters

Epoch: 126
Batch: 0 | Loss: 2.840 | Acc: 54.688,78.125,93.750,%
Batch: 20 | Loss: 3.429 | Acc: 47.024,71.615,90.030,%
Batch: 40 | Loss: 3.483 | Acc: 46.361,71.189,90.358,%
Batch: 60 | Loss: 3.521 | Acc: 46.414,71.183,90.459,%
Batch: 80 | Loss: 3.558 | Acc: 45.727,71.103,90.374,%
Batch: 100 | Loss: 3.550 | Acc: 46.055,70.738,90.207,%
Batch: 120 | Loss: 3.569 | Acc: 46.042,70.390,90.018,%
Batch: 140 | Loss: 3.592 | Acc: 45.667,70.235,89.921,%
Batch: 160 | Loss: 3.597 | Acc: 45.642,70.143,89.747,%
Batch: 180 | Loss: 3.610 | Acc: 45.541,70.066,89.749,%
Batch: 200 | Loss: 3.601 | Acc: 45.592,70.227,89.739,%
Batch: 220 | Loss: 3.601 | Acc: 45.645,70.090,89.632,%
Batch: 240 | Loss: 3.615 | Acc: 45.650,70.008,89.510,%
Batch: 260 | Loss: 3.604 | Acc: 45.714,70.118,89.598,%
Batch: 280 | Loss: 3.603 | Acc: 45.796,69.982,89.516,%
Batch: 300 | Loss: 3.600 | Acc: 45.811,69.988,89.506,%
Batch: 320 | Loss: 3.600 | Acc: 45.853,69.957,89.376,%
Batch: 340 | Loss: 3.606 | Acc: 45.775,69.870,89.312,%
Batch: 360 | Loss: 3.615 | Acc: 45.728,69.845,89.186,%
Batch: 380 | Loss: 3.611 | Acc: 45.757,69.806,89.130,%
Batch: 0 | Loss: 5.351 | Acc: 42.188,58.594,68.750,%
Batch: 20 | Loss: 5.698 | Acc: 36.458,56.362,62.760,%
Batch: 40 | Loss: 5.676 | Acc: 37.005,56.040,62.271,%
Batch: 60 | Loss: 5.703 | Acc: 36.783,55.264,62.026,%
Train all parameters

Epoch: 127
Batch: 0 | Loss: 3.313 | Acc: 50.000,70.312,92.969,%
Batch: 20 | Loss: 3.533 | Acc: 47.024,69.978,90.625,%
Batch: 40 | Loss: 3.535 | Acc: 46.856,70.675,91.235,%
Batch: 60 | Loss: 3.515 | Acc: 46.977,70.799,91.329,%
Batch: 80 | Loss: 3.504 | Acc: 47.270,70.679,91.522,%
Batch: 100 | Loss: 3.489 | Acc: 47.146,71.202,91.437,%
Batch: 120 | Loss: 3.501 | Acc: 46.707,71.358,91.245,%
Batch: 140 | Loss: 3.510 | Acc: 46.764,71.238,91.046,%
Batch: 160 | Loss: 3.516 | Acc: 46.501,71.011,90.965,%
Batch: 180 | Loss: 3.541 | Acc: 46.335,70.740,90.724,%
Batch: 200 | Loss: 3.548 | Acc: 46.339,70.604,90.551,%
Batch: 220 | Loss: 3.555 | Acc: 46.278,70.429,90.349,%
Batch: 240 | Loss: 3.549 | Acc: 46.411,70.442,90.259,%
Batch: 260 | Loss: 3.545 | Acc: 46.483,70.486,90.185,%
Batch: 280 | Loss: 3.550 | Acc: 46.433,70.549,90.027,%
Batch: 300 | Loss: 3.563 | Acc: 46.361,70.471,89.914,%
Batch: 320 | Loss: 3.576 | Acc: 46.169,70.274,89.751,%
Batch: 340 | Loss: 3.595 | Acc: 45.933,70.241,89.635,%
Batch: 360 | Loss: 3.592 | Acc: 46.024,70.139,89.519,%
Batch: 380 | Loss: 3.602 | Acc: 45.979,69.995,89.352,%
Batch: 0 | Loss: 4.914 | Acc: 39.062,60.938,67.188,%
Batch: 20 | Loss: 5.291 | Acc: 37.798,58.557,67.411,%
Batch: 40 | Loss: 5.325 | Acc: 37.671,58.384,66.482,%
Batch: 60 | Loss: 5.359 | Acc: 37.641,58.338,65.996,%
Train all parameters

Epoch: 128
Batch: 0 | Loss: 4.126 | Acc: 42.969,67.969,87.500,%
Batch: 20 | Loss: 3.475 | Acc: 47.842,71.243,91.592,%
Batch: 40 | Loss: 3.497 | Acc: 46.704,71.151,91.521,%
Batch: 60 | Loss: 3.458 | Acc: 46.990,71.414,91.560,%
Batch: 80 | Loss: 3.475 | Acc: 47.029,71.576,91.397,%
Batch: 100 | Loss: 3.487 | Acc: 46.921,71.218,91.282,%
Batch: 120 | Loss: 3.490 | Acc: 46.720,71.358,91.193,%
Batch: 140 | Loss: 3.515 | Acc: 46.260,71.077,91.212,%
Batch: 160 | Loss: 3.522 | Acc: 46.040,70.841,91.173,%
Batch: 180 | Loss: 3.522 | Acc: 46.016,70.740,90.949,%
Batch: 200 | Loss: 3.539 | Acc: 45.884,70.678,90.648,%
Batch: 220 | Loss: 3.552 | Acc: 45.839,70.528,90.395,%
Batch: 240 | Loss: 3.561 | Acc: 45.919,70.588,90.259,%
Batch: 260 | Loss: 3.560 | Acc: 45.905,70.552,90.179,%
Batch: 280 | Loss: 3.579 | Acc: 45.760,70.301,89.983,%
Batch: 300 | Loss: 3.589 | Acc: 45.746,70.255,89.805,%
Batch: 320 | Loss: 3.598 | Acc: 45.682,70.074,89.649,%
Batch: 340 | Loss: 3.616 | Acc: 45.560,69.955,89.502,%
Batch: 360 | Loss: 3.606 | Acc: 45.791,70.007,89.493,%
Batch: 380 | Loss: 3.609 | Acc: 45.768,69.995,89.409,%
Batch: 0 | Loss: 5.025 | Acc: 41.406,64.844,67.188,%
Batch: 20 | Loss: 5.332 | Acc: 38.876,59.301,65.402,%
Batch: 40 | Loss: 5.272 | Acc: 39.158,59.223,65.701,%
Batch: 60 | Loss: 5.289 | Acc: 38.845,59.106,65.523,%
Train all parameters

Epoch: 129
Batch: 0 | Loss: 3.222 | Acc: 50.781,73.438,90.625,%
Batch: 20 | Loss: 3.513 | Acc: 45.685,70.052,90.365,%
Batch: 40 | Loss: 3.474 | Acc: 46.322,69.970,91.273,%
Batch: 60 | Loss: 3.483 | Acc: 46.568,70.517,91.201,%
Batch: 80 | Loss: 3.496 | Acc: 46.142,70.853,91.223,%
Batch: 100 | Loss: 3.483 | Acc: 46.248,70.862,91.298,%
Batch: 120 | Loss: 3.483 | Acc: 46.262,70.945,91.006,%
Batch: 140 | Loss: 3.492 | Acc: 46.110,70.861,90.952,%
Batch: 160 | Loss: 3.495 | Acc: 46.312,70.584,90.892,%
Batch: 180 | Loss: 3.505 | Acc: 46.284,70.649,90.849,%
Batch: 200 | Loss: 3.520 | Acc: 45.985,70.503,90.847,%
Batch: 220 | Loss: 3.525 | Acc: 45.974,70.231,90.735,%
Batch: 240 | Loss: 3.540 | Acc: 45.860,70.170,90.622,%
Batch: 260 | Loss: 3.554 | Acc: 45.764,70.001,90.571,%
Batch: 280 | Loss: 3.553 | Acc: 45.807,70.087,90.439,%
Batch: 300 | Loss: 3.555 | Acc: 45.759,70.066,90.285,%
Batch: 320 | Loss: 3.565 | Acc: 45.790,69.950,90.097,%
Batch: 340 | Loss: 3.568 | Acc: 45.771,69.914,89.967,%
Batch: 360 | Loss: 3.571 | Acc: 45.760,69.862,89.820,%
Batch: 380 | Loss: 3.579 | Acc: 45.714,69.927,89.741,%
Batch: 0 | Loss: 4.914 | Acc: 42.188,64.062,69.531,%
Batch: 20 | Loss: 5.313 | Acc: 37.649,58.780,67.150,%
Batch: 40 | Loss: 5.297 | Acc: 37.938,58.479,67.130,%
Batch: 60 | Loss: 5.312 | Acc: 37.705,58.222,66.867,%
Train all parameters

Epoch: 130
Batch: 0 | Loss: 3.283 | Acc: 48.438,72.656,89.844,%
Batch: 20 | Loss: 3.251 | Acc: 48.028,73.363,91.629,%
Batch: 40 | Loss: 3.393 | Acc: 46.780,71.951,91.292,%
Batch: 60 | Loss: 3.391 | Acc: 46.849,72.016,91.445,%
Batch: 80 | Loss: 3.430 | Acc: 46.692,71.460,91.155,%
Batch: 100 | Loss: 3.442 | Acc: 46.798,71.334,91.182,%
Batch: 120 | Loss: 3.419 | Acc: 47.043,71.468,91.225,%
Batch: 140 | Loss: 3.449 | Acc: 46.709,71.293,91.013,%
Batch: 160 | Loss: 3.477 | Acc: 46.501,70.977,90.814,%
Batch: 180 | Loss: 3.486 | Acc: 46.426,70.766,90.638,%
Batch: 200 | Loss: 3.521 | Acc: 46.230,70.511,90.423,%
Batch: 220 | Loss: 3.519 | Acc: 46.401,70.493,90.268,%
Batch: 240 | Loss: 3.536 | Acc: 46.327,70.413,90.119,%
Batch: 260 | Loss: 3.561 | Acc: 46.157,70.226,89.838,%
Batch: 280 | Loss: 3.570 | Acc: 46.113,70.121,89.666,%
Batch: 300 | Loss: 3.580 | Acc: 46.109,70.126,89.496,%
Batch: 320 | Loss: 3.586 | Acc: 46.048,70.128,89.471,%
Batch: 340 | Loss: 3.587 | Acc: 46.016,70.102,89.457,%
Batch: 360 | Loss: 3.587 | Acc: 46.061,70.070,89.365,%
Batch: 380 | Loss: 3.594 | Acc: 46.004,70.025,89.294,%
Batch: 0 | Loss: 4.902 | Acc: 36.719,62.500,70.312,%
Batch: 20 | Loss: 5.219 | Acc: 39.137,59.003,67.336,%
Batch: 40 | Loss: 5.179 | Acc: 38.929,58.956,67.226,%
Batch: 60 | Loss: 5.192 | Acc: 39.011,59.106,66.534,%
Train all parameters

Epoch: 131
Batch: 0 | Loss: 4.140 | Acc: 38.281,67.969,94.531,%
Batch: 20 | Loss: 3.555 | Acc: 45.238,70.945,91.704,%
Batch: 40 | Loss: 3.429 | Acc: 46.684,72.294,91.787,%
Batch: 60 | Loss: 3.481 | Acc: 46.363,71.977,91.304,%
Batch: 80 | Loss: 3.526 | Acc: 45.766,71.885,91.348,%
Batch: 100 | Loss: 3.554 | Acc: 45.498,71.651,91.344,%
Batch: 120 | Loss: 3.533 | Acc: 45.661,71.475,91.309,%
Batch: 140 | Loss: 3.535 | Acc: 45.612,71.437,91.223,%
Batch: 160 | Loss: 3.530 | Acc: 45.754,71.390,91.159,%
Batch: 180 | Loss: 3.526 | Acc: 45.912,71.210,91.126,%
Batch: 200 | Loss: 3.545 | Acc: 45.771,71.047,90.951,%
Batch: 220 | Loss: 3.553 | Acc: 45.807,70.793,90.823,%
Batch: 240 | Loss: 3.544 | Acc: 45.977,70.841,90.735,%
Batch: 260 | Loss: 3.543 | Acc: 45.872,70.896,90.631,%
Batch: 280 | Loss: 3.548 | Acc: 45.835,70.860,90.542,%
Batch: 300 | Loss: 3.553 | Acc: 45.907,70.790,90.456,%
Batch: 320 | Loss: 3.556 | Acc: 45.936,70.692,90.418,%
Batch: 340 | Loss: 3.564 | Acc: 45.860,70.681,90.320,%
Batch: 360 | Loss: 3.566 | Acc: 45.793,70.644,90.212,%
Batch: 380 | Loss: 3.573 | Acc: 45.745,70.520,90.082,%
Batch: 0 | Loss: 4.838 | Acc: 44.531,63.281,67.188,%
Batch: 20 | Loss: 5.208 | Acc: 39.621,60.491,67.076,%
Batch: 40 | Loss: 5.220 | Acc: 39.615,60.480,67.149,%
Batch: 60 | Loss: 5.261 | Acc: 39.101,59.977,66.406,%
Train all parameters

Epoch: 132
Batch: 0 | Loss: 3.860 | Acc: 42.188,75.000,90.625,%
Batch: 20 | Loss: 3.658 | Acc: 45.945,71.354,90.327,%
Batch: 40 | Loss: 3.490 | Acc: 47.332,72.447,91.387,%
Batch: 60 | Loss: 3.454 | Acc: 47.118,72.157,91.534,%
Batch: 80 | Loss: 3.491 | Acc: 46.181,71.721,91.368,%
Batch: 100 | Loss: 3.499 | Acc: 46.016,71.481,91.290,%
Batch: 120 | Loss: 3.504 | Acc: 45.706,71.307,91.006,%
Batch: 140 | Loss: 3.524 | Acc: 45.639,70.983,90.963,%
Batch: 160 | Loss: 3.529 | Acc: 45.686,70.773,90.863,%
Batch: 180 | Loss: 3.528 | Acc: 45.783,70.679,90.724,%
Batch: 200 | Loss: 3.513 | Acc: 45.962,70.686,90.714,%
Batch: 220 | Loss: 3.518 | Acc: 45.956,70.659,90.643,%
Batch: 240 | Loss: 3.523 | Acc: 45.896,70.565,90.521,%
Batch: 260 | Loss: 3.530 | Acc: 45.854,70.615,90.478,%
Batch: 280 | Loss: 3.538 | Acc: 45.916,70.521,90.258,%
Batch: 300 | Loss: 3.542 | Acc: 45.881,70.471,90.168,%
Batch: 320 | Loss: 3.560 | Acc: 45.821,70.395,89.995,%
Batch: 340 | Loss: 3.563 | Acc: 45.835,70.409,89.846,%
Batch: 360 | Loss: 3.565 | Acc: 45.871,70.369,89.757,%
Batch: 380 | Loss: 3.568 | Acc: 45.835,70.360,89.678,%
Batch: 0 | Loss: 5.053 | Acc: 42.969,53.906,71.094,%
Batch: 20 | Loss: 5.226 | Acc: 38.765,57.515,67.336,%
Batch: 40 | Loss: 5.231 | Acc: 38.529,57.793,66.864,%
Batch: 60 | Loss: 5.262 | Acc: 38.128,57.979,66.496,%
Train all parameters

Epoch: 133
Batch: 0 | Loss: 3.817 | Acc: 39.844,66.406,85.156,%
Batch: 20 | Loss: 3.407 | Acc: 44.940,71.615,93.229,%
Batch: 40 | Loss: 3.487 | Acc: 45.122,71.875,91.902,%
Batch: 60 | Loss: 3.452 | Acc: 45.825,72.054,91.291,%
Batch: 80 | Loss: 3.492 | Acc: 45.891,71.422,91.020,%
Batch: 100 | Loss: 3.467 | Acc: 46.094,71.310,91.043,%
Batch: 120 | Loss: 3.466 | Acc: 46.287,71.268,90.786,%
Batch: 140 | Loss: 3.476 | Acc: 46.410,71.061,90.791,%
Batch: 160 | Loss: 3.483 | Acc: 46.293,70.963,90.717,%
Batch: 180 | Loss: 3.512 | Acc: 45.947,70.999,90.608,%
Batch: 200 | Loss: 3.504 | Acc: 46.082,71.113,90.598,%
Batch: 220 | Loss: 3.515 | Acc: 46.104,71.111,90.424,%
Batch: 240 | Loss: 3.534 | Acc: 46.107,70.912,90.320,%
Batch: 260 | Loss: 3.528 | Acc: 46.199,70.950,90.245,%
Batch: 280 | Loss: 3.524 | Acc: 46.236,70.935,90.275,%
Batch: 300 | Loss: 3.536 | Acc: 46.070,70.795,90.181,%
Batch: 320 | Loss: 3.543 | Acc: 45.977,70.717,90.046,%
Batch: 340 | Loss: 3.547 | Acc: 45.943,70.663,89.942,%
Batch: 360 | Loss: 3.548 | Acc: 45.947,70.670,89.889,%
Batch: 380 | Loss: 3.545 | Acc: 46.073,70.602,89.821,%
Batch: 0 | Loss: 4.862 | Acc: 43.750,66.406,71.094,%
Batch: 20 | Loss: 5.234 | Acc: 38.021,60.007,67.336,%
Batch: 40 | Loss: 5.263 | Acc: 37.957,59.280,67.188,%
Batch: 60 | Loss: 5.292 | Acc: 37.500,58.876,66.650,%
Train all parameters

Epoch: 134
Batch: 0 | Loss: 3.856 | Acc: 44.531,68.750,89.844,%
Batch: 20 | Loss: 3.456 | Acc: 45.164,71.503,91.295,%
Batch: 40 | Loss: 3.453 | Acc: 45.255,71.704,92.111,%
Batch: 60 | Loss: 3.454 | Acc: 45.287,71.094,91.778,%
Batch: 80 | Loss: 3.456 | Acc: 45.698,71.605,91.821,%
Batch: 100 | Loss: 3.466 | Acc: 45.684,71.597,91.785,%
Batch: 120 | Loss: 3.462 | Acc: 45.745,71.668,91.464,%
Batch: 140 | Loss: 3.492 | Acc: 45.667,71.310,91.318,%
Batch: 160 | Loss: 3.501 | Acc: 45.366,71.380,91.270,%
Batch: 180 | Loss: 3.517 | Acc: 45.274,71.314,91.130,%
Batch: 200 | Loss: 3.508 | Acc: 45.480,71.350,91.033,%
Batch: 220 | Loss: 3.508 | Acc: 45.624,71.271,90.826,%
Batch: 240 | Loss: 3.521 | Acc: 45.517,71.136,90.729,%
Batch: 260 | Loss: 3.533 | Acc: 45.510,71.055,90.613,%
Batch: 280 | Loss: 3.534 | Acc: 45.629,71.116,90.608,%
Batch: 300 | Loss: 3.546 | Acc: 45.562,71.008,90.430,%
Batch: 320 | Loss: 3.547 | Acc: 45.605,70.906,90.245,%
Batch: 340 | Loss: 3.561 | Acc: 45.574,70.901,90.142,%
Batch: 360 | Loss: 3.563 | Acc: 45.579,70.951,90.017,%
Batch: 380 | Loss: 3.576 | Acc: 45.520,70.854,89.918,%
Batch: 0 | Loss: 4.741 | Acc: 42.188,60.938,73.438,%
Batch: 20 | Loss: 5.093 | Acc: 40.737,60.789,66.741,%
Batch: 40 | Loss: 5.109 | Acc: 40.663,60.213,66.406,%
Batch: 60 | Loss: 5.151 | Acc: 40.561,59.798,65.817,%
Train classifier parameters

Epoch: 135
Batch: 0 | Loss: 3.762 | Acc: 49.219,63.281,89.844,%
Batch: 20 | Loss: 3.878 | Acc: 44.866,67.932,85.454,%
Batch: 40 | Loss: 4.102 | Acc: 42.778,65.892,84.032,%
Batch: 60 | Loss: 4.146 | Acc: 41.829,65.510,83.940,%
Batch: 80 | Loss: 4.198 | Acc: 41.387,65.972,83.902,%
Batch: 100 | Loss: 4.187 | Acc: 41.337,65.888,83.988,%
Batch: 120 | Loss: 4.183 | Acc: 41.135,66.064,83.923,%
Batch: 140 | Loss: 4.182 | Acc: 41.085,66.212,83.937,%
Batch: 160 | Loss: 4.148 | Acc: 41.299,66.440,84.200,%
Batch: 180 | Loss: 4.157 | Acc: 41.303,66.337,84.094,%
Batch: 200 | Loss: 4.167 | Acc: 41.371,66.216,84.095,%
Batch: 220 | Loss: 4.167 | Acc: 41.325,66.046,84.237,%
Batch: 240 | Loss: 4.169 | Acc: 41.306,65.985,84.210,%
Batch: 260 | Loss: 4.156 | Acc: 41.352,65.963,84.291,%
Batch: 280 | Loss: 4.151 | Acc: 41.470,65.978,84.247,%
Batch: 300 | Loss: 4.142 | Acc: 41.510,66.012,84.245,%
Batch: 320 | Loss: 4.144 | Acc: 41.508,66.063,84.341,%
Batch: 340 | Loss: 4.139 | Acc: 41.535,66.019,84.428,%
Batch: 360 | Loss: 4.132 | Acc: 41.558,66.012,84.581,%
Batch: 380 | Loss: 4.127 | Acc: 41.706,65.992,84.594,%
Batch: 0 | Loss: 5.212 | Acc: 40.625,59.375,69.531,%
Batch: 20 | Loss: 5.424 | Acc: 37.649,58.743,65.885,%
Batch: 40 | Loss: 5.449 | Acc: 37.595,58.232,65.206,%
Batch: 60 | Loss: 5.466 | Acc: 37.564,58.184,64.818,%
Train classifier parameters

Epoch: 136
Batch: 0 | Loss: 3.741 | Acc: 50.000,69.531,83.594,%
Batch: 20 | Loss: 3.859 | Acc: 45.610,68.229,86.942,%
Batch: 40 | Loss: 3.949 | Acc: 44.455,67.073,86.604,%
Batch: 60 | Loss: 3.993 | Acc: 43.916,67.085,86.885,%
Batch: 80 | Loss: 3.954 | Acc: 44.039,67.438,87.008,%
Batch: 100 | Loss: 3.946 | Acc: 43.889,67.744,86.982,%
Batch: 120 | Loss: 3.952 | Acc: 43.595,67.743,86.848,%
Batch: 140 | Loss: 3.937 | Acc: 43.678,67.570,86.885,%
Batch: 160 | Loss: 3.953 | Acc: 43.473,67.488,86.884,%
Batch: 180 | Loss: 3.960 | Acc: 43.374,67.373,86.909,%
Batch: 200 | Loss: 3.965 | Acc: 43.136,67.366,86.944,%
Batch: 220 | Loss: 3.978 | Acc: 42.948,67.262,86.903,%
Batch: 240 | Loss: 3.961 | Acc: 43.108,67.379,86.874,%
Batch: 260 | Loss: 3.970 | Acc: 43.041,67.298,86.791,%
Batch: 280 | Loss: 3.961 | Acc: 43.225,67.315,86.861,%
Batch: 300 | Loss: 3.979 | Acc: 43.106,67.195,86.734,%
Batch: 320 | Loss: 3.976 | Acc: 43.185,67.188,86.767,%
Batch: 340 | Loss: 3.970 | Acc: 43.255,67.256,86.822,%
Batch: 360 | Loss: 3.966 | Acc: 43.287,67.248,86.786,%
Batch: 380 | Loss: 3.959 | Acc: 43.373,67.343,86.784,%
Batch: 0 | Loss: 5.219 | Acc: 39.062,57.031,72.656,%
Batch: 20 | Loss: 5.394 | Acc: 37.909,57.254,66.518,%
Batch: 40 | Loss: 5.408 | Acc: 38.014,57.603,65.777,%
Batch: 60 | Loss: 5.425 | Acc: 38.179,57.812,65.202,%
Train classifier parameters

Epoch: 137
Batch: 0 | Loss: 4.195 | Acc: 39.062,71.875,92.188,%
Batch: 20 | Loss: 3.955 | Acc: 40.327,69.308,88.318,%
Batch: 40 | Loss: 3.952 | Acc: 41.254,68.674,87.633,%
Batch: 60 | Loss: 3.942 | Acc: 41.944,67.892,87.436,%
Batch: 80 | Loss: 3.949 | Acc: 42.197,67.757,87.114,%
Batch: 100 | Loss: 3.919 | Acc: 42.698,67.953,87.353,%
Batch: 120 | Loss: 3.925 | Acc: 42.698,67.878,87.300,%
Batch: 140 | Loss: 3.914 | Acc: 42.886,68.035,87.289,%
Batch: 160 | Loss: 3.919 | Acc: 42.930,67.920,87.146,%
Batch: 180 | Loss: 3.893 | Acc: 43.064,67.852,87.314,%
Batch: 200 | Loss: 3.895 | Acc: 43.101,67.903,87.247,%
Batch: 220 | Loss: 3.893 | Acc: 43.283,67.778,87.270,%
Batch: 240 | Loss: 3.902 | Acc: 43.286,67.891,87.286,%
Batch: 260 | Loss: 3.897 | Acc: 43.292,67.957,87.255,%
Batch: 280 | Loss: 3.909 | Acc: 43.205,67.860,87.255,%
Batch: 300 | Loss: 3.913 | Acc: 43.163,67.862,87.246,%
Batch: 320 | Loss: 3.917 | Acc: 43.188,67.913,87.232,%
Batch: 340 | Loss: 3.913 | Acc: 43.198,67.939,87.298,%
Batch: 360 | Loss: 3.916 | Acc: 43.252,67.951,87.303,%
Batch: 380 | Loss: 3.923 | Acc: 43.221,67.993,87.299,%
Batch: 0 | Loss: 5.037 | Acc: 41.406,62.500,73.438,%
Batch: 20 | Loss: 5.307 | Acc: 37.649,58.743,67.076,%
Batch: 40 | Loss: 5.330 | Acc: 38.072,58.632,66.444,%
Batch: 60 | Loss: 5.357 | Acc: 37.859,58.735,65.856,%
Train classifier parameters

Epoch: 138
Batch: 0 | Loss: 2.664 | Acc: 56.250,73.438,92.969,%
Batch: 20 | Loss: 3.929 | Acc: 43.601,67.001,86.570,%
Batch: 40 | Loss: 3.871 | Acc: 44.379,68.197,87.691,%
Batch: 60 | Loss: 3.864 | Acc: 44.160,68.020,87.910,%
Batch: 80 | Loss: 3.894 | Acc: 43.750,67.911,87.934,%
Batch: 100 | Loss: 3.888 | Acc: 43.742,68.247,88.080,%
Batch: 120 | Loss: 3.866 | Acc: 43.879,68.208,88.055,%
Batch: 140 | Loss: 3.860 | Acc: 43.822,68.157,88.037,%
Batch: 160 | Loss: 3.826 | Acc: 44.153,68.328,88.107,%
Batch: 180 | Loss: 3.800 | Acc: 44.363,68.573,88.109,%
Batch: 200 | Loss: 3.807 | Acc: 44.279,68.571,88.052,%
Batch: 220 | Loss: 3.811 | Acc: 44.220,68.527,88.101,%
Batch: 240 | Loss: 3.824 | Acc: 44.097,68.393,88.006,%
Batch: 260 | Loss: 3.822 | Acc: 44.160,68.406,87.946,%
Batch: 280 | Loss: 3.828 | Acc: 44.114,68.319,87.936,%
Batch: 300 | Loss: 3.829 | Acc: 44.272,68.189,87.957,%
Batch: 320 | Loss: 3.839 | Acc: 44.144,68.178,87.931,%
Batch: 340 | Loss: 3.842 | Acc: 44.153,68.143,87.938,%
Batch: 360 | Loss: 3.841 | Acc: 44.111,67.997,87.864,%
Batch: 380 | Loss: 3.847 | Acc: 44.078,67.946,87.851,%
Batch: 0 | Loss: 5.060 | Acc: 43.750,60.938,73.438,%
Batch: 20 | Loss: 5.305 | Acc: 37.649,58.966,67.188,%
Batch: 40 | Loss: 5.324 | Acc: 38.396,58.765,66.521,%
Batch: 60 | Loss: 5.345 | Acc: 38.384,58.530,66.060,%
Train classifier parameters

Epoch: 139
Batch: 0 | Loss: 3.316 | Acc: 47.656,71.875,87.500,%
Batch: 20 | Loss: 3.836 | Acc: 44.420,68.304,89.025,%
Batch: 40 | Loss: 3.841 | Acc: 43.960,67.854,88.815,%
Batch: 60 | Loss: 3.778 | Acc: 44.390,68.058,88.998,%
Batch: 80 | Loss: 3.837 | Acc: 43.875,67.921,88.571,%
Batch: 100 | Loss: 3.837 | Acc: 44.106,68.077,88.629,%
Batch: 120 | Loss: 3.832 | Acc: 44.312,68.266,88.669,%
Batch: 140 | Loss: 3.823 | Acc: 44.149,68.368,88.691,%
Batch: 160 | Loss: 3.816 | Acc: 44.221,68.532,88.631,%
Batch: 180 | Loss: 3.806 | Acc: 44.320,68.431,88.592,%
Batch: 200 | Loss: 3.817 | Acc: 44.092,68.486,88.608,%
Batch: 220 | Loss: 3.811 | Acc: 44.241,68.467,88.373,%
Batch: 240 | Loss: 3.799 | Acc: 44.272,68.552,88.359,%
Batch: 260 | Loss: 3.807 | Acc: 44.199,68.460,88.314,%
Batch: 280 | Loss: 3.811 | Acc: 44.114,68.377,88.265,%
Batch: 300 | Loss: 3.808 | Acc: 44.134,68.368,88.177,%
Batch: 320 | Loss: 3.808 | Acc: 44.098,68.382,88.208,%
Batch: 340 | Loss: 3.813 | Acc: 44.052,68.349,88.213,%
Batch: 360 | Loss: 3.819 | Acc: 44.105,68.330,88.132,%
Batch: 380 | Loss: 3.822 | Acc: 43.980,68.356,88.138,%
Batch: 0 | Loss: 5.078 | Acc: 41.406,59.375,71.094,%
Batch: 20 | Loss: 5.280 | Acc: 38.393,59.375,67.522,%
Batch: 40 | Loss: 5.283 | Acc: 38.453,59.585,66.806,%
Batch: 60 | Loss: 5.305 | Acc: 38.550,59.516,66.137,%
Train classifier parameters

Epoch: 140
Batch: 0 | Loss: 3.551 | Acc: 46.094,70.312,86.719,%
Batch: 20 | Loss: 3.879 | Acc: 45.871,67.522,86.942,%
Batch: 40 | Loss: 3.853 | Acc: 44.684,68.998,87.862,%
Batch: 60 | Loss: 3.820 | Acc: 44.698,69.429,88.204,%
Batch: 80 | Loss: 3.830 | Acc: 44.232,69.232,88.204,%
Batch: 100 | Loss: 3.790 | Acc: 44.640,69.400,88.374,%
Batch: 120 | Loss: 3.797 | Acc: 44.531,69.267,88.352,%
Batch: 140 | Loss: 3.806 | Acc: 44.448,69.138,88.392,%
Batch: 160 | Loss: 3.819 | Acc: 44.361,69.007,88.407,%
Batch: 180 | Loss: 3.813 | Acc: 44.410,69.009,88.355,%
Batch: 200 | Loss: 3.802 | Acc: 44.512,69.154,88.355,%
Batch: 220 | Loss: 3.799 | Acc: 44.450,69.171,88.338,%
Batch: 240 | Loss: 3.805 | Acc: 44.415,69.136,88.395,%
Batch: 260 | Loss: 3.815 | Acc: 44.220,68.974,88.305,%
Batch: 280 | Loss: 3.804 | Acc: 44.367,69.117,88.298,%
Batch: 300 | Loss: 3.802 | Acc: 44.435,69.051,88.229,%
Batch: 320 | Loss: 3.809 | Acc: 44.363,68.989,88.181,%
Batch: 340 | Loss: 3.816 | Acc: 44.320,68.871,88.141,%
Batch: 360 | Loss: 3.812 | Acc: 44.334,68.919,88.149,%
Batch: 380 | Loss: 3.816 | Acc: 44.267,68.900,88.117,%
Batch: 0 | Loss: 5.070 | Acc: 43.750,61.719,71.875,%
Batch: 20 | Loss: 5.282 | Acc: 38.430,60.007,67.411,%
Batch: 40 | Loss: 5.302 | Acc: 38.662,59.432,66.673,%
Batch: 60 | Loss: 5.315 | Acc: 38.537,59.132,65.984,%
Train classifier parameters

Epoch: 141
Batch: 0 | Loss: 3.945 | Acc: 42.969,71.094,91.406,%
Batch: 20 | Loss: 3.623 | Acc: 45.052,71.168,89.062,%
Batch: 40 | Loss: 3.711 | Acc: 44.550,70.236,88.948,%
Batch: 60 | Loss: 3.757 | Acc: 44.019,69.992,88.883,%
Batch: 80 | Loss: 3.776 | Acc: 44.078,69.956,88.879,%
Batch: 100 | Loss: 3.781 | Acc: 44.276,69.980,88.946,%
Batch: 120 | Loss: 3.797 | Acc: 44.208,69.886,88.979,%
Batch: 140 | Loss: 3.825 | Acc: 43.861,69.581,88.730,%
Batch: 160 | Loss: 3.819 | Acc: 44.012,69.468,88.674,%
Batch: 180 | Loss: 3.828 | Acc: 43.780,69.311,88.605,%
Batch: 200 | Loss: 3.819 | Acc: 43.882,69.321,88.608,%
Batch: 220 | Loss: 3.813 | Acc: 43.994,69.213,88.614,%
Batch: 240 | Loss: 3.799 | Acc: 44.184,69.120,88.557,%
Batch: 260 | Loss: 3.822 | Acc: 43.972,68.966,88.485,%
Batch: 280 | Loss: 3.824 | Acc: 43.956,68.828,88.373,%
Batch: 300 | Loss: 3.827 | Acc: 43.898,68.833,88.411,%
Batch: 320 | Loss: 3.823 | Acc: 44.093,68.862,88.393,%
Batch: 340 | Loss: 3.826 | Acc: 44.064,68.798,88.403,%
Batch: 360 | Loss: 3.822 | Acc: 44.101,68.845,88.400,%
Batch: 380 | Loss: 3.820 | Acc: 44.121,68.781,88.367,%
Batch: 0 | Loss: 4.996 | Acc: 41.406,60.938,71.875,%
Batch: 20 | Loss: 5.268 | Acc: 39.249,60.007,67.671,%
Batch: 40 | Loss: 5.275 | Acc: 39.158,59.909,67.054,%
Batch: 60 | Loss: 5.286 | Acc: 38.922,59.682,66.624,%
Train classifier parameters

Epoch: 142
Batch: 0 | Loss: 4.258 | Acc: 40.625,64.844,83.594,%
Batch: 20 | Loss: 3.807 | Acc: 43.490,69.606,89.025,%
Batch: 40 | Loss: 3.834 | Acc: 43.197,68.274,88.586,%
Batch: 60 | Loss: 3.818 | Acc: 43.404,68.161,88.717,%
Batch: 80 | Loss: 3.786 | Acc: 43.789,68.441,88.947,%
Batch: 100 | Loss: 3.804 | Acc: 43.278,68.557,88.954,%
Batch: 120 | Loss: 3.814 | Acc: 43.240,68.485,88.888,%
Batch: 140 | Loss: 3.801 | Acc: 43.467,68.523,88.702,%
Batch: 160 | Loss: 3.824 | Acc: 43.415,68.313,88.606,%
Batch: 180 | Loss: 3.823 | Acc: 43.504,68.310,88.454,%
Batch: 200 | Loss: 3.813 | Acc: 43.699,68.361,88.507,%
Batch: 220 | Loss: 3.808 | Acc: 43.955,68.386,88.444,%
Batch: 240 | Loss: 3.793 | Acc: 44.081,68.484,88.411,%
Batch: 260 | Loss: 3.807 | Acc: 44.082,68.379,88.419,%
Batch: 280 | Loss: 3.803 | Acc: 44.153,68.408,88.448,%
Batch: 300 | Loss: 3.798 | Acc: 44.267,68.436,88.442,%
Batch: 320 | Loss: 3.804 | Acc: 44.161,68.456,88.422,%
Batch: 340 | Loss: 3.808 | Acc: 44.126,68.489,88.416,%
Batch: 360 | Loss: 3.805 | Acc: 44.105,68.531,88.457,%
Batch: 380 | Loss: 3.801 | Acc: 44.123,68.551,88.419,%
Batch: 0 | Loss: 5.011 | Acc: 42.969,64.062,73.438,%
Batch: 20 | Loss: 5.288 | Acc: 39.062,60.231,66.890,%
Batch: 40 | Loss: 5.291 | Acc: 39.348,59.737,66.597,%
Batch: 60 | Loss: 5.297 | Acc: 39.357,59.554,66.150,%
Train classifier parameters

Epoch: 143
Batch: 0 | Loss: 3.419 | Acc: 49.219,65.625,91.406,%
Batch: 20 | Loss: 3.745 | Acc: 45.201,70.201,89.211,%
Batch: 40 | Loss: 3.658 | Acc: 45.370,70.198,89.520,%
Batch: 60 | Loss: 3.686 | Acc: 44.980,69.557,88.998,%
Batch: 80 | Loss: 3.683 | Acc: 45.235,69.686,88.571,%
Batch: 100 | Loss: 3.658 | Acc: 45.506,69.655,88.753,%
Batch: 120 | Loss: 3.657 | Acc: 45.861,69.525,88.772,%
Batch: 140 | Loss: 3.684 | Acc: 45.501,69.415,88.691,%
Batch: 160 | Loss: 3.672 | Acc: 45.371,69.478,88.757,%
Batch: 180 | Loss: 3.690 | Acc: 45.110,69.264,88.661,%
Batch: 200 | Loss: 3.685 | Acc: 45.165,69.100,88.608,%
Batch: 220 | Loss: 3.690 | Acc: 45.160,68.990,88.677,%
Batch: 240 | Loss: 3.699 | Acc: 45.089,69.058,88.534,%
Batch: 260 | Loss: 3.700 | Acc: 45.085,69.127,88.551,%
Batch: 280 | Loss: 3.712 | Acc: 45.001,69.047,88.534,%
Batch: 300 | Loss: 3.730 | Acc: 44.884,68.937,88.476,%
Batch: 320 | Loss: 3.739 | Acc: 44.840,68.906,88.435,%
Batch: 340 | Loss: 3.745 | Acc: 44.781,68.844,88.455,%
Batch: 360 | Loss: 3.745 | Acc: 44.810,68.873,88.474,%
Batch: 380 | Loss: 3.744 | Acc: 44.816,68.889,88.451,%
Batch: 0 | Loss: 5.059 | Acc: 43.750,61.719,70.312,%
Batch: 20 | Loss: 5.263 | Acc: 38.765,59.635,67.299,%
Batch: 40 | Loss: 5.277 | Acc: 38.986,59.699,66.787,%
Batch: 60 | Loss: 5.294 | Acc: 39.011,59.593,66.329,%
Train classifier parameters

Epoch: 144
Batch: 0 | Loss: 4.083 | Acc: 43.750,64.062,85.938,%
Batch: 20 | Loss: 3.792 | Acc: 43.601,68.080,88.207,%
Batch: 40 | Loss: 3.780 | Acc: 43.464,68.731,88.396,%
Batch: 60 | Loss: 3.817 | Acc: 43.186,68.558,88.384,%
Batch: 80 | Loss: 3.829 | Acc: 42.911,68.461,88.493,%
Batch: 100 | Loss: 3.818 | Acc: 43.263,68.588,88.668,%
Batch: 120 | Loss: 3.805 | Acc: 43.537,68.698,88.830,%
Batch: 140 | Loss: 3.775 | Acc: 44.210,68.833,88.941,%
Batch: 160 | Loss: 3.777 | Acc: 44.196,68.706,88.703,%
Batch: 180 | Loss: 3.752 | Acc: 44.492,68.879,88.898,%
Batch: 200 | Loss: 3.746 | Acc: 44.527,68.882,88.802,%
Batch: 220 | Loss: 3.750 | Acc: 44.517,68.821,88.758,%
Batch: 240 | Loss: 3.759 | Acc: 44.463,68.854,88.738,%
Batch: 260 | Loss: 3.753 | Acc: 44.651,69.010,88.721,%
Batch: 280 | Loss: 3.751 | Acc: 44.617,69.014,88.712,%
Batch: 300 | Loss: 3.753 | Acc: 44.531,69.061,88.689,%
Batch: 320 | Loss: 3.761 | Acc: 44.385,69.057,88.707,%
Batch: 340 | Loss: 3.767 | Acc: 44.385,69.103,88.623,%
Batch: 360 | Loss: 3.774 | Acc: 44.380,69.034,88.580,%
Batch: 380 | Loss: 3.773 | Acc: 44.437,69.012,88.548,%
Batch: 0 | Loss: 5.029 | Acc: 42.188,63.281,71.875,%
Batch: 20 | Loss: 5.226 | Acc: 39.211,60.193,67.560,%
Batch: 40 | Loss: 5.242 | Acc: 39.348,59.870,67.073,%
Batch: 60 | Loss: 5.259 | Acc: 39.178,59.926,66.368,%
Train classifier parameters

Epoch: 145
Batch: 0 | Loss: 3.875 | Acc: 36.719,72.656,89.062,%
Batch: 20 | Loss: 3.665 | Acc: 44.643,70.945,89.583,%
Batch: 40 | Loss: 3.785 | Acc: 43.350,70.332,89.253,%
Batch: 60 | Loss: 3.747 | Acc: 44.070,70.389,89.549,%
Batch: 80 | Loss: 3.753 | Acc: 44.271,70.100,89.284,%
Batch: 100 | Loss: 3.738 | Acc: 44.632,70.142,88.970,%
Batch: 120 | Loss: 3.728 | Acc: 44.725,69.990,89.024,%
Batch: 140 | Loss: 3.745 | Acc: 44.592,69.869,89.024,%
Batch: 160 | Loss: 3.736 | Acc: 44.657,69.910,89.082,%
Batch: 180 | Loss: 3.737 | Acc: 44.795,69.747,89.024,%
Batch: 200 | Loss: 3.730 | Acc: 44.908,69.636,88.985,%
Batch: 220 | Loss: 3.732 | Acc: 44.895,69.538,89.006,%
Batch: 240 | Loss: 3.735 | Acc: 44.875,69.531,88.985,%
Batch: 260 | Loss: 3.741 | Acc: 44.843,69.468,88.889,%
Batch: 280 | Loss: 3.741 | Acc: 44.840,69.398,88.860,%
Batch: 300 | Loss: 3.743 | Acc: 44.845,69.383,88.800,%
Batch: 320 | Loss: 3.747 | Acc: 44.714,69.368,88.770,%
Batch: 340 | Loss: 3.750 | Acc: 44.692,69.277,88.769,%
Batch: 360 | Loss: 3.756 | Acc: 44.676,69.198,88.738,%
Batch: 380 | Loss: 3.747 | Acc: 44.749,69.232,88.755,%
Batch: 0 | Loss: 4.970 | Acc: 42.969,65.625,71.875,%
Batch: 20 | Loss: 5.234 | Acc: 39.025,59.933,67.560,%
Batch: 40 | Loss: 5.263 | Acc: 39.139,59.680,66.787,%
Batch: 60 | Loss: 5.271 | Acc: 39.165,59.644,66.329,%
Train classifier parameters

Epoch: 146
Batch: 0 | Loss: 3.659 | Acc: 49.219,68.750,89.062,%
Batch: 20 | Loss: 3.692 | Acc: 45.015,70.052,90.439,%
Batch: 40 | Loss: 3.713 | Acc: 44.665,69.131,89.958,%
Batch: 60 | Loss: 3.686 | Acc: 45.300,69.185,90.061,%
Batch: 80 | Loss: 3.712 | Acc: 44.502,69.319,89.940,%
Batch: 100 | Loss: 3.734 | Acc: 44.725,69.291,89.821,%
Batch: 120 | Loss: 3.775 | Acc: 44.247,69.105,89.579,%
Batch: 140 | Loss: 3.757 | Acc: 44.398,69.238,89.467,%
Batch: 160 | Loss: 3.764 | Acc: 44.376,69.109,89.378,%
Batch: 180 | Loss: 3.779 | Acc: 44.272,68.940,89.235,%
Batch: 200 | Loss: 3.778 | Acc: 44.201,69.146,89.257,%
Batch: 220 | Loss: 3.783 | Acc: 44.188,69.096,89.130,%
Batch: 240 | Loss: 3.790 | Acc: 44.068,69.055,89.014,%
Batch: 260 | Loss: 3.785 | Acc: 44.064,69.211,89.057,%
Batch: 280 | Loss: 3.780 | Acc: 44.262,69.170,89.021,%
Batch: 300 | Loss: 3.773 | Acc: 44.279,69.207,88.987,%
Batch: 320 | Loss: 3.764 | Acc: 44.504,69.334,89.075,%
Batch: 340 | Loss: 3.768 | Acc: 44.474,69.320,89.049,%
Batch: 360 | Loss: 3.775 | Acc: 44.373,69.252,89.043,%
Batch: 380 | Loss: 3.780 | Acc: 44.310,69.236,89.026,%
Batch: 0 | Loss: 4.967 | Acc: 43.750,62.500,73.438,%
Batch: 20 | Loss: 5.241 | Acc: 39.360,59.561,67.857,%
Batch: 40 | Loss: 5.250 | Acc: 39.558,59.661,67.130,%
Batch: 60 | Loss: 5.259 | Acc: 39.421,59.477,66.650,%
Train classifier parameters

Epoch: 147
Batch: 0 | Loss: 3.200 | Acc: 46.875,80.469,94.531,%
Batch: 20 | Loss: 3.645 | Acc: 45.908,70.089,90.327,%
Batch: 40 | Loss: 3.754 | Acc: 44.646,69.436,90.015,%
Batch: 60 | Loss: 3.725 | Acc: 45.056,69.454,89.780,%
Batch: 80 | Loss: 3.750 | Acc: 44.502,69.406,89.352,%
Batch: 100 | Loss: 3.745 | Acc: 44.817,69.647,89.472,%
Batch: 120 | Loss: 3.758 | Acc: 44.615,69.493,89.405,%
Batch: 140 | Loss: 3.760 | Acc: 44.581,69.670,89.484,%
Batch: 160 | Loss: 3.760 | Acc: 44.813,69.580,89.198,%
Batch: 180 | Loss: 3.755 | Acc: 45.019,69.587,89.153,%
Batch: 200 | Loss: 3.754 | Acc: 44.904,69.492,89.160,%
Batch: 220 | Loss: 3.749 | Acc: 44.952,69.425,89.215,%
Batch: 240 | Loss: 3.749 | Acc: 44.878,69.486,89.189,%
Batch: 260 | Loss: 3.754 | Acc: 44.663,69.579,89.203,%
Batch: 280 | Loss: 3.747 | Acc: 44.756,69.579,89.099,%
Batch: 300 | Loss: 3.753 | Acc: 44.726,69.427,89.094,%
Batch: 320 | Loss: 3.748 | Acc: 44.770,69.397,89.080,%
Batch: 340 | Loss: 3.746 | Acc: 44.767,69.364,89.040,%
Batch: 360 | Loss: 3.749 | Acc: 44.728,69.302,88.995,%
Batch: 380 | Loss: 3.749 | Acc: 44.738,69.357,89.003,%
Batch: 0 | Loss: 5.039 | Acc: 44.531,62.500,72.656,%
Batch: 20 | Loss: 5.238 | Acc: 38.504,59.896,67.634,%
Batch: 40 | Loss: 5.237 | Acc: 38.662,59.947,66.959,%
Batch: 60 | Loss: 5.251 | Acc: 38.717,59.926,66.637,%
Train classifier parameters

Epoch: 148
Batch: 0 | Loss: 3.108 | Acc: 47.656,70.312,92.188,%
Batch: 20 | Loss: 3.725 | Acc: 44.829,69.010,89.546,%
Batch: 40 | Loss: 3.761 | Acc: 44.474,69.569,89.748,%
Batch: 60 | Loss: 3.726 | Acc: 44.544,69.326,89.780,%
Batch: 80 | Loss: 3.710 | Acc: 44.435,69.531,89.815,%
Batch: 100 | Loss: 3.700 | Acc: 44.524,69.787,89.635,%
Batch: 120 | Loss: 3.701 | Acc: 44.383,69.538,89.443,%
Batch: 140 | Loss: 3.683 | Acc: 44.581,69.703,89.412,%
Batch: 160 | Loss: 3.694 | Acc: 44.449,69.531,89.412,%
Batch: 180 | Loss: 3.690 | Acc: 44.665,69.609,89.468,%
Batch: 200 | Loss: 3.689 | Acc: 44.694,69.566,89.393,%
Batch: 220 | Loss: 3.685 | Acc: 44.687,69.556,89.328,%
Batch: 240 | Loss: 3.691 | Acc: 44.791,69.599,89.286,%
Batch: 260 | Loss: 3.678 | Acc: 44.932,69.588,89.299,%
Batch: 280 | Loss: 3.678 | Acc: 44.848,69.701,89.307,%
Batch: 300 | Loss: 3.684 | Acc: 44.840,69.601,89.252,%
Batch: 320 | Loss: 3.684 | Acc: 44.792,69.607,89.357,%
Batch: 340 | Loss: 3.680 | Acc: 44.811,69.575,89.328,%
Batch: 360 | Loss: 3.681 | Acc: 44.862,69.633,89.307,%
Batch: 380 | Loss: 3.692 | Acc: 44.861,69.589,89.249,%
Batch: 0 | Loss: 4.979 | Acc: 40.625,64.844,73.438,%
Batch: 20 | Loss: 5.213 | Acc: 38.393,60.193,69.048,%
Batch: 40 | Loss: 5.231 | Acc: 38.700,60.004,67.912,%
Batch: 60 | Loss: 5.252 | Acc: 38.883,59.990,67.290,%
Train classifier parameters

Epoch: 149
Batch: 0 | Loss: 4.195 | Acc: 36.719,63.281,90.625,%
Batch: 20 | Loss: 3.767 | Acc: 44.940,69.048,88.951,%
Batch: 40 | Loss: 3.701 | Acc: 45.274,69.703,89.329,%
Batch: 60 | Loss: 3.679 | Acc: 45.453,69.749,89.549,%
Batch: 80 | Loss: 3.660 | Acc: 45.467,70.052,89.632,%
Batch: 100 | Loss: 3.635 | Acc: 45.676,70.142,89.372,%
Batch: 120 | Loss: 3.623 | Acc: 45.868,70.196,89.495,%
Batch: 140 | Loss: 3.668 | Acc: 45.373,70.047,89.445,%
Batch: 160 | Loss: 3.686 | Acc: 45.230,69.919,89.417,%
Batch: 180 | Loss: 3.706 | Acc: 45.075,69.829,89.326,%
Batch: 200 | Loss: 3.716 | Acc: 44.854,69.885,89.315,%
Batch: 220 | Loss: 3.705 | Acc: 44.874,69.980,89.441,%
Batch: 240 | Loss: 3.696 | Acc: 45.053,69.917,89.377,%
Batch: 260 | Loss: 3.704 | Acc: 45.025,69.959,89.428,%
Batch: 280 | Loss: 3.704 | Acc: 44.893,69.990,89.432,%
Batch: 300 | Loss: 3.715 | Acc: 44.866,69.840,89.335,%
Batch: 320 | Loss: 3.705 | Acc: 45.020,69.855,89.318,%
Batch: 340 | Loss: 3.706 | Acc: 45.038,69.767,89.278,%
Batch: 360 | Loss: 3.718 | Acc: 44.942,69.611,89.214,%
Batch: 380 | Loss: 3.721 | Acc: 44.851,69.628,89.270,%
Batch: 0 | Loss: 4.830 | Acc: 46.094,64.062,73.438,%
Batch: 20 | Loss: 5.238 | Acc: 39.286,60.268,67.746,%
Batch: 40 | Loss: 5.241 | Acc: 39.653,60.099,67.245,%
Batch: 60 | Loss: 5.256 | Acc: 39.588,59.682,66.701,%
Train all parameters

Epoch: 150
Batch: 0 | Loss: 3.537 | Acc: 42.188,77.344,86.719,%
Batch: 20 | Loss: 3.524 | Acc: 43.378,71.168,92.671,%
Batch: 40 | Loss: 3.383 | Acc: 45.732,72.694,93.388,%
Batch: 60 | Loss: 3.387 | Acc: 45.210,73.130,93.929,%
Batch: 80 | Loss: 3.344 | Acc: 45.602,73.929,94.416,%
Batch: 100 | Loss: 3.302 | Acc: 46.225,74.358,94.709,%
Batch: 120 | Loss: 3.263 | Acc: 46.533,74.322,94.957,%
Batch: 140 | Loss: 3.218 | Acc: 46.775,74.795,95.146,%
Batch: 160 | Loss: 3.201 | Acc: 47.098,75.029,95.215,%
Batch: 180 | Loss: 3.205 | Acc: 47.095,75.056,95.287,%
Batch: 200 | Loss: 3.180 | Acc: 47.287,75.206,95.441,%
Batch: 220 | Loss: 3.181 | Acc: 47.087,75.233,95.556,%
Batch: 240 | Loss: 3.170 | Acc: 47.199,75.301,95.643,%
Batch: 260 | Loss: 3.149 | Acc: 47.399,75.425,95.738,%
Batch: 280 | Loss: 3.137 | Acc: 47.542,75.620,95.802,%
Batch: 300 | Loss: 3.140 | Acc: 47.477,75.649,95.858,%
Batch: 320 | Loss: 3.134 | Acc: 47.515,75.711,95.974,%
Batch: 340 | Loss: 3.126 | Acc: 47.521,75.839,96.053,%
Batch: 360 | Loss: 3.123 | Acc: 47.583,75.749,96.107,%
Batch: 380 | Loss: 3.126 | Acc: 47.527,75.750,96.112,%
Batch: 0 | Loss: 4.247 | Acc: 44.531,66.406,76.562,%
Batch: 20 | Loss: 4.651 | Acc: 42.001,64.807,73.251,%
Batch: 40 | Loss: 4.644 | Acc: 42.188,64.672,72.389,%
Batch: 60 | Loss: 4.661 | Acc: 41.855,64.242,72.016,%
Train all parameters

Epoch: 151
Batch: 0 | Loss: 3.798 | Acc: 44.531,71.094,96.875,%
Batch: 20 | Loss: 2.784 | Acc: 50.484,78.906,97.917,%
Batch: 40 | Loss: 2.860 | Acc: 50.019,78.049,98.056,%
Batch: 60 | Loss: 2.864 | Acc: 49.846,77.907,97.951,%
Batch: 80 | Loss: 2.920 | Acc: 49.286,78.009,97.820,%
Batch: 100 | Loss: 2.925 | Acc: 49.234,78.110,97.881,%
Batch: 120 | Loss: 2.934 | Acc: 48.851,78.177,97.837,%
Batch: 140 | Loss: 2.919 | Acc: 49.213,78.186,97.856,%
Batch: 160 | Loss: 2.930 | Acc: 48.986,77.892,97.875,%
Batch: 180 | Loss: 2.947 | Acc: 48.757,77.775,97.932,%
Batch: 200 | Loss: 2.954 | Acc: 48.690,77.717,97.909,%
Batch: 220 | Loss: 2.960 | Acc: 48.540,77.722,97.946,%
Batch: 240 | Loss: 2.965 | Acc: 48.512,77.742,97.945,%
Batch: 260 | Loss: 2.972 | Acc: 48.467,77.667,97.974,%
Batch: 280 | Loss: 2.965 | Acc: 48.577,77.716,97.990,%
Batch: 300 | Loss: 2.962 | Acc: 48.585,77.751,97.981,%
Batch: 320 | Loss: 2.953 | Acc: 48.637,77.697,97.953,%
Batch: 340 | Loss: 2.954 | Acc: 48.662,77.626,97.924,%
Batch: 360 | Loss: 2.951 | Acc: 48.621,77.558,97.933,%
Batch: 380 | Loss: 2.949 | Acc: 48.649,77.563,97.935,%
Batch: 0 | Loss: 4.269 | Acc: 42.969,71.094,73.438,%
Batch: 20 | Loss: 4.667 | Acc: 43.080,64.955,73.214,%
Batch: 40 | Loss: 4.651 | Acc: 42.988,64.920,72.485,%
Batch: 60 | Loss: 4.675 | Acc: 42.674,64.460,72.106,%
Train all parameters

Epoch: 152
Batch: 0 | Loss: 3.520 | Acc: 42.188,74.219,99.219,%
Batch: 20 | Loss: 2.942 | Acc: 50.223,77.716,98.735,%
Batch: 40 | Loss: 2.844 | Acc: 50.038,78.830,98.723,%
Batch: 60 | Loss: 2.879 | Acc: 49.462,78.624,98.630,%
Batch: 80 | Loss: 2.885 | Acc: 49.142,78.231,98.553,%
Batch: 100 | Loss: 2.872 | Acc: 49.489,78.303,98.499,%
Batch: 120 | Loss: 2.868 | Acc: 49.445,78.183,98.463,%
Batch: 140 | Loss: 2.866 | Acc: 49.501,78.197,98.382,%
Batch: 160 | Loss: 2.891 | Acc: 49.034,78.159,98.404,%
Batch: 180 | Loss: 2.900 | Acc: 49.003,78.181,98.416,%
Batch: 200 | Loss: 2.888 | Acc: 49.281,78.280,98.410,%
Batch: 220 | Loss: 2.898 | Acc: 49.190,78.242,98.402,%
Batch: 240 | Loss: 2.899 | Acc: 49.089,78.209,98.434,%
Batch: 260 | Loss: 2.898 | Acc: 49.081,78.116,98.440,%
Batch: 280 | Loss: 2.905 | Acc: 48.971,78.067,98.415,%
Batch: 300 | Loss: 2.908 | Acc: 48.967,78.029,98.383,%
Batch: 320 | Loss: 2.909 | Acc: 48.934,78.020,98.367,%
Batch: 340 | Loss: 2.905 | Acc: 48.987,78.114,98.357,%
Batch: 360 | Loss: 2.900 | Acc: 48.957,78.227,98.344,%
Batch: 380 | Loss: 2.905 | Acc: 48.887,78.131,98.337,%
Batch: 0 | Loss: 4.243 | Acc: 45.312,70.312,76.562,%
Batch: 20 | Loss: 4.695 | Acc: 41.815,64.807,72.768,%
Batch: 40 | Loss: 4.703 | Acc: 41.368,64.367,72.085,%
Batch: 60 | Loss: 4.726 | Acc: 41.035,63.883,71.696,%
Train all parameters

Epoch: 153
Batch: 0 | Loss: 2.901 | Acc: 50.781,80.469,98.438,%
Batch: 20 | Loss: 2.737 | Acc: 49.814,78.423,98.735,%
Batch: 40 | Loss: 2.817 | Acc: 49.066,78.506,98.761,%
Batch: 60 | Loss: 2.776 | Acc: 49.526,79.201,98.732,%
Batch: 80 | Loss: 2.784 | Acc: 49.450,79.128,98.775,%
Batch: 100 | Loss: 2.794 | Acc: 49.497,79.131,98.755,%
Batch: 120 | Loss: 2.794 | Acc: 49.593,79.300,98.767,%
Batch: 140 | Loss: 2.787 | Acc: 49.717,79.167,98.726,%
Batch: 160 | Loss: 2.802 | Acc: 49.432,79.042,98.695,%
Batch: 180 | Loss: 2.817 | Acc: 49.374,78.863,98.701,%
Batch: 200 | Loss: 2.834 | Acc: 49.285,78.887,98.721,%
Batch: 220 | Loss: 2.834 | Acc: 49.364,78.811,98.688,%
Batch: 240 | Loss: 2.838 | Acc: 49.407,78.819,98.658,%
Batch: 260 | Loss: 2.844 | Acc: 49.183,78.781,98.668,%
Batch: 280 | Loss: 2.828 | Acc: 49.388,78.862,98.677,%
Batch: 300 | Loss: 2.839 | Acc: 49.374,78.787,98.671,%
Batch: 320 | Loss: 2.840 | Acc: 49.416,78.777,98.640,%
Batch: 340 | Loss: 2.844 | Acc: 49.434,78.730,98.623,%
Batch: 360 | Loss: 2.845 | Acc: 49.459,78.735,98.608,%
Batch: 380 | Loss: 2.853 | Acc: 49.432,78.683,98.608,%
Batch: 0 | Loss: 4.322 | Acc: 41.406,71.094,73.438,%
Batch: 20 | Loss: 4.653 | Acc: 40.960,64.955,73.103,%
Batch: 40 | Loss: 4.685 | Acc: 41.273,64.539,72.142,%
Batch: 60 | Loss: 4.704 | Acc: 41.291,64.114,71.747,%
Train all parameters

Epoch: 154
Batch: 0 | Loss: 2.844 | Acc: 50.000,85.156,100.000,%
Batch: 20 | Loss: 2.746 | Acc: 50.000,79.464,99.144,%
Batch: 40 | Loss: 2.805 | Acc: 49.085,78.849,99.143,%
Batch: 60 | Loss: 2.831 | Acc: 49.027,78.599,98.899,%
Batch: 80 | Loss: 2.798 | Acc: 49.277,79.138,98.900,%
Batch: 100 | Loss: 2.784 | Acc: 49.698,79.541,98.817,%
Batch: 120 | Loss: 2.783 | Acc: 49.858,79.494,98.831,%
Batch: 140 | Loss: 2.788 | Acc: 49.878,79.527,98.842,%
Batch: 160 | Loss: 2.793 | Acc: 49.854,79.217,98.850,%
Batch: 180 | Loss: 2.799 | Acc: 49.853,79.372,98.873,%
Batch: 200 | Loss: 2.803 | Acc: 49.872,79.206,98.865,%
Batch: 220 | Loss: 2.807 | Acc: 49.816,79.182,98.897,%
Batch: 240 | Loss: 2.810 | Acc: 49.857,79.123,98.904,%
Batch: 260 | Loss: 2.807 | Acc: 49.859,79.176,98.878,%
Batch: 280 | Loss: 2.829 | Acc: 49.602,79.115,98.868,%
Batch: 300 | Loss: 2.831 | Acc: 49.556,79.132,98.848,%
Batch: 320 | Loss: 2.833 | Acc: 49.525,79.045,98.837,%
Batch: 340 | Loss: 2.834 | Acc: 49.501,79.028,98.852,%
Batch: 360 | Loss: 2.843 | Acc: 49.405,78.934,98.836,%
Batch: 380 | Loss: 2.847 | Acc: 49.336,78.919,98.839,%
Batch: 0 | Loss: 4.338 | Acc: 47.656,70.312,75.781,%
Batch: 20 | Loss: 4.574 | Acc: 43.713,65.439,73.698,%
Batch: 40 | Loss: 4.574 | Acc: 43.864,65.568,73.266,%
Batch: 60 | Loss: 4.594 | Acc: 43.545,65.369,72.925,%
Train all parameters

Epoch: 155
Batch: 0 | Loss: 2.893 | Acc: 46.875,82.031,100.000,%
Batch: 20 | Loss: 2.792 | Acc: 49.740,80.729,99.293,%
Batch: 40 | Loss: 2.871 | Acc: 48.819,80.030,99.162,%
Batch: 60 | Loss: 2.854 | Acc: 49.014,79.854,99.193,%
Batch: 80 | Loss: 2.857 | Acc: 49.084,79.572,99.199,%
Batch: 100 | Loss: 2.847 | Acc: 49.211,79.347,99.188,%
Batch: 120 | Loss: 2.843 | Acc: 49.090,79.326,99.109,%
Batch: 140 | Loss: 2.835 | Acc: 49.202,79.316,99.064,%
Batch: 160 | Loss: 2.820 | Acc: 49.233,79.401,99.078,%
Batch: 180 | Loss: 2.819 | Acc: 49.102,79.316,99.046,%
Batch: 200 | Loss: 2.806 | Acc: 49.452,79.361,99.005,%
Batch: 220 | Loss: 2.798 | Acc: 49.441,79.412,98.993,%
Batch: 240 | Loss: 2.796 | Acc: 49.442,79.373,99.021,%
Batch: 260 | Loss: 2.809 | Acc: 49.368,79.253,99.006,%
Batch: 280 | Loss: 2.811 | Acc: 49.491,79.234,98.966,%
Batch: 300 | Loss: 2.806 | Acc: 49.593,79.174,98.954,%
Batch: 320 | Loss: 2.807 | Acc: 49.596,79.172,98.966,%
Batch: 340 | Loss: 2.797 | Acc: 49.748,79.268,98.953,%
Batch: 360 | Loss: 2.797 | Acc: 49.736,79.265,98.950,%
Batch: 380 | Loss: 2.800 | Acc: 49.746,79.165,98.932,%
Batch: 0 | Loss: 4.253 | Acc: 46.875,68.750,78.125,%
Batch: 20 | Loss: 4.667 | Acc: 43.452,65.439,72.545,%
Batch: 40 | Loss: 4.655 | Acc: 43.388,65.091,72.561,%
Batch: 60 | Loss: 4.670 | Acc: 43.122,64.562,72.080,%
Train all parameters

Epoch: 156
Batch: 0 | Loss: 2.478 | Acc: 55.469,82.031,98.438,%
Batch: 20 | Loss: 2.816 | Acc: 49.033,79.836,99.182,%
Batch: 40 | Loss: 2.787 | Acc: 49.619,80.545,99.333,%
Batch: 60 | Loss: 2.781 | Acc: 50.026,80.097,99.103,%
Batch: 80 | Loss: 2.762 | Acc: 50.299,80.575,99.055,%
Batch: 100 | Loss: 2.780 | Acc: 50.131,80.105,99.049,%
Batch: 120 | Loss: 2.794 | Acc: 49.626,79.913,99.051,%
Batch: 140 | Loss: 2.781 | Acc: 49.784,79.981,99.041,%
Batch: 160 | Loss: 2.778 | Acc: 50.010,79.911,99.068,%
Batch: 180 | Loss: 2.795 | Acc: 49.577,79.787,99.033,%
Batch: 200 | Loss: 2.804 | Acc: 49.635,79.653,99.032,%
Batch: 220 | Loss: 2.808 | Acc: 49.537,79.550,99.000,%
Batch: 240 | Loss: 2.819 | Acc: 49.391,79.412,98.985,%
Batch: 260 | Loss: 2.808 | Acc: 49.497,79.514,98.970,%
Batch: 280 | Loss: 2.809 | Acc: 49.469,79.409,98.938,%
Batch: 300 | Loss: 2.804 | Acc: 49.473,79.397,98.941,%
Batch: 320 | Loss: 2.802 | Acc: 49.482,79.359,98.951,%
Batch: 340 | Loss: 2.811 | Acc: 49.402,79.227,98.951,%
Batch: 360 | Loss: 2.810 | Acc: 49.435,79.224,98.948,%
Batch: 380 | Loss: 2.808 | Acc: 49.547,79.232,98.962,%
Batch: 0 | Loss: 4.159 | Acc: 45.312,72.656,77.344,%
Batch: 20 | Loss: 4.537 | Acc: 43.713,66.592,73.214,%
Batch: 40 | Loss: 4.559 | Acc: 43.712,66.235,72.752,%
Batch: 60 | Loss: 4.575 | Acc: 43.430,65.753,72.579,%
Train all parameters

Epoch: 157
Batch: 0 | Loss: 3.356 | Acc: 48.438,81.250,99.219,%
Batch: 20 | Loss: 2.722 | Acc: 50.558,81.101,99.293,%
Batch: 40 | Loss: 2.735 | Acc: 50.572,79.840,99.104,%
Batch: 60 | Loss: 2.728 | Acc: 50.499,79.636,99.244,%
Batch: 80 | Loss: 2.717 | Acc: 50.502,79.929,99.219,%
Batch: 100 | Loss: 2.728 | Acc: 50.549,80.113,99.157,%
Batch: 120 | Loss: 2.768 | Acc: 49.923,80.114,99.115,%
Batch: 140 | Loss: 2.759 | Acc: 50.089,80.075,99.163,%
Batch: 160 | Loss: 2.778 | Acc: 49.922,79.920,99.165,%
Batch: 180 | Loss: 2.788 | Acc: 49.875,79.860,99.154,%
Batch: 200 | Loss: 2.786 | Acc: 50.008,79.754,99.137,%
Batch: 220 | Loss: 2.800 | Acc: 49.869,79.511,99.123,%
Batch: 240 | Loss: 2.789 | Acc: 49.961,79.584,99.112,%
Batch: 260 | Loss: 2.779 | Acc: 49.919,79.670,99.102,%
Batch: 280 | Loss: 2.766 | Acc: 50.133,79.618,99.083,%
Batch: 300 | Loss: 2.769 | Acc: 50.112,79.553,99.040,%
Batch: 320 | Loss: 2.767 | Acc: 50.046,79.561,99.036,%
Batch: 340 | Loss: 2.763 | Acc: 50.018,79.481,99.024,%
Batch: 360 | Loss: 2.769 | Acc: 49.985,79.501,99.028,%
Batch: 380 | Loss: 2.769 | Acc: 50.002,79.480,99.022,%
Batch: 0 | Loss: 4.127 | Acc: 48.438,69.531,77.344,%
Batch: 20 | Loss: 4.532 | Acc: 43.899,66.406,72.991,%
Batch: 40 | Loss: 4.547 | Acc: 43.998,65.987,72.523,%
Batch: 60 | Loss: 4.578 | Acc: 43.519,65.510,72.003,%
Train all parameters

Epoch: 158
Batch: 0 | Loss: 2.018 | Acc: 59.375,83.594,99.219,%
Batch: 20 | Loss: 2.767 | Acc: 49.442,80.171,99.182,%
Batch: 40 | Loss: 2.790 | Acc: 50.000,80.316,99.314,%
Batch: 60 | Loss: 2.814 | Acc: 49.731,79.764,99.334,%
Batch: 80 | Loss: 2.754 | Acc: 50.502,80.083,99.344,%
Batch: 100 | Loss: 2.760 | Acc: 50.642,79.718,99.319,%
Batch: 120 | Loss: 2.747 | Acc: 50.646,79.901,99.303,%
Batch: 140 | Loss: 2.756 | Acc: 50.565,79.904,99.241,%
Batch: 160 | Loss: 2.736 | Acc: 50.772,79.950,99.272,%
Batch: 180 | Loss: 2.757 | Acc: 50.557,79.808,99.258,%
Batch: 200 | Loss: 2.777 | Acc: 50.381,79.707,99.219,%
Batch: 220 | Loss: 2.773 | Acc: 50.389,79.698,99.198,%
Batch: 240 | Loss: 2.779 | Acc: 50.256,79.671,99.203,%
Batch: 260 | Loss: 2.784 | Acc: 50.236,79.535,99.165,%
Batch: 280 | Loss: 2.784 | Acc: 50.272,79.568,99.180,%
Batch: 300 | Loss: 2.783 | Acc: 50.215,79.534,99.177,%
Batch: 320 | Loss: 2.788 | Acc: 50.165,79.561,99.175,%
Batch: 340 | Loss: 2.789 | Acc: 50.154,79.474,99.168,%
Batch: 360 | Loss: 2.784 | Acc: 50.214,79.536,99.156,%
Batch: 380 | Loss: 2.788 | Acc: 50.178,79.464,99.124,%
Batch: 0 | Loss: 4.244 | Acc: 45.312,69.531,76.562,%
Batch: 20 | Loss: 4.594 | Acc: 43.341,65.290,73.103,%
Batch: 40 | Loss: 4.597 | Acc: 43.121,64.710,72.828,%
Batch: 60 | Loss: 4.616 | Acc: 42.866,64.524,72.772,%
Train all parameters

Epoch: 159
Batch: 0 | Loss: 2.107 | Acc: 55.469,85.156,98.438,%
Batch: 20 | Loss: 2.674 | Acc: 51.525,82.366,99.070,%
Batch: 40 | Loss: 2.691 | Acc: 50.724,81.688,99.066,%
Batch: 60 | Loss: 2.696 | Acc: 50.525,81.532,99.129,%
Batch: 80 | Loss: 2.688 | Acc: 50.492,81.395,99.190,%
Batch: 100 | Loss: 2.725 | Acc: 50.155,81.095,99.134,%
Batch: 120 | Loss: 2.726 | Acc: 50.387,80.733,99.070,%
Batch: 140 | Loss: 2.704 | Acc: 50.593,80.873,99.113,%
Batch: 160 | Loss: 2.709 | Acc: 50.573,80.784,99.161,%
Batch: 180 | Loss: 2.731 | Acc: 50.384,80.594,99.145,%
Batch: 200 | Loss: 2.720 | Acc: 50.416,80.702,99.172,%
Batch: 220 | Loss: 2.728 | Acc: 50.276,80.600,99.169,%
Batch: 240 | Loss: 2.731 | Acc: 50.340,80.401,99.157,%
Batch: 260 | Loss: 2.740 | Acc: 50.254,80.208,99.147,%
Batch: 280 | Loss: 2.745 | Acc: 50.247,80.257,99.158,%
Batch: 300 | Loss: 2.749 | Acc: 50.213,80.173,99.143,%
Batch: 320 | Loss: 2.736 | Acc: 50.392,80.186,99.131,%
Batch: 340 | Loss: 2.738 | Acc: 50.257,80.056,99.139,%
Batch: 360 | Loss: 2.741 | Acc: 50.247,80.066,99.128,%
Batch: 380 | Loss: 2.747 | Acc: 50.160,79.944,99.110,%
Batch: 0 | Loss: 4.122 | Acc: 46.875,71.094,79.688,%
Batch: 20 | Loss: 4.509 | Acc: 43.006,66.332,73.512,%
Batch: 40 | Loss: 4.542 | Acc: 43.121,65.835,72.523,%
Batch: 60 | Loss: 4.575 | Acc: 42.969,65.254,72.080,%
Train all parameters

Epoch: 160
Batch: 0 | Loss: 2.523 | Acc: 43.750,86.719,99.219,%
Batch: 20 | Loss: 2.749 | Acc: 48.735,80.766,99.442,%
Batch: 40 | Loss: 2.655 | Acc: 49.848,81.879,99.447,%
Batch: 60 | Loss: 2.670 | Acc: 50.333,81.442,99.411,%
Batch: 80 | Loss: 2.718 | Acc: 50.019,81.424,99.431,%
Batch: 100 | Loss: 2.734 | Acc: 49.752,81.281,99.366,%
Batch: 120 | Loss: 2.735 | Acc: 50.006,81.153,99.367,%
Batch: 140 | Loss: 2.739 | Acc: 50.183,80.785,99.330,%
Batch: 160 | Loss: 2.722 | Acc: 50.291,80.745,99.330,%
Batch: 180 | Loss: 2.715 | Acc: 50.432,80.728,99.292,%
Batch: 200 | Loss: 2.736 | Acc: 50.237,80.531,99.281,%
Batch: 220 | Loss: 2.735 | Acc: 50.300,80.592,99.286,%
Batch: 240 | Loss: 2.741 | Acc: 50.327,80.566,99.274,%
Batch: 260 | Loss: 2.733 | Acc: 50.446,80.538,99.240,%
Batch: 280 | Loss: 2.743 | Acc: 50.353,80.458,99.235,%
Batch: 300 | Loss: 2.741 | Acc: 50.475,80.482,99.227,%
Batch: 320 | Loss: 2.739 | Acc: 50.472,80.391,99.241,%
Batch: 340 | Loss: 2.737 | Acc: 50.426,80.402,99.244,%
Batch: 360 | Loss: 2.734 | Acc: 50.372,80.343,99.236,%
Batch: 380 | Loss: 2.738 | Acc: 50.303,80.253,99.243,%
Batch: 0 | Loss: 4.162 | Acc: 46.094,67.969,76.562,%
Batch: 20 | Loss: 4.572 | Acc: 42.411,65.476,72.619,%
Batch: 40 | Loss: 4.581 | Acc: 42.969,65.492,72.237,%
Batch: 60 | Loss: 4.591 | Acc: 42.764,65.010,72.246,%
Train all parameters

Epoch: 161
Batch: 0 | Loss: 3.170 | Acc: 46.875,78.906,99.219,%
Batch: 20 | Loss: 2.861 | Acc: 48.996,79.055,99.256,%
Batch: 40 | Loss: 2.742 | Acc: 50.076,80.488,99.238,%
Batch: 60 | Loss: 2.784 | Acc: 49.411,80.405,99.244,%
Batch: 80 | Loss: 2.773 | Acc: 49.566,80.498,99.248,%
Batch: 100 | Loss: 2.760 | Acc: 49.954,80.484,99.273,%
Batch: 120 | Loss: 2.755 | Acc: 49.871,80.527,99.316,%
Batch: 140 | Loss: 2.753 | Acc: 49.922,80.607,99.352,%
Batch: 160 | Loss: 2.761 | Acc: 49.825,80.517,99.369,%
Batch: 180 | Loss: 2.780 | Acc: 49.655,80.193,99.370,%
Batch: 200 | Loss: 2.768 | Acc: 49.658,80.154,99.347,%
Batch: 220 | Loss: 2.752 | Acc: 49.901,80.126,99.353,%
Batch: 240 | Loss: 2.743 | Acc: 49.987,80.180,99.339,%
Batch: 260 | Loss: 2.753 | Acc: 49.970,80.068,99.306,%
Batch: 280 | Loss: 2.749 | Acc: 50.025,80.088,99.308,%
Batch: 300 | Loss: 2.743 | Acc: 50.026,80.105,99.323,%
Batch: 320 | Loss: 2.742 | Acc: 50.107,80.057,99.304,%
Batch: 340 | Loss: 2.735 | Acc: 50.275,80.088,99.297,%
Batch: 360 | Loss: 2.736 | Acc: 50.266,80.099,99.299,%
Batch: 380 | Loss: 2.737 | Acc: 50.291,80.102,99.280,%
Batch: 0 | Loss: 4.284 | Acc: 46.094,71.875,74.219,%
Batch: 20 | Loss: 4.608 | Acc: 43.266,65.885,73.289,%
Batch: 40 | Loss: 4.596 | Acc: 43.674,65.358,72.466,%
Batch: 60 | Loss: 4.616 | Acc: 43.353,64.857,72.170,%
Train all parameters

Epoch: 162
Batch: 0 | Loss: 2.800 | Acc: 46.094,81.250,100.000,%
Batch: 20 | Loss: 2.641 | Acc: 51.004,82.217,99.665,%
Batch: 40 | Loss: 2.769 | Acc: 49.466,81.231,99.371,%
Batch: 60 | Loss: 2.709 | Acc: 50.218,81.327,99.372,%
Batch: 80 | Loss: 2.677 | Acc: 50.395,81.366,99.392,%
Batch: 100 | Loss: 2.674 | Acc: 50.302,81.498,99.358,%
Batch: 120 | Loss: 2.706 | Acc: 50.077,81.340,99.374,%
Batch: 140 | Loss: 2.700 | Acc: 50.094,81.250,99.341,%
Batch: 160 | Loss: 2.686 | Acc: 50.388,81.061,99.262,%
Batch: 180 | Loss: 2.689 | Acc: 50.414,81.039,99.253,%
Batch: 200 | Loss: 2.689 | Acc: 50.478,81.040,99.262,%
Batch: 220 | Loss: 2.674 | Acc: 50.742,81.063,99.279,%
Batch: 240 | Loss: 2.684 | Acc: 50.561,81.085,99.293,%
Batch: 260 | Loss: 2.693 | Acc: 50.476,80.999,99.285,%
Batch: 280 | Loss: 2.705 | Acc: 50.411,81.019,99.297,%
Batch: 300 | Loss: 2.709 | Acc: 50.405,80.977,99.312,%
Batch: 320 | Loss: 2.720 | Acc: 50.329,80.907,99.306,%
Batch: 340 | Loss: 2.723 | Acc: 50.332,80.856,99.306,%
Batch: 360 | Loss: 2.730 | Acc: 50.309,80.739,99.269,%
Batch: 380 | Loss: 2.730 | Acc: 50.265,80.664,99.258,%
Batch: 0 | Loss: 4.211 | Acc: 45.312,69.531,77.344,%
Batch: 20 | Loss: 4.582 | Acc: 43.080,66.257,72.656,%
Batch: 40 | Loss: 4.586 | Acc: 43.502,65.701,72.618,%
Batch: 60 | Loss: 4.598 | Acc: 43.315,65.369,72.823,%
Train all parameters

Epoch: 163
Batch: 0 | Loss: 3.143 | Acc: 49.219,78.906,100.000,%
Batch: 20 | Loss: 2.683 | Acc: 51.228,81.734,99.740,%
Batch: 40 | Loss: 2.750 | Acc: 50.076,80.488,99.581,%
Batch: 60 | Loss: 2.731 | Acc: 50.243,81.071,99.539,%
Batch: 80 | Loss: 2.730 | Acc: 50.068,80.893,99.498,%
Batch: 100 | Loss: 2.721 | Acc: 50.085,80.948,99.513,%
Batch: 120 | Loss: 2.718 | Acc: 50.161,80.985,99.509,%
Batch: 140 | Loss: 2.701 | Acc: 50.349,81.023,99.512,%
Batch: 160 | Loss: 2.721 | Acc: 50.160,80.842,99.476,%
Batch: 180 | Loss: 2.741 | Acc: 49.892,80.689,99.460,%
Batch: 200 | Loss: 2.741 | Acc: 49.949,80.554,99.471,%
Batch: 220 | Loss: 2.745 | Acc: 50.057,80.511,99.434,%
Batch: 240 | Loss: 2.742 | Acc: 50.156,80.485,99.416,%
Batch: 260 | Loss: 2.742 | Acc: 50.141,80.448,99.404,%
Batch: 280 | Loss: 2.735 | Acc: 50.181,80.497,99.380,%
Batch: 300 | Loss: 2.727 | Acc: 50.361,80.617,99.372,%
Batch: 320 | Loss: 2.729 | Acc: 50.285,80.600,99.372,%
Batch: 340 | Loss: 2.732 | Acc: 50.202,80.515,99.365,%
Batch: 360 | Loss: 2.732 | Acc: 50.141,80.473,99.357,%
Batch: 380 | Loss: 2.737 | Acc: 50.100,80.487,99.344,%
Batch: 0 | Loss: 4.201 | Acc: 47.656,71.875,74.219,%
Batch: 20 | Loss: 4.567 | Acc: 43.676,65.960,73.549,%
Batch: 40 | Loss: 4.563 | Acc: 43.902,65.625,72.809,%
Batch: 60 | Loss: 4.584 | Acc: 43.340,65.177,72.349,%
Train all parameters

Epoch: 164
Batch: 0 | Loss: 3.222 | Acc: 46.094,78.125,100.000,%
Batch: 20 | Loss: 2.720 | Acc: 49.330,81.585,99.368,%
Batch: 40 | Loss: 2.740 | Acc: 49.695,81.269,99.371,%
Batch: 60 | Loss: 2.655 | Acc: 50.551,81.749,99.296,%
Batch: 80 | Loss: 2.646 | Acc: 50.907,81.520,99.315,%
Batch: 100 | Loss: 2.675 | Acc: 50.619,81.242,99.304,%
Batch: 120 | Loss: 2.696 | Acc: 50.077,81.166,99.290,%
Batch: 140 | Loss: 2.691 | Acc: 50.116,81.256,99.269,%
Batch: 160 | Loss: 2.713 | Acc: 50.170,81.056,99.267,%
Batch: 180 | Loss: 2.713 | Acc: 50.138,80.844,99.253,%
Batch: 200 | Loss: 2.721 | Acc: 50.144,80.811,99.269,%
Batch: 220 | Loss: 2.727 | Acc: 50.088,80.734,99.265,%
Batch: 240 | Loss: 2.739 | Acc: 49.900,80.563,99.261,%
Batch: 260 | Loss: 2.736 | Acc: 50.018,80.615,99.297,%
Batch: 280 | Loss: 2.733 | Acc: 50.028,80.652,99.285,%
Batch: 300 | Loss: 2.733 | Acc: 50.016,80.801,99.278,%
Batch: 320 | Loss: 2.737 | Acc: 50.027,80.802,99.299,%
Batch: 340 | Loss: 2.733 | Acc: 50.057,80.870,99.292,%
Batch: 360 | Loss: 2.734 | Acc: 50.030,80.822,99.286,%
Batch: 380 | Loss: 2.736 | Acc: 50.062,80.778,99.284,%
Batch: 0 | Loss: 4.242 | Acc: 45.312,70.312,75.781,%
Batch: 20 | Loss: 4.582 | Acc: 43.229,66.667,73.103,%
Batch: 40 | Loss: 4.593 | Acc: 43.502,65.930,72.752,%
Batch: 60 | Loss: 4.619 | Acc: 43.251,65.228,72.451,%
Train all parameters

Epoch: 165
Batch: 0 | Loss: 2.220 | Acc: 58.594,79.688,98.438,%
Batch: 20 | Loss: 2.746 | Acc: 49.144,81.101,99.256,%
Batch: 40 | Loss: 2.641 | Acc: 51.029,81.174,99.257,%
Batch: 60 | Loss: 2.625 | Acc: 51.601,81.609,99.257,%
Batch: 80 | Loss: 2.614 | Acc: 51.379,82.089,99.286,%
Batch: 100 | Loss: 2.596 | Acc: 51.199,82.047,99.343,%
Batch: 120 | Loss: 2.607 | Acc: 50.994,81.993,99.322,%
Batch: 140 | Loss: 2.628 | Acc: 50.875,81.649,99.335,%
Batch: 160 | Loss: 2.649 | Acc: 50.815,81.468,99.350,%
Batch: 180 | Loss: 2.673 | Acc: 50.604,81.310,99.344,%
Batch: 200 | Loss: 2.678 | Acc: 50.637,81.328,99.363,%
Batch: 220 | Loss: 2.702 | Acc: 50.283,81.172,99.357,%
Batch: 240 | Loss: 2.701 | Acc: 50.454,81.188,99.355,%
Batch: 260 | Loss: 2.713 | Acc: 50.404,80.990,99.368,%
Batch: 280 | Loss: 2.709 | Acc: 50.503,81.069,99.333,%
Batch: 300 | Loss: 2.705 | Acc: 50.449,80.926,99.336,%
Batch: 320 | Loss: 2.710 | Acc: 50.389,80.870,99.319,%
Batch: 340 | Loss: 2.711 | Acc: 50.357,80.920,99.313,%
Batch: 360 | Loss: 2.713 | Acc: 50.366,80.895,99.303,%
Batch: 380 | Loss: 2.724 | Acc: 50.203,80.840,99.293,%
Batch: 0 | Loss: 4.170 | Acc: 46.094,72.656,75.781,%
Batch: 20 | Loss: 4.550 | Acc: 43.713,65.737,72.656,%
Batch: 40 | Loss: 4.566 | Acc: 43.540,65.739,72.313,%
Batch: 60 | Loss: 4.587 | Acc: 43.468,65.151,71.965,%
Train all parameters

Epoch: 166
Batch: 0 | Loss: 2.613 | Acc: 47.656,87.500,100.000,%
Batch: 20 | Loss: 2.631 | Acc: 51.302,82.589,99.516,%
Batch: 40 | Loss: 2.574 | Acc: 51.810,82.489,99.428,%
Batch: 60 | Loss: 2.613 | Acc: 51.498,82.454,99.462,%
Batch: 80 | Loss: 2.641 | Acc: 50.936,82.243,99.460,%
Batch: 100 | Loss: 2.642 | Acc: 51.207,81.962,99.466,%
Batch: 120 | Loss: 2.680 | Acc: 50.904,81.528,99.451,%
Batch: 140 | Loss: 2.682 | Acc: 50.892,81.477,99.463,%
Batch: 160 | Loss: 2.658 | Acc: 51.291,81.366,99.461,%
Batch: 180 | Loss: 2.662 | Acc: 51.260,81.220,99.448,%
Batch: 200 | Loss: 2.663 | Acc: 51.135,81.137,99.433,%
Batch: 220 | Loss: 2.678 | Acc: 50.976,81.077,99.424,%
Batch: 240 | Loss: 2.693 | Acc: 50.849,81.036,99.416,%
Batch: 260 | Loss: 2.690 | Acc: 50.805,81.055,99.404,%
Batch: 280 | Loss: 2.694 | Acc: 50.809,80.975,99.402,%
Batch: 300 | Loss: 2.687 | Acc: 50.862,81.055,99.393,%
Batch: 320 | Loss: 2.691 | Acc: 50.745,81.053,99.379,%
Batch: 340 | Loss: 2.698 | Acc: 50.646,80.925,99.363,%
Batch: 360 | Loss: 2.704 | Acc: 50.543,80.850,99.355,%
Batch: 380 | Loss: 2.706 | Acc: 50.474,80.854,99.352,%
Batch: 0 | Loss: 4.159 | Acc: 48.438,67.188,77.344,%
Batch: 20 | Loss: 4.541 | Acc: 43.787,65.588,73.103,%
Batch: 40 | Loss: 4.551 | Acc: 43.769,65.434,72.866,%
Batch: 60 | Loss: 4.573 | Acc: 43.545,64.908,72.643,%
Train all parameters

Epoch: 167
Batch: 0 | Loss: 3.052 | Acc: 50.000,82.031,100.000,%
Batch: 20 | Loss: 2.672 | Acc: 51.674,81.957,99.330,%
Batch: 40 | Loss: 2.643 | Acc: 51.200,82.260,99.524,%
Batch: 60 | Loss: 2.629 | Acc: 50.832,82.351,99.475,%
Batch: 80 | Loss: 2.620 | Acc: 50.656,82.186,99.450,%
Batch: 100 | Loss: 2.624 | Acc: 50.657,82.140,99.397,%
Batch: 120 | Loss: 2.662 | Acc: 50.478,81.812,99.374,%
Batch: 140 | Loss: 2.651 | Acc: 50.637,81.976,99.368,%
Batch: 160 | Loss: 2.678 | Acc: 50.349,81.818,99.389,%
Batch: 180 | Loss: 2.669 | Acc: 50.604,81.725,99.378,%
Batch: 200 | Loss: 2.673 | Acc: 50.634,81.437,99.370,%
Batch: 220 | Loss: 2.674 | Acc: 50.548,81.519,99.360,%
Batch: 240 | Loss: 2.677 | Acc: 50.551,81.483,99.361,%
Batch: 260 | Loss: 2.670 | Acc: 50.620,81.495,99.344,%
Batch: 280 | Loss: 2.656 | Acc: 50.734,81.561,99.322,%
Batch: 300 | Loss: 2.653 | Acc: 50.779,81.403,99.323,%
Batch: 320 | Loss: 2.668 | Acc: 50.638,81.282,99.323,%
Batch: 340 | Loss: 2.664 | Acc: 50.745,81.202,99.326,%
Batch: 360 | Loss: 2.671 | Acc: 50.718,81.196,99.318,%
Batch: 380 | Loss: 2.678 | Acc: 50.671,81.094,99.301,%
Batch: 0 | Loss: 4.145 | Acc: 41.406,71.094,75.000,%
Batch: 20 | Loss: 4.548 | Acc: 42.708,66.220,73.140,%
Batch: 40 | Loss: 4.582 | Acc: 42.893,65.415,72.561,%
Batch: 60 | Loss: 4.612 | Acc: 42.751,64.857,72.041,%
Train all parameters

Epoch: 168
Batch: 0 | Loss: 3.361 | Acc: 42.188,76.562,100.000,%
Batch: 20 | Loss: 2.604 | Acc: 51.749,81.622,99.479,%
Batch: 40 | Loss: 2.600 | Acc: 50.953,81.593,99.600,%
Batch: 60 | Loss: 2.608 | Acc: 51.294,81.416,99.552,%
Batch: 80 | Loss: 2.583 | Acc: 51.456,81.713,99.537,%
Batch: 100 | Loss: 2.631 | Acc: 50.882,81.420,99.513,%
Batch: 120 | Loss: 2.643 | Acc: 50.349,81.463,99.503,%
Batch: 140 | Loss: 2.639 | Acc: 50.532,81.605,99.490,%
Batch: 160 | Loss: 2.645 | Acc: 50.505,81.396,99.481,%
Batch: 180 | Loss: 2.627 | Acc: 50.807,81.513,99.469,%
Batch: 200 | Loss: 2.630 | Acc: 50.808,81.382,99.464,%
Batch: 220 | Loss: 2.649 | Acc: 50.516,81.406,99.424,%
Batch: 240 | Loss: 2.645 | Acc: 50.690,81.376,99.436,%
Batch: 260 | Loss: 2.649 | Acc: 50.638,81.358,99.419,%
Batch: 280 | Loss: 2.656 | Acc: 50.664,81.389,99.408,%
Batch: 300 | Loss: 2.664 | Acc: 50.545,81.229,99.400,%
Batch: 320 | Loss: 2.663 | Acc: 50.611,81.201,99.389,%
Batch: 340 | Loss: 2.664 | Acc: 50.680,81.241,99.391,%
Batch: 360 | Loss: 2.672 | Acc: 50.656,81.124,99.377,%
Batch: 380 | Loss: 2.677 | Acc: 50.533,81.100,99.364,%
Batch: 0 | Loss: 4.231 | Acc: 44.531,70.312,77.344,%
Batch: 20 | Loss: 4.499 | Acc: 43.750,66.815,73.400,%
Batch: 40 | Loss: 4.514 | Acc: 43.731,66.368,72.675,%
Batch: 60 | Loss: 4.542 | Acc: 43.532,65.856,72.285,%
Train all parameters

Epoch: 169
Batch: 0 | Loss: 3.009 | Acc: 46.094,78.125,98.438,%
Batch: 20 | Loss: 2.549 | Acc: 51.860,83.333,99.442,%
Batch: 40 | Loss: 2.595 | Acc: 50.953,83.441,99.333,%
Batch: 60 | Loss: 2.565 | Acc: 51.524,83.427,99.360,%
Batch: 80 | Loss: 2.570 | Acc: 51.302,83.382,99.392,%
Batch: 100 | Loss: 2.590 | Acc: 51.191,82.828,99.373,%
Batch: 120 | Loss: 2.629 | Acc: 50.904,82.490,99.406,%
Batch: 140 | Loss: 2.656 | Acc: 50.537,82.270,99.440,%
Batch: 160 | Loss: 2.653 | Acc: 50.641,82.143,99.427,%
Batch: 180 | Loss: 2.661 | Acc: 50.609,82.070,99.426,%
Batch: 200 | Loss: 2.642 | Acc: 50.793,81.942,99.440,%
Batch: 220 | Loss: 2.655 | Acc: 50.781,81.752,99.452,%
Batch: 240 | Loss: 2.654 | Acc: 50.875,81.762,99.433,%
Batch: 260 | Loss: 2.657 | Acc: 50.772,81.693,99.398,%
Batch: 280 | Loss: 2.652 | Acc: 50.834,81.664,99.369,%
Batch: 300 | Loss: 2.657 | Acc: 50.786,81.590,99.377,%
Batch: 320 | Loss: 2.659 | Acc: 50.847,81.549,99.345,%
Batch: 340 | Loss: 2.657 | Acc: 50.809,81.541,99.345,%
Batch: 360 | Loss: 2.660 | Acc: 50.816,81.460,99.344,%
Batch: 380 | Loss: 2.666 | Acc: 50.761,81.410,99.340,%
Batch: 0 | Loss: 4.191 | Acc: 44.531,70.312,76.562,%
Batch: 20 | Loss: 4.518 | Acc: 43.229,66.555,73.251,%
Batch: 40 | Loss: 4.516 | Acc: 43.369,65.968,72.790,%
Batch: 60 | Loss: 4.540 | Acc: 43.379,65.382,72.413,%
Train all parameters

Epoch: 170
Batch: 0 | Loss: 2.776 | Acc: 45.312,85.156,100.000,%
Batch: 20 | Loss: 2.367 | Acc: 53.869,82.738,99.516,%
Batch: 40 | Loss: 2.501 | Acc: 52.115,82.374,99.524,%
Batch: 60 | Loss: 2.567 | Acc: 51.409,81.967,99.539,%
Batch: 80 | Loss: 2.580 | Acc: 51.524,82.137,99.585,%
Batch: 100 | Loss: 2.615 | Acc: 50.913,82.194,99.505,%
Batch: 120 | Loss: 2.622 | Acc: 50.794,82.277,99.406,%
Batch: 140 | Loss: 2.626 | Acc: 50.970,81.970,99.385,%
Batch: 160 | Loss: 2.626 | Acc: 50.937,81.929,99.374,%
Batch: 180 | Loss: 2.659 | Acc: 50.552,81.487,99.331,%
Batch: 200 | Loss: 2.652 | Acc: 50.680,81.573,99.351,%
Batch: 220 | Loss: 2.655 | Acc: 50.696,81.476,99.350,%
Batch: 240 | Loss: 2.669 | Acc: 50.681,81.350,99.368,%
Batch: 260 | Loss: 2.669 | Acc: 50.668,81.301,99.356,%
Batch: 280 | Loss: 2.671 | Acc: 50.653,81.314,99.358,%
Batch: 300 | Loss: 2.672 | Acc: 50.680,81.362,99.362,%
Batch: 320 | Loss: 2.685 | Acc: 50.479,81.265,99.357,%
Batch: 340 | Loss: 2.688 | Acc: 50.525,81.103,99.363,%
Batch: 360 | Loss: 2.690 | Acc: 50.502,81.075,99.355,%
Batch: 380 | Loss: 2.686 | Acc: 50.486,81.150,99.354,%
Batch: 0 | Loss: 4.251 | Acc: 46.094,70.312,75.000,%
Batch: 20 | Loss: 4.523 | Acc: 43.229,66.369,73.661,%
Batch: 40 | Loss: 4.532 | Acc: 43.197,65.568,72.942,%
Batch: 60 | Loss: 4.557 | Acc: 43.097,64.997,72.413,%
Train all parameters

Epoch: 171
Batch: 0 | Loss: 2.696 | Acc: 46.094,85.938,98.438,%
Batch: 20 | Loss: 2.695 | Acc: 50.967,81.957,99.777,%
Batch: 40 | Loss: 2.613 | Acc: 51.372,82.660,99.486,%
Batch: 60 | Loss: 2.582 | Acc: 52.088,82.736,99.462,%
Batch: 80 | Loss: 2.608 | Acc: 51.408,82.359,99.431,%
Batch: 100 | Loss: 2.625 | Acc: 51.245,82.379,99.443,%
Batch: 120 | Loss: 2.631 | Acc: 51.149,82.483,99.451,%
Batch: 140 | Loss: 2.637 | Acc: 50.975,82.336,99.446,%
Batch: 160 | Loss: 2.662 | Acc: 50.849,82.046,99.418,%
Batch: 180 | Loss: 2.668 | Acc: 50.803,81.850,99.426,%
Batch: 200 | Loss: 2.683 | Acc: 50.595,81.736,99.433,%
Batch: 220 | Loss: 2.692 | Acc: 50.502,81.734,99.417,%
Batch: 240 | Loss: 2.692 | Acc: 50.473,81.752,99.407,%
Batch: 260 | Loss: 2.697 | Acc: 50.413,81.627,99.413,%
Batch: 280 | Loss: 2.702 | Acc: 50.336,81.648,99.405,%
Batch: 300 | Loss: 2.699 | Acc: 50.462,81.567,99.387,%
Batch: 320 | Loss: 2.691 | Acc: 50.540,81.596,99.377,%
Batch: 340 | Loss: 2.694 | Acc: 50.570,81.511,99.379,%
Batch: 360 | Loss: 2.700 | Acc: 50.485,81.410,99.375,%
Batch: 380 | Loss: 2.704 | Acc: 50.404,81.297,99.385,%
Batch: 0 | Loss: 4.192 | Acc: 45.312,71.875,78.125,%
Batch: 20 | Loss: 4.505 | Acc: 43.824,67.039,73.400,%
Batch: 40 | Loss: 4.539 | Acc: 43.921,65.987,72.618,%
Batch: 60 | Loss: 4.576 | Acc: 43.596,65.369,71.990,%
Train all parameters

Epoch: 172
Batch: 0 | Loss: 1.916 | Acc: 61.719,85.938,100.000,%
Batch: 20 | Loss: 2.516 | Acc: 52.307,81.659,99.256,%
Batch: 40 | Loss: 2.574 | Acc: 51.715,82.527,99.524,%
Batch: 60 | Loss: 2.571 | Acc: 51.358,82.172,99.513,%
Batch: 80 | Loss: 2.595 | Acc: 51.447,82.079,99.508,%
Batch: 100 | Loss: 2.624 | Acc: 51.075,81.931,99.513,%
Batch: 120 | Loss: 2.629 | Acc: 50.994,81.896,99.496,%
Batch: 140 | Loss: 2.621 | Acc: 51.069,81.821,99.507,%
Batch: 160 | Loss: 2.619 | Acc: 51.155,81.745,99.534,%
Batch: 180 | Loss: 2.611 | Acc: 51.230,81.738,99.517,%
Batch: 200 | Loss: 2.608 | Acc: 51.232,81.654,99.499,%
Batch: 220 | Loss: 2.609 | Acc: 51.294,81.660,99.498,%
Batch: 240 | Loss: 2.606 | Acc: 51.329,81.522,99.481,%
Batch: 260 | Loss: 2.603 | Acc: 51.296,81.480,99.488,%
Batch: 280 | Loss: 2.601 | Acc: 51.309,81.503,99.475,%
Batch: 300 | Loss: 2.603 | Acc: 51.311,81.517,99.468,%
Batch: 320 | Loss: 2.601 | Acc: 51.402,81.518,99.443,%
Batch: 340 | Loss: 2.609 | Acc: 51.411,81.461,99.423,%
Batch: 360 | Loss: 2.609 | Acc: 51.359,81.475,99.420,%
Batch: 380 | Loss: 2.610 | Acc: 51.245,81.525,99.414,%
Batch: 0 | Loss: 4.232 | Acc: 48.438,67.188,75.000,%
Batch: 20 | Loss: 4.586 | Acc: 44.048,65.588,73.065,%
Batch: 40 | Loss: 4.593 | Acc: 43.998,65.549,72.504,%
Batch: 60 | Loss: 4.612 | Acc: 43.635,64.946,71.990,%
Train all parameters

Epoch: 173
Batch: 0 | Loss: 2.696 | Acc: 46.875,85.938,99.219,%
Batch: 20 | Loss: 2.617 | Acc: 50.372,81.622,99.442,%
Batch: 40 | Loss: 2.628 | Acc: 50.457,82.298,99.524,%
Batch: 60 | Loss: 2.694 | Acc: 50.026,82.070,99.398,%
Batch: 80 | Loss: 2.663 | Acc: 50.723,82.186,99.373,%
Batch: 100 | Loss: 2.701 | Acc: 50.495,82.024,99.373,%
Batch: 120 | Loss: 2.698 | Acc: 50.639,81.934,99.380,%
Batch: 140 | Loss: 2.690 | Acc: 50.682,81.915,99.363,%
Batch: 160 | Loss: 2.672 | Acc: 50.776,82.119,99.330,%
Batch: 180 | Loss: 2.666 | Acc: 50.941,81.975,99.301,%
Batch: 200 | Loss: 2.660 | Acc: 50.987,81.977,99.308,%
Batch: 220 | Loss: 2.664 | Acc: 50.866,81.982,99.314,%
Batch: 240 | Loss: 2.666 | Acc: 50.882,81.853,99.290,%
Batch: 260 | Loss: 2.676 | Acc: 50.811,81.729,99.294,%
Batch: 280 | Loss: 2.675 | Acc: 50.817,81.706,99.297,%
Batch: 300 | Loss: 2.676 | Acc: 50.825,81.707,99.312,%
Batch: 320 | Loss: 2.676 | Acc: 50.908,81.593,99.306,%
Batch: 340 | Loss: 2.673 | Acc: 50.820,81.582,99.326,%
Batch: 360 | Loss: 2.685 | Acc: 50.699,81.525,99.320,%
Batch: 380 | Loss: 2.681 | Acc: 50.697,81.599,99.311,%
Batch: 0 | Loss: 4.167 | Acc: 51.562,68.750,77.344,%
Batch: 20 | Loss: 4.527 | Acc: 44.159,66.257,72.545,%
Batch: 40 | Loss: 4.536 | Acc: 44.074,65.968,72.294,%
Batch: 60 | Loss: 4.549 | Acc: 43.916,65.817,71.798,%
Train all parameters

Epoch: 174
Batch: 0 | Loss: 2.958 | Acc: 41.406,83.594,100.000,%
Batch: 20 | Loss: 2.713 | Acc: 49.554,82.440,99.554,%
Batch: 40 | Loss: 2.604 | Acc: 50.705,81.993,99.486,%
Batch: 60 | Loss: 2.648 | Acc: 50.564,81.993,99.539,%
Batch: 80 | Loss: 2.659 | Acc: 50.328,82.128,99.527,%
Batch: 100 | Loss: 2.606 | Acc: 50.673,82.317,99.513,%
Batch: 120 | Loss: 2.616 | Acc: 50.807,82.122,99.445,%
Batch: 140 | Loss: 2.616 | Acc: 50.665,82.015,99.463,%
Batch: 160 | Loss: 2.633 | Acc: 50.461,82.012,99.447,%
Batch: 180 | Loss: 2.630 | Acc: 50.552,82.023,99.452,%
Batch: 200 | Loss: 2.634 | Acc: 50.669,81.821,99.456,%
Batch: 220 | Loss: 2.626 | Acc: 50.930,81.681,99.452,%
Batch: 240 | Loss: 2.635 | Acc: 50.817,81.665,99.446,%
Batch: 260 | Loss: 2.643 | Acc: 50.736,81.561,99.434,%
Batch: 280 | Loss: 2.643 | Acc: 50.784,81.528,99.427,%
Batch: 300 | Loss: 2.646 | Acc: 50.698,81.416,99.432,%
Batch: 320 | Loss: 2.650 | Acc: 50.742,81.437,99.418,%
Batch: 340 | Loss: 2.655 | Acc: 50.841,81.328,99.413,%
Batch: 360 | Loss: 2.650 | Acc: 50.961,81.313,99.420,%
Batch: 380 | Loss: 2.652 | Acc: 51.013,81.309,99.418,%
Batch: 0 | Loss: 4.112 | Acc: 48.438,71.875,77.344,%
Batch: 20 | Loss: 4.499 | Acc: 43.601,66.071,73.512,%
Batch: 40 | Loss: 4.505 | Acc: 43.807,65.758,72.961,%
Batch: 60 | Loss: 4.542 | Acc: 43.430,65.394,72.195,%
Train all parameters

Epoch: 175
Batch: 0 | Loss: 2.302 | Acc: 57.031,82.031,99.219,%
Batch: 20 | Loss: 2.489 | Acc: 53.497,82.701,99.740,%
Batch: 40 | Loss: 2.552 | Acc: 51.925,83.327,99.695,%
Batch: 60 | Loss: 2.612 | Acc: 51.037,83.222,99.641,%
Batch: 80 | Loss: 2.640 | Acc: 50.945,82.841,99.595,%
Batch: 100 | Loss: 2.679 | Acc: 50.464,82.302,99.567,%
Batch: 120 | Loss: 2.697 | Acc: 50.200,82.160,99.567,%
Batch: 140 | Loss: 2.690 | Acc: 50.211,82.048,99.557,%
Batch: 160 | Loss: 2.679 | Acc: 50.505,81.973,99.529,%
Batch: 180 | Loss: 2.672 | Acc: 50.639,81.958,99.530,%
Batch: 200 | Loss: 2.679 | Acc: 50.435,81.817,99.530,%
Batch: 220 | Loss: 2.680 | Acc: 50.368,81.685,99.502,%
Batch: 240 | Loss: 2.685 | Acc: 50.370,81.616,99.514,%
Batch: 260 | Loss: 2.690 | Acc: 50.404,81.606,99.509,%
Batch: 280 | Loss: 2.679 | Acc: 50.648,81.584,99.486,%
Batch: 300 | Loss: 2.681 | Acc: 50.636,81.512,99.473,%
Batch: 320 | Loss: 2.684 | Acc: 50.664,81.508,99.469,%
Batch: 340 | Loss: 2.676 | Acc: 50.765,81.449,99.459,%
Batch: 360 | Loss: 2.671 | Acc: 50.816,81.434,99.465,%
Batch: 380 | Loss: 2.669 | Acc: 50.857,81.344,99.465,%
Batch: 0 | Loss: 4.121 | Acc: 46.875,69.531,78.125,%
Batch: 20 | Loss: 4.516 | Acc: 44.382,65.997,73.065,%
Batch: 40 | Loss: 4.521 | Acc: 44.417,65.396,72.637,%
Batch: 60 | Loss: 4.547 | Acc: 44.057,65.254,72.349,%
Train all parameters

Epoch: 176
Batch: 0 | Loss: 3.128 | Acc: 42.969,82.031,100.000,%
Batch: 20 | Loss: 2.678 | Acc: 50.446,81.473,99.219,%
Batch: 40 | Loss: 2.678 | Acc: 50.857,81.040,99.428,%
Batch: 60 | Loss: 2.680 | Acc: 50.307,81.327,99.488,%
Batch: 80 | Loss: 2.648 | Acc: 50.627,81.414,99.498,%
Batch: 100 | Loss: 2.685 | Acc: 50.286,81.335,99.466,%
Batch: 120 | Loss: 2.661 | Acc: 50.710,81.482,99.477,%
Batch: 140 | Loss: 2.646 | Acc: 51.080,81.377,99.407,%
Batch: 160 | Loss: 2.654 | Acc: 50.970,81.386,99.379,%
Batch: 180 | Loss: 2.654 | Acc: 50.993,81.341,99.383,%
Batch: 200 | Loss: 2.647 | Acc: 51.014,81.468,99.386,%
Batch: 220 | Loss: 2.646 | Acc: 51.068,81.490,99.374,%
Batch: 240 | Loss: 2.639 | Acc: 51.164,81.623,99.384,%
Batch: 260 | Loss: 2.646 | Acc: 51.122,81.567,99.380,%
Batch: 280 | Loss: 2.641 | Acc: 51.332,81.528,99.386,%
Batch: 300 | Loss: 2.653 | Acc: 51.168,81.546,99.382,%
Batch: 320 | Loss: 2.636 | Acc: 51.270,81.588,99.382,%
Batch: 340 | Loss: 2.636 | Acc: 51.324,81.628,99.372,%
Batch: 360 | Loss: 2.636 | Acc: 51.225,81.577,99.364,%
Batch: 380 | Loss: 2.641 | Acc: 51.085,81.572,99.379,%
Batch: 0 | Loss: 4.124 | Acc: 47.656,70.312,78.125,%
Batch: 20 | Loss: 4.559 | Acc: 44.122,65.811,73.438,%
Batch: 40 | Loss: 4.546 | Acc: 44.245,65.701,72.999,%
Batch: 60 | Loss: 4.566 | Acc: 44.198,65.446,72.349,%
Train all parameters

Epoch: 177
Batch: 0 | Loss: 2.576 | Acc: 51.562,78.906,100.000,%
Batch: 20 | Loss: 2.618 | Acc: 50.967,82.961,99.479,%
Batch: 40 | Loss: 2.638 | Acc: 51.143,81.822,99.390,%
Batch: 60 | Loss: 2.590 | Acc: 51.742,82.531,99.436,%
Batch: 80 | Loss: 2.599 | Acc: 51.312,82.841,99.421,%
Batch: 100 | Loss: 2.573 | Acc: 51.957,82.696,99.466,%
Batch: 120 | Loss: 2.579 | Acc: 51.821,82.477,99.458,%
Batch: 140 | Loss: 2.578 | Acc: 51.701,82.430,99.407,%
Batch: 160 | Loss: 2.597 | Acc: 51.480,82.216,99.423,%
Batch: 180 | Loss: 2.599 | Acc: 51.606,82.105,99.430,%
Batch: 200 | Loss: 2.620 | Acc: 51.438,82.121,99.421,%
Batch: 220 | Loss: 2.629 | Acc: 51.329,81.946,99.427,%
Batch: 240 | Loss: 2.630 | Acc: 51.248,81.986,99.436,%
Batch: 260 | Loss: 2.638 | Acc: 51.122,81.897,99.446,%
Batch: 280 | Loss: 2.641 | Acc: 50.981,81.845,99.416,%
Batch: 300 | Loss: 2.644 | Acc: 50.906,81.855,99.413,%
Batch: 320 | Loss: 2.642 | Acc: 50.886,81.871,99.411,%
Batch: 340 | Loss: 2.644 | Acc: 50.887,81.816,99.404,%
Batch: 360 | Loss: 2.655 | Acc: 50.736,81.681,99.407,%
Batch: 380 | Loss: 2.656 | Acc: 50.753,81.746,99.405,%
Batch: 0 | Loss: 4.043 | Acc: 47.656,71.094,78.125,%
Batch: 20 | Loss: 4.509 | Acc: 44.606,66.220,73.140,%
Batch: 40 | Loss: 4.528 | Acc: 44.836,65.644,72.694,%
Batch: 60 | Loss: 4.558 | Acc: 44.185,65.318,72.234,%
Train all parameters

Epoch: 178
Batch: 0 | Loss: 2.356 | Acc: 57.031,78.125,99.219,%
Batch: 20 | Loss: 2.672 | Acc: 51.042,82.775,99.665,%
Batch: 40 | Loss: 2.669 | Acc: 50.972,82.584,99.657,%
Batch: 60 | Loss: 2.617 | Acc: 51.665,82.659,99.552,%
Batch: 80 | Loss: 2.562 | Acc: 52.025,82.745,99.479,%
Batch: 100 | Loss: 2.567 | Acc: 51.856,82.812,99.482,%
Batch: 120 | Loss: 2.583 | Acc: 51.530,82.593,99.419,%
Batch: 140 | Loss: 2.573 | Acc: 51.773,82.702,99.429,%
Batch: 160 | Loss: 2.552 | Acc: 51.907,82.769,99.432,%
Batch: 180 | Loss: 2.559 | Acc: 51.908,82.618,99.422,%
Batch: 200 | Loss: 2.574 | Acc: 51.842,82.331,99.429,%
Batch: 220 | Loss: 2.585 | Acc: 51.601,82.194,99.413,%
Batch: 240 | Loss: 2.581 | Acc: 51.634,82.223,99.410,%
Batch: 260 | Loss: 2.578 | Acc: 51.673,82.157,99.413,%
Batch: 280 | Loss: 2.583 | Acc: 51.718,82.084,99.399,%
Batch: 300 | Loss: 2.584 | Acc: 51.765,82.021,99.416,%
Batch: 320 | Loss: 2.592 | Acc: 51.670,81.878,99.418,%
Batch: 340 | Loss: 2.591 | Acc: 51.666,81.839,99.418,%
Batch: 360 | Loss: 2.589 | Acc: 51.662,81.821,99.414,%
Batch: 380 | Loss: 2.598 | Acc: 51.540,81.732,99.389,%
Batch: 0 | Loss: 4.108 | Acc: 48.438,74.219,78.906,%
Batch: 20 | Loss: 4.520 | Acc: 43.899,66.704,72.284,%
Batch: 40 | Loss: 4.525 | Acc: 44.169,66.063,72.008,%
Batch: 60 | Loss: 4.561 | Acc: 43.788,65.459,71.619,%
Train all parameters

Epoch: 179
Batch: 0 | Loss: 1.769 | Acc: 58.594,85.938,97.656,%
Batch: 20 | Loss: 2.552 | Acc: 53.013,82.961,99.405,%
Batch: 40 | Loss: 2.641 | Acc: 51.220,82.050,99.466,%
Batch: 60 | Loss: 2.690 | Acc: 50.397,82.415,99.398,%
Batch: 80 | Loss: 2.719 | Acc: 50.395,82.523,99.412,%
Batch: 100 | Loss: 2.707 | Acc: 50.147,82.550,99.412,%
Batch: 120 | Loss: 2.703 | Acc: 50.226,82.315,99.419,%
Batch: 140 | Loss: 2.709 | Acc: 50.199,82.164,99.418,%
Batch: 160 | Loss: 2.699 | Acc: 50.170,82.085,99.423,%
Batch: 180 | Loss: 2.701 | Acc: 50.177,81.997,99.456,%
Batch: 200 | Loss: 2.689 | Acc: 50.381,81.934,99.468,%
Batch: 220 | Loss: 2.685 | Acc: 50.410,81.833,99.480,%
Batch: 240 | Loss: 2.678 | Acc: 50.447,81.827,99.462,%
Batch: 260 | Loss: 2.673 | Acc: 50.509,81.852,99.452,%
Batch: 280 | Loss: 2.667 | Acc: 50.631,81.892,99.452,%
Batch: 300 | Loss: 2.663 | Acc: 50.602,81.933,99.445,%
Batch: 320 | Loss: 2.660 | Acc: 50.696,81.924,99.440,%
Batch: 340 | Loss: 2.660 | Acc: 50.804,81.843,99.432,%
Batch: 360 | Loss: 2.660 | Acc: 50.777,81.867,99.420,%
Batch: 380 | Loss: 2.659 | Acc: 50.869,81.843,99.407,%
Batch: 0 | Loss: 4.334 | Acc: 45.312,69.531,75.781,%
Batch: 20 | Loss: 4.606 | Acc: 44.234,65.662,72.954,%
Batch: 40 | Loss: 4.574 | Acc: 44.512,65.663,72.275,%
Batch: 60 | Loss: 4.595 | Acc: 43.788,65.330,71.811,%
Train all parameters

Epoch: 180
Batch: 0 | Loss: 1.727 | Acc: 58.594,94.531,100.000,%
Batch: 20 | Loss: 2.542 | Acc: 52.195,84.226,99.554,%
Batch: 40 | Loss: 2.583 | Acc: 51.258,83.155,99.562,%
Batch: 60 | Loss: 2.630 | Acc: 50.986,83.120,99.616,%
Batch: 80 | Loss: 2.635 | Acc: 50.945,82.504,99.585,%
Batch: 100 | Loss: 2.600 | Acc: 51.446,82.580,99.544,%
Batch: 120 | Loss: 2.580 | Acc: 51.743,82.574,99.529,%
Batch: 140 | Loss: 2.606 | Acc: 51.490,82.480,99.518,%
Batch: 160 | Loss: 2.595 | Acc: 51.577,82.385,99.495,%
Batch: 180 | Loss: 2.598 | Acc: 51.493,82.428,99.504,%
Batch: 200 | Loss: 2.591 | Acc: 51.516,82.366,99.487,%
Batch: 220 | Loss: 2.590 | Acc: 51.594,82.187,99.484,%
Batch: 240 | Loss: 2.599 | Acc: 51.595,82.119,99.459,%
Batch: 260 | Loss: 2.608 | Acc: 51.338,82.148,99.446,%
Batch: 280 | Loss: 2.618 | Acc: 51.251,82.145,99.444,%
Batch: 300 | Loss: 2.621 | Acc: 51.191,82.075,99.442,%
Batch: 320 | Loss: 2.619 | Acc: 51.178,82.170,99.440,%
Batch: 340 | Loss: 2.624 | Acc: 51.168,82.089,99.439,%
Batch: 360 | Loss: 2.625 | Acc: 51.145,82.020,99.429,%
Batch: 380 | Loss: 2.629 | Acc: 51.126,81.888,99.424,%
Batch: 0 | Loss: 4.193 | Acc: 47.656,67.969,77.344,%
Batch: 20 | Loss: 4.585 | Acc: 43.824,65.997,72.284,%
Batch: 40 | Loss: 4.582 | Acc: 44.150,65.587,72.027,%
Batch: 60 | Loss: 4.586 | Acc: 43.942,65.266,72.041,%
Train all parameters

Epoch: 181
Batch: 0 | Loss: 3.558 | Acc: 39.844,72.656,100.000,%
Batch: 20 | Loss: 2.687 | Acc: 50.037,81.064,99.702,%
Batch: 40 | Loss: 2.646 | Acc: 50.743,81.803,99.466,%
Batch: 60 | Loss: 2.688 | Acc: 50.423,81.352,99.526,%
Batch: 80 | Loss: 2.700 | Acc: 50.386,81.578,99.537,%
Batch: 100 | Loss: 2.647 | Acc: 51.091,81.745,99.513,%
Batch: 120 | Loss: 2.626 | Acc: 51.265,82.038,99.438,%
Batch: 140 | Loss: 2.618 | Acc: 51.236,82.026,99.457,%
Batch: 160 | Loss: 2.617 | Acc: 51.398,82.036,99.471,%
Batch: 180 | Loss: 2.605 | Acc: 51.588,82.031,99.486,%
Batch: 200 | Loss: 2.598 | Acc: 51.726,81.969,99.495,%
Batch: 220 | Loss: 2.599 | Acc: 51.707,81.936,99.498,%
Batch: 240 | Loss: 2.602 | Acc: 51.699,81.834,99.498,%
Batch: 260 | Loss: 2.608 | Acc: 51.709,81.762,99.482,%
Batch: 280 | Loss: 2.614 | Acc: 51.596,81.767,99.463,%
Batch: 300 | Loss: 2.617 | Acc: 51.565,81.642,99.439,%
Batch: 320 | Loss: 2.622 | Acc: 51.516,81.622,99.445,%
Batch: 340 | Loss: 2.621 | Acc: 51.466,81.692,99.443,%
Batch: 360 | Loss: 2.616 | Acc: 51.424,81.739,99.450,%
Batch: 380 | Loss: 2.618 | Acc: 51.468,81.666,99.434,%
Batch: 0 | Loss: 4.081 | Acc: 50.000,71.094,81.250,%
Batch: 20 | Loss: 4.549 | Acc: 44.754,66.778,72.359,%
Batch: 40 | Loss: 4.557 | Acc: 44.970,66.159,72.104,%
Batch: 60 | Loss: 4.590 | Acc: 44.493,65.753,71.632,%
Train all parameters

Epoch: 182
Batch: 0 | Loss: 2.487 | Acc: 46.094,92.188,100.000,%
Batch: 20 | Loss: 2.657 | Acc: 51.079,82.217,99.516,%
Batch: 40 | Loss: 2.548 | Acc: 52.591,83.003,99.505,%
Batch: 60 | Loss: 2.586 | Acc: 52.369,82.748,99.565,%
Batch: 80 | Loss: 2.565 | Acc: 52.247,82.919,99.498,%
Batch: 100 | Loss: 2.580 | Acc: 51.911,82.836,99.505,%
Batch: 120 | Loss: 2.600 | Acc: 51.847,82.503,99.503,%
Batch: 140 | Loss: 2.588 | Acc: 51.867,82.691,99.479,%
Batch: 160 | Loss: 2.591 | Acc: 51.868,82.686,99.432,%
Batch: 180 | Loss: 2.575 | Acc: 51.947,82.623,99.426,%
Batch: 200 | Loss: 2.595 | Acc: 51.749,82.459,99.417,%
Batch: 220 | Loss: 2.593 | Acc: 51.877,82.381,99.417,%
Batch: 240 | Loss: 2.609 | Acc: 51.650,82.096,99.397,%
Batch: 260 | Loss: 2.598 | Acc: 51.691,82.223,99.398,%
Batch: 280 | Loss: 2.611 | Acc: 51.549,82.234,99.397,%
Batch: 300 | Loss: 2.616 | Acc: 51.539,82.208,99.393,%
Batch: 320 | Loss: 2.612 | Acc: 51.482,82.153,99.387,%
Batch: 340 | Loss: 2.614 | Acc: 51.336,82.157,99.384,%
Batch: 360 | Loss: 2.615 | Acc: 51.331,82.081,99.385,%
Batch: 380 | Loss: 2.623 | Acc: 51.185,81.992,99.368,%
Batch: 0 | Loss: 4.212 | Acc: 46.094,68.750,78.906,%
Batch: 20 | Loss: 4.550 | Acc: 44.680,66.109,72.656,%
Batch: 40 | Loss: 4.547 | Acc: 44.493,66.063,72.409,%
Batch: 60 | Loss: 4.571 | Acc: 44.211,65.497,72.041,%
Train all parameters

Epoch: 183
Batch: 0 | Loss: 2.174 | Acc: 63.281,84.375,98.438,%
Batch: 20 | Loss: 2.527 | Acc: 50.260,85.007,99.442,%
Batch: 40 | Loss: 2.524 | Acc: 50.572,85.232,99.543,%
Batch: 60 | Loss: 2.565 | Acc: 50.192,84.401,99.513,%
Batch: 80 | Loss: 2.602 | Acc: 50.058,84.066,99.479,%
Batch: 100 | Loss: 2.590 | Acc: 50.565,83.718,99.466,%
Batch: 120 | Loss: 2.606 | Acc: 50.407,83.574,99.516,%
Batch: 140 | Loss: 2.628 | Acc: 50.344,83.333,99.507,%
Batch: 160 | Loss: 2.622 | Acc: 50.514,83.215,99.471,%
Batch: 180 | Loss: 2.626 | Acc: 50.764,82.933,99.482,%
Batch: 200 | Loss: 2.622 | Acc: 50.785,82.812,99.491,%
Batch: 220 | Loss: 2.637 | Acc: 50.636,82.625,99.452,%
Batch: 240 | Loss: 2.641 | Acc: 50.587,82.475,99.426,%
Batch: 260 | Loss: 2.645 | Acc: 50.551,82.364,99.419,%
Batch: 280 | Loss: 2.642 | Acc: 50.595,82.454,99.416,%
Batch: 300 | Loss: 2.634 | Acc: 50.709,82.382,99.426,%
Batch: 320 | Loss: 2.642 | Acc: 50.589,82.331,99.406,%
Batch: 340 | Loss: 2.640 | Acc: 50.570,82.329,99.388,%
Batch: 360 | Loss: 2.638 | Acc: 50.628,82.293,99.388,%
Batch: 380 | Loss: 2.635 | Acc: 50.763,82.273,99.370,%
Batch: 0 | Loss: 4.128 | Acc: 45.312,67.969,76.562,%
Batch: 20 | Loss: 4.525 | Acc: 43.527,65.923,72.247,%
Batch: 40 | Loss: 4.569 | Acc: 43.407,65.587,71.780,%
Batch: 60 | Loss: 4.584 | Acc: 43.353,65.126,71.709,%
Train all parameters

Epoch: 184
Batch: 0 | Loss: 1.827 | Acc: 61.719,82.031,100.000,%
Batch: 20 | Loss: 2.525 | Acc: 49.851,83.408,99.591,%
Batch: 40 | Loss: 2.630 | Acc: 49.867,83.213,99.638,%
Batch: 60 | Loss: 2.624 | Acc: 50.141,82.877,99.565,%
Batch: 80 | Loss: 2.636 | Acc: 50.193,82.774,99.527,%
Batch: 100 | Loss: 2.649 | Acc: 50.147,82.650,99.528,%
Batch: 120 | Loss: 2.649 | Acc: 50.161,82.735,99.542,%
Batch: 140 | Loss: 2.632 | Acc: 50.355,82.630,99.523,%
Batch: 160 | Loss: 2.606 | Acc: 50.776,82.667,99.486,%
Batch: 180 | Loss: 2.615 | Acc: 50.781,82.588,99.469,%
Batch: 200 | Loss: 2.609 | Acc: 51.026,82.509,99.471,%
Batch: 220 | Loss: 2.597 | Acc: 51.213,82.558,99.470,%
Batch: 240 | Loss: 2.581 | Acc: 51.426,82.592,99.452,%
Batch: 260 | Loss: 2.582 | Acc: 51.287,82.486,99.434,%
Batch: 280 | Loss: 2.593 | Acc: 51.095,82.387,99.436,%
Batch: 300 | Loss: 2.586 | Acc: 51.152,82.509,99.429,%
Batch: 320 | Loss: 2.585 | Acc: 51.156,82.464,99.413,%
Batch: 340 | Loss: 2.588 | Acc: 51.249,82.322,99.384,%
Batch: 360 | Loss: 2.599 | Acc: 51.244,82.289,99.375,%
Batch: 380 | Loss: 2.601 | Acc: 51.275,82.269,99.381,%
Batch: 0 | Loss: 4.131 | Acc: 47.656,70.312,78.906,%
Batch: 20 | Loss: 4.555 | Acc: 43.452,65.997,72.991,%
Batch: 40 | Loss: 4.540 | Acc: 43.693,65.663,72.409,%
Batch: 60 | Loss: 4.579 | Acc: 43.635,65.241,71.901,%
Train classifier parameters

Epoch: 185
Batch: 0 | Loss: 3.054 | Acc: 46.094,82.031,100.000,%
Batch: 20 | Loss: 2.571 | Acc: 51.935,82.478,99.256,%
Batch: 40 | Loss: 2.586 | Acc: 51.486,82.412,98.933,%
Batch: 60 | Loss: 2.595 | Acc: 51.447,82.313,98.911,%
Batch: 80 | Loss: 2.658 | Acc: 50.849,81.530,98.891,%
Batch: 100 | Loss: 2.687 | Acc: 50.743,81.629,98.878,%
Batch: 120 | Loss: 2.687 | Acc: 50.400,81.444,98.909,%
Batch: 140 | Loss: 2.696 | Acc: 50.393,81.455,98.914,%
Batch: 160 | Loss: 2.708 | Acc: 50.044,81.493,98.937,%
Batch: 180 | Loss: 2.713 | Acc: 50.056,81.500,98.934,%
Batch: 200 | Loss: 2.717 | Acc: 49.891,81.604,98.935,%
Batch: 220 | Loss: 2.702 | Acc: 50.113,81.632,98.957,%
Batch: 240 | Loss: 2.695 | Acc: 50.295,81.506,98.985,%
Batch: 260 | Loss: 2.684 | Acc: 50.401,81.546,98.994,%
Batch: 280 | Loss: 2.689 | Acc: 50.339,81.595,99.005,%
Batch: 300 | Loss: 2.691 | Acc: 50.467,81.590,99.006,%
Batch: 320 | Loss: 2.679 | Acc: 50.623,81.681,99.014,%
Batch: 340 | Loss: 2.677 | Acc: 50.708,81.701,99.015,%
Batch: 360 | Loss: 2.681 | Acc: 50.643,81.694,99.007,%
Batch: 380 | Loss: 2.686 | Acc: 50.632,81.597,99.020,%
Batch: 0 | Loss: 4.229 | Acc: 47.656,71.875,78.125,%
Batch: 20 | Loss: 4.632 | Acc: 44.196,65.141,71.503,%
Batch: 40 | Loss: 4.617 | Acc: 43.902,65.168,71.494,%
Batch: 60 | Loss: 4.650 | Acc: 43.673,64.921,71.107,%
Train classifier parameters

Epoch: 186
Batch: 0 | Loss: 2.570 | Acc: 54.688,77.344,100.000,%
Batch: 20 | Loss: 2.869 | Acc: 49.219,80.506,99.293,%
Batch: 40 | Loss: 2.840 | Acc: 49.200,80.716,99.123,%
Batch: 60 | Loss: 2.782 | Acc: 49.769,81.301,99.142,%
Batch: 80 | Loss: 2.753 | Acc: 49.961,81.520,99.132,%
Batch: 100 | Loss: 2.745 | Acc: 49.776,81.459,99.141,%
Batch: 120 | Loss: 2.701 | Acc: 50.555,81.689,99.128,%
Batch: 140 | Loss: 2.666 | Acc: 50.931,81.992,99.113,%
Batch: 160 | Loss: 2.657 | Acc: 50.907,82.250,99.088,%
Batch: 180 | Loss: 2.651 | Acc: 50.924,82.364,99.063,%
Batch: 200 | Loss: 2.663 | Acc: 50.820,82.183,99.079,%
Batch: 220 | Loss: 2.663 | Acc: 50.817,82.197,99.102,%
Batch: 240 | Loss: 2.658 | Acc: 50.772,82.265,99.099,%
Batch: 260 | Loss: 2.655 | Acc: 50.925,82.322,99.102,%
Batch: 280 | Loss: 2.659 | Acc: 50.876,82.306,99.105,%
Batch: 300 | Loss: 2.668 | Acc: 50.869,82.293,99.120,%
Batch: 320 | Loss: 2.663 | Acc: 50.898,82.306,99.129,%
Batch: 340 | Loss: 2.673 | Acc: 50.653,82.203,99.113,%
Batch: 360 | Loss: 2.661 | Acc: 50.736,82.207,99.121,%
Batch: 380 | Loss: 2.667 | Acc: 50.660,82.140,99.104,%
Batch: 0 | Loss: 4.206 | Acc: 46.094,71.094,78.906,%
Batch: 20 | Loss: 4.608 | Acc: 44.308,66.071,71.726,%
Batch: 40 | Loss: 4.595 | Acc: 44.017,65.720,71.475,%
Batch: 60 | Loss: 4.630 | Acc: 43.840,65.369,71.132,%
Train classifier parameters

Epoch: 187
Batch: 0 | Loss: 3.180 | Acc: 46.094,82.031,96.875,%
Batch: 20 | Loss: 2.680 | Acc: 49.963,83.073,98.661,%
Batch: 40 | Loss: 2.651 | Acc: 50.724,82.832,98.742,%
Batch: 60 | Loss: 2.643 | Acc: 50.871,82.928,98.937,%
Batch: 80 | Loss: 2.616 | Acc: 51.119,82.465,98.987,%
Batch: 100 | Loss: 2.630 | Acc: 51.183,82.410,99.103,%
Batch: 120 | Loss: 2.628 | Acc: 50.968,82.277,99.135,%
Batch: 140 | Loss: 2.642 | Acc: 50.826,82.369,99.158,%
Batch: 160 | Loss: 2.663 | Acc: 50.510,82.274,99.156,%
Batch: 180 | Loss: 2.675 | Acc: 50.570,82.131,99.167,%
Batch: 200 | Loss: 2.674 | Acc: 50.634,82.156,99.160,%
Batch: 220 | Loss: 2.680 | Acc: 50.601,82.166,99.169,%
Batch: 240 | Loss: 2.665 | Acc: 50.746,82.187,99.193,%
Batch: 260 | Loss: 2.661 | Acc: 50.748,82.142,99.195,%
Batch: 280 | Loss: 2.650 | Acc: 50.934,82.126,99.216,%
Batch: 300 | Loss: 2.648 | Acc: 50.953,82.127,99.206,%
Batch: 320 | Loss: 2.653 | Acc: 50.910,82.141,99.209,%
Batch: 340 | Loss: 2.652 | Acc: 50.873,82.150,99.219,%
Batch: 360 | Loss: 2.651 | Acc: 50.928,82.075,99.210,%
Batch: 380 | Loss: 2.635 | Acc: 51.173,82.165,99.211,%
Batch: 0 | Loss: 4.218 | Acc: 47.656,71.094,78.125,%
Batch: 20 | Loss: 4.604 | Acc: 44.234,65.737,71.503,%
Batch: 40 | Loss: 4.593 | Acc: 43.902,65.530,71.437,%
Batch: 60 | Loss: 4.626 | Acc: 43.712,65.241,71.260,%
Train classifier parameters

Epoch: 188
Batch: 0 | Loss: 2.183 | Acc: 57.031,78.906,99.219,%
Batch: 20 | Loss: 2.667 | Acc: 51.042,83.929,99.479,%
Batch: 40 | Loss: 2.627 | Acc: 51.372,83.479,99.333,%
Batch: 60 | Loss: 2.636 | Acc: 51.422,83.210,99.283,%
Batch: 80 | Loss: 2.644 | Acc: 51.370,82.909,99.306,%
Batch: 100 | Loss: 2.656 | Acc: 51.052,82.820,99.319,%
Batch: 120 | Loss: 2.658 | Acc: 51.059,82.677,99.290,%
Batch: 140 | Loss: 2.660 | Acc: 51.346,82.391,99.258,%
Batch: 160 | Loss: 2.656 | Acc: 51.330,82.405,99.243,%
Batch: 180 | Loss: 2.675 | Acc: 51.226,82.290,99.236,%
Batch: 200 | Loss: 2.662 | Acc: 51.368,82.377,99.234,%
Batch: 220 | Loss: 2.668 | Acc: 51.273,82.427,99.236,%
Batch: 240 | Loss: 2.664 | Acc: 51.284,82.436,99.232,%
Batch: 260 | Loss: 2.666 | Acc: 51.179,82.384,99.255,%
Batch: 280 | Loss: 2.655 | Acc: 51.226,82.446,99.233,%
Batch: 300 | Loss: 2.653 | Acc: 51.173,82.415,99.250,%
Batch: 320 | Loss: 2.664 | Acc: 51.047,82.377,99.231,%
Batch: 340 | Loss: 2.665 | Acc: 51.118,82.327,99.212,%
Batch: 360 | Loss: 2.658 | Acc: 51.179,82.328,99.238,%
Batch: 380 | Loss: 2.656 | Acc: 51.284,82.259,99.231,%
Batch: 0 | Loss: 4.225 | Acc: 46.875,71.094,78.906,%
Batch: 20 | Loss: 4.594 | Acc: 44.271,65.625,71.801,%
Batch: 40 | Loss: 4.584 | Acc: 44.074,65.358,71.532,%
Batch: 60 | Loss: 4.616 | Acc: 43.904,65.190,71.171,%
Train classifier parameters

Epoch: 189
Batch: 0 | Loss: 2.859 | Acc: 38.281,86.719,100.000,%
Batch: 20 | Loss: 2.506 | Acc: 51.190,83.668,99.554,%
Batch: 40 | Loss: 2.514 | Acc: 51.753,83.175,99.466,%
Batch: 60 | Loss: 2.541 | Acc: 51.498,83.414,99.501,%
Batch: 80 | Loss: 2.600 | Acc: 50.810,82.485,99.402,%
Batch: 100 | Loss: 2.609 | Acc: 50.828,82.557,99.420,%
Batch: 120 | Loss: 2.604 | Acc: 51.014,82.619,99.406,%
Batch: 140 | Loss: 2.609 | Acc: 51.086,82.641,99.396,%
Batch: 160 | Loss: 2.617 | Acc: 50.946,82.609,99.374,%
Batch: 180 | Loss: 2.645 | Acc: 50.647,82.437,99.344,%
Batch: 200 | Loss: 2.628 | Acc: 50.824,82.638,99.366,%
Batch: 220 | Loss: 2.640 | Acc: 50.732,82.434,99.318,%
Batch: 240 | Loss: 2.659 | Acc: 50.580,82.323,99.316,%
Batch: 260 | Loss: 2.648 | Acc: 50.653,82.289,99.309,%
Batch: 280 | Loss: 2.643 | Acc: 50.728,82.315,99.297,%
Batch: 300 | Loss: 2.643 | Acc: 50.794,82.273,99.286,%
Batch: 320 | Loss: 2.642 | Acc: 50.762,82.360,99.263,%
Batch: 340 | Loss: 2.627 | Acc: 50.841,82.448,99.265,%
Batch: 360 | Loss: 2.622 | Acc: 50.989,82.475,99.277,%
Batch: 380 | Loss: 2.619 | Acc: 51.062,82.468,99.278,%
Batch: 0 | Loss: 4.223 | Acc: 46.094,70.312,79.688,%
Batch: 20 | Loss: 4.582 | Acc: 44.494,66.109,71.726,%
Batch: 40 | Loss: 4.572 | Acc: 44.264,65.682,71.437,%
Batch: 60 | Loss: 4.606 | Acc: 44.045,65.330,71.119,%
Train classifier parameters

Epoch: 190
Batch: 0 | Loss: 2.427 | Acc: 61.719,82.031,99.219,%
Batch: 20 | Loss: 2.591 | Acc: 51.265,84.226,99.182,%
Batch: 40 | Loss: 2.640 | Acc: 50.857,82.812,99.276,%
Batch: 60 | Loss: 2.604 | Acc: 51.025,83.210,99.321,%
Batch: 80 | Loss: 2.658 | Acc: 50.482,82.755,99.277,%
Batch: 100 | Loss: 2.662 | Acc: 50.286,82.526,99.281,%
Batch: 120 | Loss: 2.653 | Acc: 50.413,82.496,99.290,%
Batch: 140 | Loss: 2.609 | Acc: 51.108,82.574,99.280,%
Batch: 160 | Loss: 2.617 | Acc: 50.980,82.599,99.316,%
Batch: 180 | Loss: 2.616 | Acc: 50.945,82.743,99.318,%
Batch: 200 | Loss: 2.607 | Acc: 51.049,82.875,99.316,%
Batch: 220 | Loss: 2.611 | Acc: 51.089,82.777,99.325,%
Batch: 240 | Loss: 2.618 | Acc: 50.921,82.774,99.329,%
Batch: 260 | Loss: 2.608 | Acc: 51.119,82.792,99.318,%
Batch: 280 | Loss: 2.610 | Acc: 51.126,82.765,99.333,%
Batch: 300 | Loss: 2.615 | Acc: 51.088,82.729,99.320,%
Batch: 320 | Loss: 2.614 | Acc: 51.144,82.708,99.314,%
Batch: 340 | Loss: 2.604 | Acc: 51.242,82.771,99.290,%
Batch: 360 | Loss: 2.604 | Acc: 51.244,82.789,99.294,%
Batch: 380 | Loss: 2.605 | Acc: 51.271,82.771,99.303,%
Batch: 0 | Loss: 4.200 | Acc: 46.875,71.094,78.906,%
Batch: 20 | Loss: 4.581 | Acc: 44.606,65.848,72.173,%
Batch: 40 | Loss: 4.570 | Acc: 44.169,65.549,71.875,%
Batch: 60 | Loss: 4.604 | Acc: 43.929,65.266,71.376,%
Train classifier parameters

Epoch: 191
Batch: 0 | Loss: 3.328 | Acc: 41.406,75.781,100.000,%
Batch: 20 | Loss: 2.656 | Acc: 49.702,83.185,99.293,%
Batch: 40 | Loss: 2.532 | Acc: 51.677,83.651,99.333,%
Batch: 60 | Loss: 2.570 | Acc: 52.100,83.158,99.372,%
Batch: 80 | Loss: 2.605 | Acc: 51.524,82.764,99.354,%
Batch: 100 | Loss: 2.609 | Acc: 51.361,82.727,99.304,%
Batch: 120 | Loss: 2.627 | Acc: 51.098,82.580,99.329,%
Batch: 140 | Loss: 2.635 | Acc: 51.097,82.602,99.335,%
Batch: 160 | Loss: 2.636 | Acc: 51.053,82.749,99.311,%
Batch: 180 | Loss: 2.623 | Acc: 51.135,82.873,99.318,%
Batch: 200 | Loss: 2.645 | Acc: 50.929,82.793,99.304,%
Batch: 220 | Loss: 2.662 | Acc: 50.933,82.699,99.311,%
Batch: 240 | Loss: 2.666 | Acc: 50.921,82.644,99.313,%
Batch: 260 | Loss: 2.665 | Acc: 50.946,82.624,99.327,%
Batch: 280 | Loss: 2.661 | Acc: 51.006,82.568,99.322,%
Batch: 300 | Loss: 2.669 | Acc: 50.831,82.436,99.315,%
Batch: 320 | Loss: 2.669 | Acc: 50.871,82.401,99.294,%
Batch: 340 | Loss: 2.667 | Acc: 50.951,82.391,99.297,%
Batch: 360 | Loss: 2.659 | Acc: 50.957,82.436,99.307,%
Batch: 380 | Loss: 2.661 | Acc: 50.843,82.372,99.305,%
Batch: 0 | Loss: 4.223 | Acc: 46.094,69.531,79.688,%
Batch: 20 | Loss: 4.577 | Acc: 44.643,66.109,72.061,%
Batch: 40 | Loss: 4.562 | Acc: 44.245,65.644,71.742,%
Batch: 60 | Loss: 4.592 | Acc: 44.045,65.330,71.465,%
Train classifier parameters

Epoch: 192
Batch: 0 | Loss: 2.177 | Acc: 55.469,83.594,100.000,%
Batch: 20 | Loss: 2.453 | Acc: 51.786,84.003,99.330,%
Batch: 40 | Loss: 2.455 | Acc: 51.944,83.613,99.257,%
Batch: 60 | Loss: 2.489 | Acc: 52.075,82.992,99.244,%
Batch: 80 | Loss: 2.513 | Acc: 51.842,82.591,99.286,%
Batch: 100 | Loss: 2.548 | Acc: 51.671,82.658,99.281,%
Batch: 120 | Loss: 2.565 | Acc: 51.621,82.838,99.277,%
Batch: 140 | Loss: 2.597 | Acc: 51.302,82.585,99.269,%
Batch: 160 | Loss: 2.587 | Acc: 51.417,82.691,99.277,%
Batch: 180 | Loss: 2.602 | Acc: 51.429,82.424,99.266,%
Batch: 200 | Loss: 2.596 | Acc: 51.446,82.614,99.281,%
Batch: 220 | Loss: 2.587 | Acc: 51.548,82.653,99.304,%
Batch: 240 | Loss: 2.592 | Acc: 51.475,82.641,99.322,%
Batch: 260 | Loss: 2.605 | Acc: 51.461,82.546,99.306,%
Batch: 280 | Loss: 2.600 | Acc: 51.613,82.537,99.302,%
Batch: 300 | Loss: 2.595 | Acc: 51.666,82.633,99.315,%
Batch: 320 | Loss: 2.601 | Acc: 51.606,82.486,99.299,%
Batch: 340 | Loss: 2.593 | Acc: 51.705,82.522,99.306,%
Batch: 360 | Loss: 2.597 | Acc: 51.679,82.458,99.305,%
Batch: 380 | Loss: 2.606 | Acc: 51.565,82.448,99.321,%
Batch: 0 | Loss: 4.231 | Acc: 46.875,70.312,78.906,%
Batch: 20 | Loss: 4.562 | Acc: 44.382,65.885,72.024,%
Batch: 40 | Loss: 4.553 | Acc: 44.226,65.606,71.856,%
Batch: 60 | Loss: 4.585 | Acc: 44.045,65.407,71.478,%
Train classifier parameters

Epoch: 193
Batch: 0 | Loss: 3.013 | Acc: 46.875,83.594,99.219,%
Batch: 20 | Loss: 2.641 | Acc: 50.967,83.110,99.442,%
Batch: 40 | Loss: 2.575 | Acc: 52.096,83.098,99.162,%
Batch: 60 | Loss: 2.545 | Acc: 52.152,83.799,99.360,%
Batch: 80 | Loss: 2.517 | Acc: 52.575,83.671,99.344,%
Batch: 100 | Loss: 2.538 | Acc: 52.437,83.656,99.312,%
Batch: 120 | Loss: 2.559 | Acc: 52.382,83.290,99.335,%
Batch: 140 | Loss: 2.574 | Acc: 52.172,83.150,99.307,%
Batch: 160 | Loss: 2.571 | Acc: 51.970,83.327,99.272,%
Batch: 180 | Loss: 2.564 | Acc: 51.916,83.318,99.253,%
Batch: 200 | Loss: 2.564 | Acc: 51.862,83.279,99.277,%
Batch: 220 | Loss: 2.559 | Acc: 51.877,83.208,99.289,%
Batch: 240 | Loss: 2.583 | Acc: 51.741,83.033,99.316,%
Batch: 260 | Loss: 2.580 | Acc: 51.784,83.106,99.327,%
Batch: 280 | Loss: 2.570 | Acc: 51.929,83.138,99.316,%
Batch: 300 | Loss: 2.582 | Acc: 51.825,83.033,99.336,%
Batch: 320 | Loss: 2.582 | Acc: 51.769,83.092,99.353,%
Batch: 340 | Loss: 2.586 | Acc: 51.631,83.065,99.359,%
Batch: 360 | Loss: 2.582 | Acc: 51.688,83.042,99.355,%
Batch: 380 | Loss: 2.577 | Acc: 51.755,83.005,99.348,%
Batch: 0 | Loss: 4.202 | Acc: 46.875,71.094,79.688,%
Batch: 20 | Loss: 4.578 | Acc: 44.196,65.923,71.912,%
Batch: 40 | Loss: 4.566 | Acc: 44.207,65.396,71.608,%
Batch: 60 | Loss: 4.593 | Acc: 43.993,65.343,71.478,%
Train classifier parameters

Epoch: 194
Batch: 0 | Loss: 2.410 | Acc: 48.438,89.844,97.656,%
Batch: 20 | Loss: 2.550 | Acc: 51.823,84.561,99.070,%
Batch: 40 | Loss: 2.560 | Acc: 52.077,83.708,99.219,%
Batch: 60 | Loss: 2.613 | Acc: 51.319,83.107,99.232,%
Batch: 80 | Loss: 2.614 | Acc: 51.476,82.629,99.286,%
Batch: 100 | Loss: 2.601 | Acc: 51.400,82.812,99.265,%
Batch: 120 | Loss: 2.616 | Acc: 51.446,82.716,99.238,%
Batch: 140 | Loss: 2.605 | Acc: 51.524,82.713,99.230,%
Batch: 160 | Loss: 2.595 | Acc: 51.577,82.725,99.199,%
Batch: 180 | Loss: 2.592 | Acc: 51.541,82.869,99.240,%
Batch: 200 | Loss: 2.595 | Acc: 51.664,82.719,99.234,%
Batch: 220 | Loss: 2.594 | Acc: 51.584,82.671,99.251,%
Batch: 240 | Loss: 2.603 | Acc: 51.349,82.660,99.241,%
Batch: 260 | Loss: 2.619 | Acc: 51.134,82.504,99.252,%
Batch: 280 | Loss: 2.623 | Acc: 51.145,82.459,99.252,%
Batch: 300 | Loss: 2.616 | Acc: 51.165,82.556,99.255,%
Batch: 320 | Loss: 2.607 | Acc: 51.280,82.589,99.275,%
Batch: 340 | Loss: 2.609 | Acc: 51.297,82.588,99.281,%
Batch: 360 | Loss: 2.605 | Acc: 51.459,82.644,99.284,%
Batch: 380 | Loss: 2.601 | Acc: 51.454,82.694,99.284,%
Batch: 0 | Loss: 4.200 | Acc: 46.875,71.875,78.906,%
Batch: 20 | Loss: 4.557 | Acc: 44.420,66.555,72.582,%
Batch: 40 | Loss: 4.552 | Acc: 44.226,65.854,71.989,%
Batch: 60 | Loss: 4.582 | Acc: 44.109,65.625,71.644,%
Train classifier parameters

Epoch: 195
Batch: 0 | Loss: 2.560 | Acc: 46.094,87.500,99.219,%
Batch: 20 | Loss: 2.468 | Acc: 52.567,83.891,99.516,%
Batch: 40 | Loss: 2.532 | Acc: 51.829,82.584,99.562,%
Batch: 60 | Loss: 2.564 | Acc: 51.665,82.953,99.475,%
Batch: 80 | Loss: 2.568 | Acc: 51.611,83.054,99.498,%
Batch: 100 | Loss: 2.600 | Acc: 51.377,82.658,99.466,%
Batch: 120 | Loss: 2.566 | Acc: 51.724,82.948,99.425,%
Batch: 140 | Loss: 2.562 | Acc: 51.823,82.984,99.396,%
Batch: 160 | Loss: 2.578 | Acc: 51.684,83.050,99.403,%
Batch: 180 | Loss: 2.592 | Acc: 51.662,82.916,99.383,%
Batch: 200 | Loss: 2.596 | Acc: 51.726,82.704,99.359,%
Batch: 220 | Loss: 2.602 | Acc: 51.591,82.788,99.357,%
Batch: 240 | Loss: 2.592 | Acc: 51.705,82.848,99.348,%
Batch: 260 | Loss: 2.594 | Acc: 51.763,82.630,99.353,%
Batch: 280 | Loss: 2.590 | Acc: 51.918,82.610,99.366,%
Batch: 300 | Loss: 2.591 | Acc: 51.905,82.545,99.351,%
Batch: 320 | Loss: 2.597 | Acc: 51.930,82.455,99.360,%
Batch: 340 | Loss: 2.596 | Acc: 51.895,82.489,99.354,%
Batch: 360 | Loss: 2.601 | Acc: 51.861,82.492,99.342,%
Batch: 380 | Loss: 2.598 | Acc: 51.852,82.482,99.336,%
Batch: 0 | Loss: 4.198 | Acc: 49.219,71.875,80.469,%
Batch: 20 | Loss: 4.557 | Acc: 44.494,66.295,71.875,%
Batch: 40 | Loss: 4.554 | Acc: 44.264,65.625,71.665,%
Batch: 60 | Loss: 4.585 | Acc: 44.134,65.394,71.299,%
Train classifier parameters

Epoch: 196
Batch: 0 | Loss: 2.358 | Acc: 50.000,82.812,99.219,%
Batch: 20 | Loss: 2.683 | Acc: 48.996,82.180,99.405,%
Batch: 40 | Loss: 2.607 | Acc: 50.781,82.241,99.314,%
Batch: 60 | Loss: 2.654 | Acc: 50.461,82.339,99.334,%
Batch: 80 | Loss: 2.621 | Acc: 51.148,82.350,99.296,%
Batch: 100 | Loss: 2.646 | Acc: 50.657,82.279,99.296,%
Batch: 120 | Loss: 2.617 | Acc: 50.910,82.373,99.348,%
Batch: 140 | Loss: 2.616 | Acc: 51.152,82.491,99.313,%
Batch: 160 | Loss: 2.616 | Acc: 51.189,82.774,99.306,%
Batch: 180 | Loss: 2.603 | Acc: 51.437,82.903,99.318,%
Batch: 200 | Loss: 2.616 | Acc: 51.205,82.836,99.328,%
Batch: 220 | Loss: 2.620 | Acc: 51.372,82.706,99.339,%
Batch: 240 | Loss: 2.622 | Acc: 51.342,82.676,99.310,%
Batch: 260 | Loss: 2.632 | Acc: 51.167,82.609,99.291,%
Batch: 280 | Loss: 2.632 | Acc: 51.098,82.651,99.277,%
Batch: 300 | Loss: 2.633 | Acc: 51.007,82.579,99.276,%
Batch: 320 | Loss: 2.636 | Acc: 51.032,82.589,99.294,%
Batch: 340 | Loss: 2.625 | Acc: 51.104,82.691,99.324,%
Batch: 360 | Loss: 2.621 | Acc: 51.177,82.696,99.316,%
Batch: 380 | Loss: 2.623 | Acc: 51.140,82.778,99.311,%
Batch: 0 | Loss: 4.232 | Acc: 47.656,71.094,78.125,%
Batch: 20 | Loss: 4.566 | Acc: 44.606,65.923,72.061,%
Batch: 40 | Loss: 4.553 | Acc: 44.455,65.530,71.684,%
Batch: 60 | Loss: 4.580 | Acc: 44.237,65.407,71.478,%
Train classifier parameters

Epoch: 197
Batch: 0 | Loss: 3.153 | Acc: 42.188,81.250,100.000,%
Batch: 20 | Loss: 2.709 | Acc: 50.223,83.594,99.330,%
Batch: 40 | Loss: 2.652 | Acc: 51.162,83.003,99.352,%
Batch: 60 | Loss: 2.612 | Acc: 51.652,83.235,99.308,%
Batch: 80 | Loss: 2.651 | Acc: 50.945,82.764,99.325,%
Batch: 100 | Loss: 2.609 | Acc: 51.354,82.743,99.296,%
Batch: 120 | Loss: 2.566 | Acc: 51.808,83.097,99.251,%
Batch: 140 | Loss: 2.562 | Acc: 51.995,83.034,99.280,%
Batch: 160 | Loss: 2.560 | Acc: 52.014,83.026,99.282,%
Batch: 180 | Loss: 2.570 | Acc: 51.847,83.110,99.283,%
Batch: 200 | Loss: 2.563 | Acc: 51.827,83.057,99.277,%
Batch: 220 | Loss: 2.559 | Acc: 51.976,83.028,99.254,%
Batch: 240 | Loss: 2.568 | Acc: 51.968,82.897,99.254,%
Batch: 260 | Loss: 2.584 | Acc: 51.748,82.869,99.273,%
Batch: 280 | Loss: 2.586 | Acc: 51.657,82.899,99.274,%
Batch: 300 | Loss: 2.607 | Acc: 51.448,82.763,99.276,%
Batch: 320 | Loss: 2.613 | Acc: 51.378,82.686,99.287,%
Batch: 340 | Loss: 2.607 | Acc: 51.546,82.558,99.287,%
Batch: 360 | Loss: 2.619 | Acc: 51.439,82.561,99.288,%
Batch: 380 | Loss: 2.616 | Acc: 51.452,82.591,99.288,%
Batch: 0 | Loss: 4.207 | Acc: 46.875,71.094,80.469,%
Batch: 20 | Loss: 4.557 | Acc: 44.568,66.332,71.987,%
Batch: 40 | Loss: 4.550 | Acc: 44.436,65.739,71.665,%
Batch: 60 | Loss: 4.578 | Acc: 44.352,65.612,71.311,%
Train classifier parameters

Epoch: 198
Batch: 0 | Loss: 2.498 | Acc: 50.000,86.719,98.438,%
Batch: 20 | Loss: 2.769 | Acc: 50.707,82.924,99.256,%
Batch: 40 | Loss: 2.733 | Acc: 49.924,83.422,99.333,%
Batch: 60 | Loss: 2.680 | Acc: 50.730,82.646,99.321,%
Batch: 80 | Loss: 2.671 | Acc: 50.347,82.909,99.334,%
Batch: 100 | Loss: 2.652 | Acc: 50.650,82.983,99.327,%
Batch: 120 | Loss: 2.625 | Acc: 50.755,83.051,99.367,%
Batch: 140 | Loss: 2.625 | Acc: 50.837,82.851,99.357,%
Batch: 160 | Loss: 2.627 | Acc: 50.970,82.788,99.364,%
Batch: 180 | Loss: 2.642 | Acc: 50.889,82.761,99.378,%
Batch: 200 | Loss: 2.636 | Acc: 51.077,82.781,99.374,%
Batch: 220 | Loss: 2.643 | Acc: 51.046,82.781,99.357,%
Batch: 240 | Loss: 2.635 | Acc: 51.186,82.702,99.361,%
Batch: 260 | Loss: 2.641 | Acc: 51.158,82.771,99.362,%
Batch: 280 | Loss: 2.637 | Acc: 51.134,82.765,99.361,%
Batch: 300 | Loss: 2.631 | Acc: 51.225,82.771,99.364,%
Batch: 320 | Loss: 2.621 | Acc: 51.341,82.776,99.379,%
Batch: 340 | Loss: 2.625 | Acc: 51.239,82.696,99.386,%
Batch: 360 | Loss: 2.623 | Acc: 51.236,82.665,99.392,%
Batch: 380 | Loss: 2.616 | Acc: 51.226,82.728,99.389,%
Batch: 0 | Loss: 4.219 | Acc: 47.656,71.094,78.906,%
Batch: 20 | Loss: 4.549 | Acc: 44.866,66.034,72.135,%
Batch: 40 | Loss: 4.547 | Acc: 44.512,65.377,71.856,%
Batch: 60 | Loss: 4.576 | Acc: 44.288,65.318,71.452,%
Train classifier parameters

Epoch: 199
Batch: 0 | Loss: 2.609 | Acc: 52.344,84.375,99.219,%
Batch: 20 | Loss: 2.361 | Acc: 55.729,83.668,99.293,%
Batch: 40 | Loss: 2.499 | Acc: 53.601,83.194,99.257,%
Batch: 60 | Loss: 2.564 | Acc: 52.318,82.812,99.372,%
Batch: 80 | Loss: 2.531 | Acc: 52.402,83.102,99.315,%
Batch: 100 | Loss: 2.533 | Acc: 52.065,83.192,99.296,%
Batch: 120 | Loss: 2.530 | Acc: 51.982,83.181,99.296,%
Batch: 140 | Loss: 2.527 | Acc: 51.995,83.339,99.330,%
Batch: 160 | Loss: 2.537 | Acc: 51.907,83.327,99.345,%
Batch: 180 | Loss: 2.551 | Acc: 51.757,83.184,99.292,%
Batch: 200 | Loss: 2.551 | Acc: 51.788,83.131,99.304,%
Batch: 220 | Loss: 2.565 | Acc: 51.750,83.035,99.297,%
Batch: 240 | Loss: 2.580 | Acc: 51.550,82.819,99.287,%
Batch: 260 | Loss: 2.569 | Acc: 51.667,82.875,99.294,%
Batch: 280 | Loss: 2.574 | Acc: 51.613,82.857,99.277,%
Batch: 300 | Loss: 2.581 | Acc: 51.472,82.854,99.278,%
Batch: 320 | Loss: 2.581 | Acc: 51.565,82.830,99.275,%
Batch: 340 | Loss: 2.572 | Acc: 51.750,82.888,99.281,%
Batch: 360 | Loss: 2.567 | Acc: 51.770,82.858,99.271,%
Batch: 380 | Loss: 2.573 | Acc: 51.710,82.788,99.282,%
Batch: 0 | Loss: 4.205 | Acc: 46.875,71.094,79.688,%
Batch: 20 | Loss: 4.558 | Acc: 44.978,65.997,72.061,%
Batch: 40 | Loss: 4.547 | Acc: 44.303,65.454,71.780,%
Batch: 60 | Loss: 4.573 | Acc: 44.109,65.382,71.529,%
Train all parameters

Epoch: 200
Batch: 0 | Loss: 2.736 | Acc: 54.688,77.344,100.000,%
Batch: 20 | Loss: 2.512 | Acc: 52.641,82.552,99.405,%
Batch: 40 | Loss: 2.591 | Acc: 51.391,82.317,99.371,%
Batch: 60 | Loss: 2.623 | Acc: 50.730,82.287,99.462,%
Batch: 80 | Loss: 2.623 | Acc: 51.138,81.983,99.421,%
Batch: 100 | Loss: 2.622 | Acc: 51.145,81.830,99.319,%
Batch: 120 | Loss: 2.629 | Acc: 51.065,81.773,99.329,%
Batch: 140 | Loss: 2.632 | Acc: 51.108,81.821,99.346,%
Batch: 160 | Loss: 2.637 | Acc: 51.034,81.653,99.282,%
Batch: 180 | Loss: 2.636 | Acc: 51.088,81.522,99.227,%
Batch: 200 | Loss: 2.642 | Acc: 51.123,81.405,99.219,%
Batch: 220 | Loss: 2.646 | Acc: 51.114,81.356,99.222,%
Batch: 240 | Loss: 2.648 | Acc: 51.086,81.331,99.209,%
Batch: 260 | Loss: 2.656 | Acc: 51.054,81.142,99.195,%
Batch: 280 | Loss: 2.663 | Acc: 50.943,81.203,99.171,%
Batch: 300 | Loss: 2.675 | Acc: 50.781,81.136,99.167,%
Batch: 320 | Loss: 2.674 | Acc: 50.752,81.233,99.170,%
Batch: 340 | Loss: 2.669 | Acc: 50.784,81.177,99.159,%
Batch: 360 | Loss: 2.671 | Acc: 50.755,81.161,99.156,%
Batch: 380 | Loss: 2.671 | Acc: 50.775,81.180,99.149,%
Batch: 0 | Loss: 4.182 | Acc: 48.438,69.531,73.438,%
Batch: 20 | Loss: 4.520 | Acc: 44.978,65.960,73.214,%
Batch: 40 | Loss: 4.542 | Acc: 44.912,65.282,72.542,%
Batch: 60 | Loss: 4.583 | Acc: 44.211,64.652,72.118,%
Train all parameters

Epoch: 201
Batch: 0 | Loss: 1.868 | Acc: 57.031,88.281,100.000,%
Batch: 20 | Loss: 2.580 | Acc: 52.976,82.180,99.293,%
Batch: 40 | Loss: 2.600 | Acc: 51.353,82.622,99.409,%
Batch: 60 | Loss: 2.718 | Acc: 50.564,81.647,99.411,%
Batch: 80 | Loss: 2.672 | Acc: 50.897,81.935,99.421,%
Batch: 100 | Loss: 2.650 | Acc: 50.897,82.008,99.420,%
Batch: 120 | Loss: 2.642 | Acc: 51.001,82.005,99.419,%
Batch: 140 | Loss: 2.632 | Acc: 51.136,81.904,99.418,%
Batch: 160 | Loss: 2.617 | Acc: 51.407,81.954,99.423,%
Batch: 180 | Loss: 2.630 | Acc: 51.226,81.919,99.409,%
Batch: 200 | Loss: 2.638 | Acc: 51.166,81.856,99.390,%
Batch: 220 | Loss: 2.631 | Acc: 51.227,81.876,99.392,%
Batch: 240 | Loss: 2.632 | Acc: 51.277,81.655,99.368,%
Batch: 260 | Loss: 2.634 | Acc: 51.284,81.546,99.335,%
Batch: 280 | Loss: 2.637 | Acc: 51.240,81.414,99.308,%
Batch: 300 | Loss: 2.638 | Acc: 51.215,81.471,99.284,%
Batch: 320 | Loss: 2.646 | Acc: 51.168,81.360,99.243,%
Batch: 340 | Loss: 2.640 | Acc: 51.184,81.390,99.228,%
Batch: 360 | Loss: 2.640 | Acc: 51.179,81.347,99.206,%
Batch: 380 | Loss: 2.632 | Acc: 51.316,81.305,99.213,%
Batch: 0 | Loss: 4.215 | Acc: 49.219,67.188,77.344,%
Batch: 20 | Loss: 4.588 | Acc: 43.564,65.216,73.214,%
Batch: 40 | Loss: 4.612 | Acc: 43.617,64.787,71.799,%
Batch: 60 | Loss: 4.632 | Acc: 43.174,64.575,71.606,%
Train all parameters

Epoch: 202
Batch: 0 | Loss: 3.286 | Acc: 44.531,77.344,98.438,%
Batch: 20 | Loss: 2.685 | Acc: 50.595,82.850,99.330,%
Batch: 40 | Loss: 2.611 | Acc: 51.601,82.793,99.352,%
Batch: 60 | Loss: 2.638 | Acc: 51.306,82.480,99.321,%
Batch: 80 | Loss: 2.610 | Acc: 51.717,82.639,99.334,%
Batch: 100 | Loss: 2.602 | Acc: 51.880,82.820,99.319,%
Batch: 120 | Loss: 2.574 | Acc: 52.047,83.058,99.316,%
Batch: 140 | Loss: 2.584 | Acc: 51.834,83.023,99.241,%
Batch: 160 | Loss: 2.588 | Acc: 51.524,83.089,99.228,%
Batch: 180 | Loss: 2.593 | Acc: 51.511,82.929,99.180,%
Batch: 200 | Loss: 2.616 | Acc: 51.345,82.723,99.192,%
Batch: 220 | Loss: 2.624 | Acc: 51.198,82.551,99.190,%
Batch: 240 | Loss: 2.614 | Acc: 51.336,82.550,99.193,%
Batch: 260 | Loss: 2.623 | Acc: 51.215,82.349,99.189,%
Batch: 280 | Loss: 2.619 | Acc: 51.326,82.351,99.174,%
Batch: 300 | Loss: 2.617 | Acc: 51.357,82.314,99.159,%
Batch: 320 | Loss: 2.616 | Acc: 51.472,82.318,99.143,%
Batch: 340 | Loss: 2.630 | Acc: 51.155,82.144,99.132,%
Batch: 360 | Loss: 2.629 | Acc: 51.119,82.155,99.124,%
Batch: 380 | Loss: 2.639 | Acc: 51.083,82.064,99.104,%
Batch: 0 | Loss: 4.237 | Acc: 46.875,68.750,77.344,%
Batch: 20 | Loss: 4.565 | Acc: 43.043,65.067,72.582,%
Batch: 40 | Loss: 4.581 | Acc: 43.712,64.634,71.932,%
Batch: 60 | Loss: 4.599 | Acc: 43.622,64.575,71.555,%
Train all parameters

Epoch: 203
Batch: 0 | Loss: 2.398 | Acc: 53.125,82.812,99.219,%
Batch: 20 | Loss: 2.646 | Acc: 49.888,82.068,99.107,%
Batch: 40 | Loss: 2.670 | Acc: 50.171,81.650,99.143,%
Batch: 60 | Loss: 2.768 | Acc: 49.552,81.263,99.129,%
Batch: 80 | Loss: 2.750 | Acc: 49.315,81.375,99.132,%
Batch: 100 | Loss: 2.738 | Acc: 49.683,81.459,99.126,%
Batch: 120 | Loss: 2.703 | Acc: 49.987,81.463,99.148,%
Batch: 140 | Loss: 2.692 | Acc: 50.344,81.582,99.163,%
Batch: 160 | Loss: 2.688 | Acc: 50.267,81.546,99.127,%
Batch: 180 | Loss: 2.671 | Acc: 50.557,81.479,99.163,%
Batch: 200 | Loss: 2.659 | Acc: 50.719,81.608,99.164,%
Batch: 220 | Loss: 2.649 | Acc: 50.923,81.653,99.187,%
Batch: 240 | Loss: 2.653 | Acc: 51.015,81.451,99.177,%
Batch: 260 | Loss: 2.643 | Acc: 51.143,81.543,99.183,%
Batch: 280 | Loss: 2.634 | Acc: 51.309,81.420,99.194,%
Batch: 300 | Loss: 2.632 | Acc: 51.217,81.587,99.206,%
Batch: 320 | Loss: 2.640 | Acc: 51.146,81.583,99.182,%
Batch: 340 | Loss: 2.647 | Acc: 51.146,81.479,99.182,%
Batch: 360 | Loss: 2.649 | Acc: 51.086,81.401,99.167,%
Batch: 380 | Loss: 2.645 | Acc: 51.202,81.383,99.131,%
Batch: 0 | Loss: 4.186 | Acc: 47.656,69.531,75.781,%
Batch: 20 | Loss: 4.491 | Acc: 45.238,65.737,72.210,%
Batch: 40 | Loss: 4.518 | Acc: 44.817,65.625,71.761,%
Batch: 60 | Loss: 4.536 | Acc: 44.467,65.279,71.760,%
Train all parameters

Epoch: 204
Batch: 0 | Loss: 1.742 | Acc: 57.031,86.719,98.438,%
Batch: 20 | Loss: 2.479 | Acc: 52.716,83.222,99.182,%
Batch: 40 | Loss: 2.470 | Acc: 52.706,83.880,99.295,%
Batch: 60 | Loss: 2.523 | Acc: 51.947,83.158,99.308,%
Batch: 80 | Loss: 2.547 | Acc: 51.968,82.822,99.354,%
Batch: 100 | Loss: 2.568 | Acc: 51.802,82.681,99.327,%
Batch: 120 | Loss: 2.570 | Acc: 51.866,82.774,99.341,%
Batch: 140 | Loss: 2.604 | Acc: 51.441,82.425,99.374,%
Batch: 160 | Loss: 2.591 | Acc: 51.562,82.483,99.355,%
Batch: 180 | Loss: 2.594 | Acc: 51.450,82.450,99.314,%
Batch: 200 | Loss: 2.592 | Acc: 51.388,82.552,99.300,%
Batch: 220 | Loss: 2.606 | Acc: 51.290,82.402,99.279,%
Batch: 240 | Loss: 2.615 | Acc: 51.238,82.365,99.280,%
Batch: 260 | Loss: 2.611 | Acc: 51.254,82.292,99.264,%
Batch: 280 | Loss: 2.616 | Acc: 51.237,82.170,99.233,%
Batch: 300 | Loss: 2.631 | Acc: 51.145,82.047,99.216,%
Batch: 320 | Loss: 2.638 | Acc: 51.000,81.980,99.207,%
Batch: 340 | Loss: 2.647 | Acc: 50.932,81.866,99.212,%
Batch: 360 | Loss: 2.649 | Acc: 50.896,81.865,99.186,%
Batch: 380 | Loss: 2.646 | Acc: 50.956,81.867,99.178,%
Batch: 0 | Loss: 4.305 | Acc: 47.656,68.750,75.000,%
Batch: 20 | Loss: 4.607 | Acc: 44.159,64.211,72.098,%
Batch: 40 | Loss: 4.603 | Acc: 44.264,63.986,71.208,%
Batch: 60 | Loss: 4.626 | Acc: 44.006,64.024,70.876,%
Train all parameters

Epoch: 205
Batch: 0 | Loss: 2.104 | Acc: 61.719,82.812,97.656,%
Batch: 20 | Loss: 2.462 | Acc: 51.600,83.371,99.293,%
Batch: 40 | Loss: 2.471 | Acc: 52.801,83.003,99.371,%
Batch: 60 | Loss: 2.483 | Acc: 53.061,83.005,99.283,%
Batch: 80 | Loss: 2.502 | Acc: 52.614,83.169,99.296,%
Batch: 100 | Loss: 2.532 | Acc: 52.413,82.898,99.281,%
Batch: 120 | Loss: 2.509 | Acc: 52.557,83.038,99.296,%
Batch: 140 | Loss: 2.525 | Acc: 52.460,82.735,99.274,%
Batch: 160 | Loss: 2.540 | Acc: 52.383,82.604,99.267,%
Batch: 180 | Loss: 2.540 | Acc: 52.452,82.424,99.266,%
Batch: 200 | Loss: 2.540 | Acc: 52.398,82.474,99.258,%
Batch: 220 | Loss: 2.555 | Acc: 52.238,82.544,99.279,%
Batch: 240 | Loss: 2.566 | Acc: 51.857,82.488,99.274,%
Batch: 260 | Loss: 2.568 | Acc: 51.931,82.462,99.279,%
Batch: 280 | Loss: 2.569 | Acc: 51.916,82.443,99.274,%
Batch: 300 | Loss: 2.579 | Acc: 51.882,82.335,99.273,%
Batch: 320 | Loss: 2.587 | Acc: 51.825,82.219,99.265,%
Batch: 340 | Loss: 2.592 | Acc: 51.787,82.132,99.246,%
Batch: 360 | Loss: 2.602 | Acc: 51.558,82.075,99.217,%
Batch: 380 | Loss: 2.595 | Acc: 51.694,82.093,99.217,%
Batch: 0 | Loss: 4.190 | Acc: 45.312,69.531,75.000,%
Batch: 20 | Loss: 4.545 | Acc: 43.787,65.104,72.805,%
Batch: 40 | Loss: 4.559 | Acc: 44.093,65.301,72.237,%
Batch: 60 | Loss: 4.584 | Acc: 43.852,64.754,71.696,%
Train all parameters

Epoch: 206
Batch: 0 | Loss: 2.307 | Acc: 50.000,93.750,100.000,%
Batch: 20 | Loss: 2.573 | Acc: 51.935,84.412,99.554,%
Batch: 40 | Loss: 2.564 | Acc: 52.001,83.327,99.466,%
Batch: 60 | Loss: 2.538 | Acc: 52.062,82.979,99.411,%
Batch: 80 | Loss: 2.565 | Acc: 52.112,82.610,99.334,%
Batch: 100 | Loss: 2.587 | Acc: 51.771,82.689,99.281,%
Batch: 120 | Loss: 2.599 | Acc: 51.408,82.677,99.309,%
Batch: 140 | Loss: 2.599 | Acc: 51.435,82.469,99.296,%
Batch: 160 | Loss: 2.612 | Acc: 51.368,82.279,99.282,%
Batch: 180 | Loss: 2.617 | Acc: 51.355,82.381,99.296,%
Batch: 200 | Loss: 2.609 | Acc: 51.388,82.467,99.285,%
Batch: 220 | Loss: 2.599 | Acc: 51.541,82.417,99.279,%
Batch: 240 | Loss: 2.613 | Acc: 51.323,82.297,99.258,%
Batch: 260 | Loss: 2.615 | Acc: 51.224,82.334,99.255,%
Batch: 280 | Loss: 2.606 | Acc: 51.326,82.332,99.252,%
Batch: 300 | Loss: 2.607 | Acc: 51.225,82.187,99.242,%
Batch: 320 | Loss: 2.612 | Acc: 51.239,82.228,99.238,%
Batch: 340 | Loss: 2.609 | Acc: 51.327,82.270,99.230,%
Batch: 360 | Loss: 2.614 | Acc: 51.305,82.265,99.230,%
Batch: 380 | Loss: 2.620 | Acc: 51.171,82.242,99.223,%
Batch: 0 | Loss: 4.354 | Acc: 47.656,69.531,77.344,%
Batch: 20 | Loss: 4.539 | Acc: 44.717,66.257,72.991,%
Batch: 40 | Loss: 4.562 | Acc: 44.989,65.777,71.856,%
Batch: 60 | Loss: 4.590 | Acc: 44.813,65.459,71.568,%
Train all parameters

Epoch: 207
Batch: 0 | Loss: 2.541 | Acc: 57.031,77.344,100.000,%
Batch: 20 | Loss: 2.561 | Acc: 52.865,82.812,99.033,%
Batch: 40 | Loss: 2.538 | Acc: 52.896,82.927,99.143,%
Batch: 60 | Loss: 2.549 | Acc: 52.280,82.697,99.219,%
Batch: 80 | Loss: 2.552 | Acc: 52.035,83.218,99.277,%
Batch: 100 | Loss: 2.547 | Acc: 52.088,83.215,99.250,%
Batch: 120 | Loss: 2.499 | Acc: 52.479,83.542,99.277,%
Batch: 140 | Loss: 2.513 | Acc: 52.410,83.328,99.291,%
Batch: 160 | Loss: 2.524 | Acc: 52.281,83.201,99.292,%
Batch: 180 | Loss: 2.530 | Acc: 52.232,83.071,99.214,%
Batch: 200 | Loss: 2.543 | Acc: 52.138,82.770,99.211,%
Batch: 220 | Loss: 2.536 | Acc: 52.227,82.788,99.173,%
Batch: 240 | Loss: 2.548 | Acc: 52.120,82.624,99.193,%
Batch: 260 | Loss: 2.556 | Acc: 52.041,82.558,99.195,%
Batch: 280 | Loss: 2.561 | Acc: 52.013,82.573,99.174,%
Batch: 300 | Loss: 2.563 | Acc: 52.040,82.527,99.162,%
Batch: 320 | Loss: 2.573 | Acc: 51.867,82.564,99.173,%
Batch: 340 | Loss: 2.586 | Acc: 51.698,82.430,99.168,%
Batch: 360 | Loss: 2.593 | Acc: 51.643,82.408,99.186,%
Batch: 380 | Loss: 2.602 | Acc: 51.554,82.265,99.174,%
Batch: 0 | Loss: 4.163 | Acc: 45.312,69.531,75.781,%
Batch: 20 | Loss: 4.543 | Acc: 43.676,65.179,72.507,%
Batch: 40 | Loss: 4.580 | Acc: 43.559,64.405,71.551,%
Batch: 60 | Loss: 4.595 | Acc: 43.404,64.139,71.350,%
Train all parameters

Epoch: 208
Batch: 0 | Loss: 2.855 | Acc: 50.781,76.562,99.219,%
Batch: 20 | Loss: 2.582 | Acc: 51.972,82.106,99.554,%
Batch: 40 | Loss: 2.624 | Acc: 51.620,82.222,99.447,%
Batch: 60 | Loss: 2.608 | Acc: 51.780,82.057,99.398,%
Batch: 80 | Loss: 2.607 | Acc: 51.611,82.552,99.363,%
Batch: 100 | Loss: 2.588 | Acc: 52.042,82.480,99.381,%
Batch: 120 | Loss: 2.600 | Acc: 52.124,82.309,99.380,%
Batch: 140 | Loss: 2.587 | Acc: 52.200,82.414,99.368,%
Batch: 160 | Loss: 2.567 | Acc: 52.237,82.502,99.379,%
Batch: 180 | Loss: 2.569 | Acc: 52.072,82.476,99.340,%
Batch: 200 | Loss: 2.576 | Acc: 52.006,82.389,99.324,%
Batch: 220 | Loss: 2.571 | Acc: 51.898,82.420,99.325,%
Batch: 240 | Loss: 2.578 | Acc: 51.780,82.287,99.287,%
Batch: 260 | Loss: 2.567 | Acc: 52.017,82.283,99.282,%
Batch: 280 | Loss: 2.571 | Acc: 52.030,82.293,99.263,%
Batch: 300 | Loss: 2.581 | Acc: 51.915,82.319,99.252,%
Batch: 320 | Loss: 2.583 | Acc: 51.840,82.287,99.241,%
Batch: 340 | Loss: 2.587 | Acc: 51.840,82.153,99.221,%
Batch: 360 | Loss: 2.584 | Acc: 51.822,82.202,99.208,%
Batch: 380 | Loss: 2.589 | Acc: 51.772,82.210,99.184,%
Batch: 0 | Loss: 4.230 | Acc: 46.875,68.750,76.562,%
Batch: 20 | Loss: 4.518 | Acc: 44.122,65.327,73.103,%
Batch: 40 | Loss: 4.561 | Acc: 44.207,65.149,72.008,%
Batch: 60 | Loss: 4.587 | Acc: 43.545,64.728,71.568,%
Train all parameters

Epoch: 209
Batch: 0 | Loss: 2.673 | Acc: 48.438,75.781,99.219,%
Batch: 20 | Loss: 2.719 | Acc: 51.153,81.734,99.368,%
Batch: 40 | Loss: 2.612 | Acc: 52.191,82.107,99.276,%
Batch: 60 | Loss: 2.580 | Acc: 52.113,82.556,99.193,%
Batch: 80 | Loss: 2.598 | Acc: 51.852,82.658,99.286,%
Batch: 100 | Loss: 2.609 | Acc: 51.385,82.317,99.281,%
Batch: 120 | Loss: 2.590 | Acc: 51.401,82.580,99.283,%
Batch: 140 | Loss: 2.593 | Acc: 51.479,82.402,99.291,%
Batch: 160 | Loss: 2.589 | Acc: 51.538,82.322,99.296,%
Batch: 180 | Loss: 2.599 | Acc: 51.472,82.221,99.292,%
Batch: 200 | Loss: 2.604 | Acc: 51.345,82.179,99.246,%
Batch: 220 | Loss: 2.604 | Acc: 51.361,82.275,99.229,%
Batch: 240 | Loss: 2.595 | Acc: 51.446,82.398,99.238,%
Batch: 260 | Loss: 2.601 | Acc: 51.347,82.423,99.222,%
Batch: 280 | Loss: 2.601 | Acc: 51.340,82.412,99.227,%
Batch: 300 | Loss: 2.612 | Acc: 51.215,82.335,99.229,%
Batch: 320 | Loss: 2.608 | Acc: 51.280,82.372,99.216,%
Batch: 340 | Loss: 2.596 | Acc: 51.416,82.432,99.198,%
Batch: 360 | Loss: 2.607 | Acc: 51.322,82.315,99.180,%
Batch: 380 | Loss: 2.608 | Acc: 51.333,82.249,99.170,%
Batch: 0 | Loss: 4.195 | Acc: 46.094,70.312,77.344,%
Batch: 20 | Loss: 4.539 | Acc: 44.345,65.960,72.396,%
Batch: 40 | Loss: 4.549 | Acc: 44.169,65.130,71.913,%
Batch: 60 | Loss: 4.572 | Acc: 43.942,64.664,71.465,%
Train all parameters

Epoch: 210
Batch: 0 | Loss: 2.833 | Acc: 47.656,83.594,98.438,%
Batch: 20 | Loss: 2.652 | Acc: 51.562,80.171,99.293,%
Batch: 40 | Loss: 2.616 | Acc: 51.524,81.650,99.314,%
Batch: 60 | Loss: 2.602 | Acc: 51.703,81.942,99.257,%
Batch: 80 | Loss: 2.579 | Acc: 52.006,82.301,99.228,%
Batch: 100 | Loss: 2.603 | Acc: 51.663,82.109,99.203,%
Batch: 120 | Loss: 2.568 | Acc: 51.963,82.380,99.199,%
Batch: 140 | Loss: 2.579 | Acc: 51.884,82.386,99.191,%
Batch: 160 | Loss: 2.576 | Acc: 52.096,82.410,99.233,%
Batch: 180 | Loss: 2.582 | Acc: 51.981,82.316,99.232,%
Batch: 200 | Loss: 2.574 | Acc: 52.013,82.358,99.234,%
Batch: 220 | Loss: 2.573 | Acc: 52.125,82.265,99.219,%
Batch: 240 | Loss: 2.589 | Acc: 51.903,82.025,99.219,%
Batch: 260 | Loss: 2.591 | Acc: 51.889,82.070,99.237,%
Batch: 280 | Loss: 2.586 | Acc: 51.935,82.120,99.249,%
Batch: 300 | Loss: 2.582 | Acc: 51.939,82.119,99.255,%
Batch: 320 | Loss: 2.585 | Acc: 51.842,82.107,99.248,%
Batch: 340 | Loss: 2.590 | Acc: 51.801,82.006,99.232,%
Batch: 360 | Loss: 2.601 | Acc: 51.699,81.923,99.219,%
Batch: 380 | Loss: 2.604 | Acc: 51.581,81.959,99.211,%
Batch: 0 | Loss: 4.112 | Acc: 43.750,72.656,78.125,%
Batch: 20 | Loss: 4.564 | Acc: 42.894,64.658,71.503,%
Batch: 40 | Loss: 4.593 | Acc: 43.274,64.482,71.246,%
Batch: 60 | Loss: 4.621 | Acc: 43.327,64.255,70.940,%
Train all parameters

Epoch: 211
Batch: 0 | Loss: 2.496 | Acc: 48.438,88.281,100.000,%
Batch: 20 | Loss: 2.669 | Acc: 50.000,83.259,99.554,%
Batch: 40 | Loss: 2.634 | Acc: 51.086,82.755,99.524,%
Batch: 60 | Loss: 2.624 | Acc: 51.332,82.697,99.424,%
Batch: 80 | Loss: 2.610 | Acc: 51.707,82.677,99.402,%
Batch: 100 | Loss: 2.605 | Acc: 51.578,82.743,99.389,%
Batch: 120 | Loss: 2.600 | Acc: 51.595,82.645,99.374,%
Batch: 140 | Loss: 2.596 | Acc: 51.823,82.513,99.374,%
Batch: 160 | Loss: 2.600 | Acc: 51.723,82.303,99.350,%
Batch: 180 | Loss: 2.599 | Acc: 51.571,82.269,99.344,%
Batch: 200 | Loss: 2.599 | Acc: 51.524,82.393,99.331,%
Batch: 220 | Loss: 2.608 | Acc: 51.414,82.356,99.318,%
Batch: 240 | Loss: 2.614 | Acc: 51.381,82.287,99.297,%
Batch: 260 | Loss: 2.619 | Acc: 51.392,82.163,99.237,%
Batch: 280 | Loss: 2.620 | Acc: 51.346,82.193,99.224,%
Batch: 300 | Loss: 2.619 | Acc: 51.386,82.210,99.198,%
Batch: 320 | Loss: 2.620 | Acc: 51.368,82.058,99.194,%
Batch: 340 | Loss: 2.620 | Acc: 51.402,82.036,99.175,%
Batch: 360 | Loss: 2.626 | Acc: 51.459,82.027,99.173,%
Batch: 380 | Loss: 2.616 | Acc: 51.556,82.068,99.165,%
Batch: 0 | Loss: 4.107 | Acc: 50.000,68.750,78.906,%
Batch: 20 | Loss: 4.573 | Acc: 43.862,64.249,72.359,%
Batch: 40 | Loss: 4.581 | Acc: 43.845,64.196,71.894,%
Batch: 60 | Loss: 4.613 | Acc: 43.379,64.216,71.273,%
Train all parameters

Epoch: 212
Batch: 0 | Loss: 2.182 | Acc: 56.250,84.375,99.219,%
Batch: 20 | Loss: 2.487 | Acc: 52.455,83.482,99.330,%
Batch: 40 | Loss: 2.554 | Acc: 52.191,83.079,99.371,%
Batch: 60 | Loss: 2.547 | Acc: 51.960,83.081,99.436,%
Batch: 80 | Loss: 2.552 | Acc: 51.833,83.275,99.402,%
Batch: 100 | Loss: 2.579 | Acc: 51.562,83.106,99.366,%
Batch: 120 | Loss: 2.558 | Acc: 51.730,82.845,99.329,%
Batch: 140 | Loss: 2.573 | Acc: 51.651,82.718,99.346,%
Batch: 160 | Loss: 2.569 | Acc: 51.490,82.880,99.364,%
Batch: 180 | Loss: 2.565 | Acc: 51.614,82.920,99.353,%
Batch: 200 | Loss: 2.583 | Acc: 51.353,82.863,99.324,%
Batch: 220 | Loss: 2.578 | Acc: 51.432,82.887,99.286,%
Batch: 240 | Loss: 2.573 | Acc: 51.501,82.754,99.280,%
Batch: 260 | Loss: 2.582 | Acc: 51.479,82.579,99.264,%
Batch: 280 | Loss: 2.581 | Acc: 51.546,82.487,99.258,%
Batch: 300 | Loss: 2.586 | Acc: 51.555,82.379,99.258,%
Batch: 320 | Loss: 2.590 | Acc: 51.480,82.416,99.255,%
Batch: 340 | Loss: 2.590 | Acc: 51.455,82.469,99.235,%
Batch: 360 | Loss: 2.591 | Acc: 51.461,82.410,99.210,%
Batch: 380 | Loss: 2.590 | Acc: 51.458,82.322,99.192,%
Batch: 0 | Loss: 4.208 | Acc: 46.094,70.312,77.344,%
Batch: 20 | Loss: 4.568 | Acc: 44.085,65.365,72.619,%
Batch: 40 | Loss: 4.602 | Acc: 43.750,64.901,72.180,%
Batch: 60 | Loss: 4.644 | Acc: 43.468,64.421,71.401,%
Train all parameters

Epoch: 213
Batch: 0 | Loss: 1.673 | Acc: 62.500,89.844,100.000,%
Batch: 20 | Loss: 2.394 | Acc: 54.911,83.519,99.293,%
Batch: 40 | Loss: 2.523 | Acc: 52.477,82.698,99.219,%
Batch: 60 | Loss: 2.556 | Acc: 51.614,83.299,99.180,%
Batch: 80 | Loss: 2.563 | Acc: 51.726,83.034,99.171,%
Batch: 100 | Loss: 2.577 | Acc: 51.532,83.021,99.203,%
Batch: 120 | Loss: 2.567 | Acc: 51.756,83.271,99.251,%
Batch: 140 | Loss: 2.578 | Acc: 51.635,83.195,99.252,%
Batch: 160 | Loss: 2.579 | Acc: 51.849,83.225,99.262,%
Batch: 180 | Loss: 2.575 | Acc: 51.985,83.192,99.240,%
Batch: 200 | Loss: 2.562 | Acc: 52.130,83.228,99.242,%
Batch: 220 | Loss: 2.560 | Acc: 51.973,83.279,99.229,%
Batch: 240 | Loss: 2.567 | Acc: 52.039,83.117,99.238,%
Batch: 260 | Loss: 2.575 | Acc: 51.835,83.094,99.228,%
Batch: 280 | Loss: 2.564 | Acc: 51.913,82.954,99.208,%
Batch: 300 | Loss: 2.567 | Acc: 52.004,82.841,99.177,%
Batch: 320 | Loss: 2.589 | Acc: 51.796,82.589,99.163,%
Batch: 340 | Loss: 2.584 | Acc: 51.824,82.588,99.173,%
Batch: 360 | Loss: 2.585 | Acc: 51.870,82.494,99.180,%
Batch: 380 | Loss: 2.588 | Acc: 51.761,82.448,99.182,%
Batch: 0 | Loss: 4.369 | Acc: 46.875,67.969,72.656,%
Batch: 20 | Loss: 4.585 | Acc: 43.676,65.365,72.135,%
Batch: 40 | Loss: 4.613 | Acc: 43.921,64.615,71.475,%
Batch: 60 | Loss: 4.636 | Acc: 43.814,64.139,70.927,%
Train all parameters

Epoch: 214
Batch: 0 | Loss: 2.005 | Acc: 56.250,87.500,100.000,%
Batch: 20 | Loss: 2.498 | Acc: 52.939,84.040,99.516,%
Batch: 40 | Loss: 2.541 | Acc: 52.096,83.479,99.428,%
Batch: 60 | Loss: 2.579 | Acc: 51.486,83.325,99.283,%
Batch: 80 | Loss: 2.563 | Acc: 51.611,83.536,99.277,%
Batch: 100 | Loss: 2.571 | Acc: 51.562,83.199,99.319,%
Batch: 120 | Loss: 2.594 | Acc: 51.401,82.903,99.329,%
Batch: 140 | Loss: 2.605 | Acc: 51.357,82.724,99.313,%
Batch: 160 | Loss: 2.599 | Acc: 51.364,82.711,99.326,%
Batch: 180 | Loss: 2.585 | Acc: 51.653,82.696,99.318,%
Batch: 200 | Loss: 2.588 | Acc: 51.597,82.564,99.262,%
Batch: 220 | Loss: 2.589 | Acc: 51.442,82.519,99.279,%
Batch: 240 | Loss: 2.604 | Acc: 51.280,82.372,99.277,%
Batch: 260 | Loss: 2.611 | Acc: 51.125,82.372,99.273,%
Batch: 280 | Loss: 2.611 | Acc: 51.123,82.423,99.249,%
Batch: 300 | Loss: 2.614 | Acc: 51.163,82.317,99.247,%
Batch: 320 | Loss: 2.602 | Acc: 51.234,82.335,99.236,%
Batch: 340 | Loss: 2.595 | Acc: 51.290,82.334,99.200,%
Batch: 360 | Loss: 2.597 | Acc: 51.214,82.403,99.173,%
Batch: 380 | Loss: 2.583 | Acc: 51.421,82.445,99.172,%
Batch: 0 | Loss: 4.110 | Acc: 49.219,67.188,82.031,%
Batch: 20 | Loss: 4.566 | Acc: 44.308,65.365,72.321,%
Batch: 40 | Loss: 4.570 | Acc: 44.188,64.882,72.046,%
Batch: 60 | Loss: 4.612 | Acc: 43.865,64.485,71.427,%
Train all parameters

Epoch: 215
Batch: 0 | Loss: 2.266 | Acc: 57.031,88.281,100.000,%
Batch: 20 | Loss: 2.608 | Acc: 52.009,82.887,99.219,%
Batch: 40 | Loss: 2.615 | Acc: 51.353,82.965,99.257,%
Batch: 60 | Loss: 2.560 | Acc: 51.870,83.248,99.257,%
Batch: 80 | Loss: 2.593 | Acc: 51.476,82.803,99.228,%
Batch: 100 | Loss: 2.595 | Acc: 51.477,82.936,99.242,%
Batch: 120 | Loss: 2.588 | Acc: 51.750,82.761,99.251,%
Batch: 140 | Loss: 2.591 | Acc: 51.834,82.580,99.246,%
Batch: 160 | Loss: 2.585 | Acc: 51.723,82.677,99.199,%
Batch: 180 | Loss: 2.587 | Acc: 51.468,82.890,99.219,%
Batch: 200 | Loss: 2.595 | Acc: 51.458,82.789,99.188,%
Batch: 220 | Loss: 2.596 | Acc: 51.421,82.915,99.180,%
Batch: 240 | Loss: 2.609 | Acc: 51.248,82.728,99.144,%
Batch: 260 | Loss: 2.604 | Acc: 51.314,82.753,99.147,%
Batch: 280 | Loss: 2.597 | Acc: 51.404,82.737,99.149,%
Batch: 300 | Loss: 2.594 | Acc: 51.555,82.670,99.146,%
Batch: 320 | Loss: 2.584 | Acc: 51.699,82.735,99.141,%
Batch: 340 | Loss: 2.570 | Acc: 51.863,82.872,99.150,%
Batch: 360 | Loss: 2.575 | Acc: 51.859,82.724,99.134,%
Batch: 380 | Loss: 2.578 | Acc: 51.829,82.640,99.137,%
Batch: 0 | Loss: 4.226 | Acc: 48.438,67.188,76.562,%
Batch: 20 | Loss: 4.572 | Acc: 43.601,65.885,72.768,%
Batch: 40 | Loss: 4.592 | Acc: 43.693,64.729,72.256,%
Batch: 60 | Loss: 4.615 | Acc: 43.878,64.652,71.465,%
Train all parameters

Epoch: 216
Batch: 0 | Loss: 1.950 | Acc: 62.500,85.938,98.438,%
Batch: 20 | Loss: 2.325 | Acc: 54.464,84.487,99.591,%
Batch: 40 | Loss: 2.419 | Acc: 53.144,84.337,99.466,%
Batch: 60 | Loss: 2.450 | Acc: 52.971,83.991,99.308,%
Batch: 80 | Loss: 2.435 | Acc: 52.951,83.825,99.344,%
Batch: 100 | Loss: 2.457 | Acc: 52.761,83.748,99.335,%
Batch: 120 | Loss: 2.483 | Acc: 52.428,83.587,99.296,%
Batch: 140 | Loss: 2.486 | Acc: 52.277,83.511,99.318,%
Batch: 160 | Loss: 2.507 | Acc: 52.276,83.390,99.306,%
Batch: 180 | Loss: 2.536 | Acc: 51.903,83.141,99.309,%
Batch: 200 | Loss: 2.552 | Acc: 51.671,82.855,99.312,%
Batch: 220 | Loss: 2.556 | Acc: 51.541,82.911,99.293,%
Batch: 240 | Loss: 2.559 | Acc: 51.601,82.897,99.280,%
Batch: 260 | Loss: 2.564 | Acc: 51.536,82.815,99.276,%
Batch: 280 | Loss: 2.554 | Acc: 51.702,82.843,99.272,%
Batch: 300 | Loss: 2.559 | Acc: 51.747,82.755,99.260,%
Batch: 320 | Loss: 2.566 | Acc: 51.728,82.610,99.260,%
Batch: 340 | Loss: 2.568 | Acc: 51.716,82.632,99.246,%
Batch: 360 | Loss: 2.571 | Acc: 51.762,82.600,99.240,%
Batch: 380 | Loss: 2.574 | Acc: 51.761,82.609,99.219,%
Batch: 0 | Loss: 4.201 | Acc: 44.531,69.531,76.562,%
Batch: 20 | Loss: 4.527 | Acc: 43.862,66.220,72.321,%
Batch: 40 | Loss: 4.561 | Acc: 43.826,65.263,71.418,%
Batch: 60 | Loss: 4.581 | Acc: 43.660,64.985,71.145,%
Train all parameters

Epoch: 217
Batch: 0 | Loss: 2.148 | Acc: 65.625,85.938,100.000,%
Batch: 20 | Loss: 2.556 | Acc: 53.720,82.961,99.442,%
Batch: 40 | Loss: 2.580 | Acc: 53.068,82.908,99.371,%
Batch: 60 | Loss: 2.565 | Acc: 52.613,83.145,99.385,%
Batch: 80 | Loss: 2.564 | Acc: 52.122,83.111,99.431,%
Batch: 100 | Loss: 2.568 | Acc: 51.810,82.929,99.443,%
Batch: 120 | Loss: 2.568 | Acc: 51.834,82.619,99.445,%
Batch: 140 | Loss: 2.549 | Acc: 51.912,82.868,99.446,%
Batch: 160 | Loss: 2.555 | Acc: 52.082,82.856,99.427,%
Batch: 180 | Loss: 2.573 | Acc: 52.141,82.636,99.413,%
Batch: 200 | Loss: 2.575 | Acc: 52.169,82.467,99.398,%
Batch: 220 | Loss: 2.586 | Acc: 51.976,82.438,99.381,%
Batch: 240 | Loss: 2.583 | Acc: 52.068,82.423,99.358,%
Batch: 260 | Loss: 2.586 | Acc: 52.029,82.372,99.350,%
Batch: 280 | Loss: 2.594 | Acc: 51.946,82.312,99.344,%
Batch: 300 | Loss: 2.594 | Acc: 51.887,82.335,99.315,%
Batch: 320 | Loss: 2.599 | Acc: 51.857,82.255,99.321,%
Batch: 340 | Loss: 2.601 | Acc: 51.782,82.260,99.317,%
Batch: 360 | Loss: 2.600 | Acc: 51.863,82.174,99.312,%
Batch: 380 | Loss: 2.614 | Acc: 51.739,82.148,99.303,%
Batch: 0 | Loss: 4.257 | Acc: 46.875,67.188,78.906,%
Batch: 20 | Loss: 4.559 | Acc: 44.271,65.253,72.247,%
Batch: 40 | Loss: 4.558 | Acc: 44.855,64.901,71.361,%
Batch: 60 | Loss: 4.579 | Acc: 44.301,64.741,71.196,%
Train all parameters

Epoch: 218
Batch: 0 | Loss: 2.484 | Acc: 60.156,78.125,99.219,%
Batch: 20 | Loss: 2.597 | Acc: 51.265,84.375,99.628,%
Batch: 40 | Loss: 2.624 | Acc: 50.686,83.956,99.314,%
Batch: 60 | Loss: 2.582 | Acc: 51.345,83.952,99.129,%
Batch: 80 | Loss: 2.560 | Acc: 51.688,83.758,99.190,%
Batch: 100 | Loss: 2.573 | Acc: 51.516,83.679,99.219,%
Batch: 120 | Loss: 2.573 | Acc: 51.705,83.394,99.264,%
Batch: 140 | Loss: 2.581 | Acc: 51.579,83.455,99.235,%
Batch: 160 | Loss: 2.596 | Acc: 51.611,83.186,99.262,%
Batch: 180 | Loss: 2.611 | Acc: 51.446,83.007,99.288,%
Batch: 200 | Loss: 2.604 | Acc: 51.702,82.933,99.293,%
Batch: 220 | Loss: 2.613 | Acc: 51.690,82.759,99.282,%
Batch: 240 | Loss: 2.613 | Acc: 51.757,82.683,99.264,%
Batch: 260 | Loss: 2.616 | Acc: 51.751,82.606,99.276,%
Batch: 280 | Loss: 2.627 | Acc: 51.574,82.579,99.272,%
Batch: 300 | Loss: 2.629 | Acc: 51.428,82.514,99.247,%
Batch: 320 | Loss: 2.626 | Acc: 51.504,82.593,99.224,%
Batch: 340 | Loss: 2.622 | Acc: 51.505,82.515,99.230,%
Batch: 360 | Loss: 2.623 | Acc: 51.506,82.475,99.232,%
Batch: 380 | Loss: 2.623 | Acc: 51.548,82.402,99.231,%
Batch: 0 | Loss: 4.141 | Acc: 46.875,69.531,77.344,%
Batch: 20 | Loss: 4.469 | Acc: 45.275,66.109,72.879,%
Batch: 40 | Loss: 4.490 | Acc: 44.950,65.663,72.218,%
Batch: 60 | Loss: 4.522 | Acc: 44.711,65.446,71.734,%
Train all parameters

Epoch: 219
Batch: 0 | Loss: 1.762 | Acc: 58.594,89.844,100.000,%
Batch: 20 | Loss: 2.300 | Acc: 55.394,84.896,99.442,%
Batch: 40 | Loss: 2.431 | Acc: 53.392,83.899,99.447,%
Batch: 60 | Loss: 2.433 | Acc: 52.728,83.722,99.424,%
Batch: 80 | Loss: 2.438 | Acc: 52.787,83.912,99.402,%
Batch: 100 | Loss: 2.485 | Acc: 52.166,83.834,99.350,%
Batch: 120 | Loss: 2.490 | Acc: 51.969,83.833,99.309,%
Batch: 140 | Loss: 2.482 | Acc: 52.172,83.821,99.335,%
Batch: 160 | Loss: 2.482 | Acc: 52.213,83.662,99.287,%
Batch: 180 | Loss: 2.488 | Acc: 52.188,83.594,99.275,%
Batch: 200 | Loss: 2.510 | Acc: 51.990,83.446,99.242,%
Batch: 220 | Loss: 2.508 | Acc: 52.139,83.484,99.258,%
Batch: 240 | Loss: 2.510 | Acc: 52.107,83.435,99.264,%
Batch: 260 | Loss: 2.515 | Acc: 52.125,83.381,99.258,%
Batch: 280 | Loss: 2.523 | Acc: 52.107,83.252,99.263,%
Batch: 300 | Loss: 2.529 | Acc: 52.136,83.132,99.258,%
Batch: 320 | Loss: 2.534 | Acc: 52.061,83.041,99.250,%
Batch: 340 | Loss: 2.534 | Acc: 52.085,83.026,99.246,%
Batch: 360 | Loss: 2.546 | Acc: 51.989,82.949,99.238,%
Batch: 380 | Loss: 2.550 | Acc: 51.964,82.903,99.239,%
Batch: 0 | Loss: 4.181 | Acc: 48.438,66.406,74.219,%
Batch: 20 | Loss: 4.615 | Acc: 43.899,64.583,71.577,%
Batch: 40 | Loss: 4.590 | Acc: 43.998,64.863,71.684,%
Batch: 60 | Loss: 4.619 | Acc: 43.712,64.472,71.350,%
Train all parameters

Epoch: 220
Batch: 0 | Loss: 2.432 | Acc: 55.469,91.406,99.219,%
Batch: 20 | Loss: 2.564 | Acc: 52.939,83.482,99.256,%
Batch: 40 | Loss: 2.545 | Acc: 52.534,83.003,99.257,%
Batch: 60 | Loss: 2.527 | Acc: 52.536,83.235,99.168,%
Batch: 80 | Loss: 2.544 | Acc: 52.083,82.996,99.142,%
Batch: 100 | Loss: 2.542 | Acc: 52.243,82.859,99.165,%
Batch: 120 | Loss: 2.548 | Acc: 51.879,82.955,99.174,%
Batch: 140 | Loss: 2.551 | Acc: 51.690,82.835,99.191,%
Batch: 160 | Loss: 2.555 | Acc: 51.825,82.846,99.219,%
Batch: 180 | Loss: 2.574 | Acc: 51.623,82.769,99.210,%
Batch: 200 | Loss: 2.572 | Acc: 51.737,82.649,99.223,%
Batch: 220 | Loss: 2.578 | Acc: 51.601,82.600,99.222,%
Batch: 240 | Loss: 2.583 | Acc: 51.669,82.492,99.196,%
Batch: 260 | Loss: 2.570 | Acc: 51.784,82.558,99.189,%
Batch: 280 | Loss: 2.573 | Acc: 51.835,82.521,99.199,%
Batch: 300 | Loss: 2.577 | Acc: 51.871,82.431,99.180,%
Batch: 320 | Loss: 2.575 | Acc: 51.884,82.474,99.175,%
Batch: 340 | Loss: 2.575 | Acc: 51.945,82.453,99.175,%
Batch: 360 | Loss: 2.564 | Acc: 52.114,82.449,99.160,%
Batch: 380 | Loss: 2.576 | Acc: 52.063,82.279,99.159,%
Batch: 0 | Loss: 4.158 | Acc: 49.219,69.531,75.781,%
Batch: 20 | Loss: 4.579 | Acc: 44.494,65.030,71.577,%
Batch: 40 | Loss: 4.544 | Acc: 44.322,64.977,71.875,%
Batch: 60 | Loss: 4.573 | Acc: 44.301,64.921,71.337,%
Train all parameters

Epoch: 221
Batch: 0 | Loss: 3.106 | Acc: 42.188,78.906,100.000,%
Batch: 20 | Loss: 2.568 | Acc: 50.967,82.999,99.516,%
Batch: 40 | Loss: 2.560 | Acc: 51.677,83.727,99.505,%
Batch: 60 | Loss: 2.571 | Acc: 51.550,83.376,99.449,%
Batch: 80 | Loss: 2.581 | Acc: 51.119,83.748,99.412,%
Batch: 100 | Loss: 2.573 | Acc: 51.245,83.617,99.373,%
Batch: 120 | Loss: 2.583 | Acc: 51.207,83.316,99.322,%
Batch: 140 | Loss: 2.580 | Acc: 51.086,83.250,99.335,%
Batch: 160 | Loss: 2.566 | Acc: 51.300,83.196,99.311,%
Batch: 180 | Loss: 2.565 | Acc: 51.502,83.162,99.327,%
Batch: 200 | Loss: 2.563 | Acc: 51.531,83.147,99.339,%
Batch: 220 | Loss: 2.553 | Acc: 51.782,83.141,99.325,%
Batch: 240 | Loss: 2.559 | Acc: 51.838,83.120,99.306,%
Batch: 260 | Loss: 2.556 | Acc: 51.913,83.091,99.291,%
Batch: 280 | Loss: 2.563 | Acc: 51.827,82.979,99.283,%
Batch: 300 | Loss: 2.564 | Acc: 51.659,83.012,99.252,%
Batch: 320 | Loss: 2.572 | Acc: 51.628,82.844,99.236,%
Batch: 340 | Loss: 2.575 | Acc: 51.542,82.709,99.230,%
Batch: 360 | Loss: 2.576 | Acc: 51.610,82.609,99.210,%
Batch: 380 | Loss: 2.582 | Acc: 51.579,82.607,99.211,%
Batch: 0 | Loss: 4.219 | Acc: 47.656,68.750,75.781,%
Batch: 20 | Loss: 4.492 | Acc: 44.792,65.439,72.210,%
Batch: 40 | Loss: 4.514 | Acc: 44.684,65.149,71.704,%
Batch: 60 | Loss: 4.541 | Acc: 44.045,65.036,71.491,%
Train all parameters

Epoch: 222
Batch: 0 | Loss: 3.230 | Acc: 43.750,82.812,100.000,%
Batch: 20 | Loss: 2.393 | Acc: 52.269,85.342,99.330,%
Batch: 40 | Loss: 2.504 | Acc: 51.925,84.851,99.447,%
Batch: 60 | Loss: 2.505 | Acc: 52.024,84.529,99.488,%
Batch: 80 | Loss: 2.531 | Acc: 51.755,84.182,99.498,%
Batch: 100 | Loss: 2.551 | Acc: 51.508,83.787,99.497,%
Batch: 120 | Loss: 2.550 | Acc: 51.485,83.742,99.361,%
Batch: 140 | Loss: 2.562 | Acc: 51.457,83.472,99.324,%
Batch: 160 | Loss: 2.545 | Acc: 51.795,83.448,99.321,%
Batch: 180 | Loss: 2.547 | Acc: 51.727,83.318,99.301,%
Batch: 200 | Loss: 2.558 | Acc: 51.702,83.096,99.281,%
Batch: 220 | Loss: 2.577 | Acc: 51.594,83.028,99.282,%
Batch: 240 | Loss: 2.589 | Acc: 51.439,82.881,99.267,%
Batch: 260 | Loss: 2.589 | Acc: 51.419,82.878,99.258,%
Batch: 280 | Loss: 2.585 | Acc: 51.440,82.835,99.260,%
Batch: 300 | Loss: 2.585 | Acc: 51.521,82.805,99.263,%
Batch: 320 | Loss: 2.576 | Acc: 51.575,82.929,99.265,%
Batch: 340 | Loss: 2.575 | Acc: 51.601,82.906,99.274,%
Batch: 360 | Loss: 2.579 | Acc: 51.513,82.834,99.271,%
Batch: 380 | Loss: 2.585 | Acc: 51.505,82.776,99.258,%
Batch: 0 | Loss: 4.075 | Acc: 49.219,72.656,77.344,%
Batch: 20 | Loss: 4.509 | Acc: 45.164,65.960,72.173,%
Batch: 40 | Loss: 4.521 | Acc: 44.703,65.587,71.684,%
Batch: 60 | Loss: 4.540 | Acc: 44.608,65.407,71.452,%
Train all parameters

Epoch: 223
Batch: 0 | Loss: 1.995 | Acc: 60.938,86.719,100.000,%
Batch: 20 | Loss: 2.473 | Acc: 53.051,83.743,99.665,%
Batch: 40 | Loss: 2.491 | Acc: 52.687,83.632,99.657,%
Batch: 60 | Loss: 2.500 | Acc: 52.626,83.081,99.616,%
Batch: 80 | Loss: 2.456 | Acc: 53.009,83.536,99.527,%
Batch: 100 | Loss: 2.482 | Acc: 52.746,83.416,99.505,%
Batch: 120 | Loss: 2.464 | Acc: 52.783,83.607,99.425,%
Batch: 140 | Loss: 2.462 | Acc: 52.837,83.599,99.379,%
Batch: 160 | Loss: 2.490 | Acc: 52.455,83.623,99.369,%
Batch: 180 | Loss: 2.497 | Acc: 52.396,83.520,99.383,%
Batch: 200 | Loss: 2.499 | Acc: 52.511,83.516,99.374,%
Batch: 220 | Loss: 2.515 | Acc: 52.294,83.368,99.357,%
Batch: 240 | Loss: 2.526 | Acc: 52.256,83.211,99.313,%
Batch: 260 | Loss: 2.532 | Acc: 52.227,83.148,99.315,%
Batch: 280 | Loss: 2.539 | Acc: 52.121,83.160,99.308,%
Batch: 300 | Loss: 2.549 | Acc: 52.074,83.098,99.304,%
Batch: 320 | Loss: 2.545 | Acc: 52.093,83.095,99.287,%
Batch: 340 | Loss: 2.546 | Acc: 52.087,83.152,99.292,%
Batch: 360 | Loss: 2.552 | Acc: 52.084,83.040,99.275,%
Batch: 380 | Loss: 2.562 | Acc: 51.934,82.968,99.278,%
Batch: 0 | Loss: 4.114 | Acc: 50.000,70.312,77.344,%
Batch: 20 | Loss: 4.556 | Acc: 44.345,64.769,72.210,%
Batch: 40 | Loss: 4.568 | Acc: 44.455,64.729,71.704,%
Batch: 60 | Loss: 4.586 | Acc: 43.942,64.562,71.350,%
Train all parameters

Epoch: 224
Batch: 0 | Loss: 2.147 | Acc: 56.250,85.156,98.438,%
Batch: 20 | Loss: 2.656 | Acc: 50.670,83.408,99.368,%
Batch: 40 | Loss: 2.645 | Acc: 50.495,82.946,99.333,%
Batch: 60 | Loss: 2.640 | Acc: 50.474,83.338,99.347,%
Batch: 80 | Loss: 2.611 | Acc: 51.254,83.150,99.334,%
Batch: 100 | Loss: 2.606 | Acc: 51.330,82.990,99.335,%
Batch: 120 | Loss: 2.622 | Acc: 51.046,82.955,99.322,%
Batch: 140 | Loss: 2.610 | Acc: 51.302,82.995,99.324,%
Batch: 160 | Loss: 2.598 | Acc: 51.388,82.944,99.340,%
Batch: 180 | Loss: 2.586 | Acc: 51.468,82.920,99.292,%
Batch: 200 | Loss: 2.583 | Acc: 51.586,82.984,99.281,%
Batch: 220 | Loss: 2.574 | Acc: 51.771,82.975,99.279,%
Batch: 240 | Loss: 2.571 | Acc: 51.819,83.075,99.271,%
Batch: 260 | Loss: 2.572 | Acc: 51.862,82.857,99.264,%
Batch: 280 | Loss: 2.570 | Acc: 51.943,82.812,99.260,%
Batch: 300 | Loss: 2.566 | Acc: 52.043,82.857,99.245,%
Batch: 320 | Loss: 2.570 | Acc: 51.979,82.837,99.236,%
Batch: 340 | Loss: 2.570 | Acc: 52.028,82.803,99.239,%
Batch: 360 | Loss: 2.568 | Acc: 52.023,82.821,99.223,%
Batch: 380 | Loss: 2.572 | Acc: 52.012,82.778,99.219,%
Batch: 0 | Loss: 4.164 | Acc: 46.875,70.312,77.344,%
Batch: 20 | Loss: 4.554 | Acc: 44.680,66.146,72.135,%
Batch: 40 | Loss: 4.558 | Acc: 44.950,65.796,71.818,%
Batch: 60 | Loss: 4.580 | Acc: 44.711,65.446,71.529,%
Train all parameters

Epoch: 225
Batch: 0 | Loss: 2.428 | Acc: 50.781,85.156,99.219,%
Batch: 20 | Loss: 2.378 | Acc: 54.390,84.561,99.293,%
Batch: 40 | Loss: 2.432 | Acc: 53.639,83.537,99.295,%
Batch: 60 | Loss: 2.462 | Acc: 52.971,83.901,99.436,%
Batch: 80 | Loss: 2.437 | Acc: 53.299,84.558,99.470,%
Batch: 100 | Loss: 2.411 | Acc: 53.674,84.824,99.513,%
Batch: 120 | Loss: 2.402 | Acc: 53.693,85.182,99.522,%
Batch: 140 | Loss: 2.404 | Acc: 53.579,85.306,99.551,%
Batch: 160 | Loss: 2.393 | Acc: 53.470,85.341,99.549,%
Batch: 180 | Loss: 2.388 | Acc: 53.686,85.480,99.551,%
Batch: 200 | Loss: 2.394 | Acc: 53.572,85.553,99.576,%
Batch: 220 | Loss: 2.412 | Acc: 53.362,85.407,99.579,%
Batch: 240 | Loss: 2.421 | Acc: 53.119,85.422,99.585,%
Batch: 260 | Loss: 2.419 | Acc: 53.080,85.357,99.602,%
Batch: 280 | Loss: 2.433 | Acc: 52.905,85.298,99.614,%
Batch: 300 | Loss: 2.437 | Acc: 52.827,85.219,99.613,%
Batch: 320 | Loss: 2.429 | Acc: 52.974,85.239,99.618,%
Batch: 340 | Loss: 2.420 | Acc: 53.104,85.399,99.617,%
Batch: 360 | Loss: 2.420 | Acc: 53.173,85.442,99.632,%
Batch: 380 | Loss: 2.420 | Acc: 53.271,85.423,99.635,%
Batch: 0 | Loss: 3.961 | Acc: 50.000,71.875,77.344,%
Batch: 20 | Loss: 4.380 | Acc: 45.350,67.150,73.140,%
Batch: 40 | Loss: 4.386 | Acc: 45.427,66.845,72.999,%
Batch: 60 | Loss: 4.412 | Acc: 45.146,66.368,72.669,%
Train all parameters

Epoch: 226
Batch: 0 | Loss: 2.146 | Acc: 59.375,81.250,100.000,%
Batch: 20 | Loss: 2.490 | Acc: 53.237,85.454,99.851,%
Batch: 40 | Loss: 2.535 | Acc: 52.058,85.614,99.905,%
Batch: 60 | Loss: 2.505 | Acc: 52.075,86.053,99.859,%
Batch: 80 | Loss: 2.528 | Acc: 52.315,85.503,99.817,%
Batch: 100 | Loss: 2.496 | Acc: 52.382,85.667,99.783,%
Batch: 120 | Loss: 2.459 | Acc: 52.893,85.486,99.787,%
Batch: 140 | Loss: 2.424 | Acc: 53.158,85.749,99.756,%
Batch: 160 | Loss: 2.423 | Acc: 53.120,85.928,99.786,%
Batch: 180 | Loss: 2.422 | Acc: 52.987,86.089,99.784,%
Batch: 200 | Loss: 2.445 | Acc: 52.810,85.910,99.786,%
Batch: 220 | Loss: 2.425 | Acc: 53.097,86.072,99.788,%
Batch: 240 | Loss: 2.423 | Acc: 53.099,86.116,99.783,%
Batch: 260 | Loss: 2.420 | Acc: 53.083,86.174,99.796,%
Batch: 280 | Loss: 2.424 | Acc: 53.144,86.196,99.797,%
Batch: 300 | Loss: 2.431 | Acc: 53.117,86.117,99.800,%
Batch: 320 | Loss: 2.430 | Acc: 53.062,86.093,99.793,%
Batch: 340 | Loss: 2.426 | Acc: 53.136,86.132,99.798,%
Batch: 360 | Loss: 2.418 | Acc: 53.242,86.067,99.797,%
Batch: 380 | Loss: 2.408 | Acc: 53.326,86.089,99.801,%
Batch: 0 | Loss: 3.992 | Acc: 50.781,71.875,78.125,%
Batch: 20 | Loss: 4.370 | Acc: 46.205,67.374,73.549,%
Batch: 40 | Loss: 4.374 | Acc: 46.018,67.016,73.075,%
Batch: 60 | Loss: 4.392 | Acc: 45.774,66.675,72.861,%
Train all parameters

Epoch: 227
Batch: 0 | Loss: 2.152 | Acc: 57.812,82.812,99.219,%
Batch: 20 | Loss: 2.276 | Acc: 55.804,86.347,99.740,%
Batch: 40 | Loss: 2.307 | Acc: 54.840,86.966,99.771,%
Batch: 60 | Loss: 2.274 | Acc: 55.418,87.205,99.795,%
Batch: 80 | Loss: 2.275 | Acc: 55.266,87.278,99.807,%
Batch: 100 | Loss: 2.251 | Acc: 55.283,87.500,99.791,%
Batch: 120 | Loss: 2.282 | Acc: 54.855,87.145,99.806,%
Batch: 140 | Loss: 2.289 | Acc: 54.704,87.195,99.812,%
Batch: 160 | Loss: 2.297 | Acc: 54.421,87.189,99.825,%
Batch: 180 | Loss: 2.295 | Acc: 54.597,87.124,99.827,%
Batch: 200 | Loss: 2.306 | Acc: 54.419,86.925,99.837,%
Batch: 220 | Loss: 2.325 | Acc: 54.118,86.910,99.827,%
Batch: 240 | Loss: 2.328 | Acc: 54.256,86.878,99.838,%
Batch: 260 | Loss: 2.331 | Acc: 54.176,86.970,99.841,%
Batch: 280 | Loss: 2.336 | Acc: 54.115,86.980,99.836,%
Batch: 300 | Loss: 2.339 | Acc: 54.005,86.989,99.834,%
Batch: 320 | Loss: 2.343 | Acc: 53.928,86.943,99.827,%
Batch: 340 | Loss: 2.337 | Acc: 53.989,86.985,99.828,%
Batch: 360 | Loss: 2.345 | Acc: 53.976,86.959,99.825,%
Batch: 380 | Loss: 2.345 | Acc: 53.947,86.928,99.822,%
Batch: 0 | Loss: 3.957 | Acc: 49.219,72.656,79.688,%
Batch: 20 | Loss: 4.347 | Acc: 45.536,67.076,73.289,%
Batch: 40 | Loss: 4.361 | Acc: 45.598,66.997,72.885,%
Batch: 60 | Loss: 4.383 | Acc: 45.415,66.842,72.772,%
Train all parameters

Epoch: 228
Batch: 0 | Loss: 1.779 | Acc: 58.594,90.625,100.000,%
Batch: 20 | Loss: 2.149 | Acc: 55.134,88.765,99.926,%
Batch: 40 | Loss: 2.208 | Acc: 55.564,87.767,99.848,%
Batch: 60 | Loss: 2.268 | Acc: 54.623,87.487,99.846,%
Batch: 80 | Loss: 2.279 | Acc: 54.138,87.220,99.846,%
Batch: 100 | Loss: 2.312 | Acc: 53.852,87.129,99.845,%
Batch: 120 | Loss: 2.305 | Acc: 53.855,87.216,99.858,%
Batch: 140 | Loss: 2.320 | Acc: 53.723,87.267,99.834,%
Batch: 160 | Loss: 2.326 | Acc: 53.741,87.267,99.820,%
Batch: 180 | Loss: 2.337 | Acc: 53.509,87.237,99.832,%
Batch: 200 | Loss: 2.344 | Acc: 53.405,87.135,99.837,%
Batch: 220 | Loss: 2.338 | Acc: 53.471,87.161,99.837,%
Batch: 240 | Loss: 2.341 | Acc: 53.530,87.095,99.831,%
Batch: 260 | Loss: 2.346 | Acc: 53.400,87.144,99.826,%
Batch: 280 | Loss: 2.335 | Acc: 53.592,86.997,99.830,%
Batch: 300 | Loss: 2.341 | Acc: 53.673,86.958,99.826,%
Batch: 320 | Loss: 2.340 | Acc: 53.658,86.935,99.832,%
Batch: 340 | Loss: 2.343 | Acc: 53.592,86.996,99.837,%
Batch: 360 | Loss: 2.336 | Acc: 53.718,87.065,99.840,%
Batch: 380 | Loss: 2.343 | Acc: 53.615,87.061,99.836,%
Batch: 0 | Loss: 3.943 | Acc: 49.219,71.875,78.125,%
Batch: 20 | Loss: 4.356 | Acc: 46.243,67.448,73.586,%
Batch: 40 | Loss: 4.361 | Acc: 45.998,67.168,73.171,%
Batch: 60 | Loss: 4.383 | Acc: 45.594,66.829,73.092,%
Train all parameters

Epoch: 229
Batch: 0 | Loss: 1.502 | Acc: 65.625,92.969,100.000,%
Batch: 20 | Loss: 2.366 | Acc: 53.162,87.165,99.926,%
Batch: 40 | Loss: 2.334 | Acc: 53.316,87.214,99.943,%
Batch: 60 | Loss: 2.336 | Acc: 53.189,87.372,99.885,%
Batch: 80 | Loss: 2.322 | Acc: 53.636,87.490,99.904,%
Batch: 100 | Loss: 2.327 | Acc: 54.053,87.106,99.892,%
Batch: 120 | Loss: 2.354 | Acc: 53.777,86.893,99.884,%
Batch: 140 | Loss: 2.373 | Acc: 53.674,86.863,99.878,%
Batch: 160 | Loss: 2.380 | Acc: 53.552,86.792,99.874,%
Batch: 180 | Loss: 2.367 | Acc: 53.669,86.887,99.879,%
Batch: 200 | Loss: 2.367 | Acc: 53.545,86.859,99.880,%
Batch: 220 | Loss: 2.380 | Acc: 53.334,86.772,99.883,%
Batch: 240 | Loss: 2.378 | Acc: 53.352,86.677,99.883,%
Batch: 260 | Loss: 2.379 | Acc: 53.388,86.755,99.883,%
Batch: 280 | Loss: 2.396 | Acc: 53.261,86.549,99.883,%
Batch: 300 | Loss: 2.395 | Acc: 53.377,86.555,99.878,%
Batch: 320 | Loss: 2.395 | Acc: 53.371,86.568,99.876,%
Batch: 340 | Loss: 2.396 | Acc: 53.395,86.561,99.879,%
Batch: 360 | Loss: 2.402 | Acc: 53.344,86.453,99.874,%
Batch: 380 | Loss: 2.397 | Acc: 53.390,86.385,99.871,%
Batch: 0 | Loss: 3.952 | Acc: 49.219,72.656,78.125,%
Batch: 20 | Loss: 4.338 | Acc: 46.131,67.225,73.772,%
Batch: 40 | Loss: 4.357 | Acc: 45.979,66.883,73.323,%
Batch: 60 | Loss: 4.381 | Acc: 45.581,66.778,73.040,%
Train all parameters

Epoch: 230
Batch: 0 | Loss: 2.875 | Acc: 53.125,89.062,98.438,%
Batch: 20 | Loss: 2.403 | Acc: 54.762,85.268,99.851,%
Batch: 40 | Loss: 2.422 | Acc: 54.973,85.899,99.829,%
Batch: 60 | Loss: 2.348 | Acc: 55.149,87.001,99.846,%
Batch: 80 | Loss: 2.412 | Acc: 54.196,86.391,99.846,%
Batch: 100 | Loss: 2.376 | Acc: 54.285,86.734,99.853,%
Batch: 120 | Loss: 2.380 | Acc: 53.951,87.016,99.839,%
Batch: 140 | Loss: 2.369 | Acc: 53.834,86.963,99.828,%
Batch: 160 | Loss: 2.386 | Acc: 53.440,86.758,99.825,%
Batch: 180 | Loss: 2.380 | Acc: 53.652,86.676,99.827,%
Batch: 200 | Loss: 2.371 | Acc: 53.708,86.680,99.841,%
Batch: 220 | Loss: 2.370 | Acc: 53.779,86.807,99.841,%
Batch: 240 | Loss: 2.375 | Acc: 53.673,86.732,99.854,%
Batch: 260 | Loss: 2.374 | Acc: 53.703,86.761,99.856,%
Batch: 280 | Loss: 2.386 | Acc: 53.509,86.719,99.853,%
Batch: 300 | Loss: 2.391 | Acc: 53.548,86.605,99.857,%
Batch: 320 | Loss: 2.402 | Acc: 53.395,86.534,99.842,%
Batch: 340 | Loss: 2.402 | Acc: 53.377,86.519,99.837,%
Batch: 360 | Loss: 2.402 | Acc: 53.344,86.522,99.831,%
Batch: 380 | Loss: 2.393 | Acc: 53.404,86.536,99.832,%
Batch: 0 | Loss: 3.971 | Acc: 48.438,71.875,78.125,%
Batch: 20 | Loss: 4.343 | Acc: 45.833,67.188,73.810,%
Batch: 40 | Loss: 4.357 | Acc: 45.655,66.883,73.114,%
Batch: 60 | Loss: 4.377 | Acc: 45.569,66.586,73.002,%
Train all parameters

Epoch: 231
Batch: 0 | Loss: 1.865 | Acc: 61.719,85.156,100.000,%
Batch: 20 | Loss: 2.361 | Acc: 52.976,86.496,99.926,%
Batch: 40 | Loss: 2.401 | Acc: 52.287,86.681,99.924,%
Batch: 60 | Loss: 2.374 | Acc: 52.421,86.821,99.898,%
Batch: 80 | Loss: 2.389 | Acc: 53.019,86.603,99.913,%
Batch: 100 | Loss: 2.378 | Acc: 53.086,86.556,99.884,%
Batch: 120 | Loss: 2.330 | Acc: 53.687,87.197,99.897,%
Batch: 140 | Loss: 2.328 | Acc: 53.912,87.184,99.900,%
Batch: 160 | Loss: 2.324 | Acc: 53.984,87.291,99.893,%
Batch: 180 | Loss: 2.325 | Acc: 53.924,87.358,99.892,%
Batch: 200 | Loss: 2.322 | Acc: 53.860,87.372,99.891,%
Batch: 220 | Loss: 2.327 | Acc: 53.874,87.242,99.887,%
Batch: 240 | Loss: 2.322 | Acc: 53.858,87.283,99.887,%
Batch: 260 | Loss: 2.329 | Acc: 53.718,87.326,99.889,%
Batch: 280 | Loss: 2.332 | Acc: 53.748,87.250,99.883,%
Batch: 300 | Loss: 2.320 | Acc: 53.839,87.290,99.883,%
Batch: 320 | Loss: 2.317 | Acc: 53.763,87.325,99.881,%
Batch: 340 | Loss: 2.325 | Acc: 53.753,87.269,99.885,%
Batch: 360 | Loss: 2.329 | Acc: 53.785,87.240,99.883,%
Batch: 380 | Loss: 2.332 | Acc: 53.675,87.279,99.883,%
Batch: 0 | Loss: 3.974 | Acc: 50.781,71.094,78.125,%
Batch: 20 | Loss: 4.363 | Acc: 46.205,67.076,72.842,%
Batch: 40 | Loss: 4.371 | Acc: 45.865,66.749,72.809,%
Batch: 60 | Loss: 4.390 | Acc: 45.517,66.547,72.746,%
Train all parameters

Epoch: 232
Batch: 0 | Loss: 1.585 | Acc: 57.031,93.750,100.000,%
Batch: 20 | Loss: 2.363 | Acc: 52.939,87.277,99.851,%
Batch: 40 | Loss: 2.351 | Acc: 53.277,87.633,99.886,%
Batch: 60 | Loss: 2.333 | Acc: 53.535,87.500,99.859,%
Batch: 80 | Loss: 2.345 | Acc: 53.212,87.432,99.836,%
Batch: 100 | Loss: 2.335 | Acc: 53.210,87.686,99.830,%
Batch: 120 | Loss: 2.317 | Acc: 53.538,87.571,99.832,%
Batch: 140 | Loss: 2.322 | Acc: 53.679,87.456,99.828,%
Batch: 160 | Loss: 2.332 | Acc: 53.683,87.354,99.845,%
Batch: 180 | Loss: 2.320 | Acc: 53.699,87.478,99.840,%
Batch: 200 | Loss: 2.305 | Acc: 53.922,87.617,99.848,%
Batch: 220 | Loss: 2.321 | Acc: 53.864,87.373,99.834,%
Batch: 240 | Loss: 2.322 | Acc: 53.935,87.296,99.835,%
Batch: 260 | Loss: 2.333 | Acc: 53.736,87.240,99.835,%
Batch: 280 | Loss: 2.340 | Acc: 53.684,87.216,99.839,%
Batch: 300 | Loss: 2.331 | Acc: 53.678,87.191,99.847,%
Batch: 320 | Loss: 2.331 | Acc: 53.702,87.171,99.852,%
Batch: 340 | Loss: 2.334 | Acc: 53.693,87.179,99.853,%
Batch: 360 | Loss: 2.347 | Acc: 53.612,87.126,99.853,%
Batch: 380 | Loss: 2.342 | Acc: 53.769,87.125,99.856,%
Batch: 0 | Loss: 3.937 | Acc: 50.781,71.875,78.125,%
Batch: 20 | Loss: 4.347 | Acc: 46.019,67.039,73.698,%
Batch: 40 | Loss: 4.359 | Acc: 45.827,66.749,73.285,%
Batch: 60 | Loss: 4.382 | Acc: 45.505,66.522,73.105,%
Train all parameters

Epoch: 233
Batch: 0 | Loss: 1.661 | Acc: 58.594,90.625,100.000,%
Batch: 20 | Loss: 2.262 | Acc: 53.088,87.426,99.851,%
Batch: 40 | Loss: 2.292 | Acc: 52.572,87.500,99.867,%
Batch: 60 | Loss: 2.286 | Acc: 52.792,87.372,99.872,%
Batch: 80 | Loss: 2.334 | Acc: 52.566,87.240,99.875,%
Batch: 100 | Loss: 2.327 | Acc: 52.986,87.492,99.861,%
Batch: 120 | Loss: 2.317 | Acc: 53.280,87.519,99.871,%
Batch: 140 | Loss: 2.325 | Acc: 53.336,87.478,99.867,%
Batch: 160 | Loss: 2.306 | Acc: 53.571,87.524,99.854,%
Batch: 180 | Loss: 2.290 | Acc: 53.608,87.750,99.858,%
Batch: 200 | Loss: 2.291 | Acc: 53.735,87.687,99.868,%
Batch: 220 | Loss: 2.285 | Acc: 53.779,87.723,99.862,%
Batch: 240 | Loss: 2.300 | Acc: 53.683,87.643,99.867,%
Batch: 260 | Loss: 2.306 | Acc: 53.634,87.569,99.865,%
Batch: 280 | Loss: 2.305 | Acc: 53.723,87.561,99.864,%
Batch: 300 | Loss: 2.310 | Acc: 53.675,87.505,99.857,%
Batch: 320 | Loss: 2.318 | Acc: 53.607,87.459,99.849,%
Batch: 340 | Loss: 2.323 | Acc: 53.686,87.388,99.849,%
Batch: 360 | Loss: 2.319 | Acc: 53.625,87.476,99.855,%
Batch: 380 | Loss: 2.327 | Acc: 53.584,87.410,99.854,%
Batch: 0 | Loss: 3.947 | Acc: 50.000,71.094,77.344,%
Batch: 20 | Loss: 4.340 | Acc: 45.871,66.778,73.028,%
Batch: 40 | Loss: 4.354 | Acc: 45.713,66.654,72.999,%
Batch: 60 | Loss: 4.381 | Acc: 45.697,66.432,72.925,%
Train all parameters

Epoch: 234
Batch: 0 | Loss: 1.940 | Acc: 55.469,89.844,100.000,%
Batch: 20 | Loss: 2.526 | Acc: 50.000,86.496,99.963,%
Batch: 40 | Loss: 2.471 | Acc: 51.410,86.471,99.924,%
Batch: 60 | Loss: 2.369 | Acc: 52.600,87.052,99.910,%
Batch: 80 | Loss: 2.378 | Acc: 52.932,86.680,99.894,%
Batch: 100 | Loss: 2.375 | Acc: 53.009,86.703,99.899,%
Batch: 120 | Loss: 2.350 | Acc: 53.280,87.048,99.910,%
Batch: 140 | Loss: 2.352 | Acc: 53.385,87.029,99.911,%
Batch: 160 | Loss: 2.347 | Acc: 53.314,87.107,99.918,%
Batch: 180 | Loss: 2.365 | Acc: 53.198,86.939,99.914,%
Batch: 200 | Loss: 2.350 | Acc: 53.428,87.034,99.911,%
Batch: 220 | Loss: 2.345 | Acc: 53.387,87.079,99.905,%
Batch: 240 | Loss: 2.351 | Acc: 53.394,87.169,99.896,%
Batch: 260 | Loss: 2.337 | Acc: 53.496,87.204,99.901,%
Batch: 280 | Loss: 2.332 | Acc: 53.506,87.280,99.905,%
Batch: 300 | Loss: 2.330 | Acc: 53.535,87.253,99.904,%
Batch: 320 | Loss: 2.329 | Acc: 53.607,87.176,99.905,%
Batch: 340 | Loss: 2.327 | Acc: 53.549,87.209,99.895,%
Batch: 360 | Loss: 2.333 | Acc: 53.608,87.126,99.894,%
Batch: 380 | Loss: 2.323 | Acc: 53.822,87.166,99.889,%
Batch: 0 | Loss: 3.948 | Acc: 49.219,72.656,78.906,%
Batch: 20 | Loss: 4.334 | Acc: 45.796,67.262,73.400,%
Batch: 40 | Loss: 4.354 | Acc: 45.560,66.787,72.942,%
Batch: 60 | Loss: 4.376 | Acc: 45.428,66.560,72.797,%
Train classifier parameters

Epoch: 235
Batch: 0 | Loss: 2.358 | Acc: 53.125,94.531,100.000,%
Batch: 20 | Loss: 2.295 | Acc: 55.357,87.388,99.926,%
Batch: 40 | Loss: 2.274 | Acc: 54.649,88.548,99.905,%
Batch: 60 | Loss: 2.247 | Acc: 55.815,87.884,99.936,%
Batch: 80 | Loss: 2.289 | Acc: 55.102,87.375,99.942,%
Batch: 100 | Loss: 2.318 | Acc: 54.842,87.191,99.915,%
Batch: 120 | Loss: 2.322 | Acc: 54.668,87.190,99.916,%
Batch: 140 | Loss: 2.332 | Acc: 54.305,87.345,99.917,%
Batch: 160 | Loss: 2.341 | Acc: 54.222,87.243,99.918,%
Batch: 180 | Loss: 2.343 | Acc: 54.277,87.271,99.909,%
Batch: 200 | Loss: 2.358 | Acc: 54.058,87.177,99.899,%
Batch: 220 | Loss: 2.360 | Acc: 54.023,87.146,99.894,%
Batch: 240 | Loss: 2.360 | Acc: 53.897,87.160,99.893,%
Batch: 260 | Loss: 2.367 | Acc: 53.858,87.006,99.895,%
Batch: 280 | Loss: 2.361 | Acc: 53.962,87.036,99.897,%
Batch: 300 | Loss: 2.370 | Acc: 53.782,86.986,99.899,%
Batch: 320 | Loss: 2.367 | Acc: 53.775,87.055,99.900,%
Batch: 340 | Loss: 2.363 | Acc: 53.764,87.010,99.901,%
Batch: 360 | Loss: 2.364 | Acc: 53.668,87.009,99.905,%
Batch: 380 | Loss: 2.372 | Acc: 53.621,87.018,99.900,%
Batch: 0 | Loss: 3.939 | Acc: 48.438,72.656,78.125,%
Batch: 20 | Loss: 4.348 | Acc: 45.647,67.150,73.400,%
Batch: 40 | Loss: 4.362 | Acc: 45.522,66.768,72.942,%
Batch: 60 | Loss: 4.383 | Acc: 45.312,66.598,72.951,%
Train classifier parameters

Epoch: 236
Batch: 0 | Loss: 2.322 | Acc: 55.469,92.188,100.000,%
Batch: 20 | Loss: 2.354 | Acc: 54.985,87.240,99.926,%
Batch: 40 | Loss: 2.273 | Acc: 55.011,88.205,99.943,%
Batch: 60 | Loss: 2.303 | Acc: 54.470,87.833,99.923,%
Batch: 80 | Loss: 2.311 | Acc: 53.935,87.867,99.923,%
Batch: 100 | Loss: 2.326 | Acc: 54.262,87.508,99.907,%
Batch: 120 | Loss: 2.330 | Acc: 54.152,87.661,99.890,%
Batch: 140 | Loss: 2.330 | Acc: 54.023,87.555,99.895,%
Batch: 160 | Loss: 2.335 | Acc: 53.872,87.641,99.898,%
Batch: 180 | Loss: 2.324 | Acc: 53.971,87.746,99.901,%
Batch: 200 | Loss: 2.332 | Acc: 53.786,87.675,99.891,%
Batch: 220 | Loss: 2.324 | Acc: 53.783,87.656,99.883,%
Batch: 240 | Loss: 2.323 | Acc: 53.715,87.678,99.887,%
Batch: 260 | Loss: 2.322 | Acc: 53.775,87.605,99.877,%
Batch: 280 | Loss: 2.326 | Acc: 53.717,87.645,99.878,%
Batch: 300 | Loss: 2.315 | Acc: 53.893,87.671,99.881,%
Batch: 320 | Loss: 2.319 | Acc: 53.789,87.700,99.878,%
Batch: 340 | Loss: 2.323 | Acc: 53.734,87.706,99.881,%
Batch: 360 | Loss: 2.336 | Acc: 53.586,87.498,99.872,%
Batch: 380 | Loss: 2.333 | Acc: 53.609,87.510,99.877,%
Batch: 0 | Loss: 3.941 | Acc: 47.656,73.438,78.906,%
Batch: 20 | Loss: 4.349 | Acc: 45.499,67.522,73.326,%
Batch: 40 | Loss: 4.366 | Acc: 45.522,67.035,72.885,%
Batch: 60 | Loss: 4.387 | Acc: 45.389,66.701,72.759,%
Train classifier parameters

Epoch: 237
Batch: 0 | Loss: 2.389 | Acc: 50.000,91.406,100.000,%
Batch: 20 | Loss: 2.361 | Acc: 52.641,87.463,100.000,%
Batch: 40 | Loss: 2.304 | Acc: 53.601,87.824,100.000,%
Batch: 60 | Loss: 2.356 | Acc: 53.509,87.999,99.974,%
Batch: 80 | Loss: 2.334 | Acc: 53.771,87.876,99.981,%
Batch: 100 | Loss: 2.310 | Acc: 53.868,88.049,99.954,%
Batch: 120 | Loss: 2.310 | Acc: 53.932,88.004,99.942,%
Batch: 140 | Loss: 2.306 | Acc: 53.740,87.965,99.945,%
Batch: 160 | Loss: 2.314 | Acc: 53.824,87.878,99.932,%
Batch: 180 | Loss: 2.326 | Acc: 53.591,87.888,99.914,%
Batch: 200 | Loss: 2.321 | Acc: 53.720,87.850,99.911,%
Batch: 220 | Loss: 2.326 | Acc: 53.761,87.684,99.908,%
Batch: 240 | Loss: 2.332 | Acc: 53.689,87.652,99.912,%
Batch: 260 | Loss: 2.348 | Acc: 53.583,87.530,99.907,%
Batch: 280 | Loss: 2.357 | Acc: 53.492,87.514,99.903,%
Batch: 300 | Loss: 2.355 | Acc: 53.473,87.505,99.899,%
Batch: 320 | Loss: 2.350 | Acc: 53.519,87.502,99.900,%
Batch: 340 | Loss: 2.351 | Acc: 53.533,87.445,99.897,%
Batch: 360 | Loss: 2.347 | Acc: 53.502,87.446,99.892,%
Batch: 380 | Loss: 2.357 | Acc: 53.445,87.404,99.889,%
Batch: 0 | Loss: 3.952 | Acc: 48.438,71.875,78.125,%
Batch: 20 | Loss: 4.346 | Acc: 46.019,67.113,73.475,%
Batch: 40 | Loss: 4.362 | Acc: 45.655,66.730,73.114,%
Batch: 60 | Loss: 4.386 | Acc: 45.377,66.547,72.951,%
Train classifier parameters

Epoch: 238
Batch: 0 | Loss: 2.851 | Acc: 47.656,78.906,100.000,%
Batch: 20 | Loss: 2.299 | Acc: 53.832,86.905,99.851,%
Batch: 40 | Loss: 2.245 | Acc: 54.573,87.252,99.848,%
Batch: 60 | Loss: 2.317 | Acc: 53.701,87.564,99.872,%
Batch: 80 | Loss: 2.305 | Acc: 54.003,87.355,99.884,%
Batch: 100 | Loss: 2.297 | Acc: 54.216,87.601,99.907,%
Batch: 120 | Loss: 2.296 | Acc: 54.707,87.119,99.903,%
Batch: 140 | Loss: 2.321 | Acc: 54.316,87.256,99.900,%
Batch: 160 | Loss: 2.323 | Acc: 54.566,87.277,99.903,%
Batch: 180 | Loss: 2.343 | Acc: 54.467,86.900,99.896,%
Batch: 200 | Loss: 2.338 | Acc: 54.544,86.859,99.895,%
Batch: 220 | Loss: 2.343 | Acc: 54.465,86.832,99.894,%
Batch: 240 | Loss: 2.335 | Acc: 54.412,86.878,99.893,%
Batch: 260 | Loss: 2.347 | Acc: 54.206,86.832,99.895,%
Batch: 280 | Loss: 2.346 | Acc: 54.162,86.822,99.900,%
Batch: 300 | Loss: 2.348 | Acc: 54.085,86.882,99.904,%
Batch: 320 | Loss: 2.338 | Acc: 54.193,86.938,99.905,%
Batch: 340 | Loss: 2.339 | Acc: 54.094,86.968,99.906,%
Batch: 360 | Loss: 2.340 | Acc: 54.084,86.965,99.905,%
Batch: 380 | Loss: 2.339 | Acc: 54.146,86.967,99.904,%
Batch: 0 | Loss: 3.941 | Acc: 48.438,71.094,78.125,%
Batch: 20 | Loss: 4.349 | Acc: 45.759,66.853,73.214,%
Batch: 40 | Loss: 4.361 | Acc: 45.732,66.730,72.618,%
Batch: 60 | Loss: 4.380 | Acc: 45.505,66.509,72.669,%
Train classifier parameters

Epoch: 239
Batch: 0 | Loss: 2.557 | Acc: 42.969,90.625,99.219,%
Batch: 20 | Loss: 2.532 | Acc: 52.195,86.347,99.814,%
Batch: 40 | Loss: 2.450 | Acc: 52.934,86.357,99.809,%
Batch: 60 | Loss: 2.396 | Acc: 53.151,86.949,99.859,%
Batch: 80 | Loss: 2.379 | Acc: 53.356,87.201,99.875,%
Batch: 100 | Loss: 2.333 | Acc: 53.674,87.345,99.876,%
Batch: 120 | Loss: 2.326 | Acc: 53.790,87.261,99.871,%
Batch: 140 | Loss: 2.345 | Acc: 53.757,87.068,99.889,%
Batch: 160 | Loss: 2.337 | Acc: 53.863,87.005,99.898,%
Batch: 180 | Loss: 2.331 | Acc: 53.803,87.142,99.888,%
Batch: 200 | Loss: 2.318 | Acc: 54.000,87.123,99.883,%
Batch: 220 | Loss: 2.339 | Acc: 53.846,87.122,99.880,%
Batch: 240 | Loss: 2.333 | Acc: 53.939,87.176,99.887,%
Batch: 260 | Loss: 2.335 | Acc: 53.861,87.177,99.880,%
Batch: 280 | Loss: 2.331 | Acc: 53.967,87.216,99.878,%
Batch: 300 | Loss: 2.329 | Acc: 53.953,87.331,99.873,%
Batch: 320 | Loss: 2.314 | Acc: 54.055,87.420,99.873,%
Batch: 340 | Loss: 2.312 | Acc: 54.103,87.411,99.872,%
Batch: 360 | Loss: 2.320 | Acc: 53.988,87.392,99.877,%
Batch: 380 | Loss: 2.325 | Acc: 53.947,87.354,99.881,%
Batch: 0 | Loss: 3.968 | Acc: 48.438,72.656,76.562,%
Batch: 20 | Loss: 4.340 | Acc: 45.871,67.076,73.289,%
Batch: 40 | Loss: 4.358 | Acc: 45.713,66.711,72.923,%
Batch: 60 | Loss: 4.377 | Acc: 45.466,66.509,72.887,%
Train classifier parameters

Epoch: 240
Batch: 0 | Loss: 1.946 | Acc: 58.594,87.500,98.438,%
Batch: 20 | Loss: 2.209 | Acc: 55.804,87.500,99.665,%
Batch: 40 | Loss: 2.273 | Acc: 54.535,87.843,99.771,%
Batch: 60 | Loss: 2.299 | Acc: 54.073,87.526,99.821,%
Batch: 80 | Loss: 2.278 | Acc: 54.456,87.712,99.817,%
Batch: 100 | Loss: 2.270 | Acc: 54.223,87.709,99.822,%
Batch: 120 | Loss: 2.299 | Acc: 53.887,87.694,99.839,%
Batch: 140 | Loss: 2.288 | Acc: 54.000,87.810,99.850,%
Batch: 160 | Loss: 2.288 | Acc: 54.120,87.781,99.859,%
Batch: 180 | Loss: 2.293 | Acc: 54.092,87.724,99.866,%
Batch: 200 | Loss: 2.301 | Acc: 54.031,87.690,99.848,%
Batch: 220 | Loss: 2.294 | Acc: 54.147,87.641,99.848,%
Batch: 240 | Loss: 2.306 | Acc: 54.055,87.588,99.844,%
Batch: 260 | Loss: 2.312 | Acc: 53.993,87.557,99.850,%
Batch: 280 | Loss: 2.318 | Acc: 53.981,87.428,99.855,%
Batch: 300 | Loss: 2.333 | Acc: 53.901,87.264,99.855,%
Batch: 320 | Loss: 2.333 | Acc: 53.862,87.330,99.864,%
Batch: 340 | Loss: 2.334 | Acc: 53.899,87.296,99.869,%
Batch: 360 | Loss: 2.323 | Acc: 54.086,87.333,99.866,%
Batch: 380 | Loss: 2.325 | Acc: 54.037,87.276,99.873,%
Batch: 0 | Loss: 3.940 | Acc: 48.438,71.875,78.125,%
Batch: 20 | Loss: 4.341 | Acc: 46.168,67.113,73.512,%
Batch: 40 | Loss: 4.360 | Acc: 45.789,66.730,73.152,%
Batch: 60 | Loss: 4.383 | Acc: 45.415,66.496,73.002,%
Train classifier parameters

Epoch: 241
Batch: 0 | Loss: 2.246 | Acc: 60.938,92.188,100.000,%
Batch: 20 | Loss: 2.238 | Acc: 54.390,89.472,99.814,%
Batch: 40 | Loss: 2.290 | Acc: 54.059,88.072,99.829,%
Batch: 60 | Loss: 2.354 | Acc: 52.959,87.769,99.834,%
Batch: 80 | Loss: 2.334 | Acc: 53.270,87.982,99.846,%
Batch: 100 | Loss: 2.362 | Acc: 53.512,87.454,99.876,%
Batch: 120 | Loss: 2.343 | Acc: 53.642,87.577,99.884,%
Batch: 140 | Loss: 2.341 | Acc: 53.457,87.688,99.878,%
Batch: 160 | Loss: 2.342 | Acc: 53.445,87.781,99.884,%
Batch: 180 | Loss: 2.320 | Acc: 53.854,87.768,99.888,%
Batch: 200 | Loss: 2.321 | Acc: 53.895,87.644,99.872,%
Batch: 220 | Loss: 2.304 | Acc: 54.136,87.694,99.869,%
Batch: 240 | Loss: 2.306 | Acc: 54.179,87.597,99.870,%
Batch: 260 | Loss: 2.306 | Acc: 54.191,87.641,99.865,%
Batch: 280 | Loss: 2.312 | Acc: 54.154,87.589,99.872,%
Batch: 300 | Loss: 2.318 | Acc: 54.109,87.567,99.868,%
Batch: 320 | Loss: 2.319 | Acc: 54.101,87.507,99.869,%
Batch: 340 | Loss: 2.309 | Acc: 54.165,87.679,99.869,%
Batch: 360 | Loss: 2.306 | Acc: 54.203,87.671,99.874,%
Batch: 380 | Loss: 2.308 | Acc: 54.191,87.678,99.871,%
Batch: 0 | Loss: 3.958 | Acc: 48.438,71.875,78.906,%
Batch: 20 | Loss: 4.343 | Acc: 45.982,66.890,73.363,%
Batch: 40 | Loss: 4.359 | Acc: 45.598,66.730,72.847,%
Batch: 60 | Loss: 4.383 | Acc: 45.453,66.586,72.797,%
Train classifier parameters

Epoch: 242
Batch: 0 | Loss: 2.419 | Acc: 53.906,78.906,99.219,%
Batch: 20 | Loss: 2.215 | Acc: 57.812,87.835,99.963,%
Batch: 40 | Loss: 2.240 | Acc: 55.831,87.691,99.962,%
Batch: 60 | Loss: 2.258 | Acc: 55.149,86.949,99.962,%
Batch: 80 | Loss: 2.291 | Acc: 54.601,87.056,99.942,%
Batch: 100 | Loss: 2.317 | Acc: 53.837,87.283,99.946,%
Batch: 120 | Loss: 2.333 | Acc: 53.506,87.313,99.935,%
Batch: 140 | Loss: 2.328 | Acc: 53.579,87.184,99.922,%
Batch: 160 | Loss: 2.329 | Acc: 53.664,87.219,99.922,%
Batch: 180 | Loss: 2.314 | Acc: 53.919,87.362,99.922,%
Batch: 200 | Loss: 2.307 | Acc: 54.077,87.348,99.922,%
Batch: 220 | Loss: 2.304 | Acc: 54.231,87.387,99.915,%
Batch: 240 | Loss: 2.303 | Acc: 54.130,87.490,99.906,%
Batch: 260 | Loss: 2.307 | Acc: 54.164,87.431,99.907,%
Batch: 280 | Loss: 2.309 | Acc: 54.170,87.319,99.911,%
Batch: 300 | Loss: 2.312 | Acc: 54.119,87.282,99.914,%
Batch: 320 | Loss: 2.315 | Acc: 54.123,87.332,99.915,%
Batch: 340 | Loss: 2.322 | Acc: 53.973,87.333,99.920,%
Batch: 360 | Loss: 2.321 | Acc: 54.051,87.346,99.922,%
Batch: 380 | Loss: 2.325 | Acc: 54.089,87.332,99.924,%
Batch: 0 | Loss: 3.935 | Acc: 48.438,71.875,78.906,%
Batch: 20 | Loss: 4.330 | Acc: 46.019,67.113,73.512,%
Batch: 40 | Loss: 4.350 | Acc: 45.770,66.825,72.980,%
Batch: 60 | Loss: 4.374 | Acc: 45.569,66.573,72.836,%
Train classifier parameters

Epoch: 243
Batch: 0 | Loss: 2.867 | Acc: 48.438,86.719,100.000,%
Batch: 20 | Loss: 2.296 | Acc: 54.874,85.640,99.851,%
Batch: 40 | Loss: 2.290 | Acc: 54.859,86.509,99.848,%
Batch: 60 | Loss: 2.307 | Acc: 54.355,86.847,99.846,%
Batch: 80 | Loss: 2.313 | Acc: 54.591,86.458,99.846,%
Batch: 100 | Loss: 2.314 | Acc: 54.618,86.541,99.853,%
Batch: 120 | Loss: 2.315 | Acc: 54.694,86.402,99.845,%
Batch: 140 | Loss: 2.291 | Acc: 55.003,86.336,99.861,%
Batch: 160 | Loss: 2.308 | Acc: 54.629,86.360,99.869,%
Batch: 180 | Loss: 2.331 | Acc: 54.403,86.278,99.879,%
Batch: 200 | Loss: 2.339 | Acc: 54.283,86.229,99.876,%
Batch: 220 | Loss: 2.340 | Acc: 54.164,86.312,99.883,%
Batch: 240 | Loss: 2.341 | Acc: 54.175,86.391,99.877,%
Batch: 260 | Loss: 2.341 | Acc: 54.179,86.416,99.880,%
Batch: 280 | Loss: 2.328 | Acc: 54.181,86.697,99.878,%
Batch: 300 | Loss: 2.325 | Acc: 54.251,86.768,99.875,%
Batch: 320 | Loss: 2.330 | Acc: 54.269,86.714,99.881,%
Batch: 340 | Loss: 2.325 | Acc: 54.298,86.751,99.872,%
Batch: 360 | Loss: 2.322 | Acc: 54.317,86.838,99.870,%
Batch: 380 | Loss: 2.327 | Acc: 54.208,86.893,99.867,%
Batch: 0 | Loss: 3.921 | Acc: 49.219,71.875,78.125,%
Batch: 20 | Loss: 4.347 | Acc: 45.722,67.522,73.326,%
Batch: 40 | Loss: 4.361 | Acc: 45.675,67.016,72.980,%
Batch: 60 | Loss: 4.383 | Acc: 45.466,66.611,72.951,%
Train classifier parameters

Epoch: 244
Batch: 0 | Loss: 1.839 | Acc: 60.938,91.406,100.000,%
Batch: 20 | Loss: 2.368 | Acc: 54.204,87.016,99.851,%
Batch: 40 | Loss: 2.391 | Acc: 52.611,87.176,99.886,%
Batch: 60 | Loss: 2.283 | Acc: 53.919,87.590,99.898,%
Batch: 80 | Loss: 2.333 | Acc: 53.492,87.529,99.875,%
Batch: 100 | Loss: 2.292 | Acc: 53.728,87.670,99.876,%
Batch: 120 | Loss: 2.298 | Acc: 53.809,87.668,99.884,%
Batch: 140 | Loss: 2.297 | Acc: 53.729,87.771,99.889,%
Batch: 160 | Loss: 2.308 | Acc: 53.766,87.680,99.888,%
Batch: 180 | Loss: 2.303 | Acc: 53.911,87.599,99.888,%
Batch: 200 | Loss: 2.293 | Acc: 54.229,87.551,99.883,%
Batch: 220 | Loss: 2.308 | Acc: 53.991,87.486,99.880,%
Batch: 240 | Loss: 2.316 | Acc: 53.890,87.403,99.880,%
Batch: 260 | Loss: 2.317 | Acc: 53.793,87.428,99.880,%
Batch: 280 | Loss: 2.319 | Acc: 53.667,87.503,99.872,%
Batch: 300 | Loss: 2.305 | Acc: 53.784,87.580,99.875,%
Batch: 320 | Loss: 2.310 | Acc: 53.882,87.614,99.878,%
Batch: 340 | Loss: 2.314 | Acc: 53.783,87.596,99.885,%
Batch: 360 | Loss: 2.319 | Acc: 53.753,87.582,99.883,%
Batch: 380 | Loss: 2.315 | Acc: 53.826,87.555,99.875,%
Batch: 0 | Loss: 3.957 | Acc: 49.219,71.875,78.125,%
Batch: 20 | Loss: 4.343 | Acc: 46.280,67.225,73.549,%
Batch: 40 | Loss: 4.362 | Acc: 45.808,66.902,72.980,%
Batch: 60 | Loss: 4.384 | Acc: 45.517,66.637,72.900,%
Train classifier parameters

Epoch: 245
Batch: 0 | Loss: 3.128 | Acc: 46.094,78.125,100.000,%
Batch: 20 | Loss: 2.458 | Acc: 50.558,86.868,99.963,%
Batch: 40 | Loss: 2.341 | Acc: 52.306,88.224,99.867,%
Batch: 60 | Loss: 2.336 | Acc: 52.805,88.384,99.898,%
Batch: 80 | Loss: 2.341 | Acc: 52.961,88.329,99.875,%
Batch: 100 | Loss: 2.310 | Acc: 53.643,87.972,99.884,%
Batch: 120 | Loss: 2.303 | Acc: 53.745,88.062,99.884,%
Batch: 140 | Loss: 2.309 | Acc: 53.962,87.816,99.889,%
Batch: 160 | Loss: 2.318 | Acc: 53.707,87.743,99.884,%
Batch: 180 | Loss: 2.328 | Acc: 53.669,87.543,99.879,%
Batch: 200 | Loss: 2.319 | Acc: 53.770,87.679,99.883,%
Batch: 220 | Loss: 2.308 | Acc: 54.065,87.698,99.890,%
Batch: 240 | Loss: 2.314 | Acc: 54.049,87.675,99.890,%
Batch: 260 | Loss: 2.325 | Acc: 54.053,87.470,99.892,%
Batch: 280 | Loss: 2.311 | Acc: 54.204,87.503,99.897,%
Batch: 300 | Loss: 2.318 | Acc: 54.070,87.458,99.901,%
Batch: 320 | Loss: 2.317 | Acc: 54.055,87.468,99.898,%
Batch: 340 | Loss: 2.318 | Acc: 54.071,87.475,99.904,%
Batch: 360 | Loss: 2.311 | Acc: 54.118,87.582,99.907,%
Batch: 380 | Loss: 2.311 | Acc: 54.136,87.613,99.908,%
Batch: 0 | Loss: 3.959 | Acc: 48.438,71.875,78.125,%
Batch: 20 | Loss: 4.334 | Acc: 46.057,66.853,73.549,%
Batch: 40 | Loss: 4.352 | Acc: 45.655,66.616,73.152,%
Batch: 60 | Loss: 4.375 | Acc: 45.441,66.483,73.130,%
Train classifier parameters

Epoch: 246
Batch: 0 | Loss: 2.220 | Acc: 53.125,85.938,100.000,%
Batch: 20 | Loss: 2.228 | Acc: 56.027,87.909,99.888,%
Batch: 40 | Loss: 2.326 | Acc: 54.764,86.700,99.886,%
Batch: 60 | Loss: 2.320 | Acc: 54.790,87.090,99.846,%
Batch: 80 | Loss: 2.273 | Acc: 55.035,87.490,99.855,%
Batch: 100 | Loss: 2.293 | Acc: 54.618,87.531,99.861,%
Batch: 120 | Loss: 2.341 | Acc: 54.307,87.158,99.877,%
Batch: 140 | Loss: 2.329 | Acc: 54.499,87.118,99.873,%
Batch: 160 | Loss: 2.347 | Acc: 54.105,87.219,99.869,%
Batch: 180 | Loss: 2.349 | Acc: 54.213,87.008,99.871,%
Batch: 200 | Loss: 2.341 | Acc: 54.046,87.104,99.868,%
Batch: 220 | Loss: 2.339 | Acc: 53.980,87.097,99.869,%
Batch: 240 | Loss: 2.329 | Acc: 54.036,87.234,99.870,%
Batch: 260 | Loss: 2.329 | Acc: 54.065,87.302,99.868,%
Batch: 280 | Loss: 2.327 | Acc: 54.134,87.258,99.867,%
Batch: 300 | Loss: 2.328 | Acc: 54.083,87.282,99.862,%
Batch: 320 | Loss: 2.314 | Acc: 54.191,87.322,99.869,%
Batch: 340 | Loss: 2.318 | Acc: 54.035,87.337,99.867,%
Batch: 360 | Loss: 2.317 | Acc: 54.088,87.320,99.866,%
Batch: 380 | Loss: 2.321 | Acc: 54.009,87.371,99.863,%
Batch: 0 | Loss: 3.953 | Acc: 49.219,71.875,77.344,%
Batch: 20 | Loss: 4.339 | Acc: 45.945,66.704,73.512,%
Batch: 40 | Loss: 4.353 | Acc: 45.713,66.559,72.980,%
Batch: 60 | Loss: 4.377 | Acc: 45.556,66.381,72.951,%
Train classifier parameters

Epoch: 247
Batch: 0 | Loss: 1.963 | Acc: 62.500,93.750,100.000,%
Batch: 20 | Loss: 2.339 | Acc: 52.827,87.723,99.888,%
Batch: 40 | Loss: 2.340 | Acc: 53.277,87.519,99.867,%
Batch: 60 | Loss: 2.320 | Acc: 53.612,87.385,99.885,%
Batch: 80 | Loss: 2.302 | Acc: 53.752,87.490,99.904,%
Batch: 100 | Loss: 2.326 | Acc: 53.581,87.183,99.923,%
Batch: 120 | Loss: 2.357 | Acc: 53.235,87.138,99.903,%
Batch: 140 | Loss: 2.353 | Acc: 53.396,87.312,99.895,%
Batch: 160 | Loss: 2.349 | Acc: 53.474,87.320,99.898,%
Batch: 180 | Loss: 2.355 | Acc: 53.544,87.211,99.896,%
Batch: 200 | Loss: 2.347 | Acc: 53.533,87.399,99.907,%
Batch: 220 | Loss: 2.361 | Acc: 53.436,87.090,99.908,%
Batch: 240 | Loss: 2.349 | Acc: 53.495,87.276,99.912,%
Batch: 260 | Loss: 2.342 | Acc: 53.595,87.305,99.916,%
Batch: 280 | Loss: 2.354 | Acc: 53.450,87.291,99.911,%
Batch: 300 | Loss: 2.359 | Acc: 53.488,87.214,99.909,%
Batch: 320 | Loss: 2.350 | Acc: 53.636,87.240,99.912,%
Batch: 340 | Loss: 2.342 | Acc: 53.647,87.317,99.911,%
Batch: 360 | Loss: 2.339 | Acc: 53.698,87.288,99.911,%
Batch: 380 | Loss: 2.345 | Acc: 53.683,87.285,99.906,%
Batch: 0 | Loss: 3.948 | Acc: 49.219,71.875,78.125,%
Batch: 20 | Loss: 4.334 | Acc: 45.833,67.188,74.070,%
Batch: 40 | Loss: 4.354 | Acc: 45.770,66.806,73.266,%
Batch: 60 | Loss: 4.381 | Acc: 45.607,66.522,73.117,%
Train classifier parameters

Epoch: 248
Batch: 0 | Loss: 1.832 | Acc: 57.031,90.625,98.438,%
Batch: 20 | Loss: 2.309 | Acc: 55.060,88.132,99.888,%
Batch: 40 | Loss: 2.335 | Acc: 53.811,88.224,99.886,%
Batch: 60 | Loss: 2.401 | Acc: 53.548,87.423,99.859,%
Batch: 80 | Loss: 2.386 | Acc: 53.742,87.259,99.875,%
Batch: 100 | Loss: 2.406 | Acc: 53.519,86.982,99.892,%
Batch: 120 | Loss: 2.381 | Acc: 53.454,87.416,99.897,%
Batch: 140 | Loss: 2.368 | Acc: 53.574,87.472,99.889,%
Batch: 160 | Loss: 2.360 | Acc: 53.678,87.340,99.884,%
Batch: 180 | Loss: 2.356 | Acc: 53.716,87.258,99.896,%
Batch: 200 | Loss: 2.348 | Acc: 53.805,87.255,99.895,%
Batch: 220 | Loss: 2.356 | Acc: 53.712,87.295,99.890,%
Batch: 240 | Loss: 2.349 | Acc: 53.692,87.270,99.896,%
Batch: 260 | Loss: 2.348 | Acc: 53.685,87.273,99.895,%
Batch: 280 | Loss: 2.356 | Acc: 53.689,87.066,99.889,%
Batch: 300 | Loss: 2.354 | Acc: 53.688,87.124,99.894,%
Batch: 320 | Loss: 2.356 | Acc: 53.639,87.115,99.893,%
Batch: 340 | Loss: 2.360 | Acc: 53.640,87.078,99.892,%
Batch: 360 | Loss: 2.363 | Acc: 53.528,87.126,99.890,%
Batch: 380 | Loss: 2.358 | Acc: 53.714,87.172,99.891,%
Batch: 0 | Loss: 3.936 | Acc: 49.219,71.875,78.906,%
Batch: 20 | Loss: 4.337 | Acc: 45.796,67.262,73.772,%
Batch: 40 | Loss: 4.354 | Acc: 45.675,66.825,73.190,%
Batch: 60 | Loss: 4.379 | Acc: 45.569,66.637,73.092,%
Train classifier parameters

Epoch: 249
Batch: 0 | Loss: 2.320 | Acc: 50.000,88.281,100.000,%
Batch: 20 | Loss: 2.463 | Acc: 52.976,86.458,99.926,%
Batch: 40 | Loss: 2.356 | Acc: 54.002,87.176,99.886,%
Batch: 60 | Loss: 2.334 | Acc: 53.983,87.500,99.923,%
Batch: 80 | Loss: 2.318 | Acc: 54.408,87.211,99.913,%
Batch: 100 | Loss: 2.342 | Acc: 54.084,87.005,99.876,%
Batch: 120 | Loss: 2.351 | Acc: 54.003,86.906,99.884,%
Batch: 140 | Loss: 2.345 | Acc: 53.906,87.123,99.889,%
Batch: 160 | Loss: 2.343 | Acc: 53.989,87.185,99.888,%
Batch: 180 | Loss: 2.327 | Acc: 54.070,87.112,99.892,%
Batch: 200 | Loss: 2.344 | Acc: 53.762,87.034,99.887,%
Batch: 220 | Loss: 2.354 | Acc: 53.514,87.122,99.894,%
Batch: 240 | Loss: 2.344 | Acc: 53.673,87.137,99.896,%
Batch: 260 | Loss: 2.349 | Acc: 53.673,87.081,99.901,%
Batch: 280 | Loss: 2.344 | Acc: 53.706,87.230,99.908,%
Batch: 300 | Loss: 2.343 | Acc: 53.873,87.261,99.904,%
Batch: 320 | Loss: 2.339 | Acc: 53.828,87.342,99.900,%
Batch: 340 | Loss: 2.347 | Acc: 53.760,87.241,99.901,%
Batch: 360 | Loss: 2.341 | Acc: 53.889,87.266,99.900,%
Batch: 380 | Loss: 2.341 | Acc: 53.982,87.197,99.897,%
Batch: 0 | Loss: 3.969 | Acc: 49.219,71.875,78.125,%
Batch: 20 | Loss: 4.332 | Acc: 46.094,67.262,73.661,%
Batch: 40 | Loss: 4.354 | Acc: 45.713,66.787,73.285,%
Batch: 60 | Loss: 4.380 | Acc: 45.517,66.534,73.079,%
Train all parameters

Epoch: 250
Batch: 0 | Loss: 2.950 | Acc: 36.719,85.156,99.219,%
Batch: 20 | Loss: 2.386 | Acc: 54.278,86.310,99.814,%
Batch: 40 | Loss: 2.287 | Acc: 54.954,86.966,99.829,%
Batch: 60 | Loss: 2.258 | Acc: 54.649,87.756,99.821,%
Batch: 80 | Loss: 2.293 | Acc: 54.514,87.529,99.817,%
Batch: 100 | Loss: 2.306 | Acc: 54.169,87.500,99.822,%
Batch: 120 | Loss: 2.293 | Acc: 54.158,87.565,99.851,%
Batch: 140 | Loss: 2.303 | Acc: 54.117,87.223,99.845,%
Batch: 160 | Loss: 2.276 | Acc: 54.469,87.330,99.854,%
Batch: 180 | Loss: 2.301 | Acc: 54.113,87.418,99.858,%
Batch: 200 | Loss: 2.314 | Acc: 53.945,87.399,99.864,%
Batch: 220 | Loss: 2.327 | Acc: 53.825,87.376,99.859,%
Batch: 240 | Loss: 2.345 | Acc: 53.734,87.322,99.864,%
Batch: 260 | Loss: 2.363 | Acc: 53.493,87.296,99.862,%
Batch: 280 | Loss: 2.365 | Acc: 53.536,87.258,99.864,%
Batch: 300 | Loss: 2.367 | Acc: 53.434,87.189,99.870,%
Batch: 320 | Loss: 2.362 | Acc: 53.514,87.279,99.869,%
Batch: 340 | Loss: 2.360 | Acc: 53.588,87.287,99.869,%
Batch: 360 | Loss: 2.350 | Acc: 53.770,87.303,99.877,%
Batch: 380 | Loss: 2.342 | Acc: 53.779,87.307,99.871,%
Batch: 0 | Loss: 3.941 | Acc: 48.438,71.875,78.906,%
Batch: 20 | Loss: 4.343 | Acc: 46.243,66.964,73.884,%
Batch: 40 | Loss: 4.353 | Acc: 45.922,66.635,73.285,%
Batch: 60 | Loss: 4.374 | Acc: 45.569,66.586,72.938,%
Train all parameters

Epoch: 251
Batch: 0 | Loss: 2.242 | Acc: 61.719,83.594,100.000,%
Batch: 20 | Loss: 2.345 | Acc: 53.943,87.016,99.851,%
Batch: 40 | Loss: 2.373 | Acc: 54.230,86.986,99.905,%
Batch: 60 | Loss: 2.378 | Acc: 54.265,86.552,99.898,%
Batch: 80 | Loss: 2.381 | Acc: 53.742,86.796,99.884,%
Batch: 100 | Loss: 2.354 | Acc: 53.767,87.276,99.884,%
Batch: 120 | Loss: 2.318 | Acc: 53.790,87.739,99.877,%
Batch: 140 | Loss: 2.347 | Acc: 53.773,87.361,99.873,%
Batch: 160 | Loss: 2.337 | Acc: 53.906,87.262,99.884,%
Batch: 180 | Loss: 2.343 | Acc: 53.759,87.263,99.879,%
Batch: 200 | Loss: 2.349 | Acc: 53.774,87.142,99.887,%
Batch: 220 | Loss: 2.341 | Acc: 53.864,87.185,99.894,%
Batch: 240 | Loss: 2.336 | Acc: 53.803,87.260,99.900,%
Batch: 260 | Loss: 2.343 | Acc: 53.775,87.171,99.904,%
Batch: 280 | Loss: 2.335 | Acc: 53.856,87.222,99.897,%
Batch: 300 | Loss: 2.348 | Acc: 53.766,87.157,99.899,%
Batch: 320 | Loss: 2.347 | Acc: 53.780,87.174,99.895,%
Batch: 340 | Loss: 2.350 | Acc: 53.888,87.147,99.897,%
Batch: 360 | Loss: 2.347 | Acc: 53.926,87.223,99.890,%
Batch: 380 | Loss: 2.341 | Acc: 54.050,87.207,99.889,%
Batch: 0 | Loss: 3.957 | Acc: 50.000,71.875,78.125,%
Batch: 20 | Loss: 4.342 | Acc: 46.057,67.188,73.549,%
Batch: 40 | Loss: 4.351 | Acc: 45.770,66.825,73.075,%
Batch: 60 | Loss: 4.371 | Acc: 45.479,66.637,72.938,%
Train all parameters

Epoch: 252
Batch: 0 | Loss: 2.880 | Acc: 46.094,82.812,100.000,%
Batch: 20 | Loss: 2.505 | Acc: 51.042,86.161,99.963,%
Batch: 40 | Loss: 2.391 | Acc: 51.829,87.824,99.924,%
Batch: 60 | Loss: 2.313 | Acc: 53.215,87.692,99.910,%
Batch: 80 | Loss: 2.302 | Acc: 53.318,87.519,99.913,%
Batch: 100 | Loss: 2.323 | Acc: 53.318,87.423,99.907,%
Batch: 120 | Loss: 2.323 | Acc: 53.512,87.345,99.903,%
Batch: 140 | Loss: 2.309 | Acc: 53.773,87.478,99.917,%
Batch: 160 | Loss: 2.306 | Acc: 53.979,87.408,99.913,%
Batch: 180 | Loss: 2.317 | Acc: 53.630,87.530,99.922,%
Batch: 200 | Loss: 2.314 | Acc: 53.836,87.469,99.918,%
Batch: 220 | Loss: 2.330 | Acc: 53.708,87.284,99.915,%
Batch: 240 | Loss: 2.319 | Acc: 53.751,87.455,99.909,%
Batch: 260 | Loss: 2.303 | Acc: 53.831,87.608,99.901,%
Batch: 280 | Loss: 2.307 | Acc: 53.853,87.517,99.903,%
Batch: 300 | Loss: 2.306 | Acc: 53.870,87.526,99.907,%
Batch: 320 | Loss: 2.302 | Acc: 54.013,87.522,99.912,%
Batch: 340 | Loss: 2.301 | Acc: 54.071,87.443,99.906,%
Batch: 360 | Loss: 2.309 | Acc: 53.954,87.392,99.903,%
Batch: 380 | Loss: 2.309 | Acc: 53.917,87.416,99.906,%
Batch: 0 | Loss: 3.927 | Acc: 50.000,71.094,79.688,%
Batch: 20 | Loss: 4.334 | Acc: 46.131,67.188,73.921,%
Batch: 40 | Loss: 4.349 | Acc: 45.960,66.864,73.361,%
Batch: 60 | Loss: 4.373 | Acc: 45.710,66.662,73.181,%
Train all parameters

Epoch: 253
Batch: 0 | Loss: 2.079 | Acc: 61.719,84.375,100.000,%
Batch: 20 | Loss: 2.251 | Acc: 55.729,87.165,99.926,%
Batch: 40 | Loss: 2.326 | Acc: 54.745,86.852,99.867,%
Batch: 60 | Loss: 2.356 | Acc: 53.957,87.513,99.898,%
Batch: 80 | Loss: 2.339 | Acc: 53.964,87.625,99.913,%
Batch: 100 | Loss: 2.341 | Acc: 53.759,87.662,99.923,%
Batch: 120 | Loss: 2.318 | Acc: 54.100,87.726,99.884,%
Batch: 140 | Loss: 2.310 | Acc: 54.211,87.777,99.889,%
Batch: 160 | Loss: 2.323 | Acc: 53.940,87.641,99.893,%
Batch: 180 | Loss: 2.324 | Acc: 53.919,87.547,99.896,%
Batch: 200 | Loss: 2.343 | Acc: 53.646,87.442,99.895,%
Batch: 220 | Loss: 2.349 | Acc: 53.645,87.316,99.897,%
Batch: 240 | Loss: 2.367 | Acc: 53.482,87.163,99.890,%
Batch: 260 | Loss: 2.368 | Acc: 53.400,87.168,99.892,%
Batch: 280 | Loss: 2.364 | Acc: 53.473,87.172,99.889,%
Batch: 300 | Loss: 2.368 | Acc: 53.507,87.124,99.894,%
Batch: 320 | Loss: 2.376 | Acc: 53.415,87.232,99.893,%
Batch: 340 | Loss: 2.374 | Acc: 53.579,87.042,99.892,%
Batch: 360 | Loss: 2.377 | Acc: 53.493,87.007,99.890,%
Batch: 380 | Loss: 2.369 | Acc: 53.541,87.049,99.877,%
Batch: 0 | Loss: 3.918 | Acc: 49.219,71.094,78.125,%
Batch: 20 | Loss: 4.332 | Acc: 46.205,67.336,73.661,%
Batch: 40 | Loss: 4.345 | Acc: 45.789,67.111,73.133,%
Batch: 60 | Loss: 4.366 | Acc: 45.645,66.829,73.028,%
Train all parameters

Epoch: 254
Batch: 0 | Loss: 2.611 | Acc: 48.438,88.281,100.000,%
Batch: 20 | Loss: 2.328 | Acc: 52.865,87.388,99.963,%
Batch: 40 | Loss: 2.319 | Acc: 53.144,87.233,99.962,%
Batch: 60 | Loss: 2.354 | Acc: 53.099,87.180,99.949,%
Batch: 80 | Loss: 2.326 | Acc: 53.173,87.760,99.913,%
Batch: 100 | Loss: 2.344 | Acc: 53.504,87.454,99.892,%
Batch: 120 | Loss: 2.355 | Acc: 53.467,87.351,99.890,%
Batch: 140 | Loss: 2.335 | Acc: 53.823,87.289,99.906,%
Batch: 160 | Loss: 2.351 | Acc: 53.809,87.228,99.908,%
Batch: 180 | Loss: 2.347 | Acc: 53.842,87.276,99.918,%
Batch: 200 | Loss: 2.347 | Acc: 53.910,87.177,99.914,%
Batch: 220 | Loss: 2.343 | Acc: 53.828,87.337,99.915,%
Batch: 240 | Loss: 2.358 | Acc: 53.663,87.325,99.909,%
Batch: 260 | Loss: 2.356 | Acc: 53.580,87.255,99.901,%
Batch: 280 | Loss: 2.364 | Acc: 53.448,87.233,99.905,%
Batch: 300 | Loss: 2.367 | Acc: 53.395,87.326,99.904,%
Batch: 320 | Loss: 2.368 | Acc: 53.283,87.339,99.905,%
Batch: 340 | Loss: 2.363 | Acc: 53.398,87.328,99.904,%
Batch: 360 | Loss: 2.369 | Acc: 53.413,87.245,99.898,%
Batch: 380 | Loss: 2.366 | Acc: 53.381,87.252,99.904,%
Batch: 0 | Loss: 3.980 | Acc: 51.562,72.656,78.125,%
Batch: 20 | Loss: 4.332 | Acc: 46.280,66.853,73.363,%
Batch: 40 | Loss: 4.348 | Acc: 45.846,66.749,73.304,%
Batch: 60 | Loss: 4.373 | Acc: 45.569,66.457,73.002,%
Train all parameters

Epoch: 255
Batch: 0 | Loss: 2.116 | Acc: 60.156,82.031,99.219,%
Batch: 20 | Loss: 2.414 | Acc: 52.046,87.054,99.888,%
Batch: 40 | Loss: 2.351 | Acc: 53.563,87.195,99.848,%
Batch: 60 | Loss: 2.323 | Acc: 53.983,87.372,99.846,%
Batch: 80 | Loss: 2.281 | Acc: 54.128,87.693,99.865,%
Batch: 100 | Loss: 2.269 | Acc: 54.394,87.701,99.861,%
Batch: 120 | Loss: 2.271 | Acc: 54.223,87.720,99.884,%
Batch: 140 | Loss: 2.261 | Acc: 54.128,87.949,99.895,%
Batch: 160 | Loss: 2.252 | Acc: 54.309,88.034,99.898,%
Batch: 180 | Loss: 2.249 | Acc: 54.485,88.027,99.896,%
Batch: 200 | Loss: 2.260 | Acc: 54.248,87.966,99.903,%
Batch: 220 | Loss: 2.269 | Acc: 54.263,87.811,99.908,%
Batch: 240 | Loss: 2.268 | Acc: 54.188,87.795,99.903,%
Batch: 260 | Loss: 2.277 | Acc: 54.122,87.710,99.904,%
Batch: 280 | Loss: 2.293 | Acc: 54.112,87.625,99.897,%
Batch: 300 | Loss: 2.292 | Acc: 54.052,87.661,99.896,%
Batch: 320 | Loss: 2.297 | Acc: 53.904,87.622,99.895,%
Batch: 340 | Loss: 2.307 | Acc: 53.799,87.539,99.892,%
Batch: 360 | Loss: 2.314 | Acc: 53.707,87.498,99.892,%
Batch: 380 | Loss: 2.310 | Acc: 53.888,87.434,99.895,%
Batch: 0 | Loss: 3.961 | Acc: 49.219,72.656,77.344,%
Batch: 20 | Loss: 4.343 | Acc: 45.908,66.704,73.810,%
Batch: 40 | Loss: 4.361 | Acc: 45.579,66.406,73.056,%
Batch: 60 | Loss: 4.382 | Acc: 45.377,66.099,72.925,%
Train all parameters

Epoch: 256
Batch: 0 | Loss: 3.023 | Acc: 51.562,80.469,100.000,%
Batch: 20 | Loss: 2.541 | Acc: 51.674,85.603,99.888,%
Batch: 40 | Loss: 2.407 | Acc: 53.163,86.738,99.848,%
Batch: 60 | Loss: 2.376 | Acc: 53.215,87.103,99.898,%
Batch: 80 | Loss: 2.346 | Acc: 53.723,87.027,99.884,%
Batch: 100 | Loss: 2.318 | Acc: 53.643,87.500,99.876,%
Batch: 120 | Loss: 2.313 | Acc: 53.848,87.526,99.884,%
Batch: 140 | Loss: 2.317 | Acc: 53.674,87.666,99.889,%
Batch: 160 | Loss: 2.308 | Acc: 53.906,87.587,99.888,%
Batch: 180 | Loss: 2.316 | Acc: 53.781,87.582,99.888,%
Batch: 200 | Loss: 2.297 | Acc: 54.081,87.589,99.891,%
Batch: 220 | Loss: 2.306 | Acc: 53.995,87.588,99.887,%
Batch: 240 | Loss: 2.296 | Acc: 54.101,87.575,99.880,%
Batch: 260 | Loss: 2.304 | Acc: 53.996,87.584,99.877,%
Batch: 280 | Loss: 2.310 | Acc: 54.015,87.586,99.878,%
Batch: 300 | Loss: 2.308 | Acc: 54.059,87.490,99.881,%
Batch: 320 | Loss: 2.314 | Acc: 54.008,87.481,99.883,%
Batch: 340 | Loss: 2.308 | Acc: 54.039,87.367,99.885,%
Batch: 360 | Loss: 2.306 | Acc: 54.092,87.426,99.890,%
Batch: 380 | Loss: 2.301 | Acc: 54.177,87.432,99.887,%
Batch: 0 | Loss: 3.943 | Acc: 49.219,71.875,79.688,%
Batch: 20 | Loss: 4.325 | Acc: 45.722,66.853,73.586,%
Batch: 40 | Loss: 4.344 | Acc: 45.751,66.654,73.037,%
Batch: 60 | Loss: 4.364 | Acc: 45.569,66.470,72.989,%
Train all parameters

Epoch: 257
Batch: 0 | Loss: 1.851 | Acc: 57.812,91.406,100.000,%
Batch: 20 | Loss: 2.228 | Acc: 53.943,87.537,99.777,%
Batch: 40 | Loss: 2.318 | Acc: 53.449,87.176,99.809,%
Batch: 60 | Loss: 2.312 | Acc: 53.919,87.359,99.808,%
Batch: 80 | Loss: 2.345 | Acc: 54.070,87.095,99.846,%
Batch: 100 | Loss: 2.316 | Acc: 54.270,87.167,99.845,%
Batch: 120 | Loss: 2.302 | Acc: 54.145,87.222,99.864,%
Batch: 140 | Loss: 2.304 | Acc: 54.089,87.356,99.873,%
Batch: 160 | Loss: 2.327 | Acc: 53.795,87.194,99.854,%
Batch: 180 | Loss: 2.315 | Acc: 54.023,87.306,99.853,%
Batch: 200 | Loss: 2.330 | Acc: 53.961,87.247,99.848,%
Batch: 220 | Loss: 2.320 | Acc: 54.069,87.316,99.855,%
Batch: 240 | Loss: 2.319 | Acc: 54.068,87.299,99.861,%
Batch: 260 | Loss: 2.324 | Acc: 53.975,87.302,99.865,%
Batch: 280 | Loss: 2.318 | Acc: 54.037,87.214,99.872,%
Batch: 300 | Loss: 2.316 | Acc: 54.194,87.220,99.875,%
Batch: 320 | Loss: 2.317 | Acc: 54.240,87.130,99.873,%
Batch: 340 | Loss: 2.324 | Acc: 54.165,87.147,99.867,%
Batch: 360 | Loss: 2.321 | Acc: 54.220,87.173,99.868,%
Batch: 380 | Loss: 2.314 | Acc: 54.357,87.199,99.871,%
Batch: 0 | Loss: 3.967 | Acc: 49.219,72.656,78.125,%
Batch: 20 | Loss: 4.331 | Acc: 46.205,67.113,73.512,%
Batch: 40 | Loss: 4.346 | Acc: 45.903,66.825,73.056,%
Batch: 60 | Loss: 4.369 | Acc: 45.735,66.598,72.925,%
Train all parameters

Epoch: 258
Batch: 0 | Loss: 2.764 | Acc: 50.781,85.156,98.438,%
Batch: 20 | Loss: 2.292 | Acc: 54.464,87.054,99.926,%
Batch: 40 | Loss: 2.277 | Acc: 54.916,87.214,99.924,%
Batch: 60 | Loss: 2.332 | Acc: 54.828,86.988,99.936,%
Batch: 80 | Loss: 2.311 | Acc: 55.141,87.095,99.932,%
Batch: 100 | Loss: 2.352 | Acc: 54.355,86.835,99.930,%
Batch: 120 | Loss: 2.364 | Acc: 54.035,86.699,99.897,%
Batch: 140 | Loss: 2.331 | Acc: 54.482,86.647,99.911,%
Batch: 160 | Loss: 2.332 | Acc: 54.523,86.889,99.918,%
Batch: 180 | Loss: 2.319 | Acc: 54.549,87.060,99.909,%
Batch: 200 | Loss: 2.306 | Acc: 54.684,87.158,99.907,%
Batch: 220 | Loss: 2.316 | Acc: 54.613,87.111,99.908,%
Batch: 240 | Loss: 2.310 | Acc: 54.477,87.309,99.906,%
Batch: 260 | Loss: 2.316 | Acc: 54.379,87.311,99.907,%
Batch: 280 | Loss: 2.327 | Acc: 54.154,87.219,99.908,%
Batch: 300 | Loss: 2.325 | Acc: 54.166,87.251,99.907,%
Batch: 320 | Loss: 2.321 | Acc: 54.218,87.274,99.910,%
Batch: 340 | Loss: 2.321 | Acc: 54.332,87.198,99.908,%
Batch: 360 | Loss: 2.321 | Acc: 54.261,87.158,99.907,%
Batch: 380 | Loss: 2.327 | Acc: 54.126,87.125,99.910,%
Batch: 0 | Loss: 3.947 | Acc: 51.562,71.875,78.906,%
Batch: 20 | Loss: 4.348 | Acc: 46.168,67.262,73.549,%
Batch: 40 | Loss: 4.352 | Acc: 45.846,66.921,72.923,%
Batch: 60 | Loss: 4.372 | Acc: 45.838,66.675,72.810,%
Train all parameters

Epoch: 259
Batch: 0 | Loss: 3.001 | Acc: 49.219,81.250,100.000,%
Batch: 20 | Loss: 2.131 | Acc: 56.882,88.504,99.888,%
Batch: 40 | Loss: 2.217 | Acc: 55.812,87.919,99.867,%
Batch: 60 | Loss: 2.217 | Acc: 55.763,87.884,99.859,%
Batch: 80 | Loss: 2.208 | Acc: 55.613,88.040,99.875,%
Batch: 100 | Loss: 2.236 | Acc: 55.082,87.833,99.884,%
Batch: 120 | Loss: 2.260 | Acc: 54.959,87.629,99.884,%
Batch: 140 | Loss: 2.243 | Acc: 55.147,87.738,99.873,%
Batch: 160 | Loss: 2.264 | Acc: 55.022,87.631,99.879,%
Batch: 180 | Loss: 2.268 | Acc: 55.154,87.474,99.888,%
Batch: 200 | Loss: 2.277 | Acc: 54.979,87.558,99.883,%
Batch: 220 | Loss: 2.278 | Acc: 54.857,87.528,99.887,%
Batch: 240 | Loss: 2.276 | Acc: 54.772,87.500,99.890,%
Batch: 260 | Loss: 2.286 | Acc: 54.717,87.455,99.895,%
Batch: 280 | Loss: 2.297 | Acc: 54.504,87.425,99.892,%
Batch: 300 | Loss: 2.294 | Acc: 54.514,87.495,99.899,%
Batch: 320 | Loss: 2.293 | Acc: 54.522,87.471,99.893,%
Batch: 340 | Loss: 2.296 | Acc: 54.417,87.484,99.895,%
Batch: 360 | Loss: 2.297 | Acc: 54.415,87.537,99.896,%
Batch: 380 | Loss: 2.302 | Acc: 54.345,87.523,99.900,%
Batch: 0 | Loss: 3.976 | Acc: 50.781,72.656,79.688,%
Batch: 20 | Loss: 4.344 | Acc: 46.057,66.964,73.772,%
Batch: 40 | Loss: 4.360 | Acc: 45.884,66.692,73.323,%
Batch: 60 | Loss: 4.378 | Acc: 45.594,66.419,73.028,%
Train all parameters

Epoch: 260
Batch: 0 | Loss: 2.222 | Acc: 57.031,83.594,100.000,%
Batch: 20 | Loss: 2.364 | Acc: 53.943,87.240,99.926,%
Batch: 40 | Loss: 2.317 | Acc: 54.688,87.595,99.886,%
Batch: 60 | Loss: 2.295 | Acc: 54.675,87.999,99.923,%
Batch: 80 | Loss: 2.312 | Acc: 54.533,87.712,99.913,%
Batch: 100 | Loss: 2.287 | Acc: 54.896,87.848,99.884,%
Batch: 120 | Loss: 2.291 | Acc: 54.726,87.984,99.897,%
Batch: 140 | Loss: 2.298 | Acc: 54.809,87.694,99.878,%
Batch: 160 | Loss: 2.323 | Acc: 54.489,87.505,99.879,%
Batch: 180 | Loss: 2.313 | Acc: 54.549,87.435,99.883,%
Batch: 200 | Loss: 2.313 | Acc: 54.485,87.477,99.880,%
Batch: 220 | Loss: 2.307 | Acc: 54.525,87.564,99.880,%
Batch: 240 | Loss: 2.304 | Acc: 54.376,87.623,99.883,%
Batch: 260 | Loss: 2.323 | Acc: 54.194,87.401,99.883,%
Batch: 280 | Loss: 2.319 | Acc: 54.173,87.372,99.892,%
Batch: 300 | Loss: 2.318 | Acc: 54.145,87.433,99.891,%
Batch: 320 | Loss: 2.320 | Acc: 54.103,87.427,99.888,%
Batch: 340 | Loss: 2.314 | Acc: 54.032,87.470,99.888,%
Batch: 360 | Loss: 2.309 | Acc: 54.062,87.394,99.887,%
Batch: 380 | Loss: 2.318 | Acc: 53.947,87.365,99.891,%
Batch: 0 | Loss: 3.959 | Acc: 50.781,71.875,79.688,%
Batch: 20 | Loss: 4.331 | Acc: 46.317,67.448,73.735,%
Batch: 40 | Loss: 4.339 | Acc: 46.113,67.016,73.152,%
Batch: 60 | Loss: 4.361 | Acc: 45.914,66.739,72.989,%
Train all parameters

Epoch: 261
Batch: 0 | Loss: 2.261 | Acc: 53.906,90.625,100.000,%
Batch: 20 | Loss: 2.307 | Acc: 53.832,88.170,99.851,%
Batch: 40 | Loss: 2.300 | Acc: 54.421,87.824,99.905,%
Batch: 60 | Loss: 2.270 | Acc: 54.419,88.256,99.923,%
Batch: 80 | Loss: 2.282 | Acc: 54.475,87.924,99.923,%
Batch: 100 | Loss: 2.280 | Acc: 54.293,88.072,99.915,%
Batch: 120 | Loss: 2.293 | Acc: 54.197,87.726,99.910,%
Batch: 140 | Loss: 2.296 | Acc: 54.266,87.583,99.911,%
Batch: 160 | Loss: 2.327 | Acc: 53.950,87.466,99.908,%
Batch: 180 | Loss: 2.320 | Acc: 54.131,87.379,99.909,%
Batch: 200 | Loss: 2.337 | Acc: 53.980,87.345,99.918,%
Batch: 220 | Loss: 2.334 | Acc: 54.016,87.394,99.919,%
Batch: 240 | Loss: 2.338 | Acc: 54.033,87.199,99.916,%
Batch: 260 | Loss: 2.337 | Acc: 54.020,87.192,99.907,%
Batch: 280 | Loss: 2.329 | Acc: 54.118,87.219,99.905,%
Batch: 300 | Loss: 2.328 | Acc: 54.163,87.194,99.909,%
Batch: 320 | Loss: 2.332 | Acc: 54.186,87.167,99.908,%
Batch: 340 | Loss: 2.333 | Acc: 54.181,87.179,99.904,%
Batch: 360 | Loss: 2.329 | Acc: 54.209,87.219,99.903,%
Batch: 380 | Loss: 2.332 | Acc: 54.224,87.235,99.904,%
Batch: 0 | Loss: 3.931 | Acc: 49.219,70.312,78.906,%
Batch: 20 | Loss: 4.327 | Acc: 46.243,66.890,73.921,%
Batch: 40 | Loss: 4.343 | Acc: 46.037,66.730,73.171,%
Batch: 60 | Loss: 4.365 | Acc: 45.786,66.470,72.938,%
Train all parameters

Epoch: 262
Batch: 0 | Loss: 1.791 | Acc: 56.250,92.969,100.000,%
Batch: 20 | Loss: 2.279 | Acc: 53.497,88.281,99.888,%
Batch: 40 | Loss: 2.324 | Acc: 53.030,87.729,99.924,%
Batch: 60 | Loss: 2.287 | Acc: 54.290,87.602,99.923,%
Batch: 80 | Loss: 2.306 | Acc: 54.196,87.249,99.904,%
Batch: 100 | Loss: 2.302 | Acc: 53.953,87.283,99.907,%
Batch: 120 | Loss: 2.308 | Acc: 53.771,87.397,99.910,%
Batch: 140 | Loss: 2.317 | Acc: 53.795,87.312,99.911,%
Batch: 160 | Loss: 2.289 | Acc: 54.105,87.612,99.903,%
Batch: 180 | Loss: 2.296 | Acc: 54.191,87.513,99.914,%
Batch: 200 | Loss: 2.287 | Acc: 54.237,87.655,99.914,%
Batch: 220 | Loss: 2.288 | Acc: 54.129,87.652,99.908,%
Batch: 240 | Loss: 2.293 | Acc: 54.075,87.717,99.912,%
Batch: 260 | Loss: 2.306 | Acc: 53.843,87.623,99.910,%
Batch: 280 | Loss: 2.300 | Acc: 54.012,87.653,99.908,%
Batch: 300 | Loss: 2.298 | Acc: 54.091,87.726,99.904,%
Batch: 320 | Loss: 2.301 | Acc: 53.970,87.748,99.910,%
Batch: 340 | Loss: 2.295 | Acc: 54.039,87.802,99.911,%
Batch: 360 | Loss: 2.301 | Acc: 53.971,87.773,99.913,%
Batch: 380 | Loss: 2.307 | Acc: 53.855,87.783,99.908,%
Batch: 0 | Loss: 3.931 | Acc: 50.000,71.094,78.906,%
Batch: 20 | Loss: 4.337 | Acc: 46.205,67.113,73.884,%
Batch: 40 | Loss: 4.349 | Acc: 45.827,66.825,73.418,%
Batch: 60 | Loss: 4.369 | Acc: 45.505,66.598,73.220,%
Train all parameters

Epoch: 263
Batch: 0 | Loss: 2.382 | Acc: 53.125,91.406,100.000,%
Batch: 20 | Loss: 2.195 | Acc: 55.692,88.132,99.963,%
Batch: 40 | Loss: 2.225 | Acc: 54.916,88.014,99.924,%
Batch: 60 | Loss: 2.261 | Acc: 54.316,88.153,99.923,%
Batch: 80 | Loss: 2.265 | Acc: 54.282,88.137,99.923,%
Batch: 100 | Loss: 2.272 | Acc: 54.339,87.925,99.923,%
Batch: 120 | Loss: 2.289 | Acc: 54.455,87.649,99.935,%
Batch: 140 | Loss: 2.290 | Acc: 54.338,87.744,99.939,%
Batch: 160 | Loss: 2.278 | Acc: 54.421,87.864,99.942,%
Batch: 180 | Loss: 2.266 | Acc: 54.614,87.854,99.948,%
Batch: 200 | Loss: 2.267 | Acc: 54.571,87.994,99.953,%
Batch: 220 | Loss: 2.260 | Acc: 54.702,87.974,99.951,%
Batch: 240 | Loss: 2.280 | Acc: 54.438,87.844,99.945,%
Batch: 260 | Loss: 2.284 | Acc: 54.343,87.802,99.940,%
Batch: 280 | Loss: 2.293 | Acc: 54.276,87.703,99.933,%
Batch: 300 | Loss: 2.293 | Acc: 54.337,87.697,99.927,%
Batch: 320 | Loss: 2.288 | Acc: 54.305,87.736,99.927,%
Batch: 340 | Loss: 2.283 | Acc: 54.344,87.750,99.920,%
Batch: 360 | Loss: 2.293 | Acc: 54.274,87.734,99.916,%
Batch: 380 | Loss: 2.296 | Acc: 54.212,87.752,99.918,%
Batch: 0 | Loss: 3.973 | Acc: 50.000,71.875,78.906,%
Batch: 20 | Loss: 4.327 | Acc: 46.243,67.299,73.884,%
Batch: 40 | Loss: 4.342 | Acc: 45.808,66.864,73.266,%
Batch: 60 | Loss: 4.365 | Acc: 45.543,66.586,73.040,%
Train all parameters

Epoch: 264
Batch: 0 | Loss: 1.721 | Acc: 60.938,92.188,100.000,%
Batch: 20 | Loss: 2.229 | Acc: 56.176,87.984,99.888,%
Batch: 40 | Loss: 2.235 | Acc: 55.678,88.548,99.905,%
Batch: 60 | Loss: 2.178 | Acc: 55.584,88.998,99.923,%
Batch: 80 | Loss: 2.136 | Acc: 56.173,89.120,99.932,%
Batch: 100 | Loss: 2.182 | Acc: 55.322,88.869,99.930,%
Batch: 120 | Loss: 2.223 | Acc: 54.597,88.759,99.916,%
Batch: 140 | Loss: 2.231 | Acc: 54.649,88.525,99.922,%
Batch: 160 | Loss: 2.223 | Acc: 54.833,88.568,99.922,%
Batch: 180 | Loss: 2.229 | Acc: 54.787,88.402,99.914,%
Batch: 200 | Loss: 2.240 | Acc: 54.726,88.266,99.911,%
Batch: 220 | Loss: 2.253 | Acc: 54.564,88.186,99.908,%
Batch: 240 | Loss: 2.247 | Acc: 54.561,88.310,99.906,%
Batch: 260 | Loss: 2.249 | Acc: 54.568,88.269,99.898,%
Batch: 280 | Loss: 2.253 | Acc: 54.596,88.223,99.894,%
Batch: 300 | Loss: 2.253 | Acc: 54.514,88.310,99.899,%
Batch: 320 | Loss: 2.252 | Acc: 54.605,88.228,99.900,%
Batch: 340 | Loss: 2.248 | Acc: 54.671,88.158,99.901,%
Batch: 360 | Loss: 2.253 | Acc: 54.679,88.099,99.900,%
Batch: 380 | Loss: 2.255 | Acc: 54.651,88.062,99.895,%
Batch: 0 | Loss: 3.957 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.329 | Acc: 46.243,67.188,73.735,%
Batch: 40 | Loss: 4.341 | Acc: 45.846,66.921,73.247,%
Batch: 60 | Loss: 4.360 | Acc: 45.722,66.752,73.066,%
Train all parameters

Epoch: 265
Batch: 0 | Loss: 3.057 | Acc: 46.875,81.250,100.000,%
Batch: 20 | Loss: 2.218 | Acc: 54.576,88.207,99.963,%
Batch: 40 | Loss: 2.254 | Acc: 54.611,88.186,99.924,%
Batch: 60 | Loss: 2.281 | Acc: 53.727,88.256,99.885,%
Batch: 80 | Loss: 2.293 | Acc: 53.646,88.108,99.904,%
Batch: 100 | Loss: 2.318 | Acc: 53.458,87.887,99.915,%
Batch: 120 | Loss: 2.263 | Acc: 54.184,88.159,99.910,%
Batch: 140 | Loss: 2.284 | Acc: 53.984,87.988,99.911,%
Batch: 160 | Loss: 2.276 | Acc: 54.338,87.912,99.913,%
Batch: 180 | Loss: 2.270 | Acc: 54.467,87.992,99.909,%
Batch: 200 | Loss: 2.251 | Acc: 54.769,87.982,99.911,%
Batch: 220 | Loss: 2.253 | Acc: 54.670,87.960,99.915,%
Batch: 240 | Loss: 2.256 | Acc: 54.736,87.931,99.919,%
Batch: 260 | Loss: 2.264 | Acc: 54.735,87.925,99.916,%
Batch: 280 | Loss: 2.263 | Acc: 54.757,87.917,99.914,%
Batch: 300 | Loss: 2.281 | Acc: 54.589,87.801,99.914,%
Batch: 320 | Loss: 2.280 | Acc: 54.439,87.758,99.915,%
Batch: 340 | Loss: 2.275 | Acc: 54.584,87.660,99.913,%
Batch: 360 | Loss: 2.275 | Acc: 54.586,87.630,99.911,%
Batch: 380 | Loss: 2.280 | Acc: 54.605,87.502,99.914,%
Batch: 0 | Loss: 3.950 | Acc: 49.219,71.875,78.906,%
Batch: 20 | Loss: 4.334 | Acc: 46.205,67.336,73.735,%
Batch: 40 | Loss: 4.345 | Acc: 45.884,66.921,73.133,%
Batch: 60 | Loss: 4.366 | Acc: 45.658,66.509,73.066,%
Train all parameters

Epoch: 266
Batch: 0 | Loss: 2.364 | Acc: 49.219,88.281,100.000,%
Batch: 20 | Loss: 2.334 | Acc: 55.208,85.714,99.926,%
Batch: 40 | Loss: 2.224 | Acc: 55.488,87.214,99.924,%
Batch: 60 | Loss: 2.230 | Acc: 54.816,87.705,99.898,%
Batch: 80 | Loss: 2.209 | Acc: 55.276,88.011,99.904,%
Batch: 100 | Loss: 2.210 | Acc: 55.113,88.157,99.907,%
Batch: 120 | Loss: 2.216 | Acc: 55.062,88.075,99.903,%
Batch: 140 | Loss: 2.228 | Acc: 54.998,87.888,99.900,%
Batch: 160 | Loss: 2.229 | Acc: 54.998,88.063,99.903,%
Batch: 180 | Loss: 2.221 | Acc: 55.072,88.018,99.905,%
Batch: 200 | Loss: 2.228 | Acc: 54.983,87.986,99.914,%
Batch: 220 | Loss: 2.243 | Acc: 54.942,87.846,99.912,%
Batch: 240 | Loss: 2.258 | Acc: 54.778,87.750,99.912,%
Batch: 260 | Loss: 2.248 | Acc: 54.813,87.892,99.901,%
Batch: 280 | Loss: 2.255 | Acc: 54.738,87.786,99.900,%
Batch: 300 | Loss: 2.258 | Acc: 54.778,87.684,99.899,%
Batch: 320 | Loss: 2.255 | Acc: 54.717,87.780,99.898,%
Batch: 340 | Loss: 2.257 | Acc: 54.704,87.782,99.899,%
Batch: 360 | Loss: 2.261 | Acc: 54.720,87.699,99.896,%
Batch: 380 | Loss: 2.264 | Acc: 54.710,87.742,99.897,%
Batch: 0 | Loss: 3.949 | Acc: 49.219,71.094,79.688,%
Batch: 20 | Loss: 4.336 | Acc: 46.057,67.039,73.624,%
Batch: 40 | Loss: 4.352 | Acc: 45.789,66.730,72.999,%
Batch: 60 | Loss: 4.373 | Acc: 45.543,66.598,72.912,%
Train all parameters

Epoch: 267
Batch: 0 | Loss: 2.259 | Acc: 50.781,81.250,100.000,%
Batch: 20 | Loss: 2.423 | Acc: 53.981,86.644,99.963,%
Batch: 40 | Loss: 2.323 | Acc: 53.525,88.053,99.943,%
Batch: 60 | Loss: 2.331 | Acc: 54.265,87.398,99.949,%
Batch: 80 | Loss: 2.329 | Acc: 53.964,87.510,99.961,%
Batch: 100 | Loss: 2.315 | Acc: 54.332,87.740,99.930,%
Batch: 120 | Loss: 2.313 | Acc: 54.158,87.726,99.923,%
Batch: 140 | Loss: 2.320 | Acc: 54.361,87.666,99.911,%
Batch: 160 | Loss: 2.312 | Acc: 54.353,87.849,99.903,%
Batch: 180 | Loss: 2.308 | Acc: 54.377,87.794,99.905,%
Batch: 200 | Loss: 2.313 | Acc: 54.404,87.780,99.895,%
Batch: 220 | Loss: 2.307 | Acc: 54.599,87.822,99.897,%
Batch: 240 | Loss: 2.294 | Acc: 54.736,87.853,99.900,%
Batch: 260 | Loss: 2.303 | Acc: 54.670,87.787,99.898,%
Batch: 280 | Loss: 2.295 | Acc: 54.607,87.875,99.894,%
Batch: 300 | Loss: 2.293 | Acc: 54.599,87.822,99.899,%
Batch: 320 | Loss: 2.302 | Acc: 54.546,87.675,99.895,%
Batch: 340 | Loss: 2.309 | Acc: 54.429,87.612,99.895,%
Batch: 360 | Loss: 2.304 | Acc: 54.462,87.632,99.894,%
Batch: 380 | Loss: 2.299 | Acc: 54.558,87.660,99.893,%
Batch: 0 | Loss: 3.977 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.338 | Acc: 46.280,67.188,73.772,%
Batch: 40 | Loss: 4.347 | Acc: 45.960,66.883,73.399,%
Batch: 60 | Loss: 4.365 | Acc: 45.748,66.586,73.258,%
Train all parameters

Epoch: 268
Batch: 0 | Loss: 2.116 | Acc: 55.469,85.156,100.000,%
Batch: 20 | Loss: 2.352 | Acc: 53.869,87.463,100.000,%
Batch: 40 | Loss: 2.323 | Acc: 54.402,87.424,100.000,%
Batch: 60 | Loss: 2.280 | Acc: 54.892,87.666,99.974,%
Batch: 80 | Loss: 2.249 | Acc: 55.035,88.146,99.942,%
Batch: 100 | Loss: 2.261 | Acc: 54.858,88.134,99.954,%
Batch: 120 | Loss: 2.241 | Acc: 55.049,88.152,99.942,%
Batch: 140 | Loss: 2.236 | Acc: 55.037,88.182,99.922,%
Batch: 160 | Loss: 2.263 | Acc: 54.794,88.213,99.932,%
Batch: 180 | Loss: 2.283 | Acc: 54.446,88.178,99.935,%
Batch: 200 | Loss: 2.289 | Acc: 54.481,88.149,99.938,%
Batch: 220 | Loss: 2.304 | Acc: 54.334,88.030,99.929,%
Batch: 240 | Loss: 2.290 | Acc: 54.506,87.999,99.935,%
Batch: 260 | Loss: 2.298 | Acc: 54.415,87.946,99.934,%
Batch: 280 | Loss: 2.291 | Acc: 54.487,88.000,99.936,%
Batch: 300 | Loss: 2.292 | Acc: 54.490,87.965,99.935,%
Batch: 320 | Loss: 2.292 | Acc: 54.515,87.953,99.939,%
Batch: 340 | Loss: 2.295 | Acc: 54.369,87.997,99.938,%
Batch: 360 | Loss: 2.292 | Acc: 54.432,88.013,99.937,%
Batch: 380 | Loss: 2.310 | Acc: 54.210,87.980,99.936,%
Batch: 0 | Loss: 3.971 | Acc: 49.219,71.875,77.344,%
Batch: 20 | Loss: 4.326 | Acc: 46.466,67.188,74.219,%
Batch: 40 | Loss: 4.341 | Acc: 45.979,66.825,73.457,%
Batch: 60 | Loss: 4.362 | Acc: 45.786,66.560,73.092,%
Train all parameters

Epoch: 269
Batch: 0 | Loss: 2.735 | Acc: 57.031,88.281,100.000,%
Batch: 20 | Loss: 2.389 | Acc: 53.051,86.124,100.000,%
Batch: 40 | Loss: 2.372 | Acc: 52.553,87.367,99.981,%
Batch: 60 | Loss: 2.357 | Acc: 52.920,87.551,99.974,%
Batch: 80 | Loss: 2.348 | Acc: 53.212,87.539,99.961,%
Batch: 100 | Loss: 2.305 | Acc: 54.177,87.608,99.938,%
Batch: 120 | Loss: 2.296 | Acc: 54.261,87.707,99.923,%
Batch: 140 | Loss: 2.287 | Acc: 54.444,87.572,99.911,%
Batch: 160 | Loss: 2.276 | Acc: 54.484,87.490,99.922,%
Batch: 180 | Loss: 2.270 | Acc: 54.662,87.491,99.914,%
Batch: 200 | Loss: 2.273 | Acc: 54.769,87.531,99.918,%
Batch: 220 | Loss: 2.270 | Acc: 54.769,87.585,99.919,%
Batch: 240 | Loss: 2.258 | Acc: 54.931,87.565,99.916,%
Batch: 260 | Loss: 2.259 | Acc: 54.837,87.650,99.916,%
Batch: 280 | Loss: 2.270 | Acc: 54.635,87.620,99.919,%
Batch: 300 | Loss: 2.261 | Acc: 54.846,87.715,99.922,%
Batch: 320 | Loss: 2.263 | Acc: 54.739,87.794,99.917,%
Batch: 340 | Loss: 2.259 | Acc: 54.820,87.809,99.920,%
Batch: 360 | Loss: 2.269 | Acc: 54.629,87.784,99.918,%
Batch: 380 | Loss: 2.273 | Acc: 54.558,87.775,99.916,%
Batch: 0 | Loss: 3.943 | Acc: 49.219,72.656,78.906,%
Batch: 20 | Loss: 4.318 | Acc: 46.205,67.374,73.958,%
Batch: 40 | Loss: 4.335 | Acc: 45.979,66.959,73.418,%
Batch: 60 | Loss: 4.359 | Acc: 45.710,66.624,73.130,%
Train all parameters

Epoch: 270
Batch: 0 | Loss: 2.309 | Acc: 48.438,91.406,100.000,%
Batch: 20 | Loss: 2.500 | Acc: 51.786,86.607,99.888,%
Batch: 40 | Loss: 2.406 | Acc: 52.992,86.852,99.924,%
Batch: 60 | Loss: 2.372 | Acc: 53.266,87.282,99.898,%
Batch: 80 | Loss: 2.332 | Acc: 53.405,87.548,99.904,%
Batch: 100 | Loss: 2.345 | Acc: 53.481,87.523,99.876,%
Batch: 120 | Loss: 2.331 | Acc: 53.642,87.829,99.871,%
Batch: 140 | Loss: 2.328 | Acc: 53.646,87.821,99.867,%
Batch: 160 | Loss: 2.316 | Acc: 53.863,87.903,99.874,%
Batch: 180 | Loss: 2.331 | Acc: 53.859,87.461,99.879,%
Batch: 200 | Loss: 2.327 | Acc: 54.000,87.356,99.864,%
Batch: 220 | Loss: 2.310 | Acc: 54.079,87.567,99.866,%
Batch: 240 | Loss: 2.306 | Acc: 54.039,87.669,99.867,%
Batch: 260 | Loss: 2.299 | Acc: 54.176,87.751,99.868,%
Batch: 280 | Loss: 2.304 | Acc: 54.179,87.639,99.872,%
Batch: 300 | Loss: 2.296 | Acc: 54.345,87.630,99.875,%
Batch: 320 | Loss: 2.292 | Acc: 54.461,87.661,99.883,%
Batch: 340 | Loss: 2.293 | Acc: 54.431,87.722,99.888,%
Batch: 360 | Loss: 2.293 | Acc: 54.367,87.829,99.892,%
Batch: 380 | Loss: 2.296 | Acc: 54.296,87.785,99.891,%
Batch: 0 | Loss: 3.961 | Acc: 49.219,71.875,78.125,%
Batch: 20 | Loss: 4.329 | Acc: 46.131,67.262,73.661,%
Batch: 40 | Loss: 4.340 | Acc: 46.056,66.902,73.266,%
Batch: 60 | Loss: 4.361 | Acc: 45.876,66.662,73.143,%
Train all parameters

Epoch: 271
Batch: 0 | Loss: 2.239 | Acc: 57.031,86.719,100.000,%
Batch: 20 | Loss: 2.365 | Acc: 53.981,87.314,99.888,%
Batch: 40 | Loss: 2.365 | Acc: 53.639,87.786,99.943,%
Batch: 60 | Loss: 2.343 | Acc: 53.676,88.204,99.923,%
Batch: 80 | Loss: 2.350 | Acc: 53.385,87.992,99.932,%
Batch: 100 | Loss: 2.350 | Acc: 53.543,87.949,99.938,%
Batch: 120 | Loss: 2.368 | Acc: 53.196,87.765,99.916,%
Batch: 140 | Loss: 2.363 | Acc: 53.363,87.694,99.917,%
Batch: 160 | Loss: 2.343 | Acc: 53.702,87.519,99.913,%
Batch: 180 | Loss: 2.348 | Acc: 53.617,87.642,99.909,%
Batch: 200 | Loss: 2.331 | Acc: 53.829,87.811,99.918,%
Batch: 220 | Loss: 2.343 | Acc: 53.701,87.868,99.919,%
Batch: 240 | Loss: 2.339 | Acc: 53.770,87.889,99.922,%
Batch: 260 | Loss: 2.334 | Acc: 53.867,87.940,99.916,%
Batch: 280 | Loss: 2.314 | Acc: 54.093,87.970,99.905,%
Batch: 300 | Loss: 2.310 | Acc: 54.135,87.983,99.907,%
Batch: 320 | Loss: 2.314 | Acc: 54.074,87.967,99.908,%
Batch: 340 | Loss: 2.315 | Acc: 54.048,87.889,99.911,%
Batch: 360 | Loss: 2.310 | Acc: 54.066,87.885,99.905,%
Batch: 380 | Loss: 2.307 | Acc: 54.064,87.947,99.906,%
Batch: 0 | Loss: 3.934 | Acc: 50.000,71.875,78.125,%
Batch: 20 | Loss: 4.325 | Acc: 46.205,67.262,73.661,%
Batch: 40 | Loss: 4.343 | Acc: 45.998,66.940,73.190,%
Batch: 60 | Loss: 4.366 | Acc: 45.748,66.714,73.066,%
Train all parameters

Epoch: 272
Batch: 0 | Loss: 2.495 | Acc: 54.688,78.906,100.000,%
Batch: 20 | Loss: 2.297 | Acc: 54.613,87.202,99.814,%
Batch: 40 | Loss: 2.348 | Acc: 54.516,87.043,99.848,%
Batch: 60 | Loss: 2.357 | Acc: 54.726,86.872,99.834,%
Batch: 80 | Loss: 2.322 | Acc: 54.958,87.172,99.875,%
Batch: 100 | Loss: 2.317 | Acc: 54.541,87.477,99.892,%
Batch: 120 | Loss: 2.332 | Acc: 54.688,87.138,99.910,%
Batch: 140 | Loss: 2.328 | Acc: 54.615,87.145,99.917,%
Batch: 160 | Loss: 2.336 | Acc: 54.387,87.029,99.913,%
Batch: 180 | Loss: 2.329 | Acc: 54.381,87.228,99.909,%
Batch: 200 | Loss: 2.339 | Acc: 54.260,87.310,99.914,%
Batch: 220 | Loss: 2.329 | Acc: 54.341,87.415,99.919,%
Batch: 240 | Loss: 2.322 | Acc: 54.496,87.542,99.909,%
Batch: 260 | Loss: 2.310 | Acc: 54.475,87.650,99.916,%
Batch: 280 | Loss: 2.298 | Acc: 54.654,87.681,99.917,%
Batch: 300 | Loss: 2.307 | Acc: 54.558,87.635,99.920,%
Batch: 320 | Loss: 2.306 | Acc: 54.510,87.668,99.922,%
Batch: 340 | Loss: 2.300 | Acc: 54.447,87.734,99.922,%
Batch: 360 | Loss: 2.313 | Acc: 54.311,87.615,99.924,%
Batch: 380 | Loss: 2.320 | Acc: 54.259,87.588,99.928,%
Batch: 0 | Loss: 3.952 | Acc: 49.219,71.875,78.906,%
Batch: 20 | Loss: 4.325 | Acc: 46.131,67.262,74.070,%
Batch: 40 | Loss: 4.341 | Acc: 46.037,66.883,73.457,%
Batch: 60 | Loss: 4.363 | Acc: 45.799,66.688,73.194,%
Train all parameters

Epoch: 273
Batch: 0 | Loss: 2.199 | Acc: 55.469,96.094,100.000,%
Batch: 20 | Loss: 2.334 | Acc: 53.683,87.984,99.926,%
Batch: 40 | Loss: 2.255 | Acc: 54.726,88.167,99.905,%
Batch: 60 | Loss: 2.325 | Acc: 54.162,87.526,99.885,%
Batch: 80 | Loss: 2.323 | Acc: 53.935,87.780,99.894,%
Batch: 100 | Loss: 2.303 | Acc: 54.138,88.304,99.884,%
Batch: 120 | Loss: 2.292 | Acc: 54.171,88.385,99.884,%
Batch: 140 | Loss: 2.302 | Acc: 53.895,88.193,99.900,%
Batch: 160 | Loss: 2.299 | Acc: 53.882,88.407,99.903,%
Batch: 180 | Loss: 2.297 | Acc: 54.113,88.048,99.905,%
Batch: 200 | Loss: 2.277 | Acc: 54.404,88.118,99.911,%
Batch: 220 | Loss: 2.275 | Acc: 54.383,87.977,99.915,%
Batch: 240 | Loss: 2.278 | Acc: 54.334,88.041,99.919,%
Batch: 260 | Loss: 2.281 | Acc: 54.203,88.009,99.910,%
Batch: 280 | Loss: 2.283 | Acc: 54.198,87.981,99.911,%
Batch: 300 | Loss: 2.285 | Acc: 54.285,87.879,99.907,%
Batch: 320 | Loss: 2.283 | Acc: 54.313,87.809,99.903,%
Batch: 340 | Loss: 2.283 | Acc: 54.284,87.860,99.908,%
Batch: 360 | Loss: 2.280 | Acc: 54.387,87.907,99.907,%
Batch: 380 | Loss: 2.272 | Acc: 54.491,87.961,99.910,%
Batch: 0 | Loss: 3.929 | Acc: 50.000,71.875,78.906,%
Batch: 20 | Loss: 4.326 | Acc: 46.726,67.411,73.772,%
Batch: 40 | Loss: 4.338 | Acc: 46.322,67.035,73.190,%
Batch: 60 | Loss: 4.360 | Acc: 46.004,66.675,72.976,%
Train all parameters

Epoch: 274
Batch: 0 | Loss: 2.183 | Acc: 57.031,84.375,100.000,%
Batch: 20 | Loss: 2.347 | Acc: 53.981,86.719,99.963,%
Batch: 40 | Loss: 2.315 | Acc: 54.954,87.081,99.905,%
Batch: 60 | Loss: 2.255 | Acc: 55.418,87.910,99.910,%
Batch: 80 | Loss: 2.220 | Acc: 55.662,88.079,99.904,%
Batch: 100 | Loss: 2.249 | Acc: 55.159,87.670,99.899,%
Batch: 120 | Loss: 2.274 | Acc: 55.204,87.694,99.910,%
Batch: 140 | Loss: 2.287 | Acc: 55.153,87.633,99.917,%
Batch: 160 | Loss: 2.318 | Acc: 54.717,87.325,99.918,%
Batch: 180 | Loss: 2.315 | Acc: 54.631,87.431,99.927,%
Batch: 200 | Loss: 2.306 | Acc: 54.575,87.609,99.914,%
Batch: 220 | Loss: 2.319 | Acc: 54.391,87.542,99.915,%
Batch: 240 | Loss: 2.329 | Acc: 54.276,87.438,99.919,%
Batch: 260 | Loss: 2.314 | Acc: 54.343,87.509,99.919,%
Batch: 280 | Loss: 2.313 | Acc: 54.326,87.556,99.919,%
Batch: 300 | Loss: 2.307 | Acc: 54.371,87.557,99.914,%
Batch: 320 | Loss: 2.310 | Acc: 54.395,87.554,99.910,%
Batch: 340 | Loss: 2.301 | Acc: 54.374,87.619,99.911,%
Batch: 360 | Loss: 2.292 | Acc: 54.493,87.669,99.905,%
Batch: 380 | Loss: 2.291 | Acc: 54.493,87.705,99.904,%
Batch: 0 | Loss: 3.945 | Acc: 49.219,71.875,79.688,%
Batch: 20 | Loss: 4.327 | Acc: 46.243,67.411,73.661,%
Batch: 40 | Loss: 4.345 | Acc: 46.037,66.883,73.133,%
Batch: 60 | Loss: 4.368 | Acc: 45.722,66.598,73.079,%
Train all parameters

Epoch: 275
Batch: 0 | Loss: 2.387 | Acc: 49.219,91.406,100.000,%
Batch: 20 | Loss: 2.267 | Acc: 54.353,87.835,99.888,%
Batch: 40 | Loss: 2.347 | Acc: 53.735,87.367,99.924,%
Batch: 60 | Loss: 2.311 | Acc: 54.764,86.808,99.898,%
Batch: 80 | Loss: 2.339 | Acc: 54.495,86.709,99.884,%
Batch: 100 | Loss: 2.359 | Acc: 54.107,86.556,99.876,%
Batch: 120 | Loss: 2.358 | Acc: 53.868,86.725,99.877,%
Batch: 140 | Loss: 2.329 | Acc: 54.205,86.913,99.889,%
Batch: 160 | Loss: 2.344 | Acc: 53.950,87.107,99.893,%
Batch: 180 | Loss: 2.334 | Acc: 54.247,87.155,99.901,%
Batch: 200 | Loss: 2.331 | Acc: 54.229,87.341,99.899,%
Batch: 220 | Loss: 2.343 | Acc: 54.171,87.295,99.897,%
Batch: 240 | Loss: 2.338 | Acc: 54.234,87.318,99.900,%
Batch: 260 | Loss: 2.320 | Acc: 54.475,87.371,99.907,%
Batch: 280 | Loss: 2.317 | Acc: 54.454,87.428,99.911,%
Batch: 300 | Loss: 2.322 | Acc: 54.480,87.287,99.912,%
Batch: 320 | Loss: 2.312 | Acc: 54.527,87.381,99.912,%
Batch: 340 | Loss: 2.310 | Acc: 54.527,87.429,99.913,%
Batch: 360 | Loss: 2.315 | Acc: 54.532,87.407,99.911,%
Batch: 380 | Loss: 2.310 | Acc: 54.554,87.332,99.912,%
Batch: 0 | Loss: 3.940 | Acc: 50.000,71.094,79.688,%
Batch: 20 | Loss: 4.336 | Acc: 46.168,67.299,73.810,%
Batch: 40 | Loss: 4.345 | Acc: 46.037,66.978,73.190,%
Batch: 60 | Loss: 4.367 | Acc: 45.786,66.726,72.925,%
Train all parameters

Epoch: 276
Batch: 0 | Loss: 2.893 | Acc: 48.438,88.281,100.000,%
Batch: 20 | Loss: 2.268 | Acc: 55.915,87.984,99.888,%
Batch: 40 | Loss: 2.294 | Acc: 55.259,88.129,99.848,%
Batch: 60 | Loss: 2.284 | Acc: 54.688,88.012,99.859,%
Batch: 80 | Loss: 2.276 | Acc: 54.639,87.963,99.875,%
Batch: 100 | Loss: 2.310 | Acc: 54.015,88.088,99.861,%
Batch: 120 | Loss: 2.324 | Acc: 53.880,87.984,99.858,%
Batch: 140 | Loss: 2.318 | Acc: 53.989,87.794,99.878,%
Batch: 160 | Loss: 2.320 | Acc: 54.042,87.680,99.869,%
Batch: 180 | Loss: 2.318 | Acc: 54.044,87.724,99.866,%
Batch: 200 | Loss: 2.315 | Acc: 54.213,87.609,99.860,%
Batch: 220 | Loss: 2.316 | Acc: 54.122,87.641,99.866,%
Batch: 240 | Loss: 2.311 | Acc: 54.078,87.730,99.870,%
Batch: 260 | Loss: 2.312 | Acc: 54.053,87.802,99.862,%
Batch: 280 | Loss: 2.315 | Acc: 53.931,87.778,99.867,%
Batch: 300 | Loss: 2.307 | Acc: 54.052,87.835,99.870,%
Batch: 320 | Loss: 2.304 | Acc: 54.133,87.802,99.878,%
Batch: 340 | Loss: 2.313 | Acc: 53.893,87.789,99.881,%
Batch: 360 | Loss: 2.318 | Acc: 53.930,87.755,99.881,%
Batch: 380 | Loss: 2.314 | Acc: 54.054,87.730,99.883,%
Batch: 0 | Loss: 3.927 | Acc: 48.438,71.094,78.906,%
Batch: 20 | Loss: 4.317 | Acc: 46.131,67.411,73.847,%
Batch: 40 | Loss: 4.334 | Acc: 45.922,66.940,73.342,%
Batch: 60 | Loss: 4.358 | Acc: 45.633,66.534,73.207,%
Train all parameters

Epoch: 277
Batch: 0 | Loss: 2.790 | Acc: 47.656,88.281,100.000,%
Batch: 20 | Loss: 2.371 | Acc: 52.493,88.281,99.888,%
Batch: 40 | Loss: 2.375 | Acc: 52.706,87.881,99.905,%
Batch: 60 | Loss: 2.360 | Acc: 53.227,87.590,99.885,%
Batch: 80 | Loss: 2.321 | Acc: 53.520,87.818,99.884,%
Batch: 100 | Loss: 2.311 | Acc: 53.875,87.492,99.884,%
Batch: 120 | Loss: 2.315 | Acc: 53.441,87.836,99.897,%
Batch: 140 | Loss: 2.303 | Acc: 53.762,87.733,99.906,%
Batch: 160 | Loss: 2.283 | Acc: 53.950,88.043,99.918,%
Batch: 180 | Loss: 2.271 | Acc: 53.898,88.234,99.918,%
Batch: 200 | Loss: 2.277 | Acc: 53.852,88.091,99.918,%
Batch: 220 | Loss: 2.289 | Acc: 53.726,87.998,99.922,%
Batch: 240 | Loss: 2.297 | Acc: 53.747,87.895,99.909,%
Batch: 260 | Loss: 2.288 | Acc: 54.077,87.832,99.898,%
Batch: 280 | Loss: 2.303 | Acc: 54.076,87.700,99.894,%
Batch: 300 | Loss: 2.291 | Acc: 54.189,87.754,99.896,%
Batch: 320 | Loss: 2.287 | Acc: 54.257,87.739,99.895,%
Batch: 340 | Loss: 2.285 | Acc: 54.383,87.734,99.897,%
Batch: 360 | Loss: 2.283 | Acc: 54.488,87.788,99.900,%
Batch: 380 | Loss: 2.284 | Acc: 54.534,87.730,99.902,%
Batch: 0 | Loss: 3.939 | Acc: 49.219,71.875,79.688,%
Batch: 20 | Loss: 4.327 | Acc: 46.317,67.225,73.735,%
Batch: 40 | Loss: 4.345 | Acc: 46.094,66.692,73.190,%
Batch: 60 | Loss: 4.366 | Acc: 45.927,66.355,73.015,%
Train all parameters

Epoch: 278
Batch: 0 | Loss: 2.711 | Acc: 56.250,81.250,100.000,%
Batch: 20 | Loss: 2.273 | Acc: 54.576,87.612,100.000,%
Batch: 40 | Loss: 2.275 | Acc: 54.516,87.290,99.943,%
Batch: 60 | Loss: 2.304 | Acc: 54.495,87.231,99.898,%
Batch: 80 | Loss: 2.284 | Acc: 54.755,87.596,99.913,%
Batch: 100 | Loss: 2.305 | Acc: 54.262,87.809,99.923,%
Batch: 120 | Loss: 2.304 | Acc: 54.281,87.868,99.935,%
Batch: 140 | Loss: 2.308 | Acc: 54.145,87.783,99.928,%
Batch: 160 | Loss: 2.308 | Acc: 54.188,87.786,99.927,%
Batch: 180 | Loss: 2.324 | Acc: 54.070,87.759,99.927,%
Batch: 200 | Loss: 2.312 | Acc: 54.147,87.951,99.926,%
Batch: 220 | Loss: 2.294 | Acc: 54.401,87.952,99.915,%
Batch: 240 | Loss: 2.295 | Acc: 54.344,88.002,99.906,%
Batch: 260 | Loss: 2.295 | Acc: 54.307,87.934,99.907,%
Batch: 280 | Loss: 2.293 | Acc: 54.309,87.998,99.914,%
Batch: 300 | Loss: 2.284 | Acc: 54.438,87.972,99.914,%
Batch: 320 | Loss: 2.287 | Acc: 54.430,87.902,99.910,%
Batch: 340 | Loss: 2.301 | Acc: 54.266,87.837,99.911,%
Batch: 360 | Loss: 2.300 | Acc: 54.207,87.803,99.911,%
Batch: 380 | Loss: 2.298 | Acc: 54.339,87.781,99.912,%
Batch: 0 | Loss: 3.941 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.321 | Acc: 46.280,67.336,73.958,%
Batch: 40 | Loss: 4.339 | Acc: 45.922,66.825,73.476,%
Batch: 60 | Loss: 4.361 | Acc: 45.671,66.598,73.194,%
Train all parameters

Epoch: 279
Batch: 0 | Loss: 2.836 | Acc: 51.562,82.031,100.000,%
Batch: 20 | Loss: 2.284 | Acc: 54.241,87.537,99.888,%
Batch: 40 | Loss: 2.309 | Acc: 54.116,88.224,99.886,%
Batch: 60 | Loss: 2.284 | Acc: 53.906,88.730,99.885,%
Batch: 80 | Loss: 2.299 | Acc: 53.810,88.156,99.884,%
Batch: 100 | Loss: 2.318 | Acc: 53.628,88.142,99.892,%
Batch: 120 | Loss: 2.301 | Acc: 53.816,88.288,99.884,%
Batch: 140 | Loss: 2.304 | Acc: 54.067,88.220,99.889,%
Batch: 160 | Loss: 2.306 | Acc: 54.037,88.170,99.893,%
Batch: 180 | Loss: 2.311 | Acc: 54.006,88.018,99.896,%
Batch: 200 | Loss: 2.311 | Acc: 53.910,88.056,99.891,%
Batch: 220 | Loss: 2.309 | Acc: 53.977,88.108,99.894,%
Batch: 240 | Loss: 2.301 | Acc: 54.114,88.203,99.890,%
Batch: 260 | Loss: 2.293 | Acc: 54.182,88.185,99.889,%
Batch: 280 | Loss: 2.277 | Acc: 54.334,88.195,99.892,%
Batch: 300 | Loss: 2.284 | Acc: 54.122,88.177,99.896,%
Batch: 320 | Loss: 2.278 | Acc: 54.120,88.313,99.903,%
Batch: 340 | Loss: 2.284 | Acc: 54.044,88.288,99.904,%
Batch: 360 | Loss: 2.284 | Acc: 54.027,88.244,99.907,%
Batch: 380 | Loss: 2.280 | Acc: 54.117,88.216,99.908,%
Batch: 0 | Loss: 3.967 | Acc: 48.438,71.875,78.125,%
Batch: 20 | Loss: 4.328 | Acc: 46.131,67.374,73.363,%
Batch: 40 | Loss: 4.342 | Acc: 45.922,66.845,72.828,%
Batch: 60 | Loss: 4.361 | Acc: 45.735,66.714,72.759,%
Train all parameters

Epoch: 280
Batch: 0 | Loss: 2.758 | Acc: 51.562,82.031,100.000,%
Batch: 20 | Loss: 2.284 | Acc: 54.018,86.756,99.926,%
Batch: 40 | Loss: 2.271 | Acc: 54.440,87.367,99.924,%
Batch: 60 | Loss: 2.265 | Acc: 55.251,87.398,99.923,%
Batch: 80 | Loss: 2.254 | Acc: 55.324,87.182,99.904,%
Batch: 100 | Loss: 2.275 | Acc: 54.819,87.338,99.907,%
Batch: 120 | Loss: 2.291 | Acc: 54.616,87.390,99.897,%
Batch: 140 | Loss: 2.303 | Acc: 54.460,87.339,99.889,%
Batch: 160 | Loss: 2.297 | Acc: 54.382,87.684,99.893,%
Batch: 180 | Loss: 2.314 | Acc: 54.273,87.535,99.888,%
Batch: 200 | Loss: 2.309 | Acc: 54.248,87.803,99.891,%
Batch: 220 | Loss: 2.323 | Acc: 54.140,87.673,99.894,%
Batch: 240 | Loss: 2.316 | Acc: 54.286,87.568,99.900,%
Batch: 260 | Loss: 2.319 | Acc: 54.295,87.605,99.898,%
Batch: 280 | Loss: 2.317 | Acc: 54.284,87.650,99.897,%
Batch: 300 | Loss: 2.319 | Acc: 54.277,87.596,99.901,%
Batch: 320 | Loss: 2.325 | Acc: 54.303,87.558,99.905,%
Batch: 340 | Loss: 2.316 | Acc: 54.392,87.651,99.906,%
Batch: 360 | Loss: 2.318 | Acc: 54.367,87.669,99.909,%
Batch: 380 | Loss: 2.322 | Acc: 54.277,87.656,99.910,%
Batch: 0 | Loss: 3.939 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.330 | Acc: 46.205,66.815,73.735,%
Batch: 40 | Loss: 4.341 | Acc: 46.037,66.521,73.247,%
Batch: 60 | Loss: 4.361 | Acc: 45.927,66.278,72.964,%
Train all parameters

Epoch: 281
Batch: 0 | Loss: 1.936 | Acc: 54.688,91.406,100.000,%
Batch: 20 | Loss: 2.392 | Acc: 52.753,87.798,99.963,%
Batch: 40 | Loss: 2.256 | Acc: 54.402,88.624,99.924,%
Batch: 60 | Loss: 2.325 | Acc: 53.714,87.948,99.936,%
Batch: 80 | Loss: 2.329 | Acc: 53.057,88.088,99.932,%
Batch: 100 | Loss: 2.304 | Acc: 53.643,88.266,99.923,%
Batch: 120 | Loss: 2.313 | Acc: 53.771,88.004,99.935,%
Batch: 140 | Loss: 2.342 | Acc: 53.491,87.755,99.934,%
Batch: 160 | Loss: 2.335 | Acc: 53.474,87.621,99.937,%
Batch: 180 | Loss: 2.319 | Acc: 53.583,87.819,99.940,%
Batch: 200 | Loss: 2.311 | Acc: 53.677,87.900,99.934,%
Batch: 220 | Loss: 2.299 | Acc: 53.836,88.023,99.926,%
Batch: 240 | Loss: 2.305 | Acc: 53.825,87.980,99.929,%
Batch: 260 | Loss: 2.302 | Acc: 53.903,88.012,99.931,%
Batch: 280 | Loss: 2.299 | Acc: 53.945,88.112,99.925,%
Batch: 300 | Loss: 2.303 | Acc: 54.007,88.053,99.930,%
Batch: 320 | Loss: 2.305 | Acc: 54.011,87.975,99.932,%
Batch: 340 | Loss: 2.299 | Acc: 54.046,87.961,99.922,%
Batch: 360 | Loss: 2.305 | Acc: 54.064,87.827,99.920,%
Batch: 380 | Loss: 2.304 | Acc: 54.078,87.818,99.922,%
Batch: 0 | Loss: 3.947 | Acc: 49.219,72.656,78.906,%
Batch: 20 | Loss: 4.323 | Acc: 46.280,67.113,73.475,%
Batch: 40 | Loss: 4.339 | Acc: 45.903,66.673,73.018,%
Batch: 60 | Loss: 4.362 | Acc: 45.620,66.586,72.887,%
Train all parameters

Epoch: 282
Batch: 0 | Loss: 3.035 | Acc: 49.219,82.031,99.219,%
Batch: 20 | Loss: 2.268 | Acc: 55.580,87.165,99.777,%
Batch: 40 | Loss: 2.347 | Acc: 54.573,87.481,99.886,%
Batch: 60 | Loss: 2.335 | Acc: 54.316,87.628,99.910,%
Batch: 80 | Loss: 2.256 | Acc: 55.285,88.117,99.923,%
Batch: 100 | Loss: 2.299 | Acc: 54.718,87.717,99.915,%
Batch: 120 | Loss: 2.308 | Acc: 54.481,87.752,99.916,%
Batch: 140 | Loss: 2.311 | Acc: 54.410,88.015,99.922,%
Batch: 160 | Loss: 2.330 | Acc: 53.960,87.874,99.913,%
Batch: 180 | Loss: 2.342 | Acc: 53.820,87.824,99.896,%
Batch: 200 | Loss: 2.333 | Acc: 53.871,87.842,99.891,%
Batch: 220 | Loss: 2.340 | Acc: 53.733,87.825,99.894,%
Batch: 240 | Loss: 2.342 | Acc: 53.725,87.850,99.887,%
Batch: 260 | Loss: 2.341 | Acc: 53.807,87.760,99.883,%
Batch: 280 | Loss: 2.338 | Acc: 53.862,87.811,99.886,%
Batch: 300 | Loss: 2.341 | Acc: 53.860,87.728,99.888,%
Batch: 320 | Loss: 2.345 | Acc: 53.707,87.683,99.893,%
Batch: 340 | Loss: 2.345 | Acc: 53.689,87.697,99.888,%
Batch: 360 | Loss: 2.344 | Acc: 53.716,87.634,99.892,%
Batch: 380 | Loss: 2.343 | Acc: 53.755,87.600,99.889,%
Batch: 0 | Loss: 3.979 | Acc: 50.781,71.875,78.125,%
Batch: 20 | Loss: 4.330 | Acc: 46.019,67.336,73.698,%
Batch: 40 | Loss: 4.348 | Acc: 45.865,66.787,73.304,%
Batch: 60 | Loss: 4.369 | Acc: 45.684,66.509,73.194,%
Train all parameters

Epoch: 283
Batch: 0 | Loss: 2.011 | Acc: 56.250,88.281,100.000,%
Batch: 20 | Loss: 2.383 | Acc: 52.381,87.314,99.963,%
Batch: 40 | Loss: 2.293 | Acc: 53.296,88.605,99.981,%
Batch: 60 | Loss: 2.246 | Acc: 54.290,88.832,99.923,%
Batch: 80 | Loss: 2.237 | Acc: 54.909,88.194,99.932,%
Batch: 100 | Loss: 2.253 | Acc: 54.950,87.879,99.930,%
Batch: 120 | Loss: 2.262 | Acc: 54.752,87.958,99.942,%
Batch: 140 | Loss: 2.282 | Acc: 54.449,87.866,99.928,%
Batch: 160 | Loss: 2.293 | Acc: 54.455,87.689,99.922,%
Batch: 180 | Loss: 2.302 | Acc: 54.290,87.711,99.927,%
Batch: 200 | Loss: 2.307 | Acc: 54.069,87.869,99.926,%
Batch: 220 | Loss: 2.307 | Acc: 54.122,87.818,99.929,%
Batch: 240 | Loss: 2.294 | Acc: 54.269,87.769,99.925,%
Batch: 260 | Loss: 2.292 | Acc: 54.221,87.916,99.916,%
Batch: 280 | Loss: 2.294 | Acc: 54.204,87.939,99.911,%
Batch: 300 | Loss: 2.294 | Acc: 54.104,88.035,99.912,%
Batch: 320 | Loss: 2.283 | Acc: 54.283,88.099,99.908,%
Batch: 340 | Loss: 2.284 | Acc: 54.328,88.050,99.908,%
Batch: 360 | Loss: 2.288 | Acc: 54.222,88.069,99.905,%
Batch: 380 | Loss: 2.290 | Acc: 54.296,88.082,99.906,%
Batch: 0 | Loss: 3.941 | Acc: 48.438,71.875,79.688,%
Batch: 20 | Loss: 4.324 | Acc: 46.131,67.299,73.549,%
Batch: 40 | Loss: 4.334 | Acc: 45.865,66.940,73.018,%
Batch: 60 | Loss: 4.358 | Acc: 45.774,66.752,72.951,%
Train all parameters

Epoch: 284
Batch: 0 | Loss: 2.184 | Acc: 59.375,85.156,100.000,%
Batch: 20 | Loss: 2.370 | Acc: 53.423,87.277,99.851,%
Batch: 40 | Loss: 2.320 | Acc: 54.287,87.614,99.924,%
Batch: 60 | Loss: 2.262 | Acc: 54.572,88.076,99.923,%
Batch: 80 | Loss: 2.288 | Acc: 54.215,88.194,99.932,%
Batch: 100 | Loss: 2.255 | Acc: 54.556,88.328,99.915,%
Batch: 120 | Loss: 2.255 | Acc: 54.791,88.275,99.910,%
Batch: 140 | Loss: 2.276 | Acc: 54.588,88.226,99.906,%
Batch: 160 | Loss: 2.284 | Acc: 54.532,88.262,99.903,%
Batch: 180 | Loss: 2.278 | Acc: 54.679,88.212,99.901,%
Batch: 200 | Loss: 2.284 | Acc: 54.742,88.032,99.899,%
Batch: 220 | Loss: 2.283 | Acc: 54.772,87.885,99.908,%
Batch: 240 | Loss: 2.277 | Acc: 54.791,87.883,99.909,%
Batch: 260 | Loss: 2.278 | Acc: 54.655,88.006,99.907,%
Batch: 280 | Loss: 2.288 | Acc: 54.596,87.875,99.903,%
Batch: 300 | Loss: 2.302 | Acc: 54.402,87.749,99.909,%
Batch: 320 | Loss: 2.297 | Acc: 54.383,87.831,99.912,%
Batch: 340 | Loss: 2.295 | Acc: 54.337,87.873,99.913,%
Batch: 360 | Loss: 2.291 | Acc: 54.365,87.909,99.911,%
Batch: 380 | Loss: 2.294 | Acc: 54.310,87.873,99.910,%
Batch: 0 | Loss: 3.963 | Acc: 49.219,72.656,79.688,%
Batch: 20 | Loss: 4.327 | Acc: 46.243,67.299,73.661,%
Batch: 40 | Loss: 4.339 | Acc: 45.865,67.035,73.152,%
Batch: 60 | Loss: 4.362 | Acc: 45.710,66.624,73.117,%
Train classifier parameters

Epoch: 285
Batch: 0 | Loss: 2.143 | Acc: 60.938,82.031,100.000,%
Batch: 20 | Loss: 2.113 | Acc: 57.254,88.504,99.926,%
Batch: 40 | Loss: 2.214 | Acc: 55.297,88.834,99.886,%
Batch: 60 | Loss: 2.276 | Acc: 55.213,88.371,99.885,%
Batch: 80 | Loss: 2.271 | Acc: 54.813,88.146,99.904,%
Batch: 100 | Loss: 2.280 | Acc: 55.105,87.925,99.915,%
Batch: 120 | Loss: 2.259 | Acc: 55.223,87.810,99.910,%
Batch: 140 | Loss: 2.274 | Acc: 54.992,87.666,99.900,%
Batch: 160 | Loss: 2.281 | Acc: 54.988,87.665,99.908,%
Batch: 180 | Loss: 2.286 | Acc: 54.778,87.625,99.909,%
Batch: 200 | Loss: 2.293 | Acc: 54.586,87.628,99.911,%
Batch: 220 | Loss: 2.286 | Acc: 54.698,87.645,99.901,%
Batch: 240 | Loss: 2.285 | Acc: 54.681,87.659,99.906,%
Batch: 260 | Loss: 2.290 | Acc: 54.667,87.599,99.901,%
Batch: 280 | Loss: 2.302 | Acc: 54.493,87.472,99.905,%
Batch: 300 | Loss: 2.297 | Acc: 54.449,87.573,99.909,%
Batch: 320 | Loss: 2.297 | Acc: 54.369,87.600,99.912,%
Batch: 340 | Loss: 2.292 | Acc: 54.413,87.640,99.908,%
Batch: 360 | Loss: 2.301 | Acc: 54.263,87.619,99.907,%
Batch: 380 | Loss: 2.301 | Acc: 54.263,87.648,99.910,%
Batch: 0 | Loss: 3.937 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.330 | Acc: 46.391,67.076,73.549,%
Batch: 40 | Loss: 4.343 | Acc: 46.094,66.711,73.095,%
Batch: 60 | Loss: 4.362 | Acc: 45.774,66.432,72.938,%
Train classifier parameters

Epoch: 286
Batch: 0 | Loss: 2.876 | Acc: 49.219,85.938,100.000,%
Batch: 20 | Loss: 2.265 | Acc: 53.311,89.137,99.851,%
Batch: 40 | Loss: 2.304 | Acc: 53.487,87.405,99.886,%
Batch: 60 | Loss: 2.302 | Acc: 53.906,87.001,99.898,%
Batch: 80 | Loss: 2.314 | Acc: 53.684,87.548,99.923,%
Batch: 100 | Loss: 2.332 | Acc: 53.806,87.500,99.923,%
Batch: 120 | Loss: 2.309 | Acc: 53.990,87.816,99.916,%
Batch: 140 | Loss: 2.301 | Acc: 54.017,87.749,99.928,%
Batch: 160 | Loss: 2.307 | Acc: 53.945,87.835,99.927,%
Batch: 180 | Loss: 2.303 | Acc: 54.010,87.953,99.931,%
Batch: 200 | Loss: 2.302 | Acc: 54.217,87.931,99.926,%
Batch: 220 | Loss: 2.298 | Acc: 54.320,87.811,99.929,%
Batch: 240 | Loss: 2.289 | Acc: 54.496,87.857,99.925,%
Batch: 260 | Loss: 2.280 | Acc: 54.631,87.781,99.913,%
Batch: 280 | Loss: 2.273 | Acc: 54.612,87.920,99.914,%
Batch: 300 | Loss: 2.270 | Acc: 54.646,87.952,99.914,%
Batch: 320 | Loss: 2.272 | Acc: 54.678,87.892,99.917,%
Batch: 340 | Loss: 2.278 | Acc: 54.584,87.851,99.918,%
Batch: 360 | Loss: 2.280 | Acc: 54.545,87.870,99.920,%
Batch: 380 | Loss: 2.286 | Acc: 54.435,87.808,99.918,%
Batch: 0 | Loss: 3.928 | Acc: 49.219,71.875,78.906,%
Batch: 20 | Loss: 4.338 | Acc: 46.354,67.299,73.438,%
Batch: 40 | Loss: 4.347 | Acc: 45.998,67.054,73.037,%
Batch: 60 | Loss: 4.365 | Acc: 45.799,66.726,72.784,%
Train classifier parameters

Epoch: 287
Batch: 0 | Loss: 3.156 | Acc: 40.625,82.812,99.219,%
Batch: 20 | Loss: 2.401 | Acc: 51.674,87.686,99.926,%
Batch: 40 | Loss: 2.388 | Acc: 52.915,87.443,99.905,%
Batch: 60 | Loss: 2.326 | Acc: 53.240,88.051,99.923,%
Batch: 80 | Loss: 2.336 | Acc: 53.106,88.059,99.932,%
Batch: 100 | Loss: 2.350 | Acc: 53.102,87.786,99.923,%
Batch: 120 | Loss: 2.365 | Acc: 53.093,87.474,99.903,%
Batch: 140 | Loss: 2.349 | Acc: 53.469,87.566,99.900,%
Batch: 160 | Loss: 2.335 | Acc: 53.712,87.529,99.903,%
Batch: 180 | Loss: 2.325 | Acc: 53.768,87.686,99.905,%
Batch: 200 | Loss: 2.333 | Acc: 53.661,87.675,99.895,%
Batch: 220 | Loss: 2.333 | Acc: 53.645,87.705,99.897,%
Batch: 240 | Loss: 2.337 | Acc: 53.683,87.678,99.896,%
Batch: 260 | Loss: 2.335 | Acc: 53.748,87.680,99.898,%
Batch: 280 | Loss: 2.335 | Acc: 53.851,87.672,99.900,%
Batch: 300 | Loss: 2.319 | Acc: 54.049,87.809,99.907,%
Batch: 320 | Loss: 2.316 | Acc: 54.011,87.872,99.903,%
Batch: 340 | Loss: 2.311 | Acc: 54.103,87.834,99.904,%
Batch: 360 | Loss: 2.313 | Acc: 54.101,87.779,99.903,%
Batch: 380 | Loss: 2.317 | Acc: 54.113,87.760,99.900,%
Batch: 0 | Loss: 3.977 | Acc: 48.438,71.094,78.125,%
Batch: 20 | Loss: 4.327 | Acc: 46.280,67.225,73.512,%
Batch: 40 | Loss: 4.343 | Acc: 45.884,66.940,73.056,%
Batch: 60 | Loss: 4.364 | Acc: 45.761,66.624,72.925,%
Train classifier parameters

Epoch: 288
Batch: 0 | Loss: 1.787 | Acc: 57.031,93.750,99.219,%
Batch: 20 | Loss: 2.259 | Acc: 54.650,87.388,99.702,%
Batch: 40 | Loss: 2.340 | Acc: 53.982,87.195,99.733,%
Batch: 60 | Loss: 2.327 | Acc: 53.420,87.372,99.821,%
Batch: 80 | Loss: 2.322 | Acc: 53.376,87.519,99.836,%
Batch: 100 | Loss: 2.339 | Acc: 53.233,87.894,99.861,%
Batch: 120 | Loss: 2.354 | Acc: 53.183,87.842,99.871,%
Batch: 140 | Loss: 2.326 | Acc: 53.651,87.832,99.878,%
Batch: 160 | Loss: 2.298 | Acc: 54.110,87.723,99.888,%
Batch: 180 | Loss: 2.326 | Acc: 53.902,87.586,99.883,%
Batch: 200 | Loss: 2.320 | Acc: 53.945,87.593,99.880,%
Batch: 220 | Loss: 2.323 | Acc: 53.995,87.613,99.887,%
Batch: 240 | Loss: 2.311 | Acc: 54.146,87.672,99.887,%
Batch: 260 | Loss: 2.317 | Acc: 54.074,87.620,99.892,%
Batch: 280 | Loss: 2.313 | Acc: 54.220,87.578,99.897,%
Batch: 300 | Loss: 2.319 | Acc: 54.259,87.555,99.896,%
Batch: 320 | Loss: 2.317 | Acc: 54.213,87.678,99.893,%
Batch: 340 | Loss: 2.309 | Acc: 54.202,87.807,99.895,%
Batch: 360 | Loss: 2.309 | Acc: 54.240,87.786,99.894,%
Batch: 380 | Loss: 2.310 | Acc: 54.224,87.777,99.897,%
Batch: 0 | Loss: 3.948 | Acc: 50.000,71.094,78.125,%
Batch: 20 | Loss: 4.327 | Acc: 46.280,67.188,73.400,%
Batch: 40 | Loss: 4.346 | Acc: 45.960,66.978,73.075,%
Batch: 60 | Loss: 4.369 | Acc: 45.697,66.586,72.874,%
Train classifier parameters

Epoch: 289
Batch: 0 | Loss: 1.657 | Acc: 61.719,89.844,100.000,%
Batch: 20 | Loss: 2.379 | Acc: 54.539,87.686,99.926,%
Batch: 40 | Loss: 2.384 | Acc: 54.230,87.519,99.943,%
Batch: 60 | Loss: 2.265 | Acc: 54.995,88.307,99.949,%
Batch: 80 | Loss: 2.291 | Acc: 54.524,87.780,99.942,%
Batch: 100 | Loss: 2.262 | Acc: 54.718,87.871,99.946,%
Batch: 120 | Loss: 2.259 | Acc: 54.726,87.849,99.948,%
Batch: 140 | Loss: 2.256 | Acc: 54.549,88.032,99.945,%
Batch: 160 | Loss: 2.273 | Acc: 54.469,87.917,99.947,%
Batch: 180 | Loss: 2.271 | Acc: 54.463,88.027,99.940,%
Batch: 200 | Loss: 2.265 | Acc: 54.641,87.846,99.942,%
Batch: 220 | Loss: 2.279 | Acc: 54.426,87.829,99.940,%
Batch: 240 | Loss: 2.292 | Acc: 54.393,87.685,99.942,%
Batch: 260 | Loss: 2.287 | Acc: 54.535,87.683,99.937,%
Batch: 280 | Loss: 2.280 | Acc: 54.529,87.875,99.942,%
Batch: 300 | Loss: 2.293 | Acc: 54.303,87.830,99.945,%
Batch: 320 | Loss: 2.286 | Acc: 54.420,87.863,99.939,%
Batch: 340 | Loss: 2.293 | Acc: 54.426,87.814,99.936,%
Batch: 360 | Loss: 2.297 | Acc: 54.311,87.781,99.933,%
Batch: 380 | Loss: 2.291 | Acc: 54.306,87.812,99.936,%
Batch: 0 | Loss: 3.961 | Acc: 49.219,71.094,79.688,%
Batch: 20 | Loss: 4.326 | Acc: 46.354,67.448,73.549,%
Batch: 40 | Loss: 4.344 | Acc: 46.151,66.978,73.056,%
Batch: 60 | Loss: 4.366 | Acc: 45.774,66.598,72.810,%
Train classifier parameters

Epoch: 290
Batch: 0 | Loss: 2.427 | Acc: 49.219,91.406,100.000,%
Batch: 20 | Loss: 2.290 | Acc: 52.269,89.323,99.851,%
Batch: 40 | Loss: 2.302 | Acc: 52.458,88.700,99.848,%
Batch: 60 | Loss: 2.292 | Acc: 52.997,88.409,99.859,%
Batch: 80 | Loss: 2.323 | Acc: 53.202,88.002,99.884,%
Batch: 100 | Loss: 2.348 | Acc: 53.357,87.740,99.884,%
Batch: 120 | Loss: 2.319 | Acc: 53.713,87.862,99.897,%
Batch: 140 | Loss: 2.306 | Acc: 54.167,87.849,99.895,%
Batch: 160 | Loss: 2.322 | Acc: 53.858,87.893,99.898,%
Batch: 180 | Loss: 2.320 | Acc: 53.902,87.953,99.896,%
Batch: 200 | Loss: 2.301 | Acc: 54.136,88.122,99.899,%
Batch: 220 | Loss: 2.300 | Acc: 54.143,88.087,99.901,%
Batch: 240 | Loss: 2.307 | Acc: 53.961,88.142,99.890,%
Batch: 260 | Loss: 2.310 | Acc: 53.867,88.090,99.895,%
Batch: 280 | Loss: 2.308 | Acc: 53.928,87.970,99.900,%
Batch: 300 | Loss: 2.302 | Acc: 54.046,87.962,99.901,%
Batch: 320 | Loss: 2.306 | Acc: 53.979,87.967,99.903,%
Batch: 340 | Loss: 2.328 | Acc: 53.702,87.841,99.899,%
Batch: 360 | Loss: 2.339 | Acc: 53.573,87.805,99.896,%
Batch: 380 | Loss: 2.329 | Acc: 53.699,87.885,99.900,%
Batch: 0 | Loss: 3.949 | Acc: 49.219,71.094,78.125,%
Batch: 20 | Loss: 4.324 | Acc: 46.243,67.039,73.438,%
Batch: 40 | Loss: 4.339 | Acc: 45.770,66.749,73.056,%
Batch: 60 | Loss: 4.360 | Acc: 45.658,66.534,72.976,%
Train classifier parameters

Epoch: 291
Batch: 0 | Loss: 3.044 | Acc: 47.656,80.469,100.000,%
Batch: 20 | Loss: 2.338 | Acc: 53.795,86.644,99.926,%
Batch: 40 | Loss: 2.324 | Acc: 54.040,87.367,99.962,%
Batch: 60 | Loss: 2.362 | Acc: 53.612,87.602,99.949,%
Batch: 80 | Loss: 2.349 | Acc: 53.819,87.297,99.942,%
Batch: 100 | Loss: 2.336 | Acc: 54.146,87.438,99.938,%
Batch: 120 | Loss: 2.337 | Acc: 54.035,87.429,99.929,%
Batch: 140 | Loss: 2.324 | Acc: 54.117,87.572,99.928,%
Batch: 160 | Loss: 2.317 | Acc: 54.144,87.602,99.918,%
Batch: 180 | Loss: 2.309 | Acc: 54.239,87.638,99.918,%
Batch: 200 | Loss: 2.309 | Acc: 54.167,87.663,99.911,%
Batch: 220 | Loss: 2.317 | Acc: 54.132,87.549,99.901,%
Batch: 240 | Loss: 2.313 | Acc: 54.182,87.698,99.896,%
Batch: 260 | Loss: 2.305 | Acc: 54.161,87.883,99.904,%
Batch: 280 | Loss: 2.305 | Acc: 54.287,87.845,99.903,%
Batch: 300 | Loss: 2.297 | Acc: 54.425,87.954,99.904,%
Batch: 320 | Loss: 2.296 | Acc: 54.412,87.996,99.903,%
Batch: 340 | Loss: 2.296 | Acc: 54.394,88.050,99.901,%
Batch: 360 | Loss: 2.299 | Acc: 54.372,87.976,99.900,%
Batch: 380 | Loss: 2.302 | Acc: 54.372,87.924,99.904,%
Batch: 0 | Loss: 3.941 | Acc: 49.219,71.875,78.906,%
Batch: 20 | Loss: 4.335 | Acc: 46.689,67.150,73.735,%
Batch: 40 | Loss: 4.350 | Acc: 46.189,66.825,73.247,%
Batch: 60 | Loss: 4.369 | Acc: 45.953,66.547,73.040,%
Train classifier parameters

Epoch: 292
Batch: 0 | Loss: 2.129 | Acc: 49.219,90.625,99.219,%
Batch: 20 | Loss: 2.269 | Acc: 54.278,88.393,99.777,%
Batch: 40 | Loss: 2.305 | Acc: 53.792,87.614,99.829,%
Batch: 60 | Loss: 2.288 | Acc: 54.175,87.820,99.821,%
Batch: 80 | Loss: 2.327 | Acc: 53.897,87.519,99.865,%
Batch: 100 | Loss: 2.308 | Acc: 54.169,87.778,99.869,%
Batch: 120 | Loss: 2.306 | Acc: 54.384,87.803,99.871,%
Batch: 140 | Loss: 2.306 | Acc: 54.433,87.755,99.861,%
Batch: 160 | Loss: 2.304 | Acc: 54.411,87.650,99.859,%
Batch: 180 | Loss: 2.280 | Acc: 54.545,87.789,99.858,%
Batch: 200 | Loss: 2.288 | Acc: 54.454,87.846,99.864,%
Batch: 220 | Loss: 2.306 | Acc: 54.157,87.829,99.873,%
Batch: 240 | Loss: 2.305 | Acc: 54.114,87.740,99.877,%
Batch: 260 | Loss: 2.311 | Acc: 54.050,87.736,99.880,%
Batch: 280 | Loss: 2.303 | Acc: 54.251,87.784,99.883,%
Batch: 300 | Loss: 2.306 | Acc: 54.161,87.801,99.891,%
Batch: 320 | Loss: 2.313 | Acc: 54.145,87.814,99.895,%
Batch: 340 | Loss: 2.306 | Acc: 54.177,87.871,99.897,%
Batch: 360 | Loss: 2.299 | Acc: 54.330,87.896,99.896,%
Batch: 380 | Loss: 2.301 | Acc: 54.284,87.865,99.895,%
Batch: 0 | Loss: 3.940 | Acc: 49.219,71.875,78.906,%
Batch: 20 | Loss: 4.329 | Acc: 45.945,67.262,73.251,%
Batch: 40 | Loss: 4.346 | Acc: 45.808,66.845,73.037,%
Batch: 60 | Loss: 4.370 | Acc: 45.620,66.483,72.861,%
Train classifier parameters

Epoch: 293
Batch: 0 | Loss: 2.233 | Acc: 56.250,93.750,100.000,%
Batch: 20 | Loss: 2.345 | Acc: 52.418,89.100,99.926,%
Batch: 40 | Loss: 2.349 | Acc: 53.430,87.462,99.924,%
Batch: 60 | Loss: 2.339 | Acc: 53.791,87.718,99.910,%
Batch: 80 | Loss: 2.357 | Acc: 53.482,87.539,99.913,%
Batch: 100 | Loss: 2.338 | Acc: 53.581,87.631,99.930,%
Batch: 120 | Loss: 2.303 | Acc: 53.964,87.829,99.935,%
Batch: 140 | Loss: 2.321 | Acc: 54.072,87.627,99.934,%
Batch: 160 | Loss: 2.332 | Acc: 53.916,87.636,99.932,%
Batch: 180 | Loss: 2.350 | Acc: 53.755,87.457,99.914,%
Batch: 200 | Loss: 2.334 | Acc: 54.054,87.294,99.914,%
Batch: 220 | Loss: 2.333 | Acc: 54.030,87.253,99.912,%
Batch: 240 | Loss: 2.320 | Acc: 54.192,87.396,99.909,%
Batch: 260 | Loss: 2.317 | Acc: 54.334,87.323,99.904,%
Batch: 280 | Loss: 2.308 | Acc: 54.376,87.386,99.911,%
Batch: 300 | Loss: 2.314 | Acc: 54.290,87.375,99.912,%
Batch: 320 | Loss: 2.313 | Acc: 54.342,87.466,99.915,%
Batch: 340 | Loss: 2.303 | Acc: 54.440,87.610,99.911,%
Batch: 360 | Loss: 2.288 | Acc: 54.636,87.710,99.911,%
Batch: 380 | Loss: 2.295 | Acc: 54.497,87.680,99.912,%
Batch: 0 | Loss: 3.972 | Acc: 48.438,71.875,79.688,%
Batch: 20 | Loss: 4.331 | Acc: 46.280,67.374,73.772,%
Batch: 40 | Loss: 4.347 | Acc: 45.941,66.864,73.152,%
Batch: 60 | Loss: 4.368 | Acc: 45.825,66.522,72.951,%
Train classifier parameters

Epoch: 294
Batch: 0 | Loss: 2.370 | Acc: 53.906,89.062,100.000,%
Batch: 20 | Loss: 2.322 | Acc: 54.018,86.979,99.888,%
Batch: 40 | Loss: 2.277 | Acc: 55.164,87.805,99.924,%
Batch: 60 | Loss: 2.291 | Acc: 54.623,87.628,99.910,%
Batch: 80 | Loss: 2.310 | Acc: 54.572,87.336,99.913,%
Batch: 100 | Loss: 2.294 | Acc: 54.749,87.616,99.907,%
Batch: 120 | Loss: 2.280 | Acc: 54.739,87.952,99.910,%
Batch: 140 | Loss: 2.289 | Acc: 54.671,87.866,99.900,%
Batch: 160 | Loss: 2.286 | Acc: 54.852,87.830,99.898,%
Batch: 180 | Loss: 2.288 | Acc: 54.959,87.832,99.892,%
Batch: 200 | Loss: 2.281 | Acc: 54.991,87.982,99.891,%
Batch: 220 | Loss: 2.283 | Acc: 54.861,88.006,99.890,%
Batch: 240 | Loss: 2.283 | Acc: 54.769,88.200,99.890,%
Batch: 260 | Loss: 2.280 | Acc: 54.726,88.173,99.886,%
Batch: 280 | Loss: 2.285 | Acc: 54.654,88.089,99.889,%
Batch: 300 | Loss: 2.287 | Acc: 54.589,88.017,99.881,%
Batch: 320 | Loss: 2.282 | Acc: 54.585,88.070,99.886,%
Batch: 340 | Loss: 2.285 | Acc: 54.495,88.036,99.890,%
Batch: 360 | Loss: 2.283 | Acc: 54.540,88.076,99.894,%
Batch: 380 | Loss: 2.290 | Acc: 54.509,87.974,99.895,%
Batch: 0 | Loss: 3.939 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.336 | Acc: 46.317,67.076,73.772,%
Batch: 40 | Loss: 4.344 | Acc: 45.941,66.921,73.133,%
Batch: 60 | Loss: 4.363 | Acc: 45.645,66.688,73.015,%
Train classifier parameters

Epoch: 295
Batch: 0 | Loss: 2.605 | Acc: 47.656,91.406,100.000,%
Batch: 20 | Loss: 2.305 | Acc: 55.320,87.649,99.888,%
Batch: 40 | Loss: 2.315 | Acc: 54.211,87.290,99.905,%
Batch: 60 | Loss: 2.313 | Acc: 53.573,87.602,99.910,%
Batch: 80 | Loss: 2.333 | Acc: 53.492,87.847,99.904,%
Batch: 100 | Loss: 2.337 | Acc: 53.589,87.980,99.915,%
Batch: 120 | Loss: 2.321 | Acc: 53.700,88.081,99.903,%
Batch: 140 | Loss: 2.300 | Acc: 54.034,88.198,99.911,%
Batch: 160 | Loss: 2.307 | Acc: 53.897,88.228,99.922,%
Batch: 180 | Loss: 2.307 | Acc: 53.867,88.195,99.914,%
Batch: 200 | Loss: 2.324 | Acc: 53.630,88.075,99.911,%
Batch: 220 | Loss: 2.330 | Acc: 53.708,87.945,99.919,%
Batch: 240 | Loss: 2.320 | Acc: 53.760,88.019,99.916,%
Batch: 260 | Loss: 2.306 | Acc: 53.852,88.123,99.913,%
Batch: 280 | Loss: 2.294 | Acc: 54.026,88.167,99.911,%
Batch: 300 | Loss: 2.296 | Acc: 54.005,88.175,99.912,%
Batch: 320 | Loss: 2.293 | Acc: 54.108,88.125,99.910,%
Batch: 340 | Loss: 2.290 | Acc: 54.115,88.158,99.908,%
Batch: 360 | Loss: 2.289 | Acc: 54.116,88.171,99.909,%
Batch: 380 | Loss: 2.294 | Acc: 54.078,88.177,99.914,%
Batch: 0 | Loss: 3.951 | Acc: 49.219,71.094,78.906,%
Batch: 20 | Loss: 4.327 | Acc: 46.466,67.262,73.549,%
Batch: 40 | Loss: 4.342 | Acc: 46.056,66.883,73.190,%
Batch: 60 | Loss: 4.366 | Acc: 45.927,66.547,72.938,%
Train classifier parameters

Epoch: 296
Batch: 0 | Loss: 1.812 | Acc: 63.281,90.625,100.000,%
Batch: 20 | Loss: 2.215 | Acc: 56.473,86.905,99.888,%
Batch: 40 | Loss: 2.182 | Acc: 57.546,87.691,99.924,%
Batch: 60 | Loss: 2.185 | Acc: 56.288,87.987,99.898,%
Batch: 80 | Loss: 2.248 | Acc: 55.710,87.317,99.884,%
Batch: 100 | Loss: 2.242 | Acc: 55.438,87.616,99.892,%
Batch: 120 | Loss: 2.279 | Acc: 54.946,87.552,99.897,%
Batch: 140 | Loss: 2.269 | Acc: 55.164,87.555,99.900,%
Batch: 160 | Loss: 2.285 | Acc: 54.916,87.384,99.884,%
Batch: 180 | Loss: 2.282 | Acc: 54.955,87.431,99.883,%
Batch: 200 | Loss: 2.281 | Acc: 54.932,87.523,99.887,%
Batch: 220 | Loss: 2.294 | Acc: 54.624,87.581,99.890,%
Batch: 240 | Loss: 2.305 | Acc: 54.461,87.555,99.893,%
Batch: 260 | Loss: 2.309 | Acc: 54.328,87.581,99.898,%
Batch: 280 | Loss: 2.309 | Acc: 54.332,87.592,99.894,%
Batch: 300 | Loss: 2.310 | Acc: 54.347,87.622,99.894,%
Batch: 320 | Loss: 2.304 | Acc: 54.437,87.588,99.893,%
Batch: 340 | Loss: 2.306 | Acc: 54.406,87.601,99.897,%
Batch: 360 | Loss: 2.308 | Acc: 54.389,87.541,99.896,%
Batch: 380 | Loss: 2.301 | Acc: 54.359,87.607,99.900,%
Batch: 0 | Loss: 3.947 | Acc: 50.000,71.094,78.906,%
Batch: 20 | Loss: 4.337 | Acc: 45.982,66.927,73.475,%
Batch: 40 | Loss: 4.350 | Acc: 45.713,66.711,73.095,%
Batch: 60 | Loss: 4.369 | Acc: 45.517,66.483,72.912,%
Train classifier parameters

Epoch: 297
Batch: 0 | Loss: 1.777 | Acc: 55.469,92.969,100.000,%
Batch: 20 | Loss: 2.275 | Acc: 54.278,88.318,99.888,%
Batch: 40 | Loss: 2.255 | Acc: 55.202,87.881,99.924,%
Batch: 60 | Loss: 2.269 | Acc: 55.046,87.769,99.872,%
Batch: 80 | Loss: 2.245 | Acc: 55.073,88.329,99.894,%
Batch: 100 | Loss: 2.272 | Acc: 54.780,88.366,99.899,%
Batch: 120 | Loss: 2.255 | Acc: 54.939,88.346,99.910,%
Batch: 140 | Loss: 2.291 | Acc: 54.499,88.082,99.917,%
Batch: 160 | Loss: 2.308 | Acc: 54.241,87.840,99.922,%
Batch: 180 | Loss: 2.301 | Acc: 54.368,87.711,99.927,%
Batch: 200 | Loss: 2.301 | Acc: 54.349,87.815,99.930,%
Batch: 220 | Loss: 2.286 | Acc: 54.440,87.984,99.922,%
Batch: 240 | Loss: 2.279 | Acc: 54.558,87.989,99.919,%
Batch: 260 | Loss: 2.282 | Acc: 54.439,87.991,99.910,%
Batch: 280 | Loss: 2.280 | Acc: 54.446,87.900,99.905,%
Batch: 300 | Loss: 2.293 | Acc: 54.392,87.744,99.904,%
Batch: 320 | Loss: 2.297 | Acc: 54.344,87.707,99.903,%
Batch: 340 | Loss: 2.296 | Acc: 54.307,87.780,99.906,%
Batch: 360 | Loss: 2.295 | Acc: 54.326,87.738,99.898,%
Batch: 380 | Loss: 2.295 | Acc: 54.407,87.730,99.904,%
Batch: 0 | Loss: 3.985 | Acc: 48.438,71.875,78.125,%
Batch: 20 | Loss: 4.334 | Acc: 46.168,67.076,73.624,%
Batch: 40 | Loss: 4.351 | Acc: 45.808,66.768,73.209,%
Batch: 60 | Loss: 4.374 | Acc: 45.645,66.534,72.976,%
Train classifier parameters

Epoch: 298
Batch: 0 | Loss: 1.931 | Acc: 64.844,85.938,100.000,%
Batch: 20 | Loss: 2.294 | Acc: 57.031,86.272,99.963,%
Batch: 40 | Loss: 2.296 | Acc: 55.602,87.386,99.943,%
Batch: 60 | Loss: 2.278 | Acc: 55.046,87.526,99.923,%
Batch: 80 | Loss: 2.281 | Acc: 54.736,87.760,99.913,%
Batch: 100 | Loss: 2.260 | Acc: 54.896,88.088,99.923,%
Batch: 120 | Loss: 2.257 | Acc: 55.062,88.197,99.929,%
Batch: 140 | Loss: 2.263 | Acc: 55.175,87.943,99.939,%
Batch: 160 | Loss: 2.287 | Acc: 54.848,87.806,99.932,%
Batch: 180 | Loss: 2.283 | Acc: 54.843,87.888,99.931,%
Batch: 200 | Loss: 2.287 | Acc: 54.835,87.842,99.930,%
Batch: 220 | Loss: 2.287 | Acc: 54.751,87.822,99.926,%
Batch: 240 | Loss: 2.283 | Acc: 54.752,87.886,99.925,%
Batch: 260 | Loss: 2.285 | Acc: 54.673,87.862,99.916,%
Batch: 280 | Loss: 2.299 | Acc: 54.624,87.792,99.914,%
Batch: 300 | Loss: 2.303 | Acc: 54.563,87.739,99.917,%
Batch: 320 | Loss: 2.304 | Acc: 54.602,87.743,99.922,%
Batch: 340 | Loss: 2.299 | Acc: 54.545,87.839,99.924,%
Batch: 360 | Loss: 2.303 | Acc: 54.421,87.887,99.924,%
Batch: 380 | Loss: 2.304 | Acc: 54.407,87.842,99.924,%
Batch: 0 | Loss: 3.968 | Acc: 49.219,71.875,79.688,%
Batch: 20 | Loss: 4.322 | Acc: 46.466,67.299,73.847,%
Batch: 40 | Loss: 4.339 | Acc: 46.151,66.825,73.171,%
Batch: 60 | Loss: 4.361 | Acc: 45.940,66.586,73.143,%
Train classifier parameters

Epoch: 299
Batch: 0 | Loss: 2.699 | Acc: 57.031,87.500,100.000,%
Batch: 20 | Loss: 2.439 | Acc: 51.786,87.054,100.000,%
Batch: 40 | Loss: 2.394 | Acc: 52.706,87.481,99.943,%
Batch: 60 | Loss: 2.357 | Acc: 53.727,87.423,99.949,%
Batch: 80 | Loss: 2.400 | Acc: 53.270,87.114,99.952,%
Batch: 100 | Loss: 2.386 | Acc: 53.659,87.036,99.946,%
Batch: 120 | Loss: 2.364 | Acc: 54.055,87.158,99.942,%
Batch: 140 | Loss: 2.328 | Acc: 54.438,87.361,99.945,%
Batch: 160 | Loss: 2.337 | Acc: 54.202,87.490,99.947,%
Batch: 180 | Loss: 2.346 | Acc: 54.044,87.366,99.935,%
Batch: 200 | Loss: 2.349 | Acc: 53.992,87.317,99.938,%
Batch: 220 | Loss: 2.349 | Acc: 54.178,87.348,99.936,%
Batch: 240 | Loss: 2.358 | Acc: 54.101,87.412,99.919,%
Batch: 260 | Loss: 2.359 | Acc: 54.167,87.371,99.922,%
Batch: 280 | Loss: 2.358 | Acc: 54.087,87.372,99.914,%
Batch: 300 | Loss: 2.350 | Acc: 54.176,87.362,99.917,%
Batch: 320 | Loss: 2.349 | Acc: 54.128,87.364,99.915,%
Batch: 340 | Loss: 2.348 | Acc: 54.133,87.390,99.913,%
Batch: 360 | Loss: 2.345 | Acc: 54.209,87.398,99.911,%
Batch: 380 | Loss: 2.352 | Acc: 54.078,87.365,99.912,%
Batch: 0 | Loss: 3.971 | Acc: 48.438,71.094,78.906,%
Batch: 20 | Loss: 4.322 | Acc: 46.577,67.225,73.847,%
Batch: 40 | Loss: 4.336 | Acc: 46.151,66.883,73.247,%
Batch: 60 | Loss: 4.359 | Acc: 45.889,66.586,73.169,%
