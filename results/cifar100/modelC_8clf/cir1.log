
Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=0 | lr=1.0e-02 | circles=1 
Training: Epoch=0 | Loss: 31.532 |  Acc: 4.692,6.178,8.060,9.828,11.674,12.330,12.850,13.280,% 
Testing: Epoch=0 | Loss: 30.837 |  Acc: 6.160,8.280,8.930,10.830,13.000,13.110,15.710,15.590,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=1 | lr=1.0e-02 | circles=1 
Training: Epoch=1 | Loss: 27.028 |  Acc: 7.628,10.132,14.886,18.212,21.842,24.002,25.568,26.126,% 
Testing: Epoch=1 | Loss: 26.458 |  Acc: 7.870,12.060,16.600,19.860,23.290,25.110,27.620,28.030,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=2 | lr=1.0e-02 | circles=1 
Training: Epoch=2 | Loss: 24.549 |  Acc: 9.572,12.298,19.228,23.182,29.028,31.812,33.924,34.786,% 
Testing: Epoch=2 | Loss: 23.963 |  Acc: 9.400,12.180,19.030,24.110,30.210,34.260,37.160,37.330,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=3 | lr=1.0e-02 | circles=1 
Training: Epoch=3 | Loss: 22.808 |  Acc: 11.652,14.342,22.266,26.962,34.350,37.830,40.382,41.330,% 
Testing: Epoch=3 | Loss: 22.660 |  Acc: 10.960,12.560,22.080,28.060,35.860,38.290,40.670,41.950,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=4 | lr=1.0e-02 | circles=1 
Training: Epoch=4 | Loss: 21.477 |  Acc: 13.274,15.428,24.412,29.996,39.358,42.702,45.590,46.732,% 
Testing: Epoch=4 | Loss: 22.009 |  Acc: 12.480,14.290,21.820,27.430,38.720,42.400,45.980,46.580,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=5 | lr=1.0e-02 | circles=1 
Training: Epoch=5 | Loss: 20.475 |  Acc: 14.486,16.720,26.452,32.388,42.804,46.266,49.200,50.334,% 
Testing: Epoch=5 | Loss: 21.243 |  Acc: 12.060,14.020,24.590,30.240,42.070,46.290,48.620,49.880,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=6 | lr=1.0e-02 | circles=1 
Training: Epoch=6 | Loss: 19.554 |  Acc: 15.244,17.530,28.202,35.012,46.110,50.120,53.178,53.934,% 
Testing: Epoch=6 | Loss: 20.756 |  Acc: 14.010,15.860,25.600,32.160,43.260,47.110,50.370,51.060,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=7 | lr=1.0e-02 | circles=1 
Training: Epoch=7 | Loss: 18.862 |  Acc: 15.872,18.172,29.270,36.622,48.406,52.712,55.916,56.892,% 
Testing: Epoch=7 | Loss: 19.768 |  Acc: 14.870,16.880,26.670,34.990,47.510,50.840,53.180,54.140,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=8 | lr=1.0e-02 | circles=1 
Training: Epoch=8 | Loss: 18.274 |  Acc: 16.740,19.168,30.856,38.536,50.476,55.072,58.286,59.296,% 
Testing: Epoch=8 | Loss: 19.735 |  Acc: 15.640,16.870,27.670,35.040,45.750,49.710,52.940,53.970,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=9 | lr=1.0e-02 | circles=1 
Training: Epoch=9 | Loss: 17.704 |  Acc: 17.170,19.564,31.856,40.228,52.610,57.448,60.982,61.918,% 
Testing: Epoch=9 | Loss: 18.835 |  Acc: 15.840,19.400,30.740,38.490,49.310,53.650,55.930,56.710,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=10 | lr=1.0e-02 | circles=1 
Training: Epoch=10 | Loss: 17.232 |  Acc: 17.754,20.318,32.806,41.662,53.948,59.416,62.764,63.780,% 
Testing: Epoch=10 | Loss: 20.131 |  Acc: 16.810,17.830,27.010,32.710,45.280,50.230,53.180,53.940,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=11 | lr=1.0e-02 | circles=1 
Training: Epoch=11 | Loss: 16.885 |  Acc: 18.078,20.752,33.512,42.136,55.174,60.556,64.498,65.280,% 
Testing: Epoch=11 | Loss: 18.840 |  Acc: 15.110,17.290,30.160,39.200,50.140,54.200,57.420,57.700,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=12 | lr=1.0e-02 | circles=1 
Training: Epoch=12 | Loss: 16.493 |  Acc: 18.452,21.356,34.490,43.258,56.612,62.328,65.852,66.852,% 
Testing: Epoch=12 | Loss: 19.014 |  Acc: 17.080,18.010,30.440,37.290,48.770,53.040,56.920,57.400,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=13 | lr=1.0e-02 | circles=1 
Training: Epoch=13 | Loss: 16.111 |  Acc: 18.910,21.786,35.410,44.756,58.000,63.668,67.542,68.718,% 
Testing: Epoch=13 | Loss: 17.578 |  Acc: 17.940,20.660,32.980,42.080,53.070,58.620,61.350,61.970,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=14 | lr=1.0e-02 | circles=1 
Training: Epoch=14 | Loss: 15.825 |  Acc: 19.388,22.190,36.028,45.070,58.942,64.958,69.028,70.156,% 
Testing: Epoch=14 | Loss: 17.477 |  Acc: 17.610,19.380,33.990,43.250,54.370,59.200,61.540,61.640,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=15 | lr=1.0e-02 | circles=1 
Training: Epoch=15 | Loss: 15.533 |  Acc: 19.758,22.548,36.790,45.968,59.798,66.006,69.966,71.094,% 
Testing: Epoch=15 | Loss: 17.005 |  Acc: 19.020,22.990,34.860,44.250,55.460,60.510,63.320,63.540,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=16 | lr=1.0e-02 | circles=1 
Training: Epoch=16 | Loss: 15.341 |  Acc: 19.932,22.908,37.084,46.690,60.312,66.922,70.834,72.202,% 
Testing: Epoch=16 | Loss: 17.610 |  Acc: 17.940,19.540,34.470,44.470,54.670,59.280,61.470,61.590,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=17 | lr=1.0e-02 | circles=1 
Training: Epoch=17 | Loss: 15.094 |  Acc: 20.282,23.120,37.680,47.390,61.400,68.108,72.402,73.682,% 
Testing: Epoch=17 | Loss: 18.047 |  Acc: 16.140,18.170,32.110,42.000,54.850,59.010,61.440,61.740,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=18 | lr=1.0e-02 | circles=1 
Training: Epoch=18 | Loss: 14.868 |  Acc: 20.618,23.784,38.104,47.742,62.002,68.806,73.314,74.686,% 
Testing: Epoch=18 | Loss: 18.507 |  Acc: 18.150,19.940,32.340,41.610,53.220,57.020,59.330,59.530,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=19 | lr=1.0e-02 | circles=1 
Training: Epoch=19 | Loss: 14.698 |  Acc: 20.448,23.830,38.592,48.398,62.656,69.528,73.746,75.140,% 
Testing: Epoch=19 | Loss: 16.933 |  Acc: 19.230,22.580,34.370,45.900,58.240,61.880,62.910,63.700,% 

Training Setting: backend=modelC | dataset=cifar100 | batch_size=128 | epoch=20 | lr=1.0e-02 | circles=1 



==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=64, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=64, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (1): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=164, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=164, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (2): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (3): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=228, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=228, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (4): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (5): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=356, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=356, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (6): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
    (7): ClassifierModule(
      (relu): ReLU(inplace=True)
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=612, out_features=100, bias=True)
      (linear_bw): Linear(in_features=100, out_features=612, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x64x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x128x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x256x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x512x1x1])
      (relu): ReLU(inplace=True)
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Batch: 0 | Loss: 38.055 | Acc: 1.562,2.344,0.781,2.344,1.562,0.781,0.781,0.000,%
Batch: 20 | Loss: 36.361 | Acc: 1.414,2.083,2.307,3.162,3.311,4.055,4.204,4.501,%
Batch: 40 | Loss: 35.573 | Acc: 1.886,2.572,3.487,3.697,4.230,4.516,4.954,5.011,%
Batch: 60 | Loss: 35.044 | Acc: 2.190,3.099,3.881,4.393,5.251,5.482,6.032,5.968,%
Batch: 80 | Loss: 34.651 | Acc: 2.566,3.665,4.485,4.900,5.961,6.211,6.655,6.588,%
Batch: 100 | Loss: 34.322 | Acc: 2.978,3.798,4.819,5.299,6.528,6.745,7.209,7.225,%
Batch: 120 | Loss: 34.006 | Acc: 3.119,4.177,5.107,5.746,6.954,7.231,7.690,7.580,%
Batch: 140 | Loss: 33.699 | Acc: 3.385,4.433,5.530,6.300,7.425,7.801,8.150,8.128,%
Batch: 160 | Loss: 33.509 | Acc: 3.479,4.639,5.789,6.721,7.827,8.162,8.531,8.526,%
Batch: 180 | Loss: 33.305 | Acc: 3.561,4.739,6.004,7.005,8.218,8.464,8.818,8.879,%
Batch: 200 | Loss: 33.081 | Acc: 3.794,4.995,6.250,7.389,8.683,8.924,9.231,9.344,%
Batch: 220 | Loss: 32.883 | Acc: 3.892,5.136,6.441,7.770,9.106,9.354,9.679,9.926,%
Batch: 240 | Loss: 32.705 | Acc: 3.981,5.206,6.613,8.013,9.450,9.693,9.991,10.305,%
Batch: 260 | Loss: 32.517 | Acc: 4.107,5.376,6.855,8.288,9.830,10.120,10.426,10.854,%
Batch: 280 | Loss: 32.360 | Acc: 4.254,5.438,7.034,8.541,10.087,10.490,10.782,11.235,%
Batch: 300 | Loss: 32.208 | Acc: 4.311,5.580,7.254,8.796,10.395,10.826,11.156,11.628,%
Batch: 320 | Loss: 32.058 | Acc: 4.439,5.739,7.486,8.981,10.687,11.118,11.519,12.033,%
Batch: 340 | Loss: 31.901 | Acc: 4.495,5.833,7.620,9.201,10.972,11.446,11.909,12.349,%
Batch: 360 | Loss: 31.759 | Acc: 4.568,5.962,7.761,9.420,11.217,11.786,12.273,12.719,%
Batch: 380 | Loss: 31.599 | Acc: 4.661,6.129,7.968,9.736,11.547,12.184,12.707,13.148,%
Batch: 0 | Loss: 31.672 | Acc: 10.938,10.938,11.719,13.281,14.844,14.062,15.625,14.062,%
Batch: 20 | Loss: 31.072 | Acc: 6.362,8.705,9.598,10.528,13.170,13.281,15.737,15.513,%
Batch: 40 | Loss: 30.978 | Acc: 6.021,8.708,9.089,10.842,13.034,13.167,15.854,15.796,%
Batch: 60 | Loss: 30.958 | Acc: 6.058,8.350,8.799,10.745,12.846,13.115,15.523,15.599,%

Epoch: 1
Batch: 0 | Loss: 28.164 | Acc: 10.156,11.719,15.625,15.625,16.406,24.219,25.781,21.875,%
Batch: 20 | Loss: 28.565 | Acc: 6.287,8.408,11.644,14.658,18.080,19.866,20.871,21.280,%
Batch: 40 | Loss: 28.259 | Acc: 6.631,8.765,12.157,15.091,18.159,20.084,21.303,21.665,%
Batch: 60 | Loss: 28.234 | Acc: 6.801,9.106,12.654,15.548,18.391,20.108,21.657,21.888,%
Batch: 80 | Loss: 28.126 | Acc: 6.925,9.327,12.982,15.876,18.740,20.611,22.280,22.502,%
Batch: 100 | Loss: 28.047 | Acc: 6.985,9.282,13.173,16.066,19.183,20.955,22.517,22.726,%
Batch: 120 | Loss: 27.993 | Acc: 7.051,9.310,13.191,16.174,19.305,21.145,22.592,22.915,%
Batch: 140 | Loss: 27.909 | Acc: 7.081,9.386,13.265,16.301,19.454,21.459,22.867,23.221,%
Batch: 160 | Loss: 27.843 | Acc: 7.041,9.380,13.485,16.557,19.546,21.661,22.899,23.404,%
Batch: 180 | Loss: 27.758 | Acc: 7.087,9.500,13.661,16.743,19.846,21.849,23.170,23.645,%
Batch: 200 | Loss: 27.653 | Acc: 7.175,9.600,13.845,17.016,20.145,22.201,23.601,24.013,%
Batch: 220 | Loss: 27.568 | Acc: 7.296,9.619,13.935,17.078,20.284,22.441,23.858,24.297,%
Batch: 240 | Loss: 27.513 | Acc: 7.333,9.641,14.017,17.226,20.481,22.608,24.073,24.582,%
Batch: 260 | Loss: 27.446 | Acc: 7.331,9.695,14.062,17.304,20.669,22.782,24.237,24.761,%
Batch: 280 | Loss: 27.375 | Acc: 7.334,9.720,14.160,17.466,20.888,22.954,24.419,25.003,%
Batch: 300 | Loss: 27.314 | Acc: 7.389,9.798,14.335,17.582,21.013,23.108,24.642,25.213,%
Batch: 320 | Loss: 27.235 | Acc: 7.438,9.891,14.518,17.781,21.215,23.279,24.864,25.387,%
Batch: 340 | Loss: 27.194 | Acc: 7.471,9.897,14.573,17.834,21.298,23.433,24.940,25.486,%
Batch: 360 | Loss: 27.137 | Acc: 7.497,10.009,14.690,17.964,21.501,23.671,25.201,25.766,%
Batch: 380 | Loss: 27.064 | Acc: 7.595,10.103,14.792,18.149,21.734,23.897,25.451,26.005,%
Batch: 0 | Loss: 25.607 | Acc: 11.719,14.844,20.312,22.656,26.562,25.781,26.562,30.469,%
Batch: 20 | Loss: 26.481 | Acc: 8.557,12.984,17.299,19.680,23.698,26.339,28.609,28.906,%
Batch: 40 | Loss: 26.377 | Acc: 7.736,12.309,17.092,20.065,23.990,26.258,28.659,28.906,%
Batch: 60 | Loss: 26.498 | Acc: 7.838,12.039,16.496,19.813,23.079,24.898,27.677,27.946,%

Epoch: 2
Batch: 0 | Loss: 24.264 | Acc: 11.719,12.500,21.875,25.781,28.906,33.594,38.281,39.062,%
Batch: 20 | Loss: 25.345 | Acc: 8.780,11.756,17.820,21.726,26.525,29.353,30.804,32.403,%
Batch: 40 | Loss: 25.308 | Acc: 8.861,11.623,18.483,22.046,27.287,29.573,31.250,32.603,%
Batch: 60 | Loss: 25.354 | Acc: 8.811,11.834,18.084,21.798,26.806,29.380,31.199,32.082,%
Batch: 80 | Loss: 25.365 | Acc: 8.690,11.651,17.978,21.663,26.881,29.572,31.318,31.983,%
Batch: 100 | Loss: 25.319 | Acc: 8.818,11.765,18.069,21.643,27.034,29.711,31.397,32.039,%
Batch: 120 | Loss: 25.295 | Acc: 8.800,11.757,18.014,21.649,26.872,29.565,31.347,31.960,%
Batch: 140 | Loss: 25.218 | Acc: 9.031,11.907,18.179,21.875,27.105,29.765,31.483,32.186,%
Batch: 160 | Loss: 25.136 | Acc: 9.026,11.908,18.207,21.948,27.150,29.906,31.653,32.245,%
Batch: 180 | Loss: 25.042 | Acc: 9.090,12.055,18.353,22.069,27.283,30.003,31.811,32.554,%
Batch: 200 | Loss: 24.984 | Acc: 9.111,12.030,18.354,22.225,27.569,30.224,32.078,32.789,%
Batch: 220 | Loss: 24.915 | Acc: 9.181,12.079,18.587,22.430,27.782,30.508,32.480,33.254,%
Batch: 240 | Loss: 24.868 | Acc: 9.236,12.082,18.714,22.462,27.895,30.650,32.715,33.516,%
Batch: 260 | Loss: 24.818 | Acc: 9.306,12.054,18.765,22.572,28.134,30.906,32.884,33.767,%
Batch: 280 | Loss: 24.773 | Acc: 9.339,12.169,18.783,22.578,28.111,30.986,33.024,33.939,%
Batch: 300 | Loss: 24.724 | Acc: 9.404,12.272,18.911,22.719,28.265,31.107,33.210,34.105,%
Batch: 320 | Loss: 24.713 | Acc: 9.399,12.198,18.915,22.737,28.364,31.209,33.319,34.253,%
Batch: 340 | Loss: 24.666 | Acc: 9.407,12.191,19.004,22.798,28.567,31.404,33.523,34.432,%
Batch: 360 | Loss: 24.608 | Acc: 9.490,12.204,19.111,22.970,28.818,31.637,33.786,34.669,%
Batch: 380 | Loss: 24.566 | Acc: 9.519,12.262,19.170,23.105,28.982,31.761,33.840,34.740,%
Batch: 0 | Loss: 23.622 | Acc: 10.938,10.938,20.312,27.344,27.344,30.469,39.062,35.156,%
Batch: 20 | Loss: 23.991 | Acc: 9.301,11.830,19.531,23.586,30.134,33.891,37.016,36.496,%
Batch: 40 | Loss: 23.963 | Acc: 8.784,12.062,19.855,24.466,30.469,34.451,37.157,37.119,%
Batch: 60 | Loss: 24.011 | Acc: 9.234,11.872,19.326,24.155,30.174,34.170,37.103,37.205,%

Epoch: 3
Batch: 0 | Loss: 23.731 | Acc: 11.719,14.844,24.219,25.000,32.031,32.812,37.500,29.688,%
Batch: 20 | Loss: 23.227 | Acc: 10.305,12.984,21.391,26.228,32.701,37.202,39.918,40.327,%
Batch: 40 | Loss: 23.071 | Acc: 10.595,13.929,21.932,26.429,33.384,37.043,39.501,40.720,%
Batch: 60 | Loss: 23.149 | Acc: 10.336,13.332,21.465,26.153,33.466,36.757,39.229,40.484,%
Batch: 80 | Loss: 23.200 | Acc: 10.407,13.484,21.644,26.167,33.092,36.198,39.207,40.278,%
Batch: 100 | Loss: 23.198 | Acc: 10.504,13.335,21.589,26.060,33.052,36.378,39.171,40.145,%
Batch: 120 | Loss: 23.117 | Acc: 10.757,13.675,21.772,26.317,33.419,36.635,39.269,40.399,%
Batch: 140 | Loss: 23.081 | Acc: 10.865,13.841,21.936,26.180,33.477,36.741,39.306,40.359,%
Batch: 160 | Loss: 23.028 | Acc: 10.981,14.092,22.064,26.456,33.579,36.893,39.490,40.562,%
Batch: 180 | Loss: 23.010 | Acc: 11.119,14.127,22.022,26.588,33.637,37.094,39.598,40.664,%
Batch: 200 | Loss: 23.018 | Acc: 11.163,14.171,21.999,26.605,33.738,37.034,39.684,40.652,%
Batch: 220 | Loss: 23.027 | Acc: 11.270,14.165,21.960,26.577,33.735,37.055,39.649,40.682,%
Batch: 240 | Loss: 22.984 | Acc: 11.304,14.205,21.975,26.644,33.879,37.237,39.802,40.722,%
Batch: 260 | Loss: 22.956 | Acc: 11.342,14.194,22.085,26.787,33.959,37.422,39.993,40.852,%
Batch: 280 | Loss: 22.951 | Acc: 11.405,14.215,22.028,26.777,33.941,37.419,40.005,40.870,%
Batch: 300 | Loss: 22.919 | Acc: 11.444,14.197,22.039,26.819,33.986,37.448,40.044,40.918,%
Batch: 320 | Loss: 22.900 | Acc: 11.473,14.247,22.128,26.891,34.134,37.561,40.085,40.995,%
Batch: 340 | Loss: 22.888 | Acc: 11.478,14.202,22.081,26.826,34.093,37.585,40.153,41.074,%
Batch: 360 | Loss: 22.862 | Acc: 11.546,14.249,22.150,26.889,34.204,37.690,40.298,41.177,%
Batch: 380 | Loss: 22.824 | Acc: 11.622,14.298,22.246,26.938,34.309,37.836,40.358,41.283,%
Batch: 0 | Loss: 22.586 | Acc: 9.375,14.062,26.562,30.469,38.281,41.406,42.188,41.406,%
Batch: 20 | Loss: 22.811 | Acc: 11.049,12.351,22.693,28.906,35.826,38.728,40.737,42.522,%
Batch: 40 | Loss: 22.771 | Acc: 10.404,12.424,21.932,28.125,36.014,38.415,40.873,41.730,%
Batch: 60 | Loss: 22.704 | Acc: 10.886,12.474,22.067,27.959,35.886,38.332,40.830,41.778,%

Epoch: 4
Batch: 0 | Loss: 21.786 | Acc: 17.969,11.719,24.219,31.250,35.156,42.188,46.875,42.969,%
Batch: 20 | Loss: 21.593 | Acc: 14.137,15.476,23.735,29.725,38.728,41.667,45.350,46.652,%
Batch: 40 | Loss: 21.625 | Acc: 13.338,15.358,23.399,29.459,38.567,41.768,45.503,46.341,%
Batch: 60 | Loss: 21.637 | Acc: 13.371,15.574,23.284,29.124,38.115,41.419,45.351,46.209,%
Batch: 80 | Loss: 21.681 | Acc: 13.030,15.384,23.360,28.877,38.262,41.638,45.129,46.181,%
Batch: 100 | Loss: 21.663 | Acc: 13.157,15.377,23.376,29.138,38.328,41.476,45.181,46.187,%
Batch: 120 | Loss: 21.672 | Acc: 13.107,15.251,23.605,29.332,38.326,41.542,45.170,46.087,%
Batch: 140 | Loss: 21.702 | Acc: 13.071,15.132,23.570,29.156,38.176,41.384,44.781,45.645,%
Batch: 160 | Loss: 21.720 | Acc: 13.039,15.213,23.544,29.188,38.306,41.469,44.793,45.609,%
Batch: 180 | Loss: 21.659 | Acc: 13.109,15.465,23.778,29.441,38.549,41.704,45.010,45.925,%
Batch: 200 | Loss: 21.629 | Acc: 13.254,15.512,23.966,29.695,38.787,41.896,45.091,45.981,%
Batch: 220 | Loss: 21.637 | Acc: 13.235,15.551,24.031,29.666,38.702,41.799,44.906,45.878,%
Batch: 240 | Loss: 21.653 | Acc: 13.181,15.379,23.917,29.516,38.635,41.883,44.875,45.951,%
Batch: 260 | Loss: 21.621 | Acc: 13.173,15.260,23.931,29.541,38.718,41.984,44.917,46.022,%
Batch: 280 | Loss: 21.592 | Acc: 13.195,15.300,24.021,29.662,38.804,42.107,45.001,46.102,%
Batch: 300 | Loss: 21.576 | Acc: 13.167,15.256,24.105,29.706,38.868,42.195,45.211,46.252,%
Batch: 320 | Loss: 21.544 | Acc: 13.220,15.318,24.177,29.790,39.021,42.407,45.351,46.476,%
Batch: 340 | Loss: 21.529 | Acc: 13.229,15.348,24.230,29.921,39.193,42.515,45.429,46.582,%
Batch: 360 | Loss: 21.511 | Acc: 13.223,15.354,24.292,29.982,39.296,42.622,45.555,46.665,%
Batch: 380 | Loss: 21.479 | Acc: 13.312,15.459,24.465,30.046,39.382,42.739,45.616,46.758,%
Batch: 0 | Loss: 20.605 | Acc: 12.500,18.750,28.906,28.125,42.969,45.312,52.344,52.344,%
Batch: 20 | Loss: 22.074 | Acc: 12.351,14.546,21.205,27.195,38.728,41.778,45.833,46.317,%
Batch: 40 | Loss: 22.085 | Acc: 12.119,14.253,21.284,27.268,38.986,42.321,46.303,46.913,%
Batch: 60 | Loss: 22.072 | Acc: 12.282,14.050,21.529,27.408,38.909,42.136,45.838,46.542,%

Epoch: 5
Batch: 0 | Loss: 21.527 | Acc: 9.375,10.156,20.312,29.688,35.938,42.969,42.188,43.750,%
Batch: 20 | Loss: 20.525 | Acc: 14.249,16.369,25.409,31.622,42.634,47.321,49.182,50.744,%
Batch: 40 | Loss: 20.452 | Acc: 14.253,16.178,26.410,31.898,42.988,46.837,49.524,51.105,%
Batch: 60 | Loss: 20.534 | Acc: 14.050,16.291,26.383,31.929,42.764,46.709,49.296,50.832,%
Batch: 80 | Loss: 20.608 | Acc: 13.995,16.165,26.109,31.954,42.477,46.258,48.891,50.251,%
Batch: 100 | Loss: 20.690 | Acc: 13.970,16.120,25.549,31.799,41.979,45.808,48.677,49.961,%
Batch: 120 | Loss: 20.665 | Acc: 13.972,16.135,25.510,31.779,42.065,45.952,48.773,50.032,%
Batch: 140 | Loss: 20.611 | Acc: 14.051,16.329,25.731,32.192,42.204,45.911,48.737,50.089,%
Batch: 160 | Loss: 20.588 | Acc: 13.951,16.227,25.728,32.104,42.202,45.968,48.806,50.082,%
Batch: 180 | Loss: 20.620 | Acc: 13.907,16.139,25.829,32.044,42.183,45.887,48.766,49.987,%
Batch: 200 | Loss: 20.640 | Acc: 13.958,16.142,25.777,31.985,42.261,45.740,48.667,49.895,%
Batch: 220 | Loss: 20.640 | Acc: 14.048,16.194,25.852,31.999,42.318,45.726,48.646,49.869,%
Batch: 240 | Loss: 20.626 | Acc: 14.205,16.277,25.966,31.953,42.324,45.705,48.762,49.903,%
Batch: 260 | Loss: 20.629 | Acc: 14.299,16.379,26.042,32.058,42.244,45.678,48.620,49.835,%
Batch: 280 | Loss: 20.586 | Acc: 14.310,16.495,26.162,32.204,42.441,45.771,48.757,49.930,%
Batch: 300 | Loss: 20.570 | Acc: 14.330,16.528,26.256,32.249,42.502,45.826,48.822,49.984,%
Batch: 320 | Loss: 20.562 | Acc: 14.376,16.579,26.285,32.338,42.589,45.945,48.900,50.007,%
Batch: 340 | Loss: 20.535 | Acc: 14.406,16.571,26.315,32.347,42.721,46.091,48.983,50.131,%
Batch: 360 | Loss: 20.502 | Acc: 14.417,16.664,26.335,32.334,42.748,46.150,49.102,50.223,%
Batch: 380 | Loss: 20.487 | Acc: 14.454,16.677,26.374,32.320,42.741,46.180,49.135,50.248,%
Batch: 0 | Loss: 20.432 | Acc: 12.500,20.312,27.344,37.500,44.531,50.781,53.125,50.000,%
Batch: 20 | Loss: 21.351 | Acc: 12.500,14.100,24.665,30.171,42.374,47.024,49.665,50.670,%
Batch: 40 | Loss: 21.263 | Acc: 12.081,14.425,25.019,30.126,42.111,46.799,49.028,50.343,%
Batch: 60 | Loss: 21.308 | Acc: 12.052,14.101,24.782,30.123,41.842,46.158,48.604,49.872,%

Epoch: 6
Batch: 0 | Loss: 20.290 | Acc: 16.406,18.750,26.562,33.594,42.188,44.531,47.656,53.906,%
Batch: 20 | Loss: 19.609 | Acc: 14.621,17.225,27.530,34.189,45.461,49.479,52.567,54.018,%
Batch: 40 | Loss: 19.663 | Acc: 14.806,17.511,27.992,34.794,45.312,49.752,52.877,53.944,%
Batch: 60 | Loss: 19.600 | Acc: 14.946,17.674,28.535,35.067,45.594,50.218,53.112,54.034,%
Batch: 80 | Loss: 19.688 | Acc: 15.008,17.245,28.376,34.732,45.592,49.807,52.672,53.549,%
Batch: 100 | Loss: 19.740 | Acc: 14.851,17.327,27.978,34.390,45.398,49.513,52.452,53.334,%
Batch: 120 | Loss: 19.716 | Acc: 14.902,17.355,27.996,34.401,45.422,49.380,52.563,53.338,%
Batch: 140 | Loss: 19.733 | Acc: 15.027,17.365,27.937,34.386,45.429,49.235,52.371,53.097,%
Batch: 160 | Loss: 19.708 | Acc: 15.130,17.420,27.945,34.559,45.647,49.403,52.538,53.305,%
Batch: 180 | Loss: 19.686 | Acc: 15.120,17.507,27.883,34.647,45.766,49.599,52.642,53.410,%
Batch: 200 | Loss: 19.687 | Acc: 15.108,17.553,27.915,34.639,45.651,49.510,52.554,53.366,%
Batch: 220 | Loss: 19.676 | Acc: 15.095,17.530,27.899,34.711,45.708,49.498,52.627,53.362,%
Batch: 240 | Loss: 19.674 | Acc: 15.178,17.502,27.882,34.745,45.679,49.494,52.580,53.384,%
Batch: 260 | Loss: 19.674 | Acc: 15.176,17.472,27.981,34.818,45.797,49.587,52.727,53.538,%
Batch: 280 | Loss: 19.667 | Acc: 15.091,17.418,27.967,34.809,45.777,49.627,52.758,53.581,%
Batch: 300 | Loss: 19.654 | Acc: 15.176,17.442,27.985,34.855,45.881,49.686,52.788,53.592,%
Batch: 320 | Loss: 19.655 | Acc: 15.138,17.480,28.025,34.816,45.811,49.710,52.806,53.580,%
Batch: 340 | Loss: 19.639 | Acc: 15.178,17.495,28.079,34.854,45.833,49.769,52.873,53.535,%
Batch: 360 | Loss: 19.590 | Acc: 15.324,17.551,28.192,34.981,46.005,49.985,53.015,53.727,%
Batch: 380 | Loss: 19.563 | Acc: 15.315,17.587,28.209,34.988,46.063,50.055,53.117,53.863,%
Batch: 0 | Loss: 19.858 | Acc: 12.500,17.188,27.344,33.594,42.969,56.250,55.469,54.688,%
Batch: 20 | Loss: 20.802 | Acc: 14.249,16.109,26.637,32.143,43.304,47.805,51.079,51.562,%
Batch: 40 | Loss: 20.811 | Acc: 14.005,15.663,25.915,32.431,42.588,46.399,49.790,50.419,%
Batch: 60 | Loss: 20.777 | Acc: 13.947,15.715,25.679,32.070,42.764,46.849,49.974,50.525,%

Epoch: 7
Batch: 0 | Loss: 20.031 | Acc: 17.969,21.094,28.906,35.938,48.438,50.000,54.688,57.812,%
Batch: 20 | Loss: 18.837 | Acc: 16.369,17.820,30.952,37.128,48.289,51.786,55.692,57.180,%
Batch: 40 | Loss: 19.068 | Acc: 16.006,17.683,28.925,35.957,47.085,51.143,54.611,56.383,%
Batch: 60 | Loss: 19.008 | Acc: 16.022,17.930,29.137,36.091,47.733,51.921,54.944,56.545,%
Batch: 80 | Loss: 18.946 | Acc: 16.040,18.094,29.398,36.323,47.849,51.948,54.929,56.375,%
Batch: 100 | Loss: 18.954 | Acc: 15.996,18.139,29.293,36.139,47.795,52.081,55.198,56.474,%
Batch: 120 | Loss: 18.980 | Acc: 15.954,18.072,29.113,35.983,47.598,52.073,55.256,56.605,%
Batch: 140 | Loss: 18.986 | Acc: 15.802,17.841,28.912,35.943,47.507,51.961,55.291,56.538,%
Batch: 160 | Loss: 19.004 | Acc: 15.809,17.915,29.066,35.991,47.613,52.121,55.328,56.502,%
Batch: 180 | Loss: 18.956 | Acc: 15.836,17.969,29.195,36.296,47.799,52.296,55.525,56.729,%
Batch: 200 | Loss: 18.933 | Acc: 15.901,18.062,29.104,36.338,47.909,52.301,55.706,56.849,%
Batch: 220 | Loss: 18.944 | Acc: 15.887,18.018,29.058,36.323,47.858,52.199,55.621,56.710,%
Batch: 240 | Loss: 18.938 | Acc: 15.794,17.978,29.110,36.395,47.967,52.311,55.718,56.752,%
Batch: 260 | Loss: 18.952 | Acc: 15.781,17.912,28.981,36.378,48.060,52.332,55.747,56.837,%
Batch: 280 | Loss: 18.931 | Acc: 15.825,17.988,29.026,36.416,48.087,52.466,55.869,56.937,%
Batch: 300 | Loss: 18.929 | Acc: 15.840,18.083,29.122,36.431,48.134,52.544,55.853,56.925,%
Batch: 320 | Loss: 18.911 | Acc: 15.859,18.154,29.147,36.458,48.187,52.643,55.865,56.966,%
Batch: 340 | Loss: 18.908 | Acc: 15.840,18.125,29.158,36.480,48.270,52.685,55.870,56.949,%
Batch: 360 | Loss: 18.887 | Acc: 15.839,18.118,29.149,36.563,48.353,52.712,55.979,56.971,%
Batch: 380 | Loss: 18.870 | Acc: 15.879,18.205,29.240,36.639,48.386,52.711,55.947,56.941,%
Batch: 0 | Loss: 18.183 | Acc: 17.188,21.094,27.344,43.750,57.812,56.250,63.281,64.062,%
Batch: 20 | Loss: 19.837 | Acc: 14.844,16.592,26.376,35.789,47.582,49.851,52.865,53.609,%
Batch: 40 | Loss: 19.876 | Acc: 14.444,16.673,26.448,35.194,47.199,50.324,52.973,53.716,%
Batch: 60 | Loss: 19.881 | Acc: 14.690,16.816,26.575,35.015,47.234,50.397,52.754,53.445,%

Epoch: 8
Batch: 0 | Loss: 17.867 | Acc: 14.844,19.531,29.688,39.844,48.438,54.688,54.688,55.469,%
Batch: 20 | Loss: 18.057 | Acc: 16.778,18.862,30.990,38.504,51.004,55.543,58.891,60.342,%
Batch: 40 | Loss: 18.065 | Acc: 16.654,19.284,30.812,38.643,51.620,56.460,59.966,61.376,%
Batch: 60 | Loss: 18.153 | Acc: 16.739,19.429,30.341,38.140,51.281,56.160,59.580,61.142,%
Batch: 80 | Loss: 18.236 | Acc: 16.831,19.425,30.469,38.050,50.945,55.681,59.144,60.513,%
Batch: 100 | Loss: 18.290 | Acc: 16.847,19.261,30.585,38.188,50.603,55.546,58.702,60.149,%
Batch: 120 | Loss: 18.277 | Acc: 16.794,19.028,30.740,38.339,50.807,55.501,58.826,60.092,%
Batch: 140 | Loss: 18.254 | Acc: 16.783,18.938,30.757,38.436,50.632,55.474,58.599,60.001,%
Batch: 160 | Loss: 18.246 | Acc: 16.785,18.964,30.682,38.407,50.733,55.508,58.594,59.870,%
Batch: 180 | Loss: 18.205 | Acc: 16.972,19.156,30.918,38.657,50.872,55.551,58.719,59.949,%
Batch: 200 | Loss: 18.203 | Acc: 16.966,19.255,30.958,38.697,50.859,55.515,58.710,59.787,%
Batch: 220 | Loss: 18.201 | Acc: 16.929,19.174,30.942,38.614,50.834,55.515,58.749,59.782,%
Batch: 240 | Loss: 18.233 | Acc: 16.850,19.220,30.971,38.677,50.849,55.346,58.659,59.686,%
Batch: 260 | Loss: 18.231 | Acc: 16.828,19.253,31.028,38.682,50.799,55.301,58.576,59.561,%
Batch: 280 | Loss: 18.239 | Acc: 16.823,19.161,30.930,38.679,50.698,55.255,58.496,59.481,%
Batch: 300 | Loss: 18.241 | Acc: 16.801,19.142,31.003,38.684,50.706,55.243,58.415,59.391,%
Batch: 320 | Loss: 18.264 | Acc: 16.766,19.137,30.897,38.593,50.596,55.172,58.304,59.290,%
Batch: 340 | Loss: 18.268 | Acc: 16.812,19.156,30.906,38.597,50.564,55.143,58.333,59.334,%
Batch: 360 | Loss: 18.277 | Acc: 16.718,19.124,30.843,38.615,50.617,55.148,58.280,59.293,%
Batch: 380 | Loss: 18.279 | Acc: 16.716,19.148,30.871,38.548,50.478,55.096,58.284,59.293,%
Batch: 0 | Loss: 17.895 | Acc: 16.406,17.969,32.812,46.875,50.000,60.938,63.281,64.062,%
Batch: 20 | Loss: 19.599 | Acc: 15.811,16.890,27.902,36.049,46.280,50.335,53.274,55.357,%
Batch: 40 | Loss: 19.704 | Acc: 15.796,17.168,28.011,35.633,45.903,50.038,52.744,53.982,%
Batch: 60 | Loss: 19.772 | Acc: 15.471,16.778,27.459,35.092,45.838,49.565,52.561,53.420,%

Epoch: 9
Batch: 0 | Loss: 18.416 | Acc: 13.281,17.969,30.469,37.500,53.125,58.594,60.156,59.375,%
Batch: 20 | Loss: 17.722 | Acc: 16.964,19.606,32.478,41.183,52.790,56.808,60.900,62.463,%
Batch: 40 | Loss: 17.830 | Acc: 16.787,19.303,31.421,39.977,51.925,56.517,60.232,61.604,%
Batch: 60 | Loss: 17.774 | Acc: 17.303,19.326,31.673,39.805,51.857,56.737,60.195,61.668,%
Batch: 80 | Loss: 17.812 | Acc: 17.110,19.261,31.703,39.815,52.016,56.964,60.368,61.584,%
Batch: 100 | Loss: 17.742 | Acc: 17.149,19.670,31.853,40.045,52.498,57.225,60.651,61.696,%
Batch: 120 | Loss: 17.748 | Acc: 17.078,19.647,32.012,40.005,52.344,57.244,60.724,61.854,%
Batch: 140 | Loss: 17.786 | Acc: 16.855,19.393,31.909,39.849,52.222,57.270,60.838,61.896,%
Batch: 160 | Loss: 17.785 | Acc: 17.003,19.323,31.818,39.834,52.208,57.264,60.826,61.767,%
Batch: 180 | Loss: 17.788 | Acc: 17.002,19.238,31.854,39.865,52.227,57.200,60.782,61.766,%
Batch: 200 | Loss: 17.805 | Acc: 17.040,19.282,31.825,39.984,52.243,57.167,60.704,61.816,%
Batch: 220 | Loss: 17.779 | Acc: 17.117,19.347,31.741,40.091,52.280,57.127,60.697,61.814,%
Batch: 240 | Loss: 17.779 | Acc: 17.003,19.340,31.675,39.964,52.305,57.180,60.733,61.754,%
Batch: 260 | Loss: 17.758 | Acc: 17.041,19.403,31.678,40.023,52.380,57.283,60.827,61.782,%
Batch: 280 | Loss: 17.771 | Acc: 17.012,19.353,31.686,39.947,52.344,57.245,60.793,61.724,%
Batch: 300 | Loss: 17.756 | Acc: 17.037,19.430,31.704,39.989,52.461,57.288,60.849,61.752,%
Batch: 320 | Loss: 17.761 | Acc: 16.981,19.388,31.698,40.048,52.480,57.282,60.894,61.702,%
Batch: 340 | Loss: 17.724 | Acc: 17.029,19.456,31.795,40.171,52.552,57.315,60.933,61.808,%
Batch: 360 | Loss: 17.704 | Acc: 17.136,19.529,31.832,40.177,52.593,57.423,60.959,61.903,%
Batch: 380 | Loss: 17.707 | Acc: 17.165,19.558,31.828,40.190,52.625,57.431,60.997,61.924,%
Batch: 0 | Loss: 17.309 | Acc: 18.750,22.656,31.250,41.406,53.906,62.500,65.625,64.844,%
Batch: 20 | Loss: 18.795 | Acc: 16.071,20.461,30.469,38.542,50.670,55.022,57.626,58.185,%
Batch: 40 | Loss: 18.848 | Acc: 16.082,19.855,30.393,38.739,49.505,53.735,56.288,57.127,%
Batch: 60 | Loss: 18.900 | Acc: 15.740,19.352,30.571,38.217,49.027,53.330,55.597,56.378,%

Epoch: 10
Batch: 0 | Loss: 17.585 | Acc: 21.094,19.531,31.250,40.625,52.344,54.688,63.281,59.375,%
Batch: 20 | Loss: 16.941 | Acc: 19.048,20.461,33.445,43.713,55.134,60.789,64.658,65.885,%
Batch: 40 | Loss: 17.004 | Acc: 17.912,20.122,32.870,42.454,55.316,60.556,64.082,65.206,%
Batch: 60 | Loss: 17.077 | Acc: 18.058,20.223,32.979,42.098,55.277,60.041,63.614,64.626,%
Batch: 80 | Loss: 17.006 | Acc: 17.921,20.284,33.092,42.448,55.324,60.301,63.792,64.940,%
Batch: 100 | Loss: 17.119 | Acc: 17.628,20.142,32.843,41.894,54.595,59.715,63.335,64.488,%
Batch: 120 | Loss: 17.156 | Acc: 17.762,20.125,32.735,41.819,54.565,59.672,63.281,64.360,%
Batch: 140 | Loss: 17.159 | Acc: 17.730,20.163,32.812,41.922,54.416,59.730,63.353,64.378,%
Batch: 160 | Loss: 17.171 | Acc: 17.867,20.148,32.749,42.032,54.178,59.700,63.102,64.189,%
Batch: 180 | Loss: 17.181 | Acc: 17.878,20.192,32.601,41.855,53.945,59.612,63.040,64.153,%
Batch: 200 | Loss: 17.214 | Acc: 17.743,20.114,32.505,41.706,53.821,59.632,62.974,64.125,%
Batch: 220 | Loss: 17.232 | Acc: 17.612,20.100,32.519,41.735,53.892,59.580,62.945,63.953,%
Batch: 240 | Loss: 17.252 | Acc: 17.599,20.141,32.472,41.649,53.773,59.433,62.831,63.887,%
Batch: 260 | Loss: 17.252 | Acc: 17.675,20.127,32.489,41.700,53.807,59.405,62.778,63.868,%
Batch: 280 | Loss: 17.249 | Acc: 17.699,20.115,32.554,41.679,53.834,59.436,62.884,63.943,%
Batch: 300 | Loss: 17.264 | Acc: 17.678,20.141,32.605,41.666,53.841,59.388,62.848,63.881,%
Batch: 320 | Loss: 17.260 | Acc: 17.691,20.205,32.693,41.664,53.897,59.433,62.911,63.885,%
Batch: 340 | Loss: 17.243 | Acc: 17.744,20.342,32.831,41.656,53.968,59.435,62.906,63.836,%
Batch: 360 | Loss: 17.229 | Acc: 17.772,20.362,32.845,41.655,54.023,59.485,62.887,63.881,%
Batch: 380 | Loss: 17.238 | Acc: 17.704,20.292,32.796,41.646,53.968,59.416,62.773,63.812,%
Batch: 0 | Loss: 20.844 | Acc: 20.312,17.969,27.344,31.250,41.406,46.094,49.219,53.125,%
Batch: 20 | Loss: 20.325 | Acc: 17.188,17.783,26.860,32.775,44.754,48.884,52.530,53.497,%
Batch: 40 | Loss: 20.229 | Acc: 16.940,17.645,27.115,33.079,44.836,49.390,52.325,53.601,%
Batch: 60 | Loss: 20.195 | Acc: 16.829,17.649,26.895,32.390,44.813,49.629,52.497,53.484,%

Epoch: 11
Batch: 0 | Loss: 17.907 | Acc: 17.969,18.750,28.125,34.375,50.781,53.906,57.031,53.906,%
Batch: 20 | Loss: 17.168 | Acc: 15.960,18.564,32.292,40.699,54.650,60.789,63.430,64.918,%
Batch: 40 | Loss: 17.016 | Acc: 16.730,19.665,33.327,41.311,54.992,60.061,63.662,64.806,%
Batch: 60 | Loss: 16.951 | Acc: 17.482,20.108,33.414,41.726,55.072,59.990,64.127,65.164,%
Batch: 80 | Loss: 16.853 | Acc: 17.921,20.409,33.603,41.879,54.871,60.272,64.516,65.201,%
Batch: 100 | Loss: 16.822 | Acc: 18.007,20.398,33.408,41.770,54.889,60.481,64.720,65.447,%
Batch: 120 | Loss: 16.814 | Acc: 17.969,20.403,33.516,42.007,54.997,60.318,64.482,65.251,%
Batch: 140 | Loss: 16.823 | Acc: 18.091,20.501,33.350,42.032,54.992,60.505,64.556,65.276,%
Batch: 160 | Loss: 16.893 | Acc: 18.056,20.366,33.215,41.804,54.736,60.360,64.344,65.164,%
Batch: 180 | Loss: 16.859 | Acc: 18.090,20.477,33.330,41.924,55.059,60.571,64.589,65.435,%
Batch: 200 | Loss: 16.863 | Acc: 18.035,20.464,33.349,41.958,55.127,60.634,64.603,65.462,%
Batch: 220 | Loss: 16.815 | Acc: 18.177,20.744,33.626,42.110,55.341,60.839,64.699,65.558,%
Batch: 240 | Loss: 16.845 | Acc: 18.235,20.812,33.620,42.097,55.274,60.717,64.614,65.479,%
Batch: 260 | Loss: 16.863 | Acc: 18.181,20.860,33.576,42.080,55.157,60.563,64.476,65.329,%
Batch: 280 | Loss: 16.874 | Acc: 18.269,20.896,33.538,42.215,55.202,60.565,64.518,65.397,%
Batch: 300 | Loss: 16.857 | Acc: 18.221,20.930,33.529,42.206,55.295,60.675,64.571,65.480,%
Batch: 320 | Loss: 16.857 | Acc: 18.278,20.977,33.596,42.253,55.238,60.611,64.488,65.372,%
Batch: 340 | Loss: 16.861 | Acc: 18.218,20.945,33.546,42.185,55.226,60.658,64.544,65.384,%
Batch: 360 | Loss: 16.873 | Acc: 18.159,20.841,33.499,42.170,55.257,60.691,64.582,65.424,%
Batch: 380 | Loss: 16.882 | Acc: 18.069,20.743,33.493,42.142,55.180,60.595,64.548,65.346,%
Batch: 0 | Loss: 18.909 | Acc: 16.406,18.750,32.812,39.062,51.562,57.812,57.812,60.938,%
Batch: 20 | Loss: 19.057 | Acc: 15.513,16.667,29.948,39.286,49.405,53.720,56.882,57.738,%
Batch: 40 | Loss: 18.934 | Acc: 15.434,16.921,30.431,39.348,49.790,53.754,57.146,57.641,%
Batch: 60 | Loss: 18.950 | Acc: 15.126,17.034,30.059,39.395,50.000,53.650,57.057,57.480,%

Epoch: 12
Batch: 0 | Loss: 16.719 | Acc: 15.625,17.969,23.438,36.719,57.031,67.969,67.188,68.750,%
Batch: 20 | Loss: 16.168 | Acc: 17.783,22.359,34.635,43.341,57.775,62.984,67.374,68.341,%
Batch: 40 | Loss: 16.250 | Acc: 17.569,21.970,34.337,43.102,58.136,63.186,67.359,68.579,%
Batch: 60 | Loss: 16.261 | Acc: 17.930,21.747,34.413,43.635,57.774,63.243,67.059,68.379,%
Batch: 80 | Loss: 16.366 | Acc: 17.853,21.152,34.327,43.432,57.263,62.606,66.532,67.785,%
Batch: 100 | Loss: 16.350 | Acc: 18.038,21.380,34.746,43.502,56.907,62.461,66.290,67.713,%
Batch: 120 | Loss: 16.331 | Acc: 18.188,21.365,34.801,43.563,56.831,62.455,66.335,67.769,%
Batch: 140 | Loss: 16.344 | Acc: 18.406,21.426,34.863,43.562,56.715,62.312,66.229,67.614,%
Batch: 160 | Loss: 16.407 | Acc: 18.299,21.234,34.477,43.294,56.638,62.277,66.241,67.527,%
Batch: 180 | Loss: 16.480 | Acc: 18.232,21.016,34.284,43.116,56.608,62.181,66.091,67.369,%
Batch: 200 | Loss: 16.431 | Acc: 18.303,21.121,34.344,43.322,56.728,62.368,66.270,67.530,%
Batch: 220 | Loss: 16.440 | Acc: 18.354,21.207,34.470,43.319,56.710,62.373,66.219,67.322,%
Batch: 240 | Loss: 16.448 | Acc: 18.510,21.334,34.430,43.280,56.629,62.241,66.124,67.191,%
Batch: 260 | Loss: 16.447 | Acc: 18.540,21.399,34.462,43.358,56.723,62.398,66.158,67.188,%
Batch: 280 | Loss: 16.442 | Acc: 18.550,21.466,34.495,43.364,56.656,62.405,66.056,67.118,%
Batch: 300 | Loss: 16.472 | Acc: 18.527,21.416,34.453,43.293,56.577,62.326,65.895,66.993,%
Batch: 320 | Loss: 16.470 | Acc: 18.580,21.478,34.511,43.312,56.598,62.288,65.822,66.888,%
Batch: 340 | Loss: 16.485 | Acc: 18.512,21.401,34.460,43.285,56.566,62.280,65.774,66.858,%
Batch: 360 | Loss: 16.493 | Acc: 18.471,21.373,34.459,43.272,56.516,62.258,65.794,66.804,%
Batch: 380 | Loss: 16.488 | Acc: 18.490,21.403,34.510,43.297,56.553,62.289,65.871,66.868,%
Batch: 0 | Loss: 19.808 | Acc: 19.531,13.281,25.000,30.469,51.562,56.250,57.031,61.719,%
Batch: 20 | Loss: 19.143 | Acc: 18.415,18.452,30.394,37.351,48.996,54.055,56.585,57.515,%
Batch: 40 | Loss: 19.088 | Acc: 17.721,18.445,30.602,37.329,48.609,53.354,57.069,58.022,%
Batch: 60 | Loss: 19.115 | Acc: 17.188,18.033,30.174,37.026,48.438,52.702,56.673,57.159,%

Epoch: 13
Batch: 0 | Loss: 16.699 | Acc: 14.062,23.438,31.250,42.188,54.688,61.719,69.531,69.531,%
Batch: 20 | Loss: 16.047 | Acc: 19.345,22.210,35.454,44.754,57.626,64.323,67.857,69.792,%
Batch: 40 | Loss: 15.906 | Acc: 18.941,22.046,35.537,45.084,58.632,64.177,68.598,70.255,%
Batch: 60 | Loss: 15.948 | Acc: 18.981,22.234,35.848,45.351,58.658,64.101,68.315,70.056,%
Batch: 80 | Loss: 16.038 | Acc: 18.991,22.010,35.446,45.014,58.324,63.956,68.094,69.763,%
Batch: 100 | Loss: 16.049 | Acc: 19.013,21.952,35.497,44.949,58.323,64.171,68.038,69.732,%
Batch: 120 | Loss: 16.085 | Acc: 19.163,22.088,35.582,44.693,58.219,64.037,67.788,69.421,%
Batch: 140 | Loss: 16.029 | Acc: 19.182,22.074,35.644,44.969,58.461,64.317,67.941,69.548,%
Batch: 160 | Loss: 16.034 | Acc: 19.090,21.914,35.637,44.876,58.269,64.232,67.843,69.449,%
Batch: 180 | Loss: 16.062 | Acc: 18.936,21.940,35.458,44.820,58.071,64.075,67.805,69.346,%
Batch: 200 | Loss: 16.083 | Acc: 18.801,21.801,35.335,44.827,58.007,63.989,67.848,69.341,%
Batch: 220 | Loss: 16.108 | Acc: 18.803,21.826,35.160,44.637,57.968,63.872,67.721,69.167,%
Batch: 240 | Loss: 16.132 | Acc: 18.724,21.758,35.056,44.612,57.887,63.761,67.625,69.026,%
Batch: 260 | Loss: 16.149 | Acc: 18.702,21.764,35.105,44.582,57.863,63.649,67.499,68.903,%
Batch: 280 | Loss: 16.157 | Acc: 18.697,21.708,35.053,44.512,57.857,63.645,67.432,68.797,%
Batch: 300 | Loss: 16.145 | Acc: 18.703,21.717,35.206,44.656,57.968,63.730,67.556,68.867,%
Batch: 320 | Loss: 16.121 | Acc: 18.784,21.744,35.324,44.797,58.036,63.817,67.682,68.930,%
Batch: 340 | Loss: 16.132 | Acc: 18.805,21.767,35.220,44.698,57.966,63.694,67.595,68.876,%
Batch: 360 | Loss: 16.134 | Acc: 18.843,21.734,35.327,44.698,58.007,63.686,67.560,68.793,%
Batch: 380 | Loss: 16.122 | Acc: 18.840,21.725,35.374,44.728,58.007,63.632,67.524,68.727,%
Batch: 0 | Loss: 16.949 | Acc: 21.875,21.094,43.750,46.094,55.469,62.500,64.844,68.750,%
Batch: 20 | Loss: 17.654 | Acc: 18.118,20.796,31.882,41.592,52.976,58.966,61.086,61.942,%
Batch: 40 | Loss: 17.643 | Acc: 18.064,21.494,33.098,41.806,52.706,58.022,60.518,61.261,%
Batch: 60 | Loss: 17.670 | Acc: 17.815,20.876,32.723,42.034,52.766,58.043,60.873,61.335,%

Epoch: 14
Batch: 0 | Loss: 16.234 | Acc: 13.281,16.406,31.250,43.750,57.812,71.094,75.000,75.000,%
Batch: 20 | Loss: 15.655 | Acc: 19.940,22.135,36.347,44.754,59.003,66.034,70.424,71.429,%
Batch: 40 | Loss: 15.657 | Acc: 19.417,21.799,36.128,45.122,59.280,66.139,70.103,71.627,%
Batch: 60 | Loss: 15.603 | Acc: 19.275,21.952,36.386,45.453,59.273,65.907,70.082,71.644,%
Batch: 80 | Loss: 15.674 | Acc: 19.367,22.203,36.294,45.149,58.787,65.471,70.033,71.499,%
Batch: 100 | Loss: 15.697 | Acc: 19.384,22.215,36.301,44.964,58.849,65.254,69.740,71.241,%
Batch: 120 | Loss: 15.639 | Acc: 19.473,22.314,36.318,45.003,58.891,65.263,69.589,71.171,%
Batch: 140 | Loss: 15.693 | Acc: 19.487,22.540,36.120,44.963,58.760,65.154,69.432,70.933,%
Batch: 160 | Loss: 15.695 | Acc: 19.565,22.666,36.073,44.890,58.798,65.290,69.458,70.822,%
Batch: 180 | Loss: 15.691 | Acc: 19.544,22.648,36.184,45.300,58.961,65.414,69.631,70.891,%
Batch: 200 | Loss: 15.719 | Acc: 19.500,22.551,36.093,45.285,58.959,65.368,69.488,70.822,%
Batch: 220 | Loss: 15.731 | Acc: 19.482,22.476,36.174,45.256,59.053,65.307,69.492,70.761,%
Batch: 240 | Loss: 15.762 | Acc: 19.411,22.426,36.226,45.225,59.025,65.226,69.334,70.588,%
Batch: 260 | Loss: 15.782 | Acc: 19.403,22.420,36.174,45.133,59.040,65.215,69.316,70.621,%
Batch: 280 | Loss: 15.817 | Acc: 19.289,22.278,35.985,45.065,58.941,65.086,69.239,70.488,%
Batch: 300 | Loss: 15.835 | Acc: 19.316,22.267,35.995,45.061,58.944,65.044,69.150,70.336,%
Batch: 320 | Loss: 15.828 | Acc: 19.366,22.194,35.986,45.055,58.900,65.060,69.171,70.347,%
Batch: 340 | Loss: 15.824 | Acc: 19.433,22.207,36.027,45.090,58.894,65.061,69.172,70.287,%
Batch: 360 | Loss: 15.827 | Acc: 19.443,22.228,35.985,45.070,58.895,64.982,69.079,70.196,%
Batch: 380 | Loss: 15.828 | Acc: 19.398,22.211,36.009,45.060,58.946,64.983,69.045,70.151,%
Batch: 0 | Loss: 16.874 | Acc: 21.094,20.312,37.500,52.344,57.031,64.844,64.844,66.406,%
Batch: 20 | Loss: 17.549 | Acc: 18.452,19.866,33.408,43.490,55.022,59.338,61.942,61.830,%
Batch: 40 | Loss: 17.467 | Acc: 17.988,19.741,34.032,43.598,54.668,58.727,61.319,61.490,%
Batch: 60 | Loss: 17.530 | Acc: 17.546,19.262,33.824,43.404,54.239,58.632,61.245,61.373,%

Epoch: 15
Batch: 0 | Loss: 14.997 | Acc: 17.969,20.312,30.469,47.656,65.625,71.875,75.781,75.781,%
Batch: 20 | Loss: 14.962 | Acc: 19.457,22.284,38.430,48.661,62.649,67.932,72.991,75.037,%
Batch: 40 | Loss: 15.055 | Acc: 19.398,22.885,38.681,47.713,61.928,67.664,72.447,73.914,%
Batch: 60 | Loss: 15.274 | Acc: 19.531,22.682,37.743,47.016,61.091,66.624,71.580,73.169,%
Batch: 80 | Loss: 15.306 | Acc: 19.454,22.733,37.394,46.730,60.889,66.773,71.499,73.032,%
Batch: 100 | Loss: 15.334 | Acc: 19.500,22.618,37.144,46.403,60.760,66.623,71.318,72.664,%
Batch: 120 | Loss: 15.332 | Acc: 19.570,22.695,37.435,46.791,60.731,66.736,71.152,72.469,%
Batch: 140 | Loss: 15.362 | Acc: 19.648,22.678,37.356,46.454,60.184,66.295,70.844,72.113,%
Batch: 160 | Loss: 15.378 | Acc: 19.619,22.622,37.340,46.346,60.142,66.261,70.662,72.011,%
Batch: 180 | Loss: 15.381 | Acc: 19.652,22.596,37.358,46.452,60.273,66.311,70.641,71.888,%
Batch: 200 | Loss: 15.412 | Acc: 19.621,22.532,37.150,46.280,60.102,66.185,70.456,71.712,%
Batch: 220 | Loss: 15.420 | Acc: 19.697,22.614,37.090,46.129,59.881,66.000,70.369,71.592,%
Batch: 240 | Loss: 15.419 | Acc: 19.713,22.695,37.053,46.110,59.949,66.098,70.368,71.544,%
Batch: 260 | Loss: 15.446 | Acc: 19.732,22.617,36.994,46.133,59.971,66.047,70.262,71.363,%
Batch: 280 | Loss: 15.481 | Acc: 19.684,22.559,36.955,46.058,59.873,66.039,70.129,71.216,%
Batch: 300 | Loss: 15.495 | Acc: 19.726,22.581,36.898,46.122,59.920,66.020,70.128,71.221,%
Batch: 320 | Loss: 15.488 | Acc: 19.782,22.634,36.921,46.147,59.947,66.046,70.108,71.242,%
Batch: 340 | Loss: 15.512 | Acc: 19.822,22.638,36.778,46.009,59.895,65.971,70.054,71.192,%
Batch: 360 | Loss: 15.516 | Acc: 19.893,22.650,36.745,46.009,59.890,66.030,70.061,71.178,%
Batch: 380 | Loss: 15.529 | Acc: 19.798,22.564,36.760,46.001,59.838,66.029,70.003,71.112,%
Batch: 0 | Loss: 15.928 | Acc: 27.344,25.000,42.969,53.125,60.938,64.062,68.750,67.969,%
Batch: 20 | Loss: 17.117 | Acc: 19.196,23.028,35.119,44.308,55.915,60.975,63.393,64.025,%
Batch: 40 | Loss: 17.047 | Acc: 19.569,22.732,34.623,44.588,55.526,60.842,63.415,64.101,%
Batch: 60 | Loss: 17.096 | Acc: 19.032,22.912,34.618,44.301,55.161,60.041,62.884,63.473,%

Epoch: 16
Batch: 0 | Loss: 13.127 | Acc: 20.312,21.875,43.750,56.250,73.438,78.906,78.125,85.938,%
Batch: 20 | Loss: 14.871 | Acc: 20.573,24.070,37.984,47.619,61.793,68.750,73.400,74.628,%
Batch: 40 | Loss: 15.089 | Acc: 19.798,23.114,37.100,47.104,61.014,68.274,72.561,74.562,%
Batch: 60 | Loss: 15.210 | Acc: 19.736,22.823,36.898,46.452,60.207,67.636,71.837,73.668,%
Batch: 80 | Loss: 15.179 | Acc: 19.821,22.830,36.883,46.566,60.619,67.689,71.711,73.457,%
Batch: 100 | Loss: 15.071 | Acc: 20.057,22.966,37.075,47.006,61.247,68.216,72.200,73.917,%
Batch: 120 | Loss: 15.107 | Acc: 19.983,22.856,37.126,47.069,61.002,68.208,72.262,73.799,%
Batch: 140 | Loss: 15.161 | Acc: 19.797,22.806,36.940,46.964,60.921,67.963,72.008,73.471,%
Batch: 160 | Loss: 15.154 | Acc: 19.992,22.938,37.112,47.045,60.899,67.935,72.122,73.525,%
Batch: 180 | Loss: 15.201 | Acc: 19.764,22.967,37.008,47.039,60.670,67.714,71.901,73.295,%
Batch: 200 | Loss: 15.236 | Acc: 19.761,22.843,36.987,46.891,60.549,67.553,71.723,73.022,%
Batch: 220 | Loss: 15.270 | Acc: 19.648,22.738,36.857,46.670,60.414,67.417,71.652,72.978,%
Batch: 240 | Loss: 15.273 | Acc: 19.771,22.815,37.046,46.765,60.484,67.350,71.548,72.860,%
Batch: 260 | Loss: 15.300 | Acc: 19.771,22.836,37.051,46.716,60.423,67.185,71.351,72.761,%
Batch: 280 | Loss: 15.280 | Acc: 19.920,22.948,37.200,46.772,60.487,67.124,71.291,72.690,%
Batch: 300 | Loss: 15.300 | Acc: 19.845,22.937,37.105,46.699,60.398,67.066,71.120,72.576,%
Batch: 320 | Loss: 15.317 | Acc: 19.872,22.853,37.013,46.663,60.331,67.076,71.055,72.493,%
Batch: 340 | Loss: 15.333 | Acc: 19.811,22.817,36.973,46.577,60.321,67.027,70.961,72.409,%
Batch: 360 | Loss: 15.339 | Acc: 19.860,22.916,37.093,46.652,60.243,66.936,70.869,72.293,%
Batch: 380 | Loss: 15.343 | Acc: 19.919,22.908,37.104,46.664,60.275,66.913,70.809,72.193,%
Batch: 0 | Loss: 17.877 | Acc: 14.844,21.094,39.844,42.969,54.688,59.375,61.719,58.594,%
Batch: 20 | Loss: 17.713 | Acc: 18.415,18.676,34.561,42.969,54.092,58.631,61.086,61.161,%
Batch: 40 | Loss: 17.679 | Acc: 17.893,18.979,33.994,43.864,54.306,58.765,60.976,61.090,%
Batch: 60 | Loss: 17.732 | Acc: 18.097,19.134,34.337,44.416,54.278,58.517,60.835,61.053,%

Epoch: 17
Batch: 0 | Loss: 16.171 | Acc: 15.625,16.406,33.594,39.062,57.031,75.000,71.094,74.219,%
Batch: 20 | Loss: 14.640 | Acc: 20.871,24.293,38.802,48.810,62.612,70.796,74.702,76.153,%
Batch: 40 | Loss: 14.617 | Acc: 21.399,24.505,38.872,48.780,62.748,70.236,74.676,75.915,%
Batch: 60 | Loss: 14.744 | Acc: 20.710,23.911,38.461,48.655,62.538,69.839,74.449,75.435,%
Batch: 80 | Loss: 14.786 | Acc: 20.486,23.621,38.108,48.370,62.510,69.560,74.219,75.338,%
Batch: 100 | Loss: 14.828 | Acc: 20.413,23.592,38.111,48.267,62.515,69.547,74.157,75.193,%
Batch: 120 | Loss: 14.897 | Acc: 20.403,23.722,38.146,48.153,62.287,69.254,73.728,74.768,%
Batch: 140 | Loss: 14.955 | Acc: 20.229,23.432,37.921,48.172,61.985,68.833,73.327,74.579,%
Batch: 160 | Loss: 15.006 | Acc: 20.084,23.185,37.815,47.913,61.699,68.624,73.025,74.267,%
Batch: 180 | Loss: 15.010 | Acc: 20.148,23.183,37.772,47.760,61.684,68.603,73.015,74.309,%
Batch: 200 | Loss: 15.034 | Acc: 20.134,23.049,37.710,47.528,61.575,68.466,72.979,74.320,%
Batch: 220 | Loss: 15.035 | Acc: 20.164,23.084,37.829,47.508,61.556,68.474,72.904,74.183,%
Batch: 240 | Loss: 15.061 | Acc: 20.089,23.061,37.766,47.423,61.505,68.358,72.851,74.109,%
Batch: 260 | Loss: 15.070 | Acc: 20.136,23.018,37.748,47.354,61.557,68.427,72.779,74.015,%
Batch: 280 | Loss: 15.057 | Acc: 20.157,23.059,37.711,47.387,61.602,68.475,72.823,74.069,%
Batch: 300 | Loss: 15.061 | Acc: 20.152,23.001,37.731,47.438,61.605,68.366,72.667,73.962,%
Batch: 320 | Loss: 15.068 | Acc: 20.269,23.119,37.692,47.374,61.492,68.256,72.593,73.924,%
Batch: 340 | Loss: 15.082 | Acc: 20.255,23.073,37.676,47.384,61.526,68.257,72.530,73.884,%
Batch: 360 | Loss: 15.089 | Acc: 20.282,23.080,37.712,47.399,61.450,68.187,72.444,73.756,%
Batch: 380 | Loss: 15.088 | Acc: 20.292,23.138,37.689,47.375,61.421,68.149,72.445,73.743,%
Batch: 0 | Loss: 17.609 | Acc: 21.094,16.406,35.938,42.188,58.594,64.844,67.188,65.625,%
Batch: 20 | Loss: 18.116 | Acc: 16.927,18.304,31.845,41.853,54.836,59.226,61.272,61.830,%
Batch: 40 | Loss: 18.110 | Acc: 16.673,18.426,32.050,42.054,54.897,58.784,60.995,61.566,%
Batch: 60 | Loss: 18.113 | Acc: 16.163,18.225,31.711,41.906,54.547,58.414,61.066,61.437,%

Epoch: 18
Batch: 0 | Loss: 15.454 | Acc: 27.344,25.781,42.188,53.125,63.281,68.750,70.312,70.312,%
Batch: 20 | Loss: 14.591 | Acc: 20.759,23.810,38.914,47.098,62.946,70.945,75.521,76.562,%
Batch: 40 | Loss: 14.620 | Acc: 21.018,24.695,38.815,47.599,61.795,70.408,74.886,76.200,%
Batch: 60 | Loss: 14.646 | Acc: 20.966,24.052,38.614,48.002,62.218,70.248,74.641,76.294,%
Batch: 80 | Loss: 14.726 | Acc: 20.737,24.026,38.551,47.926,62.355,70.033,74.633,76.292,%
Batch: 100 | Loss: 14.695 | Acc: 20.599,24.265,38.591,48.012,62.508,69.918,74.505,76.323,%
Batch: 120 | Loss: 14.752 | Acc: 20.358,23.870,38.391,47.701,62.345,69.654,74.412,76.149,%
Batch: 140 | Loss: 14.738 | Acc: 20.545,24.108,38.486,47.850,62.522,69.603,74.407,76.108,%
Batch: 160 | Loss: 14.801 | Acc: 20.487,23.889,38.223,47.642,62.267,69.410,74.199,75.796,%
Batch: 180 | Loss: 14.770 | Acc: 20.571,23.964,38.260,47.738,62.349,69.488,74.271,75.725,%
Batch: 200 | Loss: 14.779 | Acc: 20.596,24.048,38.270,47.765,62.294,69.352,74.246,75.669,%
Batch: 220 | Loss: 14.792 | Acc: 20.613,24.024,38.324,47.787,62.337,69.319,74.127,75.534,%
Batch: 240 | Loss: 14.804 | Acc: 20.595,24.002,38.210,47.828,62.263,69.246,73.969,75.386,%
Batch: 260 | Loss: 14.816 | Acc: 20.537,23.955,38.284,47.845,62.165,69.250,73.937,75.308,%
Batch: 280 | Loss: 14.808 | Acc: 20.621,23.966,38.306,47.906,62.177,69.223,73.866,75.234,%
Batch: 300 | Loss: 14.808 | Acc: 20.637,23.975,38.302,47.900,62.209,69.132,73.775,75.145,%
Batch: 320 | Loss: 14.808 | Acc: 20.685,23.990,38.267,47.948,62.230,69.015,73.659,74.981,%
Batch: 340 | Loss: 14.826 | Acc: 20.668,23.877,38.210,47.885,62.136,68.952,73.561,74.920,%
Batch: 360 | Loss: 14.819 | Acc: 20.691,23.890,38.253,47.912,62.147,68.977,73.548,74.907,%
Batch: 380 | Loss: 14.858 | Acc: 20.632,23.809,38.144,47.751,62.045,68.834,73.347,74.727,%
Batch: 0 | Loss: 18.820 | Acc: 22.656,21.875,32.031,42.188,48.438,57.812,62.500,59.375,%
Batch: 20 | Loss: 18.633 | Acc: 19.048,20.387,32.552,42.225,52.158,57.961,59.040,59.449,%
Batch: 40 | Loss: 18.557 | Acc: 18.598,20.255,32.298,42.073,52.744,57.355,59.146,59.280,%
Batch: 60 | Loss: 18.565 | Acc: 17.994,19.941,31.903,41.483,52.856,56.814,59.119,59.170,%

Epoch: 19
Batch: 0 | Loss: 15.281 | Acc: 12.500,18.750,32.812,44.531,59.375,72.656,76.562,75.000,%
Batch: 20 | Loss: 14.761 | Acc: 20.275,24.330,38.914,48.251,61.458,69.271,74.926,76.116,%
Batch: 40 | Loss: 14.514 | Acc: 20.255,24.505,39.672,48.780,63.091,70.694,75.610,76.963,%
Batch: 60 | Loss: 14.484 | Acc: 20.146,24.219,38.768,48.540,63.025,70.633,75.666,77.011,%
Batch: 80 | Loss: 14.496 | Acc: 20.139,24.055,38.513,48.264,63.098,70.370,75.492,77.083,%
Batch: 100 | Loss: 14.418 | Acc: 20.359,24.172,38.738,48.847,63.119,70.668,75.681,77.282,%
Batch: 120 | Loss: 14.517 | Acc: 20.248,23.954,38.372,48.541,62.997,70.493,75.278,76.743,%
Batch: 140 | Loss: 14.565 | Acc: 20.229,23.775,38.464,48.620,62.927,70.229,74.967,76.452,%
Batch: 160 | Loss: 14.612 | Acc: 20.269,23.675,38.310,48.389,62.830,70.021,74.685,76.194,%
Batch: 180 | Loss: 14.595 | Acc: 20.364,23.666,38.333,48.412,62.845,70.015,74.737,76.222,%
Batch: 200 | Loss: 14.620 | Acc: 20.437,23.710,38.211,48.395,62.881,69.908,74.541,76.046,%
Batch: 220 | Loss: 14.586 | Acc: 20.472,23.724,38.416,48.586,63.030,69.963,74.562,76.082,%
Batch: 240 | Loss: 14.597 | Acc: 20.510,23.697,38.492,48.528,62.925,69.878,74.462,75.979,%
Batch: 260 | Loss: 14.613 | Acc: 20.513,23.728,38.581,48.557,62.823,69.831,74.315,75.766,%
Batch: 280 | Loss: 14.660 | Acc: 20.440,23.679,38.437,48.418,62.664,69.634,74.069,75.556,%
Batch: 300 | Loss: 14.673 | Acc: 20.427,23.676,38.504,48.396,62.625,69.534,73.933,75.379,%
Batch: 320 | Loss: 14.654 | Acc: 20.432,23.722,38.566,48.440,62.717,69.573,73.971,75.389,%
Batch: 340 | Loss: 14.676 | Acc: 20.441,23.694,38.499,48.394,62.713,69.568,73.882,75.284,%
Batch: 360 | Loss: 14.669 | Acc: 20.486,23.779,38.608,48.450,62.760,69.631,73.862,75.284,%
Batch: 380 | Loss: 14.702 | Acc: 20.450,23.788,38.566,48.380,62.666,69.533,73.759,75.166,%
Batch: 0 | Loss: 16.617 | Acc: 21.094,25.781,38.281,53.125,57.812,62.500,62.500,63.281,%
Batch: 20 | Loss: 17.029 | Acc: 20.312,23.028,34.710,45.833,57.440,61.533,62.835,64.062,%
Batch: 40 | Loss: 17.054 | Acc: 19.607,22.637,34.165,46.037,57.965,61.604,62.405,63.605,%
Batch: 60 | Loss: 17.035 | Acc: 19.275,22.541,34.080,45.812,57.915,61.552,62.526,63.384,%

Epoch: 20
Batch: 0 | Loss: 15.551 | Acc: 17.969,22.656,33.594,52.344,59.375,67.969,69.531,72.656,%
Batch: 20 | Loss: 14.353 | Acc: 20.945,24.554,39.025,48.549,63.951,71.094,75.893,77.567,%
Batch: 40 | Loss: 14.131 | Acc: 21.208,24.524,39.882,49.733,64.977,71.875,77.058,78.563,%
Batch: 60 | Loss: 14.247 | Acc: 20.441,24.385,39.331,49.590,64.011,71.196,76.358,78.125,%
Batch: 80 | Loss: 14.399 | Acc: 20.563,23.987,38.918,48.775,63.407,70.862,75.781,77.556,%
Batch: 100 | Loss: 14.361 | Acc: 20.413,23.971,39.032,49.126,63.660,70.939,75.820,77.568,%
Batch: 120 | Loss: 14.366 | Acc: 20.487,24.019,38.966,48.773,63.494,70.752,75.730,77.441,%
Batch: 140 | Loss: 14.349 | Acc: 20.506,24.080,38.952,48.892,63.459,70.961,75.936,77.610,%
Batch: 160 | Loss: 14.381 | Acc: 20.565,24.054,38.922,48.962,63.388,70.764,75.772,77.383,%
Batch: 180 | Loss: 14.417 | Acc: 20.576,24.081,38.955,48.891,63.234,70.567,75.639,77.249,%
Traceback (most recent call last):
  File "main.py", line 254, in <module>
    main_cifar(args)
  File "main.py", line 244, in main_cifar
    train(epoch)
  File "main.py", line 142, in train
    train_loss += to_python_float(loss.data)
  File "main.py", line 109, in to_python_float
    return t.item()
KeyboardInterrupt
