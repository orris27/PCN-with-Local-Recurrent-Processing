      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=128, out_features=32, bias=True)
      (linear): Linear(in_features=32, out_features=100, bias=True)
      (BN1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ClassifierModuleMiddle(
      (relu): ReLU()
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=288, out_features=72, bias=True)
      (linear): Linear(in_features=72, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x72])
      (linear_bw): Linear(in_features=72, out_features=288, bias=True)
      (BN1d): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ClassifierModuleLast(
      (relu): ReLU()
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=584, bias=True)
      (linear): Linear(in_features=584, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Traceback (most recent call last):
  File "main.py", line 445, in <module>
    main_cifar(args)
  File "main.py", line 382, in main_cifar
    train(epoch)
  File "main.py", line 193, in train
    outputs, scores = model(inputs) # outputs: a list of (B, num_classes); a list of scores: (B, 1)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 216, in forward
    r = self.classifiers[clf_id](x)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 145, in forward
    rep = self.linear(out)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py", line 1370, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [128 x 512], m2: [584 x 100] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python -m pdb main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy
> /home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/main.py(1)<module>()
-> '''Train CIFAR10 with PyTorch.'''
(Pdb) b pcn/modelG_2con3.py:145
Breakpoint 1 at /home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py:145
(Pdb) r
==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModuleFirst(
      (relu): ReLU()
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=128, out_features=32, bias=True)
      (linear): Linear(in_features=32, out_features=100, bias=True)
      (BN1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ClassifierModuleMiddle(
      (relu): ReLU()
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=288, out_features=72, bias=True)
      (linear): Linear(in_features=72, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x72])
      (linear_bw): Linear(in_features=72, out_features=288, bias=True)
      (BN1d): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ClassifierModuleLast(
      (relu): ReLU()
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=584, bias=True)
      (linear): Linear(in_features=584, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
> /home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py(145)forward()
-> rep = self.linear(out)
(Pdb) p out.shape
torch.Size([128, 512])
(Pdb) l.
140  	        out_block = F.avg_pool2d(self.relu(self.BN(x_block)), x_block.size(-1))
141  	        out_block = out_block.view(out_block.size(0), -1) # (batch_size, c_block)
142  	
143  	        out = out_block
144  	        #out = self.linear(out)
145 B->	        rep = self.linear(out)
146  	        if self.cls == 0:
147  	            pass
148  	        else:
149  	            b0 = F.relu(self.b0[0] + 1.0).expand_as(rep)
150  	            for _ in range(self.cls):
(Pdb) 
151  	                rep = self.linear_h(self.relu(out - self.linear_bw(rep))) * b0 + rep
152  	        out = self.BN1d(rep)
153  	
154  	        return out
155  	
156  	
157  	
158  	
159  	''' Architecture PredNetBpD '''
160  	class PredNetBpD(nn.Module):
161  	    def __init__(self, num_classes=10, cls=0, ge=False, score_layer=False):
(Pdb) 
162  	        '''
163  	            ge(bool): Switch of Gradient Equilibrium
164  	        '''
165  	        super().__init__()
166  	        self.ics = [3,  64, 64, 128, 128, 256, 256, 512] # input chanels
167  	        self.ocs = [64, 64, 128, 128, 256, 256, 512, 512] # output chanels
168  	        self.maxpool = [False, False, True, False, True, False, False, False] # downsample flag
169  	        self.cls = cls # num of time steps
170  	        self.nlays = len(self.ics)
171  	        self.classifiers = nn.ModuleList()
172  	        self.ge = ge
(Pdb) q
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python -m pdb main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy^C
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python -m pdb main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy
> /home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/main.py(1)<module>()
-> '''Train CIFAR10 with PyTorch.'''
(Pdb) q
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy
==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModuleFirst(
      (relu): ReLU()
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=128, out_features=32, bias=True)
      (linear): Linear(in_features=32, out_features=100, bias=True)
      (BN1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ClassifierModuleMiddle(
      (relu): ReLU()
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=288, out_features=72, bias=True)
      (linear): Linear(in_features=72, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x72])
      (linear_bw): Linear(in_features=72, out_features=288, bias=True)
      (BN1d): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ClassifierModuleLast(
      (relu): ReLU()
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=584, bias=True)
      (linear): Linear(in_features=584, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Traceback (most recent call last):
  File "main.py", line 445, in <module>
    main_cifar(args)
  File "main.py", line 382, in main_cifar
    train(epoch)
  File "main.py", line 193, in train
    outputs, scores = model(inputs) # outputs: a list of (B, num_classes); a list of scores: (B, 1)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 218, in forward
    r = self.classifiers[clf_id](x, h)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
TypeError: forward() takes 2 positional arguments but 3 were given
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy
==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModuleFirst(
      (relu): ReLU()
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=128, out_features=32, bias=True)
      (linear): Linear(in_features=32, out_features=100, bias=True)
      (BN1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ClassifierModuleMiddle(
      (relu): ReLU()
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=288, out_features=72, bias=True)
      (linear): Linear(in_features=72, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x72])
      (linear_bw): Linear(in_features=72, out_features=288, bias=True)
      (BN1d): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ClassifierModuleLast(
      (relu): ReLU()
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=584, bias=True)
      (linear): Linear(in_features=584, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Traceback (most recent call last):
  File "main.py", line 445, in <module>
    main_cifar(args)
  File "main.py", line 382, in main_cifar
    train(epoch)
  File "main.py", line 193, in train
    outputs, scores = model(inputs) # outputs: a list of (B, num_classes); a list of scores: (B, 1)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 218, in forward
    r = self.classifiers[clf_id](x, h)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 153, in forward
    rep = self.linear_h(self.relu(out - self.linear_bw(rep))) * b0 + rep
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 576, in __getattr__
    type(self).__name__, name))
AttributeError: 'ClassifierModuleLast' object has no attribute 'linear_h'
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy
==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModuleFirst(
      (relu): ReLU()
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=128, out_features=32, bias=True)
      (linear): Linear(in_features=32, out_features=100, bias=True)
      (BN1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ClassifierModuleMiddle(
      (relu): ReLU()
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=288, out_features=72, bias=True)
      (linear): Linear(in_features=72, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x72])
      (linear_bw): Linear(in_features=72, out_features=288, bias=True)
      (BN1d): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ClassifierModuleLast(
      (relu): ReLU()
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=584, bias=True)
      (linear): Linear(in_features=584, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Traceback (most recent call last):
  File "main.py", line 445, in <module>
    main_cifar(args)
  File "main.py", line 382, in main_cifar
    train(epoch)
  File "main.py", line 193, in train
    outputs, scores = model(inputs) # outputs: a list of (B, num_classes); a list of scores: (B, 1)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 218, in forward
    r = self.classifiers[clf_id](x, h)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/svu/e0509827/gitrepo/PCN-with-Local-Recurrent-Processing/pcn/modelG_2con3.py", line 154, in forward
    out = self.BN1d(rep)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 576, in __getattr__
    type(self).__name__, name))
AttributeError: 'ClassifierModuleLast' object has no attribute 'BN1d'
e0509827@volta01:~/gitrepo/PCN-with-Local-Recurrent-Processing$ python main.py --batch_size 128 --circles 5 --backend modelG_2con3 --dataset_name cifar100 --adaptive 0 --max_epoch 300 --dropout 1.0 --step_all 0 --step_clf 0 --ge 1 --loss_type cross_entropy
==> Preparing data..
Dataset: CIFAR100
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
PredNetBpD(
  (classifiers): ModuleList(
    (0): ClassifierModuleFirst(
      (relu): ReLU()
      (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=128, out_features=32, bias=True)
      (linear): Linear(in_features=32, out_features=100, bias=True)
      (BN1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ClassifierModuleMiddle(
      (relu): ReLU()
      (BN): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear_h): Linear(in_features=288, out_features=72, bias=True)
      (linear): Linear(in_features=72, out_features=100, bias=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x72])
      (linear_bw): Linear(in_features=72, out_features=288, bias=True)
      (BN1d): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ClassifierModuleLast(
      (relu): ReLU()
      (BN): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (b0): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1x100])
      (linear_bw): Linear(in_features=100, out_features=584, bias=True)
      (BN1d): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=584, out_features=100, bias=True)
    )
  )
  (PcConvs): ModuleList(
    (0): PcConvBp(
      (FFconv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): PcConvBp(
      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (2): PcConvBp(
      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (3): PcConvBp(
      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (4): PcConvBp(
      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (5): PcConvBp(
      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (6): PcConvBp(
      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (7): PcConvBp(
      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU()
      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (BNs): ModuleList(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

Epoch: 0
Batch: 0 | Loss: 14.216 | Acc: 0.000,0.000,1.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 13.555 | Acc: 1.897,2.307,6.510,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 13.327 | Acc: 2.420,3.449,7.127,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 13.154 | Acc: 2.779,4.060,8.107,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 13.012 | Acc: 3.241,4.688,9.047,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 12.903 | Acc: 3.489,5.012,9.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 12.802 | Acc: 3.842,5.385,10.318,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 12.702 | Acc: 4.050,5.735,10.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 12.616 | Acc: 4.314,6.085,11.355,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 12.544 | Acc: 4.472,6.345,11.727,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 12.471 | Acc: 4.680,6.720,12.065,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 12.403 | Acc: 4.864,7.028,12.514,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 12.336 | Acc: 5.112,7.394,13.074,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 12.279 | Acc: 5.274,7.630,13.416,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 12.218 | Acc: 5.408,7.979,13.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 12.157 | Acc: 5.523,8.220,14.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 12.106 | Acc: 5.685,8.511,14.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 12.057 | Acc: 5.783,8.724,14.984,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 12.006 | Acc: 5.982,8.921,15.307,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 11.952 | Acc: 6.150,9.170,15.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 11.254 | Acc: 8.594,12.500,19.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 11.125 | Acc: 8.705,13.207,19.717,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 11.123 | Acc: 8.270,12.957,18.960,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 11.119 | Acc: 8.274,13.461,19.045,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 1
Batch: 0 | Loss: 10.896 | Acc: 9.375,14.062,25.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 10.789 | Acc: 9.263,15.104,24.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 10.772 | Acc: 9.604,15.511,24.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 10.756 | Acc: 9.836,15.510,24.360,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 10.703 | Acc: 10.204,15.741,24.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 10.645 | Acc: 10.396,16.166,25.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 10.622 | Acc: 10.557,16.193,25.123,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 10.582 | Acc: 10.672,16.295,25.526,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 10.556 | Acc: 10.729,16.401,25.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 10.528 | Acc: 10.808,16.467,25.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 10.505 | Acc: 10.751,16.492,25.910,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 10.483 | Acc: 10.835,16.530,25.954,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 10.453 | Acc: 10.947,16.675,26.060,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 10.424 | Acc: 11.111,16.780,26.230,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 10.385 | Acc: 11.232,17.062,26.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 10.341 | Acc: 11.451,17.354,26.897,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 10.307 | Acc: 11.602,17.531,27.164,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 10.282 | Acc: 11.714,17.623,27.374,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 10.256 | Acc: 11.838,17.746,27.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 10.227 | Acc: 11.940,17.868,27.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 9.663 | Acc: 14.062,21.094,25.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 9.824 | Acc: 13.504,18.899,28.348,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 9.807 | Acc: 13.700,18.731,28.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 9.828 | Acc: 13.384,18.788,27.971,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 2
Batch: 0 | Loss: 9.871 | Acc: 11.719,17.969,33.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 9.432 | Acc: 14.993,21.280,34.152,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 9.438 | Acc: 14.996,21.189,33.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 9.428 | Acc: 15.215,21.209,33.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 9.399 | Acc: 15.586,21.730,33.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 9.358 | Acc: 15.934,21.906,33.965,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 9.352 | Acc: 16.025,21.965,33.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 9.336 | Acc: 15.969,21.842,33.932,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 9.319 | Acc: 16.008,22.055,33.987,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 9.296 | Acc: 16.121,22.186,34.164,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 9.288 | Acc: 16.189,22.264,34.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 9.265 | Acc: 16.304,22.313,34.347,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 9.231 | Acc: 16.341,22.475,34.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 9.205 | Acc: 16.529,22.632,34.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 9.182 | Acc: 16.601,22.731,34.973,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 9.154 | Acc: 16.764,22.986,35.156,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 9.137 | Acc: 16.771,23.048,35.400,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 9.105 | Acc: 16.933,23.218,35.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 9.087 | Acc: 17.034,23.338,35.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 9.065 | Acc: 17.161,23.489,35.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 8.443 | Acc: 17.969,26.562,39.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 8.739 | Acc: 18.229,24.516,38.914,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 8.707 | Acc: 18.236,24.771,39.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 8.720 | Acc: 18.302,24.821,38.499,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 3
Batch: 0 | Loss: 7.668 | Acc: 27.344,29.688,50.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 8.388 | Acc: 20.089,27.046,41.109,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 8.495 | Acc: 19.874,26.029,40.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 8.463 | Acc: 19.877,25.935,41.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 8.461 | Acc: 19.830,26.022,41.136,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 8.436 | Acc: 20.220,26.485,41.422,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 8.417 | Acc: 20.235,26.737,41.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 8.410 | Acc: 20.373,26.817,41.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 8.399 | Acc: 20.570,26.941,41.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 8.386 | Acc: 20.731,26.934,41.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 8.379 | Acc: 20.771,27.017,41.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 8.356 | Acc: 20.924,27.262,41.739,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 8.331 | Acc: 20.993,27.409,41.999,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 8.311 | Acc: 21.043,27.481,42.074,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 8.297 | Acc: 21.116,27.580,42.185,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 8.291 | Acc: 21.187,27.697,42.258,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 8.271 | Acc: 21.281,27.826,42.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 8.259 | Acc: 21.428,27.912,42.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 8.248 | Acc: 21.392,27.976,42.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 8.239 | Acc: 21.424,28.090,42.712,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 7.994 | Acc: 21.875,33.594,47.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 8.273 | Acc: 20.610,27.976,44.196,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 8.281 | Acc: 20.808,28.106,43.140,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 8.277 | Acc: 20.530,27.369,42.905,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 4
Batch: 0 | Loss: 8.480 | Acc: 20.312,27.344,38.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.889 | Acc: 22.284,29.874,44.903,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 7.850 | Acc: 22.847,29.954,45.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 7.821 | Acc: 23.425,30.469,46.247,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 7.841 | Acc: 23.216,30.170,46.142,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 7.801 | Acc: 23.492,30.531,46.318,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 7.756 | Acc: 23.554,30.940,46.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 7.740 | Acc: 23.593,31.189,47.091,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 7.725 | Acc: 23.748,31.352,47.253,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 7.719 | Acc: 23.895,31.354,47.341,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 7.713 | Acc: 24.059,31.611,47.338,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 7.691 | Acc: 24.190,31.883,47.476,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 7.688 | Acc: 24.222,31.911,47.462,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 7.684 | Acc: 24.159,31.864,47.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 7.678 | Acc: 24.235,31.881,47.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 7.673 | Acc: 24.273,31.896,47.542,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 7.668 | Acc: 24.370,31.953,47.527,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 7.666 | Acc: 24.372,31.942,47.429,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 7.654 | Acc: 24.375,32.014,47.436,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 7.644 | Acc: 24.438,32.021,47.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 7.669 | Acc: 21.875,29.688,53.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.789 | Acc: 22.731,29.464,47.991,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 7.810 | Acc: 22.123,29.745,47.942,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 7.842 | Acc: 21.555,29.598,47.836,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 5
Batch: 0 | Loss: 6.815 | Acc: 25.781,37.500,60.156,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.233 | Acc: 26.786,34.933,51.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 7.312 | Acc: 26.200,34.489,50.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 7.263 | Acc: 26.550,34.913,51.255,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 7.289 | Acc: 26.292,34.606,50.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 7.265 | Acc: 26.199,34.862,50.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 7.262 | Acc: 26.240,34.846,50.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 7.257 | Acc: 26.263,34.896,50.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 7.251 | Acc: 26.456,34.841,50.956,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 7.249 | Acc: 26.265,34.772,50.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 7.255 | Acc: 26.426,34.748,50.746,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 7.251 | Acc: 26.396,34.859,50.870,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 7.254 | Acc: 26.345,34.826,50.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 7.250 | Acc: 26.413,34.890,50.943,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 7.246 | Acc: 26.504,34.956,50.959,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 7.240 | Acc: 26.474,34.977,51.080,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 7.237 | Acc: 26.589,35.035,51.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 7.236 | Acc: 26.624,34.994,50.953,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 7.235 | Acc: 26.608,34.959,50.976,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 7.216 | Acc: 26.675,35.123,51.089,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 7.033 | Acc: 26.562,43.750,55.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.428 | Acc: 23.214,34.077,51.451,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 7.484 | Acc: 22.485,34.108,51.105,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 7.501 | Acc: 22.298,34.221,51.114,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 6
Batch: 0 | Loss: 6.463 | Acc: 32.031,39.062,53.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.687 | Acc: 28.646,39.955,56.696,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.747 | Acc: 28.525,38.662,56.345,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.796 | Acc: 28.548,38.973,55.341,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 6.796 | Acc: 29.041,38.966,55.170,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 6.836 | Acc: 28.411,38.544,54.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 6.857 | Acc: 28.196,38.378,54.681,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 6.842 | Acc: 28.446,38.619,54.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 6.859 | Acc: 28.285,38.437,54.338,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 6.853 | Acc: 28.151,38.415,54.234,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 6.866 | Acc: 28.090,38.320,54.124,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 6.866 | Acc: 28.125,38.348,54.002,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 6.868 | Acc: 28.070,38.404,53.919,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 6.865 | Acc: 27.960,38.341,54.023,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 6.862 | Acc: 28.058,38.342,54.090,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 6.856 | Acc: 28.141,38.318,54.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 6.855 | Acc: 28.125,38.357,54.067,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 6.849 | Acc: 28.180,38.405,54.204,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 6.848 | Acc: 28.259,38.452,54.233,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 6.842 | Acc: 28.301,38.507,54.361,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.983 | Acc: 29.688,39.062,53.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.151 | Acc: 26.525,35.565,53.385,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 7.150 | Acc: 26.848,35.137,53.258,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 7.163 | Acc: 26.319,34.785,53.381,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 7
Batch: 0 | Loss: 6.292 | Acc: 32.812,44.531,60.156,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.564 | Acc: 29.799,41.034,57.775,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.509 | Acc: 29.935,41.730,58.117,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.526 | Acc: 29.828,41.253,58.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 6.556 | Acc: 29.610,41.127,57.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 6.553 | Acc: 29.672,41.089,57.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 6.550 | Acc: 29.726,40.954,57.341,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 6.556 | Acc: 29.532,40.913,57.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 6.555 | Acc: 29.566,40.897,57.206,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 6.533 | Acc: 29.852,41.165,57.230,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 6.526 | Acc: 29.932,41.243,57.338,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 6.523 | Acc: 30.080,41.378,57.289,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 6.521 | Acc: 30.112,41.341,57.216,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 6.534 | Acc: 29.933,41.176,57.025,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 6.525 | Acc: 29.952,41.201,56.973,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 6.516 | Acc: 30.025,41.287,56.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 6.517 | Acc: 29.965,41.260,56.941,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 6.515 | Acc: 30.027,41.221,56.857,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 6.517 | Acc: 29.988,41.192,56.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 6.512 | Acc: 30.009,41.191,56.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.763 | Acc: 25.000,42.969,58.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.048 | Acc: 26.525,37.649,54.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 7.009 | Acc: 27.458,37.748,54.154,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 7.021 | Acc: 26.678,37.359,54.406,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 8
Batch: 0 | Loss: 5.902 | Acc: 40.625,50.000,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.332 | Acc: 29.613,42.746,60.119,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.211 | Acc: 30.755,43.902,61.242,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.233 | Acc: 30.904,44.096,60.451,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 6.212 | Acc: 31.366,44.425,60.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 6.184 | Acc: 31.521,44.377,60.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 6.172 | Acc: 31.663,44.480,60.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 6.197 | Acc: 31.632,44.110,60.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 6.218 | Acc: 31.313,43.828,60.210,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 6.217 | Acc: 31.354,43.754,60.117,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 6.219 | Acc: 31.308,43.773,60.055,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 6.228 | Acc: 31.246,43.609,60.093,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 6.233 | Acc: 31.302,43.543,60.091,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 6.229 | Acc: 31.343,43.690,60.138,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 6.230 | Acc: 31.306,43.686,60.051,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 6.227 | Acc: 31.354,43.755,60.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 6.224 | Acc: 31.342,43.833,60.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 6.228 | Acc: 31.404,43.761,60.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 6.228 | Acc: 31.347,43.705,59.944,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 6.226 | Acc: 31.342,43.652,59.931,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.856 | Acc: 26.562,42.188,58.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 7.002 | Acc: 25.744,38.318,57.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.959 | Acc: 26.181,38.014,56.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.928 | Acc: 25.948,38.140,56.749,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 9
Batch: 0 | Loss: 6.237 | Acc: 27.344,45.312,63.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.017 | Acc: 31.027,45.015,62.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.914 | Acc: 32.393,46.189,63.377,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.899 | Acc: 32.415,46.683,63.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 5.914 | Acc: 32.205,46.229,63.358,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 5.905 | Acc: 32.449,46.326,63.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 5.925 | Acc: 32.444,46.249,63.068,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 5.954 | Acc: 32.419,46.376,62.550,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 5.961 | Acc: 32.376,46.268,62.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 5.975 | Acc: 32.325,46.232,62.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 5.973 | Acc: 32.292,46.234,62.516,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 5.970 | Acc: 32.332,46.267,62.401,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 5.972 | Acc: 32.271,46.133,62.331,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 5.972 | Acc: 32.277,46.145,62.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 5.986 | Acc: 32.237,46.091,62.133,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 5.993 | Acc: 32.275,46.122,62.087,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 5.995 | Acc: 32.326,46.101,62.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 5.990 | Acc: 32.393,46.172,62.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 5.993 | Acc: 32.438,46.159,61.933,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 5.990 | Acc: 32.501,46.194,61.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.133 | Acc: 27.344,46.875,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.471 | Acc: 29.353,42.597,58.408,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.448 | Acc: 29.021,43.007,58.708,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.461 | Acc: 28.612,43.020,58.747,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 10
Batch: 0 | Loss: 6.135 | Acc: 32.031,42.969,64.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.745 | Acc: 33.854,46.987,64.881,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.728 | Acc: 33.194,47.370,65.415,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.710 | Acc: 33.248,47.848,65.343,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 5.706 | Acc: 33.304,47.955,65.326,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 5.735 | Acc: 32.828,47.649,65.091,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 5.754 | Acc: 32.754,47.475,64.721,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 5.748 | Acc: 32.979,47.656,64.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 5.760 | Acc: 33.002,47.559,64.456,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 5.773 | Acc: 33.033,47.570,64.378,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 5.764 | Acc: 33.205,47.610,64.420,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 5.760 | Acc: 33.286,47.578,64.250,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 5.761 | Acc: 33.338,47.578,64.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 5.764 | Acc: 33.285,47.593,64.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 5.774 | Acc: 33.307,47.531,64.079,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 5.774 | Acc: 33.262,47.545,64.075,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 5.770 | Acc: 33.346,47.498,64.084,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 5.769 | Acc: 33.406,47.549,64.046,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 5.759 | Acc: 33.492,47.728,64.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 5.758 | Acc: 33.504,47.806,63.974,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.271 | Acc: 25.781,51.562,61.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.513 | Acc: 27.195,42.001,59.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.478 | Acc: 28.316,42.092,59.051,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.499 | Acc: 27.754,42.341,58.773,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 11
Batch: 0 | Loss: 6.113 | Acc: 29.688,46.094,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.343 | Acc: 36.458,51.823,68.713,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.416 | Acc: 35.766,51.086,67.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.443 | Acc: 35.464,50.499,67.892,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 5.483 | Acc: 35.320,50.260,67.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 5.521 | Acc: 35.071,50.077,67.242,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 5.519 | Acc: 34.840,49.864,67.058,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 5.553 | Acc: 34.613,49.523,66.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 5.565 | Acc: 34.690,49.413,66.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 5.554 | Acc: 34.768,49.448,66.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 5.552 | Acc: 34.814,49.545,66.709,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 5.552 | Acc: 34.898,49.601,66.724,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 5.544 | Acc: 34.971,49.627,66.721,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 5.553 | Acc: 34.974,49.644,66.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 5.563 | Acc: 34.920,49.614,66.373,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 5.568 | Acc: 34.923,49.595,66.313,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 5.561 | Acc: 35.071,49.657,66.275,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 5.561 | Acc: 35.094,49.695,66.163,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 5.558 | Acc: 34.996,49.738,66.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 5.556 | Acc: 34.941,49.752,66.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.111 | Acc: 33.594,53.125,57.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.299 | Acc: 31.734,44.680,60.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.333 | Acc: 31.479,44.093,59.489,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.332 | Acc: 31.122,44.160,59.375,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 12
Batch: 0 | Loss: 5.111 | Acc: 35.156,53.125,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.199 | Acc: 38.170,52.976,70.164,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.301 | Acc: 36.509,52.477,68.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.339 | Acc: 35.848,52.036,68.737,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 5.357 | Acc: 35.667,51.775,68.461,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 5.341 | Acc: 35.922,51.918,68.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 5.319 | Acc: 36.021,51.943,68.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 5.349 | Acc: 35.827,51.707,68.357,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 5.357 | Acc: 35.690,51.660,68.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 5.359 | Acc: 35.575,51.701,68.232,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 5.364 | Acc: 35.568,51.605,68.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 5.383 | Acc: 35.404,51.474,68.174,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 5.386 | Acc: 35.467,51.391,68.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 5.386 | Acc: 35.659,51.443,67.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 5.390 | Acc: 35.657,51.415,67.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 5.386 | Acc: 35.730,51.422,67.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 5.391 | Acc: 35.619,51.375,67.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 5.393 | Acc: 35.605,51.379,67.747,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 5.397 | Acc: 35.570,51.383,67.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 5.399 | Acc: 35.579,51.372,67.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 6.003 | Acc: 37.500,51.562,63.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.976 | Acc: 32.887,47.433,62.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.975 | Acc: 33.098,47.294,62.519,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.987 | Acc: 32.505,47.579,62.551,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 13
Batch: 0 | Loss: 5.107 | Acc: 38.281,51.562,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.251 | Acc: 35.938,52.381,69.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.262 | Acc: 35.080,52.039,69.931,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.227 | Acc: 35.758,52.587,69.915,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 5.234 | Acc: 35.986,52.353,69.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 5.211 | Acc: 36.054,52.785,69.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 5.196 | Acc: 36.144,52.860,70.164,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 5.208 | Acc: 36.242,52.876,70.069,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 5.204 | Acc: 36.243,52.936,70.016,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 5.215 | Acc: 36.322,52.771,69.937,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 5.228 | Acc: 36.229,52.631,69.570,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 5.238 | Acc: 36.121,52.432,69.482,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 5.236 | Acc: 36.184,52.529,69.343,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 5.234 | Acc: 36.219,52.595,69.247,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 5.241 | Acc: 36.224,52.630,69.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 5.240 | Acc: 36.259,52.676,69.087,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 5.244 | Acc: 36.205,52.660,69.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 5.240 | Acc: 36.194,52.681,69.039,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 5.246 | Acc: 36.195,52.638,68.914,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 5.245 | Acc: 36.159,52.623,68.945,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.772 | Acc: 32.812,50.781,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.052 | Acc: 32.701,46.577,63.021,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 6.096 | Acc: 32.298,46.418,61.947,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 6.107 | Acc: 31.762,46.337,61.911,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 14
Batch: 0 | Loss: 5.195 | Acc: 33.594,55.469,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.974 | Acc: 36.830,54.092,72.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.957 | Acc: 37.271,54.935,72.066,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.978 | Acc: 37.065,54.803,72.439,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 5.000 | Acc: 36.796,54.514,72.193,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 5.000 | Acc: 36.889,54.633,72.030,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 5.029 | Acc: 37.074,54.455,71.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 5.008 | Acc: 37.350,54.848,71.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 5.037 | Acc: 37.122,54.391,71.399,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 5.041 | Acc: 37.090,54.217,71.297,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 5.042 | Acc: 37.123,54.283,71.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 5.056 | Acc: 37.192,54.260,71.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 5.066 | Acc: 37.053,54.230,70.928,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 5.069 | Acc: 37.045,54.227,70.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 5.085 | Acc: 36.897,54.093,70.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 5.096 | Acc: 36.856,54.002,70.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 5.095 | Acc: 36.965,54.028,70.407,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 5.099 | Acc: 36.895,53.986,70.308,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 5.103 | Acc: 36.927,53.973,70.243,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 5.107 | Acc: 36.889,53.939,70.161,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.803 | Acc: 35.938,57.031,61.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.887 | Acc: 31.920,49.479,64.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.886 | Acc: 32.298,48.895,63.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.906 | Acc: 31.916,48.809,63.704,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 15
Batch: 0 | Loss: 4.720 | Acc: 43.750,58.594,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.928 | Acc: 36.644,54.501,72.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.934 | Acc: 36.890,55.164,72.428,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.918 | Acc: 37.167,55.213,72.464,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.950 | Acc: 37.201,54.832,72.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.940 | Acc: 37.291,55.330,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.920 | Acc: 37.687,55.495,72.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.915 | Acc: 37.688,55.663,72.684,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.915 | Acc: 37.689,55.590,72.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.894 | Acc: 37.893,55.879,72.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.905 | Acc: 37.815,55.725,72.648,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.913 | Acc: 37.808,55.529,72.487,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.921 | Acc: 37.763,55.537,72.459,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.918 | Acc: 37.856,55.580,72.441,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.931 | Acc: 37.728,55.424,72.348,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.940 | Acc: 37.726,55.375,72.135,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.938 | Acc: 37.819,55.483,72.026,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.941 | Acc: 37.766,55.501,71.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.942 | Acc: 37.831,55.519,71.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.951 | Acc: 37.812,55.415,71.686,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.458 | Acc: 39.844,56.250,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.836 | Acc: 34.040,50.409,64.918,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.804 | Acc: 33.708,50.553,63.948,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.802 | Acc: 33.504,50.756,63.858,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 16
Batch: 0 | Loss: 4.166 | Acc: 43.750,64.062,79.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.657 | Acc: 40.104,58.929,75.186,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.633 | Acc: 39.863,58.289,75.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.679 | Acc: 39.152,57.928,75.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.676 | Acc: 39.381,57.706,75.511,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.679 | Acc: 39.318,57.526,75.255,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.710 | Acc: 38.849,57.173,74.968,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.707 | Acc: 39.046,57.386,74.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.722 | Acc: 38.999,57.419,74.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.723 | Acc: 39.024,57.325,74.534,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.738 | Acc: 38.926,57.319,74.374,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.751 | Acc: 38.918,57.187,74.148,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.767 | Acc: 38.920,57.096,73.972,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.772 | Acc: 38.934,57.019,73.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.784 | Acc: 38.796,56.909,73.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.790 | Acc: 38.782,56.865,73.718,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.794 | Acc: 38.778,56.875,73.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.806 | Acc: 38.746,56.782,73.412,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.814 | Acc: 38.686,56.724,73.282,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.824 | Acc: 38.595,56.625,73.204,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.752 | Acc: 41.406,51.562,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.747 | Acc: 33.594,50.186,65.253,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.776 | Acc: 34.070,50.133,64.253,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.777 | Acc: 33.376,50.525,64.588,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 17
Batch: 0 | Loss: 4.896 | Acc: 35.156,54.688,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.593 | Acc: 40.141,57.738,77.269,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.606 | Acc: 39.329,58.022,76.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.632 | Acc: 39.062,58.274,76.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.620 | Acc: 39.284,58.227,76.408,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.624 | Acc: 39.132,58.145,76.168,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.630 | Acc: 39.088,58.026,76.085,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.644 | Acc: 39.195,57.940,75.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.647 | Acc: 39.121,57.798,75.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.660 | Acc: 39.136,57.722,75.294,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.687 | Acc: 39.097,57.544,75.120,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.695 | Acc: 39.094,57.554,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.712 | Acc: 38.968,57.446,74.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.723 | Acc: 38.934,57.405,74.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.730 | Acc: 38.790,57.384,74.444,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.736 | Acc: 38.790,57.387,74.354,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.739 | Acc: 38.899,57.370,74.233,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.749 | Acc: 38.790,57.263,74.079,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.756 | Acc: 38.762,57.276,74.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.756 | Acc: 38.798,57.230,73.999,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.350 | Acc: 38.281,54.688,64.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.759 | Acc: 34.970,49.814,64.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.693 | Acc: 34.737,50.553,64.501,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.661 | Acc: 34.477,51.294,64.728,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 18
Batch: 0 | Loss: 4.225 | Acc: 40.625,55.469,82.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.494 | Acc: 40.030,59.338,78.385,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.451 | Acc: 40.320,59.508,77.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.507 | Acc: 39.946,59.004,77.472,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.486 | Acc: 39.931,59.317,77.537,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.508 | Acc: 39.983,59.236,77.259,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.525 | Acc: 39.837,58.949,77.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.530 | Acc: 39.816,58.943,76.961,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.540 | Acc: 39.790,58.885,76.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.547 | Acc: 39.848,58.969,76.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.565 | Acc: 39.572,58.835,76.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.574 | Acc: 39.501,58.700,76.354,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.580 | Acc: 39.597,58.600,76.261,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.582 | Acc: 39.616,58.564,76.203,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.595 | Acc: 39.560,58.521,76.020,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.603 | Acc: 39.525,58.493,75.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.621 | Acc: 39.515,58.350,75.557,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.625 | Acc: 39.500,58.252,75.392,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.624 | Acc: 39.539,58.286,75.361,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.626 | Acc: 39.505,58.350,75.340,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.379 | Acc: 38.281,59.375,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.682 | Acc: 34.598,51.972,65.402,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.669 | Acc: 34.623,52.039,64.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.669 | Acc: 34.503,52.177,64.639,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 19
Batch: 0 | Loss: 4.389 | Acc: 45.312,62.500,81.250,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.361 | Acc: 38.802,60.491,80.283,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.417 | Acc: 39.939,60.004,79.497,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.420 | Acc: 40.010,60.118,78.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.456 | Acc: 39.805,59.606,78.376,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.443 | Acc: 40.261,59.886,78.357,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.429 | Acc: 40.160,60.034,78.364,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.457 | Acc: 40.176,59.696,78.059,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.473 | Acc: 39.936,59.506,77.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.482 | Acc: 39.934,59.595,77.542,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.487 | Acc: 39.840,59.472,77.453,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.485 | Acc: 39.932,59.478,77.422,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.496 | Acc: 39.857,59.453,77.305,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.506 | Acc: 39.790,59.396,77.155,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.501 | Acc: 39.810,59.567,77.043,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.511 | Acc: 39.750,59.359,76.913,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.514 | Acc: 39.815,59.292,76.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.517 | Acc: 39.864,59.329,76.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.519 | Acc: 39.852,59.345,76.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.525 | Acc: 39.792,59.275,76.483,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.942 | Acc: 37.500,54.688,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.997 | Acc: 34.115,48.363,64.025,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.961 | Acc: 34.108,48.418,64.234,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.986 | Acc: 33.901,48.796,63.960,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 20
Batch: 0 | Loss: 4.636 | Acc: 42.969,64.062,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.463 | Acc: 40.774,60.417,78.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.424 | Acc: 40.072,60.537,78.925,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.381 | Acc: 40.036,60.784,78.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.369 | Acc: 40.181,60.725,78.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.404 | Acc: 40.037,60.481,78.373,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.411 | Acc: 40.083,60.382,77.957,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.418 | Acc: 40.088,60.217,77.986,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.398 | Acc: 40.247,60.278,77.989,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.409 | Acc: 40.180,60.148,77.918,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.414 | Acc: 40.166,60.152,77.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.428 | Acc: 40.070,60.170,77.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.433 | Acc: 40.032,60.254,77.447,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.440 | Acc: 39.943,60.258,77.404,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.445 | Acc: 40.044,60.195,77.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.451 | Acc: 40.070,60.128,77.185,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.451 | Acc: 40.048,60.110,77.200,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.453 | Acc: 40.119,60.081,77.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.459 | Acc: 40.119,60.046,77.017,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.460 | Acc: 40.147,60.070,76.995,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.120 | Acc: 46.875,57.031,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.616 | Acc: 37.202,52.827,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.594 | Acc: 37.024,52.668,64.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.568 | Acc: 36.591,53.343,64.857,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 21
Batch: 0 | Loss: 4.259 | Acc: 42.969,67.188,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.281 | Acc: 41.592,62.946,80.990,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.257 | Acc: 41.482,61.738,80.526,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.284 | Acc: 41.291,61.872,80.059,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.269 | Acc: 41.503,61.680,80.160,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.303 | Acc: 41.259,61.324,79.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.314 | Acc: 41.064,61.228,79.700,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.329 | Acc: 41.052,61.054,79.422,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.318 | Acc: 41.135,61.258,79.387,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.322 | Acc: 41.234,61.274,79.161,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.337 | Acc: 41.395,61.210,78.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.337 | Acc: 41.392,61.047,78.754,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.356 | Acc: 41.199,60.912,78.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.363 | Acc: 41.125,60.905,78.388,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.352 | Acc: 41.220,60.993,78.395,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.354 | Acc: 41.139,60.997,78.325,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.360 | Acc: 41.090,60.894,78.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.361 | Acc: 41.090,60.802,78.127,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.367 | Acc: 41.105,60.730,78.142,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.364 | Acc: 41.140,60.782,78.059,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.419 | Acc: 40.625,58.594,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.491 | Acc: 36.756,53.832,66.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.507 | Acc: 37.309,53.068,65.320,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.509 | Acc: 36.783,53.061,65.459,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 22
Batch: 0 | Loss: 4.153 | Acc: 43.750,67.188,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.261 | Acc: 41.592,61.607,81.287,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.148 | Acc: 42.797,63.434,82.069,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.165 | Acc: 42.316,62.859,81.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.158 | Acc: 42.486,62.558,81.520,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.172 | Acc: 41.886,62.469,81.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.172 | Acc: 42.020,62.461,81.282,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.187 | Acc: 41.811,62.339,81.045,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.204 | Acc: 41.838,62.238,80.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.219 | Acc: 41.743,62.047,80.464,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.221 | Acc: 41.620,62.080,80.375,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.222 | Acc: 41.671,62.203,80.228,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.229 | Acc: 41.549,62.127,80.093,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.243 | Acc: 41.475,61.910,79.957,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.254 | Acc: 41.562,61.824,79.743,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.260 | Acc: 41.567,61.843,79.685,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.273 | Acc: 41.411,61.831,79.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.286 | Acc: 41.333,61.753,79.415,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.290 | Acc: 41.428,61.751,79.300,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.300 | Acc: 41.363,61.643,79.152,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.545 | Acc: 37.500,60.156,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.722 | Acc: 35.528,50.930,64.249,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.662 | Acc: 35.042,51.620,64.291,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.668 | Acc: 35.207,51.230,64.114,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 23
Batch: 0 | Loss: 4.344 | Acc: 39.062,60.156,80.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.156 | Acc: 40.997,63.095,80.432,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.127 | Acc: 41.444,63.396,81.098,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.106 | Acc: 41.560,63.537,81.455,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.087 | Acc: 41.975,63.532,81.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.111 | Acc: 41.909,63.034,81.351,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.112 | Acc: 42.065,62.913,81.211,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.148 | Acc: 41.789,62.639,80.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.146 | Acc: 41.867,62.694,80.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.164 | Acc: 41.786,62.409,80.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.175 | Acc: 41.682,62.313,80.445,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.192 | Acc: 41.647,62.171,80.129,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.199 | Acc: 41.649,62.108,80.044,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.203 | Acc: 41.604,62.111,79.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.212 | Acc: 41.551,62.127,79.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.212 | Acc: 41.681,62.199,79.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.220 | Acc: 41.654,62.174,79.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.229 | Acc: 41.638,62.044,79.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.226 | Acc: 41.707,62.039,79.599,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.230 | Acc: 41.716,62.018,79.515,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.111 | Acc: 41.406,57.812,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.264 | Acc: 37.649,55.357,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.274 | Acc: 38.415,54.859,66.139,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.311 | Acc: 37.705,55.059,66.099,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 24
Batch: 0 | Loss: 3.729 | Acc: 49.219,67.188,85.156,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.055 | Acc: 42.374,63.132,82.515,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.040 | Acc: 41.521,63.510,82.450,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.025 | Acc: 42.136,63.819,82.454,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.024 | Acc: 42.091,63.812,82.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.058 | Acc: 41.986,63.413,82.209,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.059 | Acc: 41.761,63.507,82.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.066 | Acc: 41.794,63.447,81.915,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.082 | Acc: 41.794,63.412,81.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.107 | Acc: 41.700,63.147,81.461,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.123 | Acc: 41.900,63.095,81.308,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.121 | Acc: 42.035,63.097,81.275,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.130 | Acc: 42.019,63.038,81.195,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.138 | Acc: 41.963,62.991,81.052,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.136 | Acc: 42.057,62.984,80.953,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.140 | Acc: 42.156,62.837,80.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.148 | Acc: 42.202,62.787,80.724,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.162 | Acc: 42.133,62.628,80.563,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.168 | Acc: 42.164,62.565,80.497,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.165 | Acc: 42.216,62.617,80.440,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.112 | Acc: 33.594,59.375,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.505 | Acc: 36.719,54.204,66.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.481 | Acc: 37.424,54.249,65.987,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.469 | Acc: 36.834,54.329,66.124,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 25
Batch: 0 | Loss: 4.335 | Acc: 40.625,60.938,81.250,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.111 | Acc: 42.188,61.533,82.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.033 | Acc: 42.931,63.148,82.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.999 | Acc: 43.238,63.832,82.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 4.007 | Acc: 42.988,63.667,82.475,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 4.018 | Acc: 42.814,63.506,82.287,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 4.017 | Acc: 42.814,63.662,82.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 4.026 | Acc: 42.991,63.702,81.992,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 4.039 | Acc: 42.847,63.602,81.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 4.051 | Acc: 42.736,63.557,81.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 4.046 | Acc: 42.864,63.674,81.549,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 4.056 | Acc: 42.718,63.617,81.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.061 | Acc: 42.658,63.618,81.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.067 | Acc: 42.601,63.578,81.346,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.079 | Acc: 42.477,63.498,81.122,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.083 | Acc: 42.507,63.494,81.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.098 | Acc: 42.531,63.391,80.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.109 | Acc: 42.506,63.300,80.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.112 | Acc: 42.508,63.290,80.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.118 | Acc: 42.446,63.224,80.467,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.792 | Acc: 42.969,63.281,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.478 | Acc: 37.426,53.981,64.955,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.551 | Acc: 37.081,53.296,64.043,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.526 | Acc: 37.295,53.689,64.139,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 26
Batch: 0 | Loss: 3.692 | Acc: 48.438,67.969,85.938,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.974 | Acc: 44.010,65.030,83.519,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.986 | Acc: 43.826,64.977,83.880,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.963 | Acc: 43.660,64.831,83.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.959 | Acc: 43.808,64.516,83.372,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.965 | Acc: 43.719,64.287,83.192,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.968 | Acc: 43.666,64.398,83.077,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.973 | Acc: 43.722,64.428,82.940,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.964 | Acc: 43.896,64.655,82.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.981 | Acc: 43.603,64.572,82.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.989 | Acc: 43.536,64.370,82.684,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.995 | Acc: 43.488,64.306,82.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 4.003 | Acc: 43.322,64.202,82.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 4.005 | Acc: 43.361,64.179,82.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 4.019 | Acc: 43.288,64.032,82.287,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 4.022 | Acc: 43.252,63.990,82.153,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 4.030 | Acc: 43.275,63.909,82.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 4.033 | Acc: 43.344,63.856,81.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 4.042 | Acc: 43.239,63.762,81.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 4.053 | Acc: 43.198,63.648,81.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.182 | Acc: 38.281,62.500,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.342 | Acc: 38.318,55.878,67.262,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.373 | Acc: 38.110,55.183,65.968,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.365 | Acc: 38.179,55.674,65.689,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 27
Batch: 0 | Loss: 4.344 | Acc: 42.188,65.625,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.936 | Acc: 43.862,65.179,82.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.875 | Acc: 43.845,65.244,83.899,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.878 | Acc: 43.916,65.394,83.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.871 | Acc: 43.519,65.075,83.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.882 | Acc: 43.789,64.851,83.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.904 | Acc: 43.427,64.469,83.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.924 | Acc: 43.229,64.323,83.422,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.928 | Acc: 43.168,64.363,83.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.940 | Acc: 43.137,64.373,83.162,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.953 | Acc: 43.031,64.307,83.112,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.965 | Acc: 43.039,64.204,83.049,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.962 | Acc: 43.095,64.276,82.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.967 | Acc: 43.136,64.221,82.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.969 | Acc: 43.252,64.227,82.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.971 | Acc: 43.267,64.247,82.724,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.978 | Acc: 43.210,64.196,82.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.981 | Acc: 43.317,64.168,82.487,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.996 | Acc: 43.161,64.091,82.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.999 | Acc: 43.223,64.091,82.245,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.562 | Acc: 41.406,66.406,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.446 | Acc: 38.914,55.394,65.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.432 | Acc: 38.586,54.611,65.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.431 | Acc: 38.012,55.008,65.279,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 28
Batch: 0 | Loss: 4.063 | Acc: 39.062,67.969,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.805 | Acc: 43.304,65.811,84.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.825 | Acc: 43.426,66.006,84.585,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.818 | Acc: 43.596,66.060,84.721,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.814 | Acc: 43.451,65.828,84.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.821 | Acc: 43.433,66.004,84.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.834 | Acc: 43.447,65.793,83.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.840 | Acc: 43.639,65.686,83.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.838 | Acc: 43.881,65.644,83.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.856 | Acc: 43.707,65.504,83.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.869 | Acc: 43.668,65.400,83.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.878 | Acc: 43.838,65.370,83.403,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.894 | Acc: 43.724,65.103,83.292,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.902 | Acc: 43.744,65.056,83.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.910 | Acc: 43.792,64.983,83.060,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.918 | Acc: 43.714,64.961,82.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.927 | Acc: 43.636,64.914,82.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.937 | Acc: 43.622,64.814,82.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.950 | Acc: 43.484,64.677,82.436,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.953 | Acc: 43.537,64.680,82.359,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.349 | Acc: 36.719,55.469,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.564 | Acc: 36.533,55.320,65.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.480 | Acc: 37.138,55.926,65.339,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.472 | Acc: 37.103,55.648,65.523,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 29
Batch: 0 | Loss: 3.621 | Acc: 49.219,66.406,85.938,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.839 | Acc: 43.155,65.439,84.635,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.794 | Acc: 44.226,66.368,85.404,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.815 | Acc: 43.327,65.843,85.233,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.824 | Acc: 43.326,65.490,85.137,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.816 | Acc: 43.858,65.695,84.994,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.813 | Acc: 43.763,65.709,84.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.819 | Acc: 43.761,65.619,84.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.822 | Acc: 44.017,65.727,84.598,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.830 | Acc: 44.061,65.599,84.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.842 | Acc: 44.026,65.539,84.282,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.852 | Acc: 43.997,65.392,84.128,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.859 | Acc: 43.954,65.353,83.934,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.873 | Acc: 43.879,65.173,83.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.880 | Acc: 43.883,65.133,83.599,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.889 | Acc: 43.830,65.044,83.407,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.892 | Acc: 43.937,65.092,83.324,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.901 | Acc: 43.894,65.006,83.191,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.906 | Acc: 43.940,64.915,83.070,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.909 | Acc: 43.939,64.920,83.018,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.112 | Acc: 38.281,58.594,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.553 | Acc: 36.272,53.571,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.559 | Acc: 36.795,53.582,65.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.555 | Acc: 37.065,53.496,65.779,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 30
Batch: 0 | Loss: 3.552 | Acc: 50.000,67.969,82.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.750 | Acc: 45.015,65.588,85.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.730 | Acc: 44.722,66.673,85.328,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.753 | Acc: 44.570,66.329,84.977,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.745 | Acc: 44.724,66.281,85.166,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.749 | Acc: 44.856,66.259,85.087,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.761 | Acc: 44.686,66.122,85.079,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.786 | Acc: 44.454,66.035,84.962,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.789 | Acc: 44.488,65.945,84.914,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.801 | Acc: 44.419,65.824,84.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.810 | Acc: 44.442,65.777,84.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.812 | Acc: 44.443,65.876,84.421,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.822 | Acc: 44.424,65.764,84.265,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.824 | Acc: 44.489,65.730,84.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.832 | Acc: 44.378,65.597,84.050,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.839 | Acc: 44.409,65.552,83.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.857 | Acc: 44.302,65.408,83.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.868 | Acc: 44.183,65.291,83.527,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.870 | Acc: 44.146,65.255,83.395,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.872 | Acc: 44.166,65.207,83.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.968 | Acc: 41.406,59.375,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.456 | Acc: 37.649,56.622,65.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.442 | Acc: 37.576,55.831,65.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.429 | Acc: 37.385,56.135,65.753,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 31
Batch: 0 | Loss: 3.678 | Acc: 42.969,67.188,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.751 | Acc: 44.568,66.555,86.235,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.770 | Acc: 44.512,66.082,85.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.763 | Acc: 44.890,66.368,85.925,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.745 | Acc: 44.850,66.580,85.735,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.728 | Acc: 45.111,66.839,85.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.727 | Acc: 45.048,66.819,85.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.735 | Acc: 45.047,66.611,85.439,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.738 | Acc: 45.075,66.426,85.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.751 | Acc: 44.898,66.251,85.303,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.762 | Acc: 44.807,66.274,85.063,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.768 | Acc: 44.814,66.389,84.948,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.789 | Acc: 44.729,66.173,84.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.803 | Acc: 44.477,66.038,84.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.805 | Acc: 44.517,66.006,84.394,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.813 | Acc: 44.448,65.973,84.227,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.826 | Acc: 44.322,65.910,84.068,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.822 | Acc: 44.417,65.937,84.047,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.826 | Acc: 44.425,65.926,83.923,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.825 | Acc: 44.482,65.894,83.889,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.137 | Acc: 47.656,66.406,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.452 | Acc: 37.574,57.031,66.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.409 | Acc: 38.148,56.059,66.120,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.409 | Acc: 38.204,56.429,65.868,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 32
Batch: 0 | Loss: 3.523 | Acc: 46.094,66.406,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.672 | Acc: 45.089,67.299,85.751,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.698 | Acc: 45.141,66.540,85.728,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.715 | Acc: 44.992,66.880,85.553,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.719 | Acc: 45.004,66.165,85.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.717 | Acc: 44.887,66.282,85.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.740 | Acc: 44.764,66.238,85.724,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.740 | Acc: 44.686,66.207,85.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.736 | Acc: 44.876,66.207,85.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.739 | Acc: 45.071,66.229,85.407,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.736 | Acc: 45.157,66.282,85.366,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.741 | Acc: 45.012,66.219,85.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.745 | Acc: 44.917,66.209,85.185,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.749 | Acc: 44.864,66.218,85.165,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.762 | Acc: 44.704,66.128,84.978,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.771 | Acc: 44.736,66.038,84.762,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.772 | Acc: 44.753,66.046,84.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.778 | Acc: 44.760,65.994,84.545,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.789 | Acc: 44.676,65.900,84.440,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.796 | Acc: 44.677,65.898,84.270,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.099 | Acc: 34.375,60.938,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.383 | Acc: 37.202,56.138,67.299,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.429 | Acc: 37.309,55.583,65.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.431 | Acc: 37.013,55.610,65.663,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 33
Batch: 0 | Loss: 3.355 | Acc: 49.219,64.062,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.665 | Acc: 45.015,67.448,86.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.736 | Acc: 44.341,66.597,86.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.719 | Acc: 44.467,66.855,86.117,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.703 | Acc: 44.821,66.917,85.938,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.699 | Acc: 44.725,67.056,85.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.699 | Acc: 44.802,67.020,86.002,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.689 | Acc: 44.919,67.099,85.915,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.684 | Acc: 45.104,67.197,85.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.696 | Acc: 44.907,67.067,85.726,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.701 | Acc: 44.869,67.013,85.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.711 | Acc: 44.910,66.894,85.467,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.720 | Acc: 44.855,66.863,85.335,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.729 | Acc: 44.759,66.849,85.246,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.743 | Acc: 44.656,66.668,85.159,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.743 | Acc: 44.692,66.648,85.024,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.750 | Acc: 44.633,66.552,84.876,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.762 | Acc: 44.685,66.475,84.739,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.777 | Acc: 44.590,66.300,84.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.775 | Acc: 44.603,66.220,84.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.054 | Acc: 43.750,60.938,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.404 | Acc: 37.984,55.841,66.369,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.392 | Acc: 38.281,55.621,66.063,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.387 | Acc: 38.217,55.930,65.945,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 34
Batch: 0 | Loss: 3.605 | Acc: 43.750,63.281,88.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.641 | Acc: 44.271,66.592,86.086,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.572 | Acc: 45.198,67.645,86.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.586 | Acc: 45.517,67.918,86.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.599 | Acc: 45.843,67.882,86.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.601 | Acc: 45.893,67.868,86.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.601 | Acc: 45.984,67.904,86.570,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.627 | Acc: 45.617,67.764,86.408,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.643 | Acc: 45.444,67.687,86.335,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.661 | Acc: 45.356,67.481,86.119,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.657 | Acc: 45.429,67.572,85.957,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.661 | Acc: 45.465,67.495,85.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.666 | Acc: 45.345,67.479,85.882,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.678 | Acc: 45.336,67.388,85.728,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.689 | Acc: 45.190,67.265,85.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.697 | Acc: 45.133,67.162,85.418,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.699 | Acc: 45.198,67.097,85.263,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.703 | Acc: 45.120,67.073,85.131,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.711 | Acc: 45.092,67.034,85.052,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.721 | Acc: 44.993,66.927,84.904,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.096 | Acc: 47.656,60.156,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.275 | Acc: 39.286,57.664,67.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.290 | Acc: 39.425,57.127,66.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.316 | Acc: 38.845,56.839,66.470,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 35
Batch: 0 | Loss: 3.285 | Acc: 49.219,72.656,88.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.576 | Acc: 45.089,69.085,85.417,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.623 | Acc: 44.646,68.579,86.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.635 | Acc: 44.454,68.174,86.181,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.611 | Acc: 45.062,68.441,86.314,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.637 | Acc: 44.701,67.915,86.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.624 | Acc: 44.932,67.930,86.364,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.625 | Acc: 44.991,67.658,86.331,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.623 | Acc: 45.201,67.600,86.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.635 | Acc: 45.144,67.416,86.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.640 | Acc: 45.173,67.467,86.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.647 | Acc: 45.086,67.350,86.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.643 | Acc: 45.183,67.460,85.960,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.650 | Acc: 45.175,67.385,85.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.654 | Acc: 45.148,67.276,85.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.656 | Acc: 45.170,67.312,85.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.662 | Acc: 45.106,67.312,85.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.669 | Acc: 45.070,67.245,85.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.680 | Acc: 45.061,67.162,85.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.683 | Acc: 45.046,67.185,85.187,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.831 | Acc: 43.750,60.938,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.201 | Acc: 40.253,57.961,65.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.265 | Acc: 40.434,57.222,65.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.239 | Acc: 40.702,57.428,65.049,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 36
Batch: 0 | Loss: 3.430 | Acc: 51.562,68.750,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.484 | Acc: 47.284,69.606,88.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.501 | Acc: 46.570,69.512,88.224,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.503 | Acc: 46.337,69.160,87.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.530 | Acc: 46.267,68.875,87.307,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.550 | Acc: 45.854,68.557,87.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.552 | Acc: 45.648,68.550,87.067,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.558 | Acc: 45.673,68.368,87.029,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.563 | Acc: 45.589,68.221,86.971,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.581 | Acc: 45.403,68.185,86.766,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.596 | Acc: 45.316,68.004,86.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.600 | Acc: 45.394,67.940,86.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.614 | Acc: 45.345,67.752,86.245,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.623 | Acc: 45.444,67.604,86.126,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.630 | Acc: 45.435,67.502,86.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.639 | Acc: 45.531,67.424,85.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.640 | Acc: 45.570,67.424,85.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.637 | Acc: 45.601,67.488,85.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.643 | Acc: 45.516,67.460,85.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.648 | Acc: 45.516,67.354,85.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.608 | Acc: 46.875,64.844,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.244 | Acc: 40.811,58.333,66.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.262 | Acc: 40.587,57.165,65.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.284 | Acc: 40.202,57.082,65.779,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 37
Batch: 0 | Loss: 3.268 | Acc: 54.688,71.094,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.576 | Acc: 47.098,68.006,87.277,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.600 | Acc: 46.532,67.854,87.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.571 | Acc: 46.209,68.238,87.013,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.575 | Acc: 46.460,68.432,86.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.587 | Acc: 46.148,68.069,87.020,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.597 | Acc: 46.152,67.956,86.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.593 | Acc: 45.961,67.941,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.597 | Acc: 45.856,67.988,86.568,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.605 | Acc: 45.835,67.904,86.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.604 | Acc: 45.857,67.887,86.392,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.610 | Acc: 45.769,67.746,86.358,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.625 | Acc: 45.731,67.547,86.132,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.631 | Acc: 45.633,67.529,86.036,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.635 | Acc: 45.696,67.521,85.993,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.642 | Acc: 45.634,67.455,85.943,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.644 | Acc: 45.668,67.467,85.918,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.646 | Acc: 45.640,67.435,85.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.650 | Acc: 45.635,67.400,85.732,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.657 | Acc: 45.600,67.313,85.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.060 | Acc: 43.750,65.625,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.264 | Acc: 39.807,58.966,67.374,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.228 | Acc: 39.615,58.841,66.959,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.261 | Acc: 38.717,58.876,66.496,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 38
Batch: 0 | Loss: 3.318 | Acc: 39.844,71.094,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.468 | Acc: 45.536,69.866,86.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.527 | Acc: 45.427,68.598,87.367,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.538 | Acc: 45.594,68.289,87.295,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.545 | Acc: 45.129,68.509,87.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.522 | Acc: 45.251,68.858,87.307,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.530 | Acc: 45.377,68.685,87.100,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.540 | Acc: 45.340,68.567,87.168,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.549 | Acc: 45.366,68.430,87.126,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.550 | Acc: 45.390,68.353,86.935,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.562 | Acc: 45.270,68.190,86.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.567 | Acc: 45.267,68.160,86.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.572 | Acc: 45.332,68.034,86.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.575 | Acc: 45.402,67.948,86.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.584 | Acc: 45.354,67.874,86.505,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.587 | Acc: 45.416,67.883,86.350,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.595 | Acc: 45.490,67.913,86.215,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.603 | Acc: 45.487,67.875,86.132,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.606 | Acc: 45.496,67.923,86.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.611 | Acc: 45.464,67.917,85.985,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.534 | Acc: 44.531,65.625,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.092 | Acc: 41.741,59.449,67.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.069 | Acc: 41.540,59.223,67.873,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.080 | Acc: 41.099,59.209,67.700,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 39
Batch: 0 | Loss: 3.492 | Acc: 43.750,69.531,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.425 | Acc: 45.312,71.019,87.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.471 | Acc: 45.274,70.617,87.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.484 | Acc: 45.261,69.698,87.935,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.481 | Acc: 45.322,69.599,87.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.476 | Acc: 45.475,69.601,87.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.489 | Acc: 45.777,69.370,87.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.504 | Acc: 45.623,69.260,87.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.516 | Acc: 45.633,69.211,87.320,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.524 | Acc: 45.783,69.044,87.124,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.531 | Acc: 45.857,69.057,86.968,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.544 | Acc: 45.793,68.895,86.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.556 | Acc: 45.698,68.795,86.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.573 | Acc: 45.624,68.654,86.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.578 | Acc: 45.604,68.516,86.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.582 | Acc: 45.725,68.501,86.319,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.590 | Acc: 45.726,68.441,86.234,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.596 | Acc: 45.837,68.383,86.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.599 | Acc: 45.825,68.192,86.070,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.608 | Acc: 45.786,68.096,85.960,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.387 | Acc: 45.312,61.719,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.556 | Acc: 38.690,54.725,66.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.539 | Acc: 38.091,55.145,66.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.528 | Acc: 37.654,55.430,65.766,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 40
Batch: 0 | Loss: 3.208 | Acc: 43.750,73.438,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.410 | Acc: 46.615,68.452,89.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.470 | Acc: 45.541,68.826,89.043,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.483 | Acc: 45.850,68.776,88.550,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.502 | Acc: 45.650,68.615,88.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.484 | Acc: 45.970,68.843,88.498,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.486 | Acc: 46.094,68.950,88.359,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.489 | Acc: 45.928,69.121,88.237,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.511 | Acc: 45.701,68.930,88.010,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.511 | Acc: 45.757,68.914,88.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.519 | Acc: 45.787,68.847,87.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.519 | Acc: 45.903,68.867,87.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.532 | Acc: 45.932,68.731,87.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.532 | Acc: 46.145,68.687,87.371,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.538 | Acc: 46.149,68.689,87.241,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.543 | Acc: 46.089,68.644,87.144,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.544 | Acc: 46.135,68.565,87.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.552 | Acc: 46.107,68.436,86.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.559 | Acc: 46.035,68.373,86.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.571 | Acc: 46.006,68.241,86.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.415 | Acc: 39.062,60.938,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.463 | Acc: 37.723,56.324,66.890,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.479 | Acc: 37.824,56.441,66.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.515 | Acc: 37.372,56.327,65.894,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 41
Batch: 0 | Loss: 3.208 | Acc: 46.094,76.562,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.508 | Acc: 46.168,69.085,89.472,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.450 | Acc: 46.951,69.703,89.196,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.456 | Acc: 46.619,69.992,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.448 | Acc: 46.692,70.496,88.831,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.428 | Acc: 46.898,70.390,89.055,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.436 | Acc: 46.810,70.319,88.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.444 | Acc: 46.764,70.191,88.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.454 | Acc: 46.797,70.007,88.306,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.466 | Acc: 46.750,69.846,88.065,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.462 | Acc: 46.898,69.803,87.959,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.481 | Acc: 46.794,69.524,87.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.497 | Acc: 46.745,69.353,87.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.510 | Acc: 46.695,69.184,87.428,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.519 | Acc: 46.628,69.042,87.272,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.522 | Acc: 46.595,68.934,87.217,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.528 | Acc: 46.627,68.913,87.142,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.535 | Acc: 46.593,68.890,86.946,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.535 | Acc: 46.680,68.889,86.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.546 | Acc: 46.617,68.758,86.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.377 | Acc: 37.500,57.812,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.421 | Acc: 38.653,56.808,66.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.395 | Acc: 38.681,56.860,66.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.390 | Acc: 38.627,57.031,66.150,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 42
Batch: 0 | Loss: 3.583 | Acc: 44.531,71.875,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.460 | Acc: 47.917,69.048,88.132,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.484 | Acc: 46.989,69.684,87.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.458 | Acc: 47.093,69.864,88.128,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.460 | Acc: 47.078,69.956,87.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.474 | Acc: 46.890,69.686,87.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.472 | Acc: 46.862,69.589,87.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.468 | Acc: 46.764,69.725,87.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.475 | Acc: 46.657,69.725,87.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.480 | Acc: 46.750,69.700,87.444,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.494 | Acc: 46.622,69.356,87.306,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.502 | Acc: 46.539,69.270,87.129,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.515 | Acc: 46.424,69.058,87.059,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.522 | Acc: 46.321,69.004,86.949,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.526 | Acc: 46.352,68.970,86.955,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.528 | Acc: 46.309,68.830,86.955,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.537 | Acc: 46.264,68.670,86.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.542 | Acc: 46.247,68.697,86.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.552 | Acc: 46.178,68.594,86.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.555 | Acc: 46.161,68.506,86.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.001 | Acc: 35.938,62.500,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.401 | Acc: 39.509,57.180,67.113,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.412 | Acc: 38.872,56.764,66.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.432 | Acc: 38.755,56.711,66.253,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 43
Batch: 0 | Loss: 3.340 | Acc: 45.312,71.875,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.413 | Acc: 47.135,70.722,88.318,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.399 | Acc: 47.294,70.427,88.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.400 | Acc: 47.054,70.389,88.678,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.381 | Acc: 47.029,70.583,88.870,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.396 | Acc: 47.115,70.529,88.420,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.403 | Acc: 47.056,70.448,88.217,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.411 | Acc: 47.041,70.335,88.093,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.425 | Acc: 46.890,70.167,88.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.429 | Acc: 46.953,70.131,87.996,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.446 | Acc: 46.751,69.862,87.916,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.461 | Acc: 46.716,69.712,87.755,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.471 | Acc: 46.515,69.525,87.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.474 | Acc: 46.648,69.477,87.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.480 | Acc: 46.614,69.428,87.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.478 | Acc: 46.743,69.459,87.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.484 | Acc: 46.712,69.368,87.415,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.488 | Acc: 46.779,69.245,87.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.495 | Acc: 46.756,69.111,87.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.500 | Acc: 46.717,69.051,87.203,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.793 | Acc: 45.312,63.281,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.287 | Acc: 40.625,58.557,66.555,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.322 | Acc: 40.415,57.431,65.949,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.323 | Acc: 40.459,57.672,66.009,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 44
Batch: 0 | Loss: 3.530 | Acc: 39.062,70.312,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.434 | Acc: 46.875,69.345,87.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.410 | Acc: 46.875,69.893,88.377,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.369 | Acc: 47.759,70.441,88.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.397 | Acc: 47.618,70.042,88.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.406 | Acc: 47.293,70.003,88.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.409 | Acc: 47.482,70.093,88.372,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.426 | Acc: 47.185,70.002,88.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.431 | Acc: 47.224,69.900,88.136,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.437 | Acc: 47.281,69.665,88.117,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.448 | Acc: 47.182,69.574,87.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.450 | Acc: 47.126,69.644,87.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.461 | Acc: 47.079,69.603,87.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.467 | Acc: 47.004,69.543,87.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.467 | Acc: 46.970,69.542,87.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.466 | Acc: 46.942,69.565,87.440,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.467 | Acc: 46.941,69.522,87.383,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.477 | Acc: 46.795,69.394,87.266,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.484 | Acc: 46.708,69.349,87.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.490 | Acc: 46.697,69.256,87.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.251 | Acc: 42.188,58.594,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.514 | Acc: 37.016,54.799,66.109,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.531 | Acc: 36.681,55.469,65.873,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.536 | Acc: 36.924,55.751,65.817,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 45
Batch: 0 | Loss: 3.040 | Acc: 49.219,76.562,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.364 | Acc: 47.582,71.168,88.690,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.351 | Acc: 47.713,70.789,88.453,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.397 | Acc: 47.234,70.236,88.256,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.409 | Acc: 46.779,70.071,88.445,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.390 | Acc: 47.138,70.320,88.382,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.412 | Acc: 47.049,69.912,88.230,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.412 | Acc: 46.925,70.019,88.170,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.423 | Acc: 46.919,69.963,88.082,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.426 | Acc: 46.927,69.851,87.940,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.428 | Acc: 46.937,69.714,87.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.436 | Acc: 46.864,69.549,87.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.440 | Acc: 46.804,69.522,87.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.440 | Acc: 46.857,69.444,87.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.444 | Acc: 46.789,69.465,87.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.451 | Acc: 46.839,69.370,87.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.460 | Acc: 46.851,69.290,87.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.461 | Acc: 46.880,69.270,87.216,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.470 | Acc: 46.847,69.200,87.104,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.474 | Acc: 46.799,69.113,87.063,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.117 | Acc: 43.750,56.250,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.306 | Acc: 39.286,57.329,67.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.289 | Acc: 40.072,57.336,66.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.327 | Acc: 40.254,57.070,66.342,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 46
Batch: 0 | Loss: 3.235 | Acc: 45.312,71.094,88.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.390 | Acc: 45.833,71.615,88.356,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.419 | Acc: 45.770,70.903,88.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.418 | Acc: 46.119,70.248,88.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.386 | Acc: 46.663,70.467,88.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.405 | Acc: 46.627,70.328,88.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.410 | Acc: 46.881,70.196,88.449,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.409 | Acc: 46.714,70.052,88.464,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.412 | Acc: 46.812,69.866,88.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.426 | Acc: 46.858,69.631,88.311,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.432 | Acc: 47.007,69.570,88.176,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.434 | Acc: 47.098,69.482,87.995,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.443 | Acc: 47.050,69.356,87.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.442 | Acc: 47.120,69.367,87.736,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.443 | Acc: 47.147,69.370,87.728,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.440 | Acc: 47.212,69.414,87.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.445 | Acc: 47.196,69.390,87.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.452 | Acc: 47.090,69.398,87.509,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.448 | Acc: 47.154,69.438,87.433,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.453 | Acc: 47.148,69.398,87.340,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.264 | Acc: 39.062,57.031,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.381 | Acc: 39.286,55.543,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.435 | Acc: 39.520,55.126,66.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.456 | Acc: 39.319,55.187,65.497,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 47
Batch: 0 | Loss: 3.159 | Acc: 50.000,74.219,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.324 | Acc: 47.693,71.391,88.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.314 | Acc: 48.190,71.913,88.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.323 | Acc: 47.810,71.491,88.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.328 | Acc: 47.868,71.181,88.532,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.332 | Acc: 47.958,70.978,88.513,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.326 | Acc: 47.818,71.249,88.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.326 | Acc: 47.701,70.983,88.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.345 | Acc: 47.457,70.701,88.480,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.373 | Acc: 47.086,70.390,88.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.389 | Acc: 47.034,70.141,88.180,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.398 | Acc: 47.023,69.966,87.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.409 | Acc: 46.907,69.842,87.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.413 | Acc: 46.947,69.902,87.686,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.425 | Acc: 46.883,69.690,87.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.428 | Acc: 46.966,69.658,87.495,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.431 | Acc: 47.016,69.575,87.405,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.442 | Acc: 46.925,69.499,87.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.445 | Acc: 46.998,69.488,87.266,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.452 | Acc: 46.945,69.394,87.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.037 | Acc: 39.062,58.594,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.359 | Acc: 38.244,58.073,66.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.422 | Acc: 38.815,57.927,65.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.423 | Acc: 38.397,57.889,65.471,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 48
Batch: 0 | Loss: 2.989 | Acc: 45.312,68.750,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.306 | Acc: 45.908,71.019,89.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.242 | Acc: 46.894,71.723,89.958,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.219 | Acc: 47.336,71.849,90.138,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.219 | Acc: 47.762,72.000,90.037,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.254 | Acc: 47.687,71.597,89.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.257 | Acc: 47.579,71.552,89.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.267 | Acc: 47.784,71.432,89.412,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.278 | Acc: 47.792,71.152,89.383,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.287 | Acc: 47.760,70.999,89.179,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.303 | Acc: 47.746,71.024,89.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.319 | Acc: 47.586,70.966,88.896,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.340 | Acc: 47.468,70.841,88.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.352 | Acc: 47.402,70.609,88.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.368 | Acc: 47.356,70.471,88.501,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.380 | Acc: 47.238,70.281,88.377,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.390 | Acc: 47.160,70.171,88.199,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.395 | Acc: 47.191,70.125,88.158,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.408 | Acc: 47.107,69.997,87.972,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.418 | Acc: 47.086,69.935,87.912,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.036 | Acc: 40.625,61.719,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.208 | Acc: 40.290,57.589,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.200 | Acc: 39.977,57.622,67.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.231 | Acc: 40.190,57.313,66.586,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 49
Batch: 0 | Loss: 3.450 | Acc: 43.750,68.750,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.272 | Acc: 47.731,70.871,90.216,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.278 | Acc: 47.466,71.056,90.263,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.241 | Acc: 48.092,71.619,90.318,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.267 | Acc: 47.724,71.267,90.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.275 | Acc: 47.679,71.194,89.952,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.276 | Acc: 47.760,71.378,90.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.288 | Acc: 47.773,71.371,89.899,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.291 | Acc: 47.879,71.346,89.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.302 | Acc: 47.803,71.146,89.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.323 | Acc: 47.602,70.954,89.436,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.333 | Acc: 47.483,70.765,89.246,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.343 | Acc: 47.384,70.737,89.092,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.351 | Acc: 47.312,70.678,88.991,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.356 | Acc: 47.295,70.638,88.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.372 | Acc: 47.275,70.445,88.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.383 | Acc: 47.216,70.305,88.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.389 | Acc: 47.219,70.184,88.384,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.401 | Acc: 47.128,70.087,88.231,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.410 | Acc: 47.164,70.001,88.074,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.871 | Acc: 48.438,62.500,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.337 | Acc: 41.443,56.696,66.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.358 | Acc: 41.768,55.983,66.444,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.350 | Acc: 41.598,56.455,66.086,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 50
Batch: 0 | Loss: 2.995 | Acc: 46.875,77.344,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.273 | Acc: 47.768,70.759,90.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.251 | Acc: 48.075,71.437,90.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.298 | Acc: 47.682,70.902,89.716,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.297 | Acc: 47.840,70.862,89.410,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.313 | Acc: 47.819,70.893,89.078,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.314 | Acc: 47.953,70.771,88.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.331 | Acc: 47.640,70.556,88.874,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.345 | Acc: 47.583,70.473,88.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.349 | Acc: 47.449,70.528,88.687,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.351 | Acc: 47.423,70.522,88.717,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.345 | Acc: 47.518,70.578,88.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.360 | Acc: 47.433,70.374,88.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.369 | Acc: 47.390,70.256,88.410,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.381 | Acc: 47.389,70.185,88.276,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.383 | Acc: 47.433,70.266,88.227,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.388 | Acc: 47.381,70.171,88.152,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.391 | Acc: 47.386,70.180,88.045,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.399 | Acc: 47.366,70.105,87.920,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.404 | Acc: 47.353,70.068,87.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.883 | Acc: 42.969,60.156,63.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.338 | Acc: 39.025,58.743,65.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.307 | Acc: 39.520,58.518,65.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.300 | Acc: 39.626,58.363,65.753,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 51
Batch: 0 | Loss: 2.927 | Acc: 48.438,79.688,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.217 | Acc: 47.991,73.586,89.918,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.251 | Acc: 47.447,73.209,89.920,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.268 | Acc: 47.631,72.784,89.536,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.247 | Acc: 47.820,72.907,89.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.260 | Acc: 47.718,72.656,89.465,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.259 | Acc: 47.740,72.553,89.553,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.275 | Acc: 47.883,72.257,89.362,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.278 | Acc: 47.918,72.113,89.300,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.288 | Acc: 47.971,71.832,89.157,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.293 | Acc: 47.928,71.883,89.066,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.305 | Acc: 47.925,71.741,88.967,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.316 | Acc: 47.841,71.596,88.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.330 | Acc: 47.728,71.291,88.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.332 | Acc: 47.754,71.197,88.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.339 | Acc: 47.651,71.078,88.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.349 | Acc: 47.671,70.894,88.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.363 | Acc: 47.514,70.718,88.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.372 | Acc: 47.485,70.628,88.231,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.375 | Acc: 47.406,70.577,88.187,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.020 | Acc: 45.312,60.156,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.205 | Acc: 40.737,57.254,66.220,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.259 | Acc: 40.263,56.402,65.053,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.244 | Acc: 40.523,56.762,65.202,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 52
Batch: 0 | Loss: 3.216 | Acc: 50.000,73.438,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.178 | Acc: 50.112,74.182,88.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.206 | Acc: 48.990,73.304,89.215,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.237 | Acc: 48.245,72.759,89.203,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.280 | Acc: 47.868,71.807,89.082,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.281 | Acc: 47.904,71.813,89.124,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.280 | Acc: 47.927,71.675,89.056,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.295 | Acc: 47.806,71.432,88.896,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.315 | Acc: 47.632,71.157,88.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.318 | Acc: 47.790,71.133,88.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.315 | Acc: 47.889,71.164,88.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.316 | Acc: 47.865,71.161,88.532,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.326 | Acc: 47.818,71.068,88.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.328 | Acc: 47.788,71.052,88.239,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.337 | Acc: 47.812,70.921,88.126,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.351 | Acc: 47.633,70.697,87.998,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.357 | Acc: 47.600,70.697,87.945,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.361 | Acc: 47.581,70.654,87.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.361 | Acc: 47.587,70.574,87.870,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.368 | Acc: 47.533,70.483,87.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.829 | Acc: 51.562,62.500,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.206 | Acc: 41.964,58.036,67.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.251 | Acc: 41.902,57.565,66.978,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.245 | Acc: 41.214,57.313,66.855,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 53
Batch: 0 | Loss: 3.321 | Acc: 46.094,75.000,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.216 | Acc: 49.033,72.954,90.439,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.265 | Acc: 48.037,71.932,89.920,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.248 | Acc: 48.386,72.157,89.857,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.228 | Acc: 48.630,72.261,90.085,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.246 | Acc: 48.298,72.084,89.689,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.241 | Acc: 48.425,72.140,89.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.252 | Acc: 48.066,71.809,89.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.258 | Acc: 48.127,71.613,89.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.267 | Acc: 48.058,71.443,89.378,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.278 | Acc: 47.870,71.199,89.269,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.281 | Acc: 47.798,71.203,89.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.286 | Acc: 47.835,71.113,89.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.293 | Acc: 47.854,71.145,89.060,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.301 | Acc: 47.898,71.116,88.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.317 | Acc: 47.804,70.891,88.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.327 | Acc: 47.780,70.777,88.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.329 | Acc: 47.769,70.794,88.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.335 | Acc: 47.758,70.700,88.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.343 | Acc: 47.759,70.665,88.324,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.718 | Acc: 49.219,63.281,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.174 | Acc: 40.737,59.412,68.192,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.237 | Acc: 40.663,58.556,67.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.260 | Acc: 40.292,58.363,66.829,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 54
Batch: 0 | Loss: 3.149 | Acc: 48.438,74.219,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.239 | Acc: 48.289,72.210,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.285 | Acc: 47.580,71.704,90.015,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.231 | Acc: 48.412,72.208,90.266,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.225 | Acc: 48.601,72.299,90.230,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.221 | Acc: 48.971,72.393,90.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.235 | Acc: 48.605,72.256,89.857,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.244 | Acc: 48.609,72.014,89.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.259 | Acc: 48.578,71.880,89.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.275 | Acc: 48.256,71.551,89.365,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.284 | Acc: 48.228,71.444,89.276,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.286 | Acc: 48.275,71.543,89.165,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.287 | Acc: 48.224,71.486,89.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.294 | Acc: 48.162,71.426,88.961,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.296 | Acc: 48.193,71.430,88.860,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.302 | Acc: 48.139,71.377,88.754,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.312 | Acc: 48.128,71.198,88.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.312 | Acc: 48.257,71.140,88.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.324 | Acc: 48.070,71.061,88.483,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.330 | Acc: 47.968,71.014,88.339,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.204 | Acc: 45.312,58.594,64.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.322 | Acc: 42.039,56.213,66.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.324 | Acc: 41.787,55.507,65.892,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.351 | Acc: 41.765,55.776,65.766,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 55
Batch: 0 | Loss: 3.201 | Acc: 53.125,73.438,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.224 | Acc: 47.061,71.912,90.067,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.277 | Acc: 46.780,71.227,89.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.289 | Acc: 47.182,71.107,89.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.262 | Acc: 47.897,71.190,89.709,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.256 | Acc: 47.850,71.403,89.689,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.247 | Acc: 47.902,71.610,89.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.243 | Acc: 48.066,71.759,89.423,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.250 | Acc: 48.122,71.773,89.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.268 | Acc: 48.049,71.491,89.127,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.272 | Acc: 48.103,71.393,89.121,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.285 | Acc: 48.017,71.242,89.027,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.290 | Acc: 48.019,71.207,88.939,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.295 | Acc: 48.054,71.249,88.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.302 | Acc: 47.970,71.155,88.765,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.307 | Acc: 48.064,71.107,88.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.310 | Acc: 48.102,71.043,88.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.318 | Acc: 48.085,70.959,88.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.327 | Acc: 48.035,70.862,88.309,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.332 | Acc: 48.015,70.852,88.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.909 | Acc: 50.781,59.375,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.135 | Acc: 42.299,59.263,67.113,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.176 | Acc: 42.016,58.251,66.120,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.184 | Acc: 41.739,58.325,66.022,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 56
Batch: 0 | Loss: 3.247 | Acc: 45.312,77.344,83.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.220 | Acc: 49.628,73.363,89.397,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.202 | Acc: 49.066,72.313,89.501,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.194 | Acc: 48.489,72.426,89.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.193 | Acc: 48.679,72.444,89.959,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.181 | Acc: 48.979,72.463,90.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.191 | Acc: 49.128,72.424,90.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.189 | Acc: 48.931,72.473,90.143,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.208 | Acc: 48.704,72.365,89.931,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.230 | Acc: 48.584,71.974,89.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.233 | Acc: 48.562,71.891,89.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.246 | Acc: 48.420,71.705,89.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.254 | Acc: 48.418,71.535,89.468,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.261 | Acc: 48.348,71.531,89.308,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.266 | Acc: 48.371,71.489,89.199,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.274 | Acc: 48.365,71.447,89.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.286 | Acc: 48.345,71.308,89.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.294 | Acc: 48.263,71.240,88.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.296 | Acc: 48.386,71.239,88.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.307 | Acc: 48.310,71.077,88.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.413 | Acc: 39.844,57.812,64.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.462 | Acc: 39.435,57.515,65.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.442 | Acc: 38.815,56.993,65.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.440 | Acc: 39.062,57.364,64.728,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 57
Batch: 0 | Loss: 3.460 | Acc: 50.000,69.531,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.233 | Acc: 49.293,71.912,88.988,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.239 | Acc: 48.495,72.085,88.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.213 | Acc: 48.425,72.874,89.395,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.208 | Acc: 48.322,72.714,89.497,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.220 | Acc: 48.051,72.641,89.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.208 | Acc: 48.257,72.521,89.973,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.212 | Acc: 48.166,72.291,89.966,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.217 | Acc: 48.171,72.346,89.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.223 | Acc: 48.325,72.212,89.697,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.233 | Acc: 48.259,72.065,89.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.249 | Acc: 48.250,71.978,89.508,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.251 | Acc: 48.301,72.044,89.351,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.253 | Acc: 48.315,71.995,89.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.256 | Acc: 48.440,72.022,89.210,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.264 | Acc: 48.445,71.906,89.140,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.272 | Acc: 48.326,71.773,89.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.272 | Acc: 48.334,71.719,88.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.279 | Acc: 48.370,71.607,88.876,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.288 | Acc: 48.304,71.524,88.755,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.631 | Acc: 46.094,63.281,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.169 | Acc: 39.360,60.640,68.973,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.200 | Acc: 39.043,59.832,68.331,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.188 | Acc: 39.267,59.798,68.263,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 58
Batch: 0 | Loss: 3.000 | Acc: 47.656,73.438,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.127 | Acc: 48.996,74.070,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.103 | Acc: 48.876,74.447,90.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.094 | Acc: 48.899,74.232,90.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.122 | Acc: 48.939,73.573,90.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.155 | Acc: 48.646,73.252,90.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.177 | Acc: 48.683,72.831,90.322,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.192 | Acc: 48.543,72.540,90.115,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.217 | Acc: 48.467,72.239,89.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.231 | Acc: 48.256,71.935,89.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.242 | Acc: 48.317,71.778,89.490,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.252 | Acc: 48.218,71.705,89.377,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.264 | Acc: 48.191,71.619,89.328,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.266 | Acc: 48.276,71.588,89.308,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.267 | Acc: 48.246,71.614,89.243,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.265 | Acc: 48.321,71.709,89.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.275 | Acc: 48.145,71.602,89.050,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.277 | Acc: 48.174,71.607,88.994,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.284 | Acc: 48.113,71.542,88.935,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.281 | Acc: 48.175,71.541,88.921,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.939 | Acc: 46.094,60.938,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.268 | Acc: 41.406,58.519,67.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.269 | Acc: 41.101,57.698,66.463,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.265 | Acc: 40.856,57.928,66.304,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 59
Batch: 0 | Loss: 2.805 | Acc: 57.812,75.781,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.160 | Acc: 48.549,71.763,90.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.148 | Acc: 48.399,72.504,90.454,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.154 | Acc: 48.706,72.426,90.292,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.158 | Acc: 48.794,72.512,90.181,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.159 | Acc: 48.847,72.571,90.161,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.162 | Acc: 48.818,72.566,90.154,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.174 | Acc: 48.731,72.623,89.999,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.183 | Acc: 48.646,72.627,89.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.186 | Acc: 48.886,72.548,89.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.203 | Acc: 48.737,72.236,89.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.210 | Acc: 48.678,72.147,89.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.220 | Acc: 48.703,72.086,89.306,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.228 | Acc: 48.683,71.980,89.161,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.245 | Acc: 48.582,71.836,89.032,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.261 | Acc: 48.419,71.763,88.904,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.271 | Acc: 48.333,71.622,88.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.287 | Acc: 48.195,71.410,88.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.294 | Acc: 48.204,71.340,88.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.304 | Acc: 48.155,71.231,88.423,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.010 | Acc: 43.750,64.062,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.206 | Acc: 40.774,59.933,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.293 | Acc: 40.015,58.841,67.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.324 | Acc: 39.664,58.555,66.803,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 60
Batch: 0 | Loss: 3.046 | Acc: 50.000,75.781,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.179 | Acc: 48.251,73.363,90.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.211 | Acc: 47.123,72.637,90.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.193 | Acc: 47.682,72.477,90.407,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.187 | Acc: 47.907,72.377,90.307,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.172 | Acc: 48.314,72.618,90.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.179 | Acc: 48.547,72.514,90.192,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.196 | Acc: 48.615,72.368,89.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.201 | Acc: 48.714,72.220,89.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.209 | Acc: 48.602,72.147,89.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.226 | Acc: 48.500,71.926,89.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.235 | Acc: 48.455,71.850,89.405,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.238 | Acc: 48.480,71.865,89.218,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.247 | Acc: 48.384,71.800,89.137,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.258 | Acc: 48.454,71.655,88.965,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.266 | Acc: 48.341,71.610,88.904,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.271 | Acc: 48.382,71.478,88.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.272 | Acc: 48.405,71.479,88.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.277 | Acc: 48.308,71.496,88.718,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.277 | Acc: 48.310,71.535,88.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.028 | Acc: 44.531,59.375,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.111 | Acc: 41.406,58.929,67.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.133 | Acc: 40.720,58.822,66.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.141 | Acc: 40.484,59.106,66.983,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 61
Batch: 0 | Loss: 2.848 | Acc: 53.906,78.906,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.070 | Acc: 49.702,75.707,89.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.095 | Acc: 49.295,74.352,90.568,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.084 | Acc: 49.539,74.552,90.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.069 | Acc: 49.614,74.498,90.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.107 | Acc: 49.265,73.824,90.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.127 | Acc: 49.032,73.431,90.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.160 | Acc: 48.753,72.911,90.243,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.167 | Acc: 48.690,72.792,90.154,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.170 | Acc: 48.783,72.855,90.042,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.180 | Acc: 48.764,72.781,89.914,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.185 | Acc: 48.660,72.738,89.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.189 | Acc: 48.723,72.608,89.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.198 | Acc: 48.695,72.501,89.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.208 | Acc: 48.682,72.392,89.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.215 | Acc: 48.627,72.236,89.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.228 | Acc: 48.622,72.138,89.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.237 | Acc: 48.621,72.065,89.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.254 | Acc: 48.591,71.953,89.147,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.257 | Acc: 48.626,71.891,89.009,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.793 | Acc: 41.406,64.844,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.265 | Acc: 39.918,58.594,66.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.263 | Acc: 39.386,58.384,66.159,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.238 | Acc: 39.267,58.773,66.534,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 62
Batch: 0 | Loss: 2.993 | Acc: 43.750,76.562,88.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.187 | Acc: 48.400,72.768,89.137,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.099 | Acc: 49.695,73.857,89.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.120 | Acc: 49.219,73.553,90.036,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.157 | Acc: 49.007,73.447,89.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.172 | Acc: 48.515,73.190,89.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.166 | Acc: 48.573,73.160,89.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.167 | Acc: 48.720,73.100,89.633,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.176 | Acc: 48.729,72.967,89.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.176 | Acc: 48.973,72.855,89.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.181 | Acc: 48.943,72.781,89.455,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.200 | Acc: 48.759,72.586,89.342,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.200 | Acc: 48.849,72.692,89.332,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.210 | Acc: 48.836,72.477,89.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.220 | Acc: 48.771,72.356,89.157,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.229 | Acc: 48.666,72.275,89.078,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.238 | Acc: 48.506,72.145,88.972,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.246 | Acc: 48.506,72.054,88.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.253 | Acc: 48.531,71.938,88.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.265 | Acc: 48.495,71.738,88.589,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.107 | Acc: 41.406,64.844,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.488 | Acc: 38.951,59.449,66.443,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.557 | Acc: 38.853,58.098,65.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.527 | Acc: 38.704,58.683,65.817,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 63
Batch: 0 | Loss: 3.267 | Acc: 50.000,66.406,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.216 | Acc: 48.289,70.759,89.360,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.207 | Acc: 47.999,71.723,89.882,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.187 | Acc: 48.066,72.451,89.857,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.158 | Acc: 48.650,72.975,89.863,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.150 | Acc: 48.909,73.113,90.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.143 | Acc: 48.928,73.270,89.986,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.143 | Acc: 48.864,73.210,90.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.158 | Acc: 48.816,73.025,89.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.162 | Acc: 48.783,72.920,89.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.181 | Acc: 48.815,72.715,89.630,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.190 | Acc: 48.756,72.632,89.522,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.204 | Acc: 48.574,72.361,89.422,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.210 | Acc: 48.659,72.252,89.326,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.214 | Acc: 48.577,72.178,89.238,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.221 | Acc: 48.572,72.054,89.172,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.225 | Acc: 48.566,72.014,89.140,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.230 | Acc: 48.582,71.996,89.090,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.232 | Acc: 48.634,72.007,89.041,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.240 | Acc: 48.583,71.930,88.890,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.709 | Acc: 45.312,61.719,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.206 | Acc: 40.699,59.263,67.336,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.224 | Acc: 40.930,59.566,67.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.234 | Acc: 40.599,59.324,66.957,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 64
Batch: 0 | Loss: 3.065 | Acc: 53.906,76.562,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.125 | Acc: 50.335,74.702,90.141,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.090 | Acc: 50.400,74.543,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.126 | Acc: 49.846,73.809,90.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.142 | Acc: 49.142,73.698,90.721,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.139 | Acc: 49.018,73.554,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.144 | Acc: 49.070,73.651,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.146 | Acc: 49.163,73.465,90.431,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.141 | Acc: 49.267,73.505,90.348,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.149 | Acc: 49.184,73.295,90.185,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.163 | Acc: 49.137,73.200,90.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.169 | Acc: 49.187,73.194,89.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.175 | Acc: 49.040,73.065,89.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.187 | Acc: 49.006,72.902,89.610,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.202 | Acc: 48.849,72.698,89.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.206 | Acc: 48.832,72.586,89.397,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.218 | Acc: 48.786,72.447,89.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.220 | Acc: 48.802,72.372,89.189,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.223 | Acc: 48.825,72.345,89.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.227 | Acc: 48.831,72.252,89.036,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.768 | Acc: 47.656,67.969,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.056 | Acc: 42.783,60.603,67.746,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.105 | Acc: 42.454,59.242,66.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.104 | Acc: 41.906,59.426,66.778,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 65
Batch: 0 | Loss: 3.062 | Acc: 47.656,74.219,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.065 | Acc: 49.926,74.219,90.141,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.053 | Acc: 49.905,74.238,90.339,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.083 | Acc: 49.616,73.566,90.394,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.115 | Acc: 49.219,73.003,90.432,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.114 | Acc: 49.737,72.857,90.455,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.129 | Acc: 49.193,72.869,90.270,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.131 | Acc: 49.346,72.933,90.348,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.133 | Acc: 49.335,73.010,90.227,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.135 | Acc: 49.448,73.071,90.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.145 | Acc: 49.351,73.006,90.003,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.147 | Acc: 49.346,72.971,89.950,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.156 | Acc: 49.241,72.899,89.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.168 | Acc: 49.156,72.821,89.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.176 | Acc: 49.124,72.726,89.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.177 | Acc: 49.125,72.768,89.571,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.179 | Acc: 49.095,72.720,89.532,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.191 | Acc: 49.106,72.592,89.360,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.205 | Acc: 48.957,72.423,89.275,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.212 | Acc: 48.956,72.230,89.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.994 | Acc: 33.594,60.156,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.385 | Acc: 40.141,57.738,64.918,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.375 | Acc: 40.206,57.317,65.053,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.370 | Acc: 40.177,57.582,65.190,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 66
Batch: 0 | Loss: 3.135 | Acc: 52.344,75.000,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.143 | Acc: 48.363,73.028,90.141,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.147 | Acc: 48.399,73.780,90.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.132 | Acc: 48.809,73.847,90.907,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.117 | Acc: 49.248,73.630,90.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.142 | Acc: 49.149,73.321,90.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.158 | Acc: 48.993,72.979,90.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.158 | Acc: 49.091,72.822,90.304,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.159 | Acc: 49.102,72.661,90.213,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.165 | Acc: 48.986,72.574,90.124,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.163 | Acc: 49.024,72.571,90.112,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.162 | Acc: 49.141,72.649,90.021,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.167 | Acc: 49.086,72.611,89.899,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.177 | Acc: 49.066,72.444,89.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.181 | Acc: 48.982,72.553,89.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.189 | Acc: 48.858,72.464,89.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.194 | Acc: 48.944,72.454,89.467,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.204 | Acc: 48.884,72.418,89.349,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.215 | Acc: 48.786,72.319,89.266,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.218 | Acc: 48.768,72.318,89.155,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.885 | Acc: 44.531,66.406,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.390 | Acc: 40.327,57.217,66.555,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.364 | Acc: 40.530,57.012,66.311,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.368 | Acc: 40.459,56.916,65.932,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 67
Batch: 0 | Loss: 2.808 | Acc: 53.906,72.656,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.942 | Acc: 51.451,75.707,91.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.060 | Acc: 49.162,74.371,90.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.050 | Acc: 49.680,74.039,90.996,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.063 | Acc: 49.344,73.717,91.098,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.058 | Acc: 49.273,73.925,91.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.065 | Acc: 49.361,73.502,91.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.063 | Acc: 49.474,73.521,90.941,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.082 | Acc: 49.432,73.195,90.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.087 | Acc: 49.353,73.278,90.612,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.105 | Acc: 49.242,73.084,90.450,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.133 | Acc: 49.053,72.766,90.240,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.150 | Acc: 48.985,72.643,90.158,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.165 | Acc: 48.758,72.534,90.083,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.174 | Acc: 48.763,72.437,89.944,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.179 | Acc: 48.811,72.407,89.852,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.193 | Acc: 48.664,72.211,89.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.196 | Acc: 48.719,72.200,89.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.206 | Acc: 48.624,72.065,89.541,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.213 | Acc: 48.591,71.941,89.432,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.572 | Acc: 42.188,65.625,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.289 | Acc: 41.146,59.449,65.141,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.324 | Acc: 40.682,58.822,64.653,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.320 | Acc: 40.510,58.722,64.754,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 68
Batch: 0 | Loss: 3.173 | Acc: 46.094,74.219,85.938,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.125 | Acc: 48.884,73.996,89.211,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.055 | Acc: 50.057,74.657,90.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.068 | Acc: 50.141,74.462,90.138,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.093 | Acc: 49.566,74.257,90.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.106 | Acc: 49.265,74.080,90.053,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.109 | Acc: 49.090,73.960,90.218,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.124 | Acc: 49.058,73.737,90.126,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.137 | Acc: 49.005,73.602,90.018,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.148 | Acc: 49.055,73.399,89.965,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.162 | Acc: 48.896,73.181,89.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.175 | Acc: 48.777,73.027,89.653,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.177 | Acc: 48.807,72.935,89.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.182 | Acc: 48.904,72.917,89.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.191 | Acc: 48.827,72.820,89.488,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.194 | Acc: 48.853,72.770,89.465,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.197 | Acc: 48.846,72.720,89.430,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.203 | Acc: 48.850,72.659,89.294,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.209 | Acc: 48.885,72.591,89.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.211 | Acc: 48.907,72.531,89.145,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.694 | Acc: 46.094,64.844,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.159 | Acc: 43.676,59.152,66.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.159 | Acc: 42.950,59.204,66.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.212 | Acc: 42.328,58.927,66.086,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 69
Batch: 0 | Loss: 3.464 | Acc: 46.094,73.438,82.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.169 | Acc: 48.996,71.949,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.109 | Acc: 49.848,73.037,90.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.089 | Acc: 49.910,73.642,90.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.094 | Acc: 49.672,73.717,90.731,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.092 | Acc: 49.590,73.530,90.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.091 | Acc: 49.761,73.586,90.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.090 | Acc: 49.817,73.781,90.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.098 | Acc: 49.743,73.821,90.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.102 | Acc: 49.858,73.822,90.483,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.116 | Acc: 49.650,73.624,90.400,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.128 | Acc: 49.597,73.494,90.328,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.127 | Acc: 49.692,73.570,90.336,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.130 | Acc: 49.590,73.512,90.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.129 | Acc: 49.564,73.549,90.355,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.142 | Acc: 49.538,73.365,90.192,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.152 | Acc: 49.557,73.228,90.009,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.166 | Acc: 49.482,73.041,89.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.174 | Acc: 49.414,72.944,89.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.184 | Acc: 49.270,72.761,89.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.904 | Acc: 44.531,62.500,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.411 | Acc: 41.667,59.115,64.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.409 | Acc: 41.406,57.812,64.348,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.396 | Acc: 41.547,58.107,64.677,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 70
Batch: 0 | Loss: 2.760 | Acc: 50.781,82.031,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.139 | Acc: 49.628,73.772,90.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.099 | Acc: 49.409,73.933,90.244,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.115 | Acc: 49.526,73.604,90.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.106 | Acc: 49.470,73.717,90.538,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.113 | Acc: 49.435,73.646,90.339,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.124 | Acc: 49.380,73.476,90.160,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.122 | Acc: 49.357,73.399,90.160,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.123 | Acc: 49.282,73.321,90.174,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.122 | Acc: 49.253,73.360,90.180,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.126 | Acc: 49.331,73.278,90.112,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.133 | Acc: 49.286,73.151,90.038,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.143 | Acc: 49.267,73.023,89.912,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.151 | Acc: 49.309,72.917,89.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.151 | Acc: 49.322,72.931,89.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.159 | Acc: 49.297,72.864,89.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.163 | Acc: 49.282,72.727,89.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.169 | Acc: 49.313,72.684,89.441,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.178 | Acc: 49.258,72.617,89.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.184 | Acc: 49.252,72.548,89.229,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.687 | Acc: 41.406,64.844,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.330 | Acc: 39.583,58.668,67.299,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.331 | Acc: 39.158,58.289,66.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.310 | Acc: 39.331,58.478,66.317,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 71
Batch: 0 | Loss: 2.975 | Acc: 53.906,75.781,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.089 | Acc: 50.260,73.624,90.030,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.064 | Acc: 50.305,74.409,90.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.076 | Acc: 49.846,74.142,90.727,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.071 | Acc: 49.855,74.209,90.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.076 | Acc: 49.683,74.064,90.710,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.079 | Acc: 49.587,74.135,90.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.089 | Acc: 49.512,73.864,90.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.104 | Acc: 49.476,73.612,90.431,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.088 | Acc: 49.689,73.740,90.444,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.098 | Acc: 49.674,73.496,90.427,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.104 | Acc: 49.682,73.480,90.293,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.123 | Acc: 49.468,73.298,90.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.129 | Acc: 49.383,73.264,90.140,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.141 | Acc: 49.316,73.104,89.958,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.150 | Acc: 49.323,72.929,89.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.153 | Acc: 49.418,72.885,89.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.162 | Acc: 49.397,72.730,89.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.174 | Acc: 49.333,72.565,89.454,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.177 | Acc: 49.307,72.519,89.417,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.326 | Acc: 44.531,59.375,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.206 | Acc: 40.923,59.561,67.671,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.213 | Acc: 41.063,59.070,67.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.217 | Acc: 41.073,58.940,66.995,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 72
Batch: 0 | Loss: 3.463 | Acc: 46.094,68.750,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.123 | Acc: 50.037,73.475,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.128 | Acc: 49.047,73.609,91.120,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.127 | Acc: 49.001,73.527,90.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.137 | Acc: 48.978,73.245,90.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.132 | Acc: 49.118,73.329,90.517,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.119 | Acc: 49.374,73.360,90.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.128 | Acc: 49.330,73.166,90.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.134 | Acc: 49.301,73.006,90.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.142 | Acc: 49.171,72.850,90.163,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.156 | Acc: 49.009,72.637,90.026,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.150 | Acc: 49.152,72.875,90.024,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.149 | Acc: 49.219,72.886,89.977,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.151 | Acc: 49.261,72.956,89.949,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.146 | Acc: 49.427,72.929,89.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.148 | Acc: 49.452,72.905,89.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.148 | Acc: 49.511,72.956,89.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.155 | Acc: 49.427,72.977,89.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.153 | Acc: 49.528,73.000,89.681,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.158 | Acc: 49.471,72.980,89.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.950 | Acc: 42.969,58.594,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.303 | Acc: 40.513,59.189,66.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.377 | Acc: 40.625,58.289,65.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.404 | Acc: 40.843,58.389,65.010,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 73
Batch: 0 | Loss: 3.049 | Acc: 50.781,71.875,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.065 | Acc: 50.074,73.438,90.402,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.119 | Acc: 48.857,73.342,90.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.078 | Acc: 49.526,73.899,90.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.048 | Acc: 49.778,74.113,90.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.053 | Acc: 49.783,74.118,90.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.079 | Acc: 49.477,73.793,90.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.077 | Acc: 49.762,73.875,90.570,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.070 | Acc: 49.825,73.918,90.538,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.075 | Acc: 49.866,73.766,90.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.091 | Acc: 49.708,73.585,90.361,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.105 | Acc: 49.721,73.476,90.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.124 | Acc: 49.569,73.249,90.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.129 | Acc: 49.563,73.132,89.904,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.137 | Acc: 49.525,73.071,89.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.143 | Acc: 49.616,73.077,89.696,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.152 | Acc: 49.533,73.016,89.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.161 | Acc: 49.457,72.904,89.468,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.165 | Acc: 49.476,72.896,89.415,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.168 | Acc: 49.475,72.867,89.319,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.120 | Acc: 42.188,59.375,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.201 | Acc: 42.225,59.747,65.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.265 | Acc: 42.111,58.575,65.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.284 | Acc: 41.829,58.440,64.600,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 74
Batch: 0 | Loss: 2.737 | Acc: 52.344,85.156,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.063 | Acc: 50.446,74.368,89.583,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.068 | Acc: 50.191,74.181,89.958,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.089 | Acc: 49.834,74.155,90.279,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.083 | Acc: 49.913,73.891,90.220,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.099 | Acc: 49.482,73.817,90.238,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.093 | Acc: 49.361,73.870,90.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.113 | Acc: 49.368,73.626,90.126,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.122 | Acc: 49.190,73.481,89.999,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.115 | Acc: 49.340,73.571,89.999,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.115 | Acc: 49.262,73.527,90.026,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.123 | Acc: 49.254,73.367,89.985,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.131 | Acc: 49.134,73.259,89.899,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.132 | Acc: 49.165,73.213,89.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.136 | Acc: 49.152,73.168,89.766,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.138 | Acc: 49.164,73.201,89.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.151 | Acc: 49.104,73.094,89.535,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.162 | Acc: 49.097,72.895,89.459,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.169 | Acc: 49.089,72.821,89.294,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.176 | Acc: 49.124,72.759,89.202,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.876 | Acc: 42.969,60.938,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.133 | Acc: 42.485,59.821,67.746,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.135 | Acc: 42.092,59.546,67.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.132 | Acc: 41.816,59.964,67.175,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 75
Batch: 0 | Loss: 3.092 | Acc: 47.656,73.438,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.039 | Acc: 49.182,74.554,90.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.055 | Acc: 49.390,74.790,90.892,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.042 | Acc: 49.782,74.539,91.124,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.028 | Acc: 50.145,74.797,91.184,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.038 | Acc: 50.085,74.683,91.166,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.035 | Acc: 49.910,74.729,91.284,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.052 | Acc: 49.917,74.451,91.107,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.053 | Acc: 49.835,74.442,90.897,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.072 | Acc: 49.642,74.115,90.785,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.091 | Acc: 49.366,73.803,90.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.094 | Acc: 49.385,73.795,90.388,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.094 | Acc: 49.546,73.762,90.356,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.103 | Acc: 49.527,73.611,90.284,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.107 | Acc: 49.566,73.613,90.222,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.110 | Acc: 49.618,73.474,90.179,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.119 | Acc: 49.603,73.372,90.058,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.126 | Acc: 49.700,73.383,89.935,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.133 | Acc: 49.714,73.323,89.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.148 | Acc: 49.602,73.122,89.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.140 | Acc: 35.938,60.938,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.093 | Acc: 42.039,59.152,68.973,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.173 | Acc: 41.692,58.498,67.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.220 | Acc: 41.419,58.312,67.418,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 76
Batch: 0 | Loss: 3.007 | Acc: 57.812,78.906,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.039 | Acc: 50.000,73.884,90.327,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.037 | Acc: 50.210,73.666,90.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.039 | Acc: 50.154,74.014,90.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.052 | Acc: 49.836,74.084,90.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.037 | Acc: 50.224,74.250,90.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.037 | Acc: 50.065,74.154,90.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.037 | Acc: 50.100,74.069,90.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.059 | Acc: 49.772,73.806,90.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.072 | Acc: 49.702,73.623,90.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.072 | Acc: 49.751,73.577,90.310,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.088 | Acc: 49.548,73.462,90.165,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.096 | Acc: 49.588,73.347,90.165,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.106 | Acc: 49.488,73.327,90.053,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.108 | Acc: 49.530,73.279,89.869,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.120 | Acc: 49.421,73.095,89.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.126 | Acc: 49.292,73.063,89.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.138 | Acc: 49.255,72.945,89.599,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.142 | Acc: 49.243,72.892,89.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.146 | Acc: 49.284,72.837,89.432,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.661 | Acc: 46.094,66.406,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.301 | Acc: 40.699,59.040,66.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.333 | Acc: 40.758,58.327,66.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.342 | Acc: 40.702,58.145,66.009,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 77
Batch: 0 | Loss: 3.033 | Acc: 47.656,78.906,96.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.008 | Acc: 51.302,74.182,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.988 | Acc: 50.762,74.486,91.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.983 | Acc: 50.666,74.923,91.073,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.976 | Acc: 50.887,74.865,91.300,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.001 | Acc: 50.688,74.845,90.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.012 | Acc: 50.426,74.496,91.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.026 | Acc: 50.305,74.291,90.896,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.037 | Acc: 50.175,74.204,90.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.052 | Acc: 50.065,73.981,90.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.059 | Acc: 50.031,73.861,90.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.067 | Acc: 49.996,73.780,90.565,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.078 | Acc: 49.796,73.668,90.512,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.092 | Acc: 49.770,73.440,90.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.098 | Acc: 49.836,73.357,90.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.112 | Acc: 49.777,73.230,90.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.120 | Acc: 49.681,73.272,89.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.126 | Acc: 49.668,73.195,89.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.136 | Acc: 49.587,73.124,89.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.146 | Acc: 49.623,72.952,89.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.858 | Acc: 42.969,60.156,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.337 | Acc: 39.583,58.371,65.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.413 | Acc: 38.910,57.679,64.958,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.411 | Acc: 38.998,57.620,65.074,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 78
Batch: 0 | Loss: 3.517 | Acc: 45.312,71.875,85.938,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.063 | Acc: 50.484,76.042,90.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.065 | Acc: 50.114,75.000,90.434,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.037 | Acc: 50.499,75.141,90.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.022 | Acc: 50.627,74.797,90.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.036 | Acc: 50.549,74.567,90.710,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.075 | Acc: 50.168,74.251,90.322,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.067 | Acc: 50.288,74.208,90.459,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.067 | Acc: 50.296,74.165,90.475,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.065 | Acc: 50.294,74.193,90.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.069 | Acc: 50.109,74.308,90.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.074 | Acc: 50.117,74.240,90.416,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.083 | Acc: 49.984,74.060,90.421,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.083 | Acc: 50.135,73.952,90.356,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.088 | Acc: 50.078,73.863,90.269,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.099 | Acc: 50.003,73.770,90.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.107 | Acc: 49.954,73.652,90.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.118 | Acc: 49.853,73.531,89.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.132 | Acc: 49.732,73.347,89.766,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.136 | Acc: 49.631,73.300,89.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.651 | Acc: 46.875,63.281,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.235 | Acc: 41.667,59.040,66.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.310 | Acc: 42.168,58.175,65.187,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.327 | Acc: 41.995,58.197,64.921,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 79
Batch: 0 | Loss: 3.363 | Acc: 42.969,67.188,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.046 | Acc: 50.521,74.479,89.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.032 | Acc: 50.305,75.343,90.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.043 | Acc: 50.179,75.141,90.177,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.013 | Acc: 50.608,75.521,90.519,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.029 | Acc: 50.487,74.969,90.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.039 | Acc: 50.245,75.006,90.354,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.040 | Acc: 50.227,74.839,90.359,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.043 | Acc: 50.180,74.869,90.431,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.054 | Acc: 50.022,74.616,90.375,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.058 | Acc: 50.120,74.433,90.279,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.057 | Acc: 50.113,74.406,90.279,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.059 | Acc: 50.055,74.287,90.249,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.070 | Acc: 50.054,74.165,90.206,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.075 | Acc: 49.911,74.105,90.172,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.086 | Acc: 49.813,73.923,90.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.095 | Acc: 49.798,73.807,89.909,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.098 | Acc: 49.865,73.719,89.880,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.106 | Acc: 49.866,73.684,89.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.115 | Acc: 49.776,73.561,89.700,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.045 | Acc: 42.188,54.688,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.323 | Acc: 42.150,58.557,65.030,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.410 | Acc: 41.578,57.641,64.901,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.416 | Acc: 41.253,57.620,64.985,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 80
Batch: 0 | Loss: 3.479 | Acc: 42.188,71.094,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.030 | Acc: 50.186,74.628,90.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.954 | Acc: 51.315,75.915,91.387,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.941 | Acc: 51.191,75.961,91.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.974 | Acc: 50.849,75.820,91.011,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.011 | Acc: 50.410,75.394,90.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.016 | Acc: 50.174,75.271,90.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.032 | Acc: 49.922,74.900,90.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.034 | Acc: 49.956,74.719,90.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.045 | Acc: 50.030,74.508,90.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.054 | Acc: 50.074,74.320,90.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.061 | Acc: 50.007,74.243,90.583,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.056 | Acc: 50.075,74.222,90.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.062 | Acc: 49.916,74.126,90.445,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.077 | Acc: 49.875,73.866,90.361,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.087 | Acc: 49.743,73.650,90.269,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.094 | Acc: 49.735,73.542,90.177,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.098 | Acc: 49.780,73.495,90.050,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.104 | Acc: 49.745,73.450,89.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.113 | Acc: 49.768,73.384,89.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.702 | Acc: 39.844,63.281,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.034 | Acc: 43.862,59.859,67.113,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.083 | Acc: 43.864,59.013,67.092,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.101 | Acc: 43.468,58.658,66.547,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 81
Batch: 0 | Loss: 2.661 | Acc: 60.156,81.250,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.026 | Acc: 49.926,74.628,90.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.030 | Acc: 49.981,74.352,90.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.008 | Acc: 50.371,74.629,90.881,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.967 | Acc: 51.148,74.971,91.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.959 | Acc: 51.454,75.062,91.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.966 | Acc: 51.072,75.116,91.232,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.982 | Acc: 50.720,74.906,90.957,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.993 | Acc: 50.616,74.655,90.974,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.014 | Acc: 50.419,74.512,90.724,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.031 | Acc: 50.237,74.262,90.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.047 | Acc: 50.180,74.137,90.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.057 | Acc: 50.136,73.966,90.265,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.070 | Acc: 49.889,73.713,90.191,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.072 | Acc: 49.900,73.649,90.108,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.080 | Acc: 49.811,73.583,90.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.085 | Acc: 49.822,73.457,90.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.097 | Acc: 49.737,73.325,89.880,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.107 | Acc: 49.678,73.245,89.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.113 | Acc: 49.674,73.226,89.698,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.885 | Acc: 43.750,61.719,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.078 | Acc: 42.894,60.528,67.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.132 | Acc: 42.092,60.328,66.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.124 | Acc: 42.059,60.899,67.008,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 82
Batch: 0 | Loss: 2.869 | Acc: 50.000,76.562,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.996 | Acc: 51.004,74.963,89.993,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.992 | Acc: 50.991,74.981,90.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.040 | Acc: 50.282,74.283,90.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.034 | Acc: 50.588,74.392,90.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.040 | Acc: 50.541,74.358,90.756,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 3.032 | Acc: 50.517,74.471,90.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.053 | Acc: 50.161,74.368,90.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.067 | Acc: 50.097,74.063,90.499,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.080 | Acc: 49.974,73.865,90.293,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.087 | Acc: 49.907,73.803,90.205,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.087 | Acc: 49.926,73.844,90.236,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.095 | Acc: 49.919,73.781,90.168,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.097 | Acc: 49.904,73.806,90.089,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.097 | Acc: 49.875,73.807,90.038,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.101 | Acc: 49.958,73.754,89.919,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.103 | Acc: 49.910,73.708,89.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.106 | Acc: 49.947,73.667,89.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.113 | Acc: 50.013,73.598,89.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.116 | Acc: 50.121,73.522,89.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.107 | Acc: 40.625,55.469,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.344 | Acc: 39.844,57.961,66.369,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.321 | Acc: 40.587,58.537,66.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.323 | Acc: 40.420,58.747,65.881,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 83
Batch: 0 | Loss: 2.654 | Acc: 51.562,82.031,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.866 | Acc: 52.158,76.376,90.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.910 | Acc: 51.620,75.553,91.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.932 | Acc: 51.230,75.435,90.945,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.967 | Acc: 51.138,75.000,90.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.968 | Acc: 51.106,74.946,91.058,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.990 | Acc: 50.762,74.587,90.877,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.993 | Acc: 50.582,74.485,90.902,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.001 | Acc: 50.529,74.568,90.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.014 | Acc: 50.449,74.439,90.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.019 | Acc: 50.509,74.526,90.804,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.022 | Acc: 50.477,74.494,90.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.022 | Acc: 50.473,74.514,90.696,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.030 | Acc: 50.362,74.383,90.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.044 | Acc: 50.222,74.219,90.475,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.052 | Acc: 50.244,74.118,90.420,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.072 | Acc: 50.039,73.839,90.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.079 | Acc: 50.011,73.871,90.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.086 | Acc: 49.948,73.836,90.084,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.093 | Acc: 49.873,73.768,90.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.672 | Acc: 44.531,61.719,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.299 | Acc: 41.592,59.115,67.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.260 | Acc: 41.787,59.337,66.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.254 | Acc: 41.470,59.452,66.483,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 84
Batch: 0 | Loss: 2.875 | Acc: 42.969,72.656,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.960 | Acc: 50.856,75.260,91.220,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.978 | Acc: 50.724,75.267,91.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.981 | Acc: 50.589,75.179,91.368,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 3.002 | Acc: 50.627,74.894,91.146,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 3.009 | Acc: 50.750,74.845,90.942,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.998 | Acc: 50.620,75.116,91.148,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 3.014 | Acc: 50.460,75.022,91.146,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.020 | Acc: 50.311,74.888,91.028,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.020 | Acc: 50.341,74.689,90.992,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.022 | Acc: 50.412,74.596,91.010,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.036 | Acc: 50.283,74.452,90.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.045 | Acc: 50.233,74.352,90.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.061 | Acc: 50.057,74.165,90.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.069 | Acc: 50.078,74.030,90.544,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.072 | Acc: 50.078,74.042,90.451,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.079 | Acc: 50.090,73.944,90.294,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.086 | Acc: 50.002,73.891,90.181,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.089 | Acc: 50.024,73.814,90.084,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.097 | Acc: 49.992,73.735,89.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.863 | Acc: 39.844,65.625,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.129 | Acc: 42.671,61.272,66.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.160 | Acc: 42.569,60.537,66.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.204 | Acc: 42.213,60.003,65.971,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 85
Batch: 0 | Loss: 2.800 | Acc: 49.219,75.000,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.921 | Acc: 51.302,75.818,90.737,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.939 | Acc: 51.200,75.915,91.178,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.924 | Acc: 50.961,75.884,91.176,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.929 | Acc: 50.608,75.656,91.426,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.951 | Acc: 50.472,75.379,91.213,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.952 | Acc: 50.271,75.400,91.238,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.975 | Acc: 50.388,75.155,91.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.992 | Acc: 50.320,75.087,90.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.996 | Acc: 50.337,75.017,90.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.004 | Acc: 50.385,74.949,90.718,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.010 | Acc: 50.336,74.912,90.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.019 | Acc: 50.327,74.689,90.550,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.025 | Acc: 50.371,74.593,90.472,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.038 | Acc: 50.325,74.380,90.289,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.054 | Acc: 50.210,74.182,90.142,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.066 | Acc: 50.161,74.002,89.931,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.077 | Acc: 50.154,73.871,89.770,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.088 | Acc: 50.024,73.708,89.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.087 | Acc: 50.072,73.677,89.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.885 | Acc: 40.625,59.375,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.253 | Acc: 42.076,57.961,65.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.262 | Acc: 42.321,58.003,65.949,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.231 | Acc: 42.290,58.607,65.958,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 86
Batch: 0 | Loss: 3.123 | Acc: 48.438,77.344,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.989 | Acc: 50.074,74.554,91.295,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.938 | Acc: 51.372,75.324,90.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.943 | Acc: 50.884,75.435,91.048,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.932 | Acc: 50.907,75.685,91.146,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.951 | Acc: 50.789,75.340,91.074,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.956 | Acc: 50.775,75.155,90.980,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.960 | Acc: 50.809,75.271,90.908,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.979 | Acc: 50.626,74.981,90.746,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.984 | Acc: 50.665,74.875,90.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.003 | Acc: 50.505,74.767,90.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.010 | Acc: 50.456,74.678,90.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.027 | Acc: 50.327,74.575,90.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.035 | Acc: 50.476,74.449,90.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.047 | Acc: 50.348,74.336,90.177,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.061 | Acc: 50.273,74.099,90.036,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.070 | Acc: 50.241,73.975,89.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.080 | Acc: 50.220,73.907,89.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.083 | Acc: 50.171,73.872,89.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.093 | Acc: 50.111,73.723,89.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.133 | Acc: 46.875,57.031,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.172 | Acc: 43.564,58.668,67.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.216 | Acc: 43.464,57.927,66.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.255 | Acc: 43.071,57.633,65.920,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 87
Batch: 0 | Loss: 3.089 | Acc: 44.531,75.000,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.010 | Acc: 50.409,75.074,91.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.975 | Acc: 50.762,75.743,91.292,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.954 | Acc: 50.717,75.730,91.765,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.954 | Acc: 50.936,75.444,91.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.955 | Acc: 50.936,75.364,91.917,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.950 | Acc: 50.988,75.316,91.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.963 | Acc: 50.931,75.061,91.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.968 | Acc: 51.014,75.078,91.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.985 | Acc: 50.751,74.931,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.994 | Acc: 50.630,74.848,91.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.995 | Acc: 50.718,74.926,91.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.008 | Acc: 50.489,74.861,91.040,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.022 | Acc: 50.320,74.632,90.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.031 | Acc: 50.378,74.561,90.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.037 | Acc: 50.293,74.507,90.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.048 | Acc: 50.265,74.333,90.423,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.053 | Acc: 50.321,74.294,90.323,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.059 | Acc: 50.344,74.238,90.255,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.072 | Acc: 50.262,74.075,90.153,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.858 | Acc: 39.844,63.281,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.294 | Acc: 42.001,57.068,66.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.288 | Acc: 42.245,56.612,66.178,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.268 | Acc: 42.392,57.147,65.932,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 88
Batch: 0 | Loss: 2.846 | Acc: 55.469,73.438,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.003 | Acc: 51.190,74.554,91.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.985 | Acc: 51.258,74.905,90.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.996 | Acc: 50.346,74.718,91.048,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.999 | Acc: 50.492,74.778,91.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.989 | Acc: 50.657,74.783,91.027,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.996 | Acc: 50.749,74.761,91.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.995 | Acc: 50.903,74.540,90.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 3.010 | Acc: 50.767,74.398,90.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 3.030 | Acc: 50.466,74.240,90.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.032 | Acc: 50.358,74.262,90.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.036 | Acc: 50.385,74.194,90.424,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.038 | Acc: 50.308,74.131,90.324,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.040 | Acc: 50.407,74.165,90.200,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.045 | Acc: 50.278,74.074,90.211,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.053 | Acc: 50.197,73.936,90.205,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.060 | Acc: 50.139,73.844,90.080,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.069 | Acc: 50.087,73.742,90.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.080 | Acc: 49.978,73.621,89.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.090 | Acc: 49.902,73.509,89.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.646 | Acc: 40.625,62.500,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.474 | Acc: 39.509,58.222,65.476,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.448 | Acc: 39.558,58.537,65.244,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.466 | Acc: 39.677,58.837,65.074,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 89
Batch: 0 | Loss: 3.206 | Acc: 44.531,77.344,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.989 | Acc: 51.265,74.554,91.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.937 | Acc: 51.639,75.324,91.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.958 | Acc: 51.165,75.128,91.432,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.952 | Acc: 51.283,75.048,91.397,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.956 | Acc: 51.176,75.402,91.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.954 | Acc: 51.188,75.452,91.355,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.958 | Acc: 51.086,75.244,91.351,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.972 | Acc: 51.009,75.010,91.266,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.975 | Acc: 50.911,75.026,91.147,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.982 | Acc: 50.890,74.918,91.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.994 | Acc: 50.916,74.848,90.950,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.997 | Acc: 50.875,74.812,90.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.998 | Acc: 50.916,74.808,90.918,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.009 | Acc: 50.837,74.650,90.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.022 | Acc: 50.613,74.525,90.711,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.029 | Acc: 50.567,74.428,90.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.037 | Acc: 50.518,74.281,90.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.046 | Acc: 50.500,74.173,90.380,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.055 | Acc: 50.482,74.016,90.266,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.619 | Acc: 48.438,67.188,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.084 | Acc: 42.671,61.049,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.093 | Acc: 42.302,60.728,66.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.140 | Acc: 42.136,60.323,66.765,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 90
Batch: 0 | Loss: 3.024 | Acc: 47.656,73.438,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.939 | Acc: 50.223,75.595,90.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.952 | Acc: 50.191,75.305,91.025,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.964 | Acc: 50.435,75.243,91.022,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.951 | Acc: 50.559,75.280,91.098,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.978 | Acc: 50.333,74.683,90.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.966 | Acc: 50.323,74.955,90.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.975 | Acc: 50.255,74.884,90.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.985 | Acc: 50.238,74.879,90.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.998 | Acc: 50.393,74.702,90.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.008 | Acc: 50.299,74.499,90.508,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.016 | Acc: 50.332,74.367,90.424,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.024 | Acc: 50.408,74.235,90.346,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.033 | Acc: 50.380,74.129,90.212,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.041 | Acc: 50.331,74.038,90.097,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.047 | Acc: 50.291,73.993,89.955,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.060 | Acc: 50.226,73.936,89.873,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.067 | Acc: 50.133,73.827,89.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.075 | Acc: 50.056,73.652,89.762,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.079 | Acc: 50.115,73.632,89.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.666 | Acc: 47.656,71.094,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.132 | Acc: 41.332,61.161,67.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.138 | Acc: 41.578,60.747,67.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.160 | Acc: 41.509,60.873,67.111,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 91
Batch: 0 | Loss: 2.668 | Acc: 49.219,82.812,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.873 | Acc: 52.455,76.153,91.109,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.870 | Acc: 52.268,77.077,91.349,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.873 | Acc: 52.100,76.857,91.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.900 | Acc: 52.006,76.543,91.319,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.944 | Acc: 51.562,75.766,91.089,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.958 | Acc: 51.621,75.426,90.948,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.947 | Acc: 51.574,75.488,90.980,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.962 | Acc: 51.461,75.233,90.882,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.977 | Acc: 51.498,75.052,90.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.984 | Acc: 51.524,74.876,90.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.987 | Acc: 51.421,74.919,90.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.000 | Acc: 51.287,74.809,90.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.008 | Acc: 51.290,74.686,90.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.011 | Acc: 51.182,74.580,90.436,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.018 | Acc: 51.077,74.554,90.378,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.031 | Acc: 50.908,74.409,90.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.036 | Acc: 50.827,74.292,90.252,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.043 | Acc: 50.766,74.247,90.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.052 | Acc: 50.673,74.157,90.030,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.676 | Acc: 50.000,60.156,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.099 | Acc: 44.159,59.896,67.001,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.164 | Acc: 43.807,59.261,66.216,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.203 | Acc: 43.507,59.349,65.791,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 92
Batch: 0 | Loss: 3.059 | Acc: 50.781,73.438,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.877 | Acc: 52.679,76.749,90.737,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.927 | Acc: 51.524,76.448,90.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.916 | Acc: 51.498,76.370,91.086,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.943 | Acc: 51.389,75.955,91.146,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.960 | Acc: 51.176,75.580,90.996,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.957 | Acc: 51.130,75.471,90.967,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.961 | Acc: 51.169,75.305,90.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.980 | Acc: 50.844,75.102,90.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.980 | Acc: 50.928,75.060,90.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.985 | Acc: 50.875,75.031,90.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.000 | Acc: 50.650,74.809,90.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.004 | Acc: 50.558,74.676,90.700,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.014 | Acc: 50.533,74.518,90.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.026 | Acc: 50.487,74.316,90.403,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.027 | Acc: 50.568,74.341,90.345,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.033 | Acc: 50.596,74.275,90.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.036 | Acc: 50.502,74.246,90.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.043 | Acc: 50.472,74.256,90.147,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.052 | Acc: 50.437,74.092,90.010,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.862 | Acc: 42.188,67.188,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.310 | Acc: 41.964,58.445,66.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.376 | Acc: 41.349,57.565,65.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.365 | Acc: 41.265,57.838,65.625,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 93
Batch: 0 | Loss: 3.244 | Acc: 50.781,69.531,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.984 | Acc: 51.190,76.265,91.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.959 | Acc: 51.086,75.819,91.940,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.956 | Acc: 51.076,75.602,91.393,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.959 | Acc: 51.302,75.434,91.194,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.960 | Acc: 51.129,75.302,91.043,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.981 | Acc: 50.949,74.748,90.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.983 | Acc: 51.080,74.717,90.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.984 | Acc: 51.048,74.670,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.995 | Acc: 50.837,74.508,90.534,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.997 | Acc: 50.886,74.475,90.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.011 | Acc: 50.732,74.364,90.441,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.020 | Acc: 50.700,74.216,90.395,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.034 | Acc: 50.494,74.039,90.275,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.039 | Acc: 50.423,73.916,90.261,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.037 | Acc: 50.628,73.925,90.262,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.034 | Acc: 50.679,73.968,90.265,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.040 | Acc: 50.641,73.969,90.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.046 | Acc: 50.578,73.905,90.069,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.056 | Acc: 50.531,73.761,89.985,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.914 | Acc: 42.188,64.062,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.115 | Acc: 42.374,60.677,67.150,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.191 | Acc: 42.988,59.546,65.911,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.212 | Acc: 43.058,59.721,65.920,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 94
Batch: 0 | Loss: 3.040 | Acc: 42.188,72.656,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.036 | Acc: 51.004,74.554,90.402,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.974 | Acc: 51.200,74.867,90.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.926 | Acc: 51.652,75.295,91.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.926 | Acc: 51.553,75.666,91.127,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.934 | Acc: 51.315,75.743,91.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.927 | Acc: 51.388,75.885,91.032,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.933 | Acc: 51.191,75.781,91.212,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.937 | Acc: 51.174,75.655,91.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.941 | Acc: 51.127,75.570,91.199,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.948 | Acc: 51.057,75.408,91.126,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.956 | Acc: 50.990,75.283,91.099,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.975 | Acc: 50.674,75.052,90.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.980 | Acc: 50.596,75.006,90.882,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.989 | Acc: 50.595,74.889,90.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.995 | Acc: 50.602,74.805,90.726,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.009 | Acc: 50.475,74.586,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.012 | Acc: 50.534,74.546,90.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.014 | Acc: 50.493,74.515,90.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.024 | Acc: 50.459,74.407,90.416,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.917 | Acc: 45.312,60.938,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.267 | Acc: 41.332,58.222,66.890,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.328 | Acc: 40.835,57.927,66.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.306 | Acc: 40.881,58.145,66.265,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 95
Batch: 0 | Loss: 3.134 | Acc: 46.875,71.094,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.920 | Acc: 50.446,75.484,90.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.958 | Acc: 50.514,75.629,91.159,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.940 | Acc: 51.025,75.948,91.522,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.950 | Acc: 50.926,75.868,91.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.963 | Acc: 50.812,75.665,91.105,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.973 | Acc: 51.001,75.381,90.857,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.960 | Acc: 51.136,75.410,90.946,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.963 | Acc: 51.058,75.315,90.785,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.977 | Acc: 50.919,74.991,90.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.998 | Acc: 50.595,74.650,90.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.005 | Acc: 50.559,74.502,90.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.016 | Acc: 50.444,74.326,90.589,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.015 | Acc: 50.473,74.389,90.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.023 | Acc: 50.420,74.324,90.419,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.023 | Acc: 50.522,74.310,90.397,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.020 | Acc: 50.611,74.246,90.369,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.029 | Acc: 50.570,74.150,90.291,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.028 | Acc: 50.634,74.178,90.203,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.037 | Acc: 50.545,74.118,90.121,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.897 | Acc: 42.969,53.906,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.096 | Acc: 43.229,58.631,66.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.135 | Acc: 42.950,58.765,66.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.167 | Acc: 42.853,58.735,65.971,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 96
Batch: 0 | Loss: 2.928 | Acc: 48.438,78.906,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.985 | Acc: 49.665,75.484,90.104,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.002 | Acc: 49.657,75.133,90.263,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.004 | Acc: 49.808,74.859,90.164,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.989 | Acc: 50.260,75.154,90.230,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.980 | Acc: 50.356,75.124,90.300,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.980 | Acc: 50.588,75.077,90.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.974 | Acc: 50.632,75.122,90.237,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.981 | Acc: 50.529,75.131,90.339,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.992 | Acc: 50.423,75.035,90.357,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 3.002 | Acc: 50.439,74.872,90.267,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 3.008 | Acc: 50.474,74.816,90.194,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 3.007 | Acc: 50.441,74.848,90.239,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 3.013 | Acc: 50.467,74.811,90.191,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.008 | Acc: 50.470,74.886,90.200,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.013 | Acc: 50.506,74.795,90.142,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.021 | Acc: 50.436,74.698,89.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.024 | Acc: 50.433,74.643,89.970,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.030 | Acc: 50.411,74.539,89.919,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.033 | Acc: 50.468,74.409,89.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.840 | Acc: 44.531,58.594,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.102 | Acc: 42.969,59.821,66.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.108 | Acc: 42.721,59.794,65.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.122 | Acc: 42.367,59.529,65.920,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 97
Batch: 0 | Loss: 2.813 | Acc: 52.344,72.656,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.922 | Acc: 52.567,76.376,90.476,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.924 | Acc: 52.363,76.486,90.396,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.917 | Acc: 52.408,76.127,90.548,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.924 | Acc: 52.517,76.022,90.712,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.932 | Acc: 52.514,75.781,90.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.955 | Acc: 52.144,75.394,90.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.949 | Acc: 52.050,75.327,90.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.954 | Acc: 51.931,75.349,90.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.959 | Acc: 51.770,75.242,90.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.957 | Acc: 51.765,75.190,90.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.958 | Acc: 51.651,75.078,90.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.973 | Acc: 51.618,74.935,90.463,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.979 | Acc: 51.494,74.743,90.359,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.994 | Acc: 51.259,74.591,90.350,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.006 | Acc: 51.129,74.496,90.275,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.014 | Acc: 50.988,74.469,90.221,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.020 | Acc: 50.921,74.416,90.148,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.029 | Acc: 50.872,74.314,90.030,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.034 | Acc: 50.855,74.213,89.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.812 | Acc: 42.188,63.281,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.025 | Acc: 42.820,60.454,68.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.090 | Acc: 42.893,60.118,67.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.134 | Acc: 42.392,59.874,66.842,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 98
Batch: 0 | Loss: 2.636 | Acc: 46.875,82.031,96.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.863 | Acc: 52.976,77.530,91.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.898 | Acc: 51.734,76.543,91.711,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.876 | Acc: 51.665,76.691,91.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.923 | Acc: 50.993,75.974,91.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.932 | Acc: 50.874,75.511,91.553,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.933 | Acc: 50.897,75.465,91.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.935 | Acc: 51.025,75.310,91.367,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.944 | Acc: 50.975,75.378,91.202,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.962 | Acc: 50.807,75.194,91.100,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.959 | Acc: 50.840,75.225,91.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.971 | Acc: 50.834,75.120,91.010,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.978 | Acc: 50.820,75.075,90.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.992 | Acc: 50.778,74.880,90.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.995 | Acc: 50.823,74.794,90.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.001 | Acc: 50.810,74.655,90.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.005 | Acc: 50.798,74.562,90.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.006 | Acc: 50.724,74.542,90.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.008 | Acc: 50.703,74.513,90.571,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.015 | Acc: 50.712,74.430,90.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.285 | Acc: 47.656,68.750,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.224 | Acc: 41.332,59.412,67.336,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.304 | Acc: 41.063,59.127,66.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.304 | Acc: 40.394,58.863,66.124,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 99
Batch: 0 | Loss: 2.714 | Acc: 53.125,78.906,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.887 | Acc: 52.381,75.893,91.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.869 | Acc: 52.153,75.781,91.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.879 | Acc: 51.844,75.717,91.432,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.860 | Acc: 51.910,76.080,91.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.856 | Acc: 51.965,76.183,91.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.872 | Acc: 51.911,76.007,91.503,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.892 | Acc: 51.729,75.637,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.914 | Acc: 51.567,75.340,91.154,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.917 | Acc: 51.463,75.281,91.156,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.939 | Acc: 51.228,75.089,91.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.948 | Acc: 51.135,74.975,90.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.965 | Acc: 51.031,74.802,90.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.980 | Acc: 50.877,74.626,90.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.987 | Acc: 50.753,74.527,90.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.998 | Acc: 50.703,74.512,90.545,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.000 | Acc: 50.672,74.538,90.464,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.009 | Acc: 50.605,74.450,90.320,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.015 | Acc: 50.643,74.424,90.184,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.025 | Acc: 50.547,74.340,90.049,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.966 | Acc: 45.312,61.719,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.546 | Acc: 40.737,58.296,66.146,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.603 | Acc: 39.958,57.755,65.320,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.591 | Acc: 39.921,57.608,65.138,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 100
Batch: 0 | Loss: 2.954 | Acc: 53.125,75.000,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.899 | Acc: 50.967,75.707,91.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.895 | Acc: 51.620,75.419,91.349,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.893 | Acc: 51.460,75.448,91.522,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.907 | Acc: 51.389,75.164,91.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.927 | Acc: 51.346,74.992,91.360,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.946 | Acc: 51.162,74.877,91.180,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.945 | Acc: 51.308,74.939,91.118,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.949 | Acc: 51.213,74.913,91.067,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.948 | Acc: 51.239,74.991,91.078,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.969 | Acc: 50.937,74.720,90.975,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.974 | Acc: 50.859,74.799,90.940,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.985 | Acc: 50.814,74.695,90.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.993 | Acc: 50.781,74.482,90.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 3.001 | Acc: 50.762,74.491,90.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 3.005 | Acc: 50.760,74.494,90.423,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 3.012 | Acc: 50.745,74.472,90.352,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 3.017 | Acc: 50.722,74.400,90.252,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.019 | Acc: 50.736,74.349,90.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.026 | Acc: 50.609,74.276,90.164,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.145 | Acc: 42.188,62.500,64.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.106 | Acc: 42.039,61.421,67.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.247 | Acc: 41.425,60.118,65.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.266 | Acc: 41.509,59.810,65.612,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 101
Batch: 0 | Loss: 2.804 | Acc: 50.781,81.250,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.840 | Acc: 51.972,77.493,91.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.864 | Acc: 52.077,77.039,91.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.837 | Acc: 52.049,77.088,91.470,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.873 | Acc: 51.736,76.331,91.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.869 | Acc: 51.802,76.462,91.166,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.878 | Acc: 51.550,76.349,91.251,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.887 | Acc: 51.535,76.363,91.157,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.911 | Acc: 51.320,76.038,91.003,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.914 | Acc: 51.217,75.898,91.026,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.931 | Acc: 51.011,75.630,90.959,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.940 | Acc: 50.969,75.622,90.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.948 | Acc: 51.008,75.454,90.709,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.954 | Acc: 51.075,75.290,90.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.964 | Acc: 50.956,75.214,90.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.973 | Acc: 50.906,75.112,90.560,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.981 | Acc: 50.862,74.983,90.496,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.989 | Acc: 50.834,74.950,90.435,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.998 | Acc: 50.792,74.857,90.324,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.006 | Acc: 50.703,74.795,90.242,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.777 | Acc: 42.969,63.281,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.400 | Acc: 41.667,57.254,65.439,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.405 | Acc: 41.845,57.317,65.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.418 | Acc: 41.637,57.351,65.074,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 102
Batch: 0 | Loss: 3.079 | Acc: 53.906,75.781,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.007 | Acc: 51.153,74.665,90.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.920 | Acc: 51.925,75.362,90.911,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.929 | Acc: 52.100,75.461,90.804,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.934 | Acc: 51.958,75.087,91.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.925 | Acc: 51.903,75.364,91.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.921 | Acc: 51.872,75.426,91.206,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.925 | Acc: 51.762,75.360,91.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.921 | Acc: 51.660,75.471,91.149,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.932 | Acc: 51.416,75.505,91.039,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.945 | Acc: 51.240,75.350,90.932,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.952 | Acc: 51.297,75.293,90.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.959 | Acc: 51.216,75.276,90.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.965 | Acc: 51.194,75.171,90.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.975 | Acc: 51.176,75.033,90.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.987 | Acc: 51.134,74.844,90.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.993 | Acc: 51.166,74.774,90.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.996 | Acc: 51.120,74.725,90.433,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.005 | Acc: 51.032,74.572,90.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.010 | Acc: 50.990,74.494,90.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.431 | Acc: 42.188,60.938,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.718 | Acc: 39.435,55.655,65.365,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.705 | Acc: 39.520,55.316,64.463,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.673 | Acc: 39.203,55.494,64.408,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 103
Batch: 0 | Loss: 2.889 | Acc: 51.562,75.000,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.937 | Acc: 50.930,76.488,91.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.869 | Acc: 51.677,77.077,91.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.883 | Acc: 51.242,76.562,91.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.881 | Acc: 51.196,76.427,91.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.906 | Acc: 50.920,76.067,91.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.913 | Acc: 50.665,76.072,91.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.911 | Acc: 50.920,76.025,91.739,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.920 | Acc: 50.670,75.888,91.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.937 | Acc: 50.634,75.622,91.419,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.948 | Acc: 50.494,75.583,91.247,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.951 | Acc: 50.534,75.513,91.102,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.962 | Acc: 50.509,75.324,90.995,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.967 | Acc: 50.569,75.245,90.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.975 | Acc: 50.670,75.070,90.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.977 | Acc: 50.755,75.093,90.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.985 | Acc: 50.754,74.963,90.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.995 | Acc: 50.719,74.814,90.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 3.006 | Acc: 50.682,74.658,90.508,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.010 | Acc: 50.701,74.569,90.453,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.835 | Acc: 46.875,60.938,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.074 | Acc: 43.787,59.598,67.150,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.147 | Acc: 44.264,59.089,66.254,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.144 | Acc: 44.442,59.349,66.201,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 104
Batch: 0 | Loss: 3.324 | Acc: 42.969,69.531,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.868 | Acc: 51.190,76.190,91.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.903 | Acc: 51.543,76.029,91.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.894 | Acc: 51.319,76.114,91.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.891 | Acc: 51.321,76.042,91.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.898 | Acc: 51.431,75.905,91.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.900 | Acc: 51.582,75.794,91.342,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.913 | Acc: 51.274,75.632,91.340,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.922 | Acc: 51.490,75.568,91.081,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.931 | Acc: 51.463,75.479,90.962,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.938 | Acc: 51.376,75.412,90.893,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.942 | Acc: 51.326,75.442,90.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.952 | Acc: 51.319,75.386,90.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.955 | Acc: 51.311,75.308,90.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.965 | Acc: 51.218,75.209,90.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.974 | Acc: 51.163,75.119,90.490,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.980 | Acc: 51.105,75.002,90.450,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.988 | Acc: 51.091,74.936,90.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.998 | Acc: 51.093,74.801,90.168,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 3.004 | Acc: 51.083,74.697,90.053,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.104 | Acc: 43.750,59.375,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.329 | Acc: 42.113,57.403,65.439,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.290 | Acc: 42.340,57.755,65.549,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.284 | Acc: 42.149,57.915,65.510,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 105
Batch: 0 | Loss: 3.211 | Acc: 44.531,71.875,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.913 | Acc: 51.786,76.897,90.402,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.870 | Acc: 52.287,76.753,90.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.840 | Acc: 52.357,76.908,91.189,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.841 | Acc: 52.189,76.958,91.300,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.854 | Acc: 52.235,76.547,91.476,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.877 | Acc: 51.898,76.433,91.348,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.896 | Acc: 51.707,76.208,91.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.907 | Acc: 51.596,76.121,91.202,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.922 | Acc: 51.480,75.945,91.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.926 | Acc: 51.570,75.797,91.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.931 | Acc: 51.467,75.643,91.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.948 | Acc: 51.404,75.470,90.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.956 | Acc: 51.326,75.353,90.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.960 | Acc: 51.321,75.228,90.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.958 | Acc: 51.308,75.257,90.866,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.960 | Acc: 51.317,75.226,90.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.972 | Acc: 51.285,75.080,90.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.980 | Acc: 51.234,74.890,90.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.989 | Acc: 51.249,74.787,90.514,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.216 | Acc: 41.406,63.281,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.237 | Acc: 43.713,59.524,65.960,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.360 | Acc: 43.121,58.346,65.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.377 | Acc: 42.290,57.941,65.202,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 106
Batch: 0 | Loss: 3.425 | Acc: 49.219,68.750,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.931 | Acc: 52.790,74.888,89.993,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.916 | Acc: 51.944,75.762,90.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.933 | Acc: 51.191,75.346,90.715,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.943 | Acc: 51.042,75.203,90.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.929 | Acc: 51.199,75.394,90.942,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.926 | Acc: 51.375,75.517,91.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.937 | Acc: 51.202,75.477,90.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.933 | Acc: 51.223,75.519,90.999,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.930 | Acc: 51.329,75.587,90.970,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.941 | Acc: 51.310,75.505,90.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.948 | Acc: 51.262,75.382,90.713,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.956 | Acc: 51.190,75.240,90.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.961 | Acc: 51.188,75.227,90.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.967 | Acc: 51.054,75.136,90.428,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.971 | Acc: 51.054,75.138,90.345,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.978 | Acc: 51.095,75.090,90.289,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.984 | Acc: 50.983,75.014,90.268,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.988 | Acc: 51.030,74.985,90.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.998 | Acc: 50.912,74.861,90.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.135 | Acc: 39.062,62.500,64.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.274 | Acc: 41.741,59.077,66.220,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.304 | Acc: 41.425,58.937,66.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.326 | Acc: 41.291,58.722,65.971,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 107
Batch: 0 | Loss: 2.973 | Acc: 47.656,74.219,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.894 | Acc: 51.749,75.707,91.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.873 | Acc: 51.905,75.953,92.149,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.897 | Acc: 51.524,76.165,91.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.877 | Acc: 51.939,76.437,91.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.887 | Acc: 52.058,76.477,91.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.897 | Acc: 51.737,76.343,91.451,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.910 | Acc: 51.673,76.020,91.212,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.920 | Acc: 51.616,76.004,91.144,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.937 | Acc: 51.416,75.824,90.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.941 | Acc: 51.353,75.785,90.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.945 | Acc: 51.297,75.757,90.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.953 | Acc: 51.264,75.571,90.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.964 | Acc: 51.185,75.476,90.736,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.967 | Acc: 51.229,75.428,90.708,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.973 | Acc: 51.181,75.319,90.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.976 | Acc: 51.197,75.297,90.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.981 | Acc: 51.166,75.192,90.417,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.986 | Acc: 51.158,75.119,90.341,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.997 | Acc: 51.056,74.969,90.254,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.466 | Acc: 42.188,67.188,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.122 | Acc: 43.527,60.045,67.411,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.163 | Acc: 43.369,58.803,67.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.176 | Acc: 43.699,58.619,66.739,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 108
Batch: 0 | Loss: 2.729 | Acc: 55.469,77.344,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.901 | Acc: 50.893,77.083,90.960,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.902 | Acc: 51.353,76.181,91.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.865 | Acc: 52.011,76.294,91.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.883 | Acc: 51.794,76.032,91.078,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.873 | Acc: 51.756,76.191,91.252,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.884 | Acc: 51.575,75.956,91.129,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.884 | Acc: 51.601,76.008,91.102,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.887 | Acc: 51.621,76.000,91.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.891 | Acc: 51.645,76.109,91.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.910 | Acc: 51.524,75.898,91.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.915 | Acc: 51.513,75.817,90.989,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.921 | Acc: 51.488,75.707,90.952,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.932 | Acc: 51.494,75.554,90.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.940 | Acc: 51.454,75.475,90.756,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.944 | Acc: 51.516,75.449,90.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.952 | Acc: 51.404,75.355,90.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.958 | Acc: 51.306,75.270,90.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.963 | Acc: 51.303,75.167,90.476,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.976 | Acc: 51.142,75.016,90.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.843 | Acc: 43.750,62.500,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.052 | Acc: 41.667,61.644,67.746,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.088 | Acc: 41.216,60.804,67.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.149 | Acc: 41.060,60.681,66.650,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 109
Batch: 0 | Loss: 3.435 | Acc: 44.531,68.750,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.913 | Acc: 51.339,75.967,90.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.867 | Acc: 51.963,76.505,91.025,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.874 | Acc: 52.011,76.473,91.176,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.866 | Acc: 51.881,76.254,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.867 | Acc: 51.996,76.091,91.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.882 | Acc: 51.898,76.182,91.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.886 | Acc: 51.862,76.069,91.584,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.897 | Acc: 51.723,75.980,91.571,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.899 | Acc: 51.783,75.893,91.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.912 | Acc: 51.625,75.746,91.398,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.915 | Acc: 51.718,75.735,91.392,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.924 | Acc: 51.537,75.694,91.280,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.930 | Acc: 51.595,75.614,91.218,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.942 | Acc: 51.446,75.459,91.106,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.952 | Acc: 51.446,75.356,91.053,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.965 | Acc: 51.360,75.183,90.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.970 | Acc: 51.304,75.149,90.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.981 | Acc: 51.311,74.998,90.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.984 | Acc: 51.355,74.955,90.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.078 | Acc: 44.531,60.938,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.078 | Acc: 42.188,60.119,67.113,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.188 | Acc: 40.930,59.699,66.082,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.183 | Acc: 40.740,59.926,66.073,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 110
Batch: 0 | Loss: 2.727 | Acc: 53.125,81.250,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.929 | Acc: 51.525,76.376,90.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.939 | Acc: 50.915,76.315,91.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.930 | Acc: 51.345,75.909,91.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.898 | Acc: 51.900,76.157,91.339,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.918 | Acc: 51.284,75.812,91.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.908 | Acc: 51.182,75.820,91.116,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.916 | Acc: 51.213,75.698,91.085,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.939 | Acc: 51.034,75.422,91.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.940 | Acc: 51.351,75.401,91.039,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.943 | Acc: 51.368,75.295,91.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.942 | Acc: 51.509,75.293,90.954,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.948 | Acc: 51.572,75.159,90.878,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.952 | Acc: 51.583,75.156,90.718,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.951 | Acc: 51.510,75.225,90.761,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.953 | Acc: 51.539,75.254,90.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.955 | Acc: 51.587,75.260,90.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.962 | Acc: 51.524,75.211,90.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.973 | Acc: 51.396,75.121,90.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.976 | Acc: 51.370,75.021,90.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.082 | Acc: 36.719,59.375,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.143 | Acc: 43.192,59.524,67.262,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.194 | Acc: 43.197,59.451,67.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.199 | Acc: 42.751,59.221,66.816,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 111
Batch: 0 | Loss: 2.714 | Acc: 50.000,78.906,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.887 | Acc: 51.376,76.711,91.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.882 | Acc: 51.277,76.562,91.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.864 | Acc: 51.562,76.767,91.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.855 | Acc: 51.649,76.669,91.609,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.861 | Acc: 51.601,76.562,91.553,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.873 | Acc: 51.517,76.408,91.497,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.881 | Acc: 51.635,76.352,91.345,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.901 | Acc: 51.431,76.233,91.081,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.907 | Acc: 51.334,76.092,91.009,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.919 | Acc: 51.294,75.855,90.878,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.927 | Acc: 51.205,75.767,90.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.925 | Acc: 51.430,75.869,90.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.935 | Acc: 51.455,75.718,90.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.943 | Acc: 51.365,75.553,90.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.954 | Acc: 51.282,75.343,90.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.961 | Acc: 51.190,75.229,90.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.965 | Acc: 51.230,75.174,90.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.971 | Acc: 51.195,75.095,90.404,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.975 | Acc: 51.232,75.090,90.332,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.962 | Acc: 44.531,62.500,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.079 | Acc: 43.229,61.830,68.713,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.177 | Acc: 42.473,60.766,67.245,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.190 | Acc: 42.072,60.822,66.893,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 112
Batch: 0 | Loss: 2.555 | Acc: 59.375,82.812,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.778 | Acc: 53.906,77.493,92.262,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.782 | Acc: 52.934,77.820,92.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.783 | Acc: 53.035,77.651,92.098,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.805 | Acc: 52.701,77.334,91.898,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.837 | Acc: 52.506,77.042,91.739,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.842 | Acc: 52.441,76.969,91.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.850 | Acc: 52.349,76.790,91.728,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.866 | Acc: 52.256,76.626,91.683,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.874 | Acc: 52.257,76.524,91.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.879 | Acc: 52.196,76.500,91.449,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.890 | Acc: 52.029,76.379,91.374,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.897 | Acc: 51.958,76.261,91.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.906 | Acc: 51.922,76.140,91.227,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.919 | Acc: 51.841,76.001,91.109,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.928 | Acc: 51.801,75.955,91.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.934 | Acc: 51.777,75.837,91.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.942 | Acc: 51.638,75.754,90.937,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.944 | Acc: 51.703,75.638,90.863,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.945 | Acc: 51.729,75.574,90.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.068 | Acc: 44.531,65.625,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.326 | Acc: 40.327,57.775,66.890,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.337 | Acc: 40.320,57.279,66.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.334 | Acc: 40.535,57.787,66.739,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 113
Batch: 0 | Loss: 2.786 | Acc: 45.312,75.000,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.796 | Acc: 52.083,77.827,92.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.822 | Acc: 51.429,76.677,92.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.833 | Acc: 51.165,76.703,92.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.832 | Acc: 51.379,76.601,92.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.826 | Acc: 51.485,76.555,92.450,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.856 | Acc: 51.085,76.362,92.142,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.859 | Acc: 51.191,76.225,92.049,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.865 | Acc: 51.233,76.058,91.945,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.875 | Acc: 51.252,75.945,91.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.888 | Acc: 51.232,75.832,91.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.900 | Acc: 51.184,75.799,91.463,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.906 | Acc: 51.102,75.778,91.455,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.914 | Acc: 51.194,75.739,91.295,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.916 | Acc: 51.223,75.656,91.201,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.925 | Acc: 51.230,75.563,91.051,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.936 | Acc: 51.212,75.397,90.937,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.943 | Acc: 51.141,75.364,90.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.953 | Acc: 51.086,75.322,90.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.961 | Acc: 51.060,75.213,90.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.362 | Acc: 33.594,61.719,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.275 | Acc: 40.699,59.524,66.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.345 | Acc: 40.549,58.422,66.349,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.387 | Acc: 40.177,58.107,65.945,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 114
Batch: 0 | Loss: 3.238 | Acc: 54.688,73.438,86.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.987 | Acc: 50.707,75.335,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.942 | Acc: 51.753,74.809,91.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.921 | Acc: 51.396,75.410,91.278,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.898 | Acc: 51.331,75.955,91.300,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.879 | Acc: 51.539,76.276,91.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.887 | Acc: 51.265,76.272,91.497,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.890 | Acc: 51.247,76.291,91.451,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.886 | Acc: 51.509,76.334,91.435,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.900 | Acc: 51.437,76.036,91.272,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.915 | Acc: 51.298,75.805,91.068,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.928 | Acc: 51.248,75.612,90.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.933 | Acc: 51.284,75.457,90.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.943 | Acc: 51.332,75.311,90.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.945 | Acc: 51.354,75.247,90.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.951 | Acc: 51.402,75.171,90.560,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.959 | Acc: 51.434,75.061,90.455,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.966 | Acc: 51.409,75.025,90.341,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.972 | Acc: 51.383,74.978,90.246,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.981 | Acc: 51.327,74.861,90.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.248 | Acc: 39.062,57.812,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.199 | Acc: 42.001,58.891,66.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.257 | Acc: 41.044,58.670,66.292,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.297 | Acc: 40.971,58.376,65.945,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 115
Batch: 0 | Loss: 2.858 | Acc: 51.562,75.781,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.826 | Acc: 52.604,76.823,91.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.834 | Acc: 51.982,76.886,91.387,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.816 | Acc: 51.895,76.998,91.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.827 | Acc: 51.640,76.968,91.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.832 | Acc: 51.640,76.779,91.739,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.831 | Acc: 51.627,76.788,91.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.835 | Acc: 51.817,76.596,91.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.850 | Acc: 51.713,76.368,91.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.860 | Acc: 51.830,76.329,91.536,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.872 | Acc: 51.605,76.166,91.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.879 | Acc: 51.630,76.195,91.332,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.887 | Acc: 51.511,76.028,91.293,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.902 | Acc: 51.533,75.802,91.155,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.913 | Acc: 51.543,75.653,91.059,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.928 | Acc: 51.425,75.457,90.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.931 | Acc: 51.392,75.368,90.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.939 | Acc: 51.306,75.245,90.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.947 | Acc: 51.307,75.171,90.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.954 | Acc: 51.273,75.025,90.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.808 | Acc: 42.188,62.500,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.162 | Acc: 44.271,59.449,67.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.166 | Acc: 43.216,59.566,66.883,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.202 | Acc: 43.135,58.952,66.124,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 116
Batch: 0 | Loss: 2.750 | Acc: 55.469,75.000,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.873 | Acc: 51.488,76.265,91.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.884 | Acc: 51.410,75.953,91.120,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.875 | Acc: 52.164,76.550,90.881,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.877 | Acc: 52.093,76.649,90.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.871 | Acc: 52.065,76.779,91.112,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.880 | Acc: 51.866,76.569,91.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.893 | Acc: 51.912,76.391,91.018,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.901 | Acc: 51.800,76.208,91.067,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.897 | Acc: 51.886,76.282,91.070,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.905 | Acc: 51.831,76.182,91.029,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.916 | Acc: 51.902,75.954,90.950,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.928 | Acc: 51.802,75.775,90.920,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.930 | Acc: 51.859,75.727,90.754,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.935 | Acc: 51.946,75.634,90.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.938 | Acc: 51.910,75.594,90.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.949 | Acc: 51.730,75.499,90.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.960 | Acc: 51.707,75.309,90.520,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.966 | Acc: 51.625,75.219,90.458,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.974 | Acc: 51.589,75.059,90.371,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.822 | Acc: 43.750,65.625,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.088 | Acc: 44.457,59.077,67.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.084 | Acc: 43.921,58.746,67.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.087 | Acc: 44.198,59.247,66.816,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 117
Batch: 0 | Loss: 3.345 | Acc: 47.656,68.750,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.843 | Acc: 51.451,76.637,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.866 | Acc: 51.524,76.181,91.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.905 | Acc: 50.935,75.832,91.419,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.885 | Acc: 51.485,76.032,91.377,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.868 | Acc: 51.756,76.284,91.445,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.852 | Acc: 51.827,76.543,91.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.843 | Acc: 52.078,76.596,91.534,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.847 | Acc: 51.980,76.436,91.542,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.849 | Acc: 52.111,76.420,91.501,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.857 | Acc: 52.068,76.395,91.430,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.858 | Acc: 52.086,76.361,91.343,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.867 | Acc: 52.123,76.261,91.218,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.887 | Acc: 51.964,75.958,91.038,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.899 | Acc: 51.913,75.778,90.931,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.911 | Acc: 51.858,75.613,90.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.919 | Acc: 51.801,75.538,90.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.928 | Acc: 51.565,75.483,90.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.938 | Acc: 51.588,75.331,90.545,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.946 | Acc: 51.536,75.256,90.506,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.690 | Acc: 46.094,67.969,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.907 | Acc: 45.461,61.272,68.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.980 | Acc: 44.512,60.823,67.931,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.010 | Acc: 44.160,61.040,67.802,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 118
Batch: 0 | Loss: 2.502 | Acc: 53.906,75.781,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.774 | Acc: 52.269,78.013,91.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.731 | Acc: 53.182,78.011,91.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.781 | Acc: 52.536,77.395,91.739,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.798 | Acc: 52.749,77.083,91.570,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.812 | Acc: 52.452,76.833,91.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.817 | Acc: 52.428,76.840,91.568,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.827 | Acc: 52.299,76.679,91.506,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.825 | Acc: 52.397,76.713,91.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.834 | Acc: 52.206,76.696,91.467,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.857 | Acc: 52.006,76.415,91.356,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.865 | Acc: 51.895,76.251,91.251,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.880 | Acc: 51.760,76.073,91.124,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.898 | Acc: 51.745,75.817,91.023,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.907 | Acc: 51.740,75.776,90.911,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.920 | Acc: 51.607,75.636,90.866,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.927 | Acc: 51.531,75.587,90.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.938 | Acc: 51.508,75.440,90.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.949 | Acc: 51.472,75.236,90.510,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.956 | Acc: 51.456,75.146,90.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.314 | Acc: 48.438,69.531,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.870 | Acc: 44.792,62.054,69.754,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.957 | Acc: 44.017,61.280,67.873,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.981 | Acc: 43.686,61.117,67.879,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 119
Batch: 0 | Loss: 2.993 | Acc: 56.250,72.656,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.852 | Acc: 51.637,76.414,91.146,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.814 | Acc: 52.858,77.153,91.120,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.800 | Acc: 52.280,77.472,91.522,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.807 | Acc: 52.218,77.479,91.503,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.830 | Acc: 51.841,77.390,91.453,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.843 | Acc: 51.666,76.840,91.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.864 | Acc: 51.441,76.596,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.880 | Acc: 51.431,76.504,91.338,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.879 | Acc: 51.515,76.511,91.337,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.882 | Acc: 51.500,76.543,91.282,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.883 | Acc: 51.746,76.513,91.240,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.894 | Acc: 51.673,76.358,91.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.903 | Acc: 51.571,76.233,91.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.904 | Acc: 51.785,76.173,90.928,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.915 | Acc: 51.801,75.940,90.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.923 | Acc: 51.701,75.774,90.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.926 | Acc: 51.727,75.738,90.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.937 | Acc: 51.699,75.638,90.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.941 | Acc: 51.661,75.617,90.508,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.731 | Acc: 42.969,58.594,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.206 | Acc: 42.783,58.557,67.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.238 | Acc: 42.226,58.613,66.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.255 | Acc: 42.264,58.658,66.470,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 120
Batch: 0 | Loss: 2.910 | Acc: 43.750,77.344,90.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.943 | Acc: 50.446,75.558,91.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.931 | Acc: 50.362,75.324,91.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.876 | Acc: 51.127,76.076,91.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.864 | Acc: 51.476,76.235,91.464,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.872 | Acc: 51.609,76.292,91.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.875 | Acc: 51.498,76.291,91.690,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.874 | Acc: 51.352,76.280,91.678,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.873 | Acc: 51.490,76.291,91.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.889 | Acc: 51.472,75.963,91.402,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.894 | Acc: 51.454,75.882,91.235,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.904 | Acc: 51.435,75.728,91.070,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.914 | Acc: 51.491,75.541,90.956,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.918 | Acc: 51.551,75.587,90.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.925 | Acc: 51.462,75.542,90.975,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.930 | Acc: 51.415,75.485,90.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.933 | Acc: 51.407,75.433,90.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.939 | Acc: 51.370,75.309,90.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.946 | Acc: 51.400,75.309,90.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.951 | Acc: 51.417,75.224,90.563,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.967 | Acc: 46.875,60.156,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.422 | Acc: 44.345,57.775,64.025,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.415 | Acc: 43.750,58.270,64.005,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.407 | Acc: 43.891,57.877,63.819,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 121
Batch: 0 | Loss: 2.587 | Acc: 60.938,80.469,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.678 | Acc: 54.501,79.092,92.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.756 | Acc: 53.811,77.496,92.283,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.790 | Acc: 52.971,77.139,92.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.807 | Acc: 52.720,76.871,92.052,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.830 | Acc: 52.290,76.779,92.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.840 | Acc: 52.266,76.621,91.916,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.852 | Acc: 51.973,76.357,91.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.856 | Acc: 51.713,76.320,91.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.859 | Acc: 51.787,76.256,91.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.858 | Acc: 51.819,76.302,91.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.858 | Acc: 51.920,76.294,91.410,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.870 | Acc: 51.760,76.128,91.361,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.871 | Acc: 51.847,76.099,91.370,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.873 | Acc: 51.927,76.157,91.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.881 | Acc: 51.978,76.132,91.263,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.891 | Acc: 51.855,76.029,91.192,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.903 | Acc: 51.757,75.880,91.106,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.911 | Acc: 51.673,75.716,90.991,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.921 | Acc: 51.647,75.599,90.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.929 | Acc: 48.438,64.844,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.291 | Acc: 42.671,59.263,65.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.352 | Acc: 42.645,58.232,65.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.334 | Acc: 42.713,58.414,65.433,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 122
Batch: 0 | Loss: 2.943 | Acc: 50.000,76.562,92.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.914 | Acc: 51.414,75.818,91.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.901 | Acc: 51.791,75.991,91.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.853 | Acc: 52.011,76.665,92.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.863 | Acc: 51.842,76.341,91.889,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.859 | Acc: 51.856,76.632,91.979,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.857 | Acc: 51.827,76.608,91.955,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.864 | Acc: 51.801,76.496,91.761,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.874 | Acc: 51.645,76.276,91.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.876 | Acc: 51.696,76.278,91.527,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.882 | Acc: 51.597,76.193,91.433,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.887 | Acc: 51.531,76.230,91.325,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.897 | Acc: 51.527,76.109,91.173,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.898 | Acc: 51.530,76.102,91.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.901 | Acc: 51.604,76.009,91.048,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.916 | Acc: 51.607,75.823,90.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.923 | Acc: 51.531,75.769,90.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.923 | Acc: 51.656,75.701,90.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.929 | Acc: 51.649,75.623,90.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.939 | Acc: 51.544,75.523,90.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.979 | Acc: 49.219,60.938,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.132 | Acc: 44.420,59.040,67.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.141 | Acc: 44.055,58.594,67.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.167 | Acc: 44.045,58.261,66.560,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 123
Batch: 0 | Loss: 2.872 | Acc: 46.875,80.469,96.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.888 | Acc: 50.632,76.302,92.336,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.829 | Acc: 51.848,77.039,91.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.825 | Acc: 52.011,77.139,91.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.819 | Acc: 52.218,77.083,92.072,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.812 | Acc: 52.266,77.104,92.141,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.810 | Acc: 52.221,77.195,92.110,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.821 | Acc: 51.961,77.111,92.093,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.828 | Acc: 52.014,77.072,91.974,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.859 | Acc: 51.860,76.809,91.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.874 | Acc: 51.803,76.586,91.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.886 | Acc: 51.601,76.435,91.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.903 | Acc: 51.439,76.161,91.380,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.912 | Acc: 51.377,75.928,91.245,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.911 | Acc: 51.474,75.851,91.170,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.907 | Acc: 51.646,75.895,91.108,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.913 | Acc: 51.723,75.845,91.029,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.917 | Acc: 51.695,75.784,91.021,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.925 | Acc: 51.684,75.658,90.937,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.936 | Acc: 51.630,75.541,90.873,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.954 | Acc: 40.625,66.406,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.100 | Acc: 43.787,61.756,67.560,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.151 | Acc: 43.064,61.280,67.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.163 | Acc: 42.905,61.283,67.610,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 124
Batch: 0 | Loss: 3.081 | Acc: 46.094,71.875,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.785 | Acc: 51.749,77.195,92.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.726 | Acc: 52.801,77.744,92.378,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.773 | Acc: 51.998,77.293,92.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.798 | Acc: 52.199,77.122,91.917,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.808 | Acc: 52.119,76.864,91.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.822 | Acc: 52.118,76.698,91.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.834 | Acc: 52.139,76.485,91.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.844 | Acc: 52.087,76.436,91.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.848 | Acc: 52.249,76.347,91.337,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.857 | Acc: 52.293,76.224,91.235,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.864 | Acc: 52.156,76.184,91.159,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.879 | Acc: 51.997,76.031,91.063,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.881 | Acc: 51.964,75.994,90.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.898 | Acc: 51.824,75.840,90.881,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.908 | Acc: 51.695,75.677,90.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.915 | Acc: 51.611,75.621,90.700,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.919 | Acc: 51.617,75.554,90.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.921 | Acc: 51.651,75.558,90.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.927 | Acc: 51.626,75.465,90.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.605 | Acc: 42.969,68.750,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.080 | Acc: 43.118,60.751,67.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.161 | Acc: 42.645,60.328,66.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.164 | Acc: 42.725,60.131,66.919,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 125
Batch: 0 | Loss: 3.248 | Acc: 53.125,71.094,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.885 | Acc: 51.749,75.818,90.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.839 | Acc: 51.810,76.753,91.368,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.804 | Acc: 52.228,77.344,91.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.797 | Acc: 52.681,77.402,91.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.814 | Acc: 52.382,77.212,91.507,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.830 | Acc: 52.182,76.982,91.542,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.834 | Acc: 52.122,76.784,91.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.849 | Acc: 52.164,76.645,91.426,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.865 | Acc: 52.007,76.398,91.221,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.881 | Acc: 51.772,76.205,91.107,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.889 | Acc: 51.810,76.082,91.060,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.893 | Acc: 51.734,76.050,91.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.905 | Acc: 51.667,75.856,90.945,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.910 | Acc: 51.638,75.734,90.881,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.910 | Acc: 51.692,75.696,90.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.912 | Acc: 51.682,75.757,90.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.911 | Acc: 51.746,75.772,90.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.921 | Acc: 51.703,75.628,90.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.927 | Acc: 51.718,75.597,90.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.823 | Acc: 46.094,60.938,71.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.250 | Acc: 43.490,58.259,67.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.330 | Acc: 42.740,57.279,66.959,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.298 | Acc: 42.597,57.672,66.406,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 126
Batch: 0 | Loss: 2.868 | Acc: 53.906,76.562,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.852 | Acc: 52.716,77.046,91.443,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.864 | Acc: 52.248,77.077,91.349,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.886 | Acc: 51.678,76.767,91.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.862 | Acc: 52.103,76.698,91.541,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.867 | Acc: 51.957,76.431,91.453,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.861 | Acc: 52.047,76.466,91.535,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.870 | Acc: 52.078,76.363,91.434,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.861 | Acc: 52.169,76.524,91.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.861 | Acc: 52.154,76.468,91.393,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.872 | Acc: 52.006,76.322,91.216,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.870 | Acc: 52.093,76.435,91.123,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.873 | Acc: 52.097,76.349,91.092,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.882 | Acc: 51.958,76.218,91.104,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.886 | Acc: 52.005,76.115,90.998,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.893 | Acc: 52.006,75.937,90.898,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.903 | Acc: 51.984,75.840,90.761,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.909 | Acc: 51.886,75.774,90.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.911 | Acc: 51.907,75.816,90.610,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.919 | Acc: 51.864,75.724,90.529,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.533 | Acc: 48.438,68.750,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.116 | Acc: 43.899,59.449,67.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.111 | Acc: 43.902,59.489,67.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.114 | Acc: 43.366,59.452,66.944,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 127
Batch: 0 | Loss: 2.969 | Acc: 50.781,75.781,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.773 | Acc: 53.795,76.860,91.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.842 | Acc: 52.553,76.467,91.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.836 | Acc: 52.421,76.575,91.983,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.840 | Acc: 52.305,76.389,91.975,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.849 | Acc: 52.027,76.199,91.878,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.837 | Acc: 52.157,76.311,92.110,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.833 | Acc: 52.155,76.252,92.132,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.842 | Acc: 51.999,76.174,92.032,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.849 | Acc: 52.011,76.140,91.950,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.849 | Acc: 52.130,76.201,91.896,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.859 | Acc: 52.050,76.124,91.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.868 | Acc: 52.026,76.037,91.610,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.881 | Acc: 51.943,75.883,91.454,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.889 | Acc: 51.941,75.803,91.323,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.894 | Acc: 51.921,75.711,91.274,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.892 | Acc: 52.035,75.691,91.229,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.898 | Acc: 52.016,75.593,91.140,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.908 | Acc: 52.002,75.422,91.012,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.918 | Acc: 51.930,75.332,90.937,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.682 | Acc: 44.531,64.062,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.172 | Acc: 43.713,60.305,66.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.190 | Acc: 42.893,61.052,66.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.192 | Acc: 42.802,60.745,66.534,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 128
Batch: 0 | Loss: 2.346 | Acc: 61.719,77.344,97.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.735 | Acc: 52.418,78.943,92.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.745 | Acc: 52.496,78.239,92.359,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.750 | Acc: 52.664,77.997,92.277,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.788 | Acc: 52.238,77.508,92.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.803 | Acc: 52.127,77.483,91.901,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.808 | Acc: 52.266,77.434,91.903,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.820 | Acc: 52.144,77.122,91.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.820 | Acc: 52.266,77.004,91.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.827 | Acc: 52.033,76.865,91.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.838 | Acc: 52.076,76.726,91.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.850 | Acc: 51.969,76.616,91.463,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.859 | Acc: 51.909,76.540,91.384,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.859 | Acc: 51.970,76.542,91.301,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.868 | Acc: 51.893,76.476,91.134,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.883 | Acc: 51.729,76.282,90.991,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.893 | Acc: 51.709,76.195,90.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.901 | Acc: 51.725,76.139,90.687,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.902 | Acc: 51.850,76.082,90.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.908 | Acc: 51.839,75.937,90.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.730 | Acc: 42.969,63.281,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.020 | Acc: 45.015,60.565,67.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.055 | Acc: 44.093,60.480,67.302,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.049 | Acc: 43.686,60.605,67.213,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 129
Batch: 0 | Loss: 2.796 | Acc: 49.219,78.125,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.843 | Acc: 51.637,77.269,92.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.827 | Acc: 51.867,77.439,92.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.831 | Acc: 51.601,76.742,92.034,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.836 | Acc: 51.640,76.630,91.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.841 | Acc: 51.756,76.872,91.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.849 | Acc: 51.717,76.743,91.516,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.869 | Acc: 51.601,76.352,91.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.879 | Acc: 51.771,76.325,91.193,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.884 | Acc: 51.614,76.109,91.160,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.873 | Acc: 51.714,76.271,91.169,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.873 | Acc: 51.750,76.145,91.131,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.886 | Acc: 51.618,76.060,91.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.900 | Acc: 51.476,75.871,90.975,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.907 | Acc: 51.515,75.831,90.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.916 | Acc: 51.420,75.724,90.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.922 | Acc: 51.451,75.689,90.713,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.927 | Acc: 51.400,75.591,90.710,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.929 | Acc: 51.472,75.599,90.683,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.926 | Acc: 51.567,75.576,90.674,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.680 | Acc: 45.312,70.312,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.135 | Acc: 44.345,61.458,66.220,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.152 | Acc: 44.112,60.938,66.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.174 | Acc: 43.981,60.630,66.381,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 130
Batch: 0 | Loss: 2.640 | Acc: 54.688,77.344,88.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.810 | Acc: 52.976,76.860,92.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.800 | Acc: 52.668,77.420,91.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.829 | Acc: 52.126,76.895,91.983,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.837 | Acc: 51.900,76.861,91.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.817 | Acc: 52.390,77.081,91.932,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.815 | Acc: 52.318,77.040,91.968,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.812 | Acc: 52.056,77.122,92.088,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.812 | Acc: 52.121,77.140,92.023,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.825 | Acc: 52.089,76.981,91.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.832 | Acc: 52.025,76.920,91.775,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.833 | Acc: 52.004,76.796,91.689,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.844 | Acc: 51.990,76.627,91.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.854 | Acc: 51.886,76.443,91.541,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.866 | Acc: 51.757,76.307,91.387,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.873 | Acc: 51.765,76.298,91.321,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.883 | Acc: 51.674,76.244,91.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.889 | Acc: 51.714,76.201,91.140,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.898 | Acc: 51.651,76.019,91.082,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.903 | Acc: 51.610,75.933,91.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.532 | Acc: 44.531,64.062,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.091 | Acc: 43.155,60.603,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.123 | Acc: 43.236,60.480,67.092,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.135 | Acc: 43.135,60.105,66.765,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 131
Batch: 0 | Loss: 2.654 | Acc: 49.219,79.688,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.806 | Acc: 52.753,76.897,91.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.837 | Acc: 52.325,76.677,91.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.822 | Acc: 52.574,77.088,91.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.814 | Acc: 52.710,77.392,91.898,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.813 | Acc: 52.483,77.282,91.940,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.816 | Acc: 52.273,77.195,91.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.817 | Acc: 52.261,77.227,91.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.834 | Acc: 51.956,77.033,91.751,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.833 | Acc: 52.020,77.042,91.713,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.836 | Acc: 52.087,76.854,91.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.837 | Acc: 52.089,76.785,91.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.847 | Acc: 51.939,76.621,91.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.849 | Acc: 52.035,76.467,91.445,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.858 | Acc: 52.032,76.429,91.337,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.866 | Acc: 52.050,76.303,91.251,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.876 | Acc: 51.957,76.149,91.165,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.886 | Acc: 51.941,75.992,91.010,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.895 | Acc: 51.861,75.931,90.978,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.902 | Acc: 51.878,75.806,90.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.889 | Acc: 46.094,60.156,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.085 | Acc: 44.382,61.421,67.708,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.101 | Acc: 44.398,60.709,67.130,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.096 | Acc: 43.929,60.656,67.111,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 132
Batch: 0 | Loss: 2.824 | Acc: 57.812,76.562,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.765 | Acc: 53.981,78.981,91.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.740 | Acc: 54.192,78.735,92.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.747 | Acc: 54.086,78.253,91.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.776 | Acc: 53.356,77.807,91.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.803 | Acc: 53.117,77.468,91.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.818 | Acc: 52.918,77.182,91.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.829 | Acc: 52.876,77.105,91.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.828 | Acc: 52.955,77.062,91.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.837 | Acc: 52.814,76.882,91.527,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.855 | Acc: 52.538,76.757,91.472,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.866 | Acc: 52.464,76.647,91.311,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.871 | Acc: 52.370,76.553,91.296,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.871 | Acc: 52.356,76.509,91.242,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.882 | Acc: 52.177,76.371,91.128,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.887 | Acc: 52.071,76.280,91.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.891 | Acc: 52.042,76.258,90.993,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.901 | Acc: 51.954,76.143,90.907,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.909 | Acc: 51.959,76.019,90.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.912 | Acc: 51.969,75.968,90.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.974 | Acc: 46.094,61.719,64.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.168 | Acc: 44.271,60.863,66.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.191 | Acc: 43.845,60.709,66.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.187 | Acc: 43.609,60.118,66.317,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 133
Batch: 0 | Loss: 2.638 | Acc: 53.125,79.688,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.736 | Acc: 54.018,78.423,91.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.804 | Acc: 53.201,77.401,91.273,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.741 | Acc: 53.445,78.048,91.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.782 | Acc: 52.836,77.845,91.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.810 | Acc: 52.614,77.235,91.747,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.826 | Acc: 52.279,77.131,91.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.818 | Acc: 52.460,77.155,91.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.829 | Acc: 52.421,77.019,91.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.836 | Acc: 52.439,76.830,91.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.837 | Acc: 52.390,76.811,91.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.845 | Acc: 52.216,76.637,91.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.843 | Acc: 52.259,76.627,91.529,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.851 | Acc: 52.185,76.524,91.403,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.866 | Acc: 52.127,76.371,91.270,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.874 | Acc: 52.058,76.225,91.243,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.882 | Acc: 52.086,76.120,91.175,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.889 | Acc: 52.101,76.049,91.042,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.902 | Acc: 52.032,75.889,90.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.909 | Acc: 51.983,75.736,90.766,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.676 | Acc: 44.531,62.500,68.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.921 | Acc: 44.234,62.277,66.443,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.941 | Acc: 44.093,62.214,66.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.938 | Acc: 44.672,61.860,66.983,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 134
Batch: 0 | Loss: 3.336 | Acc: 48.438,71.094,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.851 | Acc: 51.265,76.451,92.113,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.850 | Acc: 51.410,76.239,91.711,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.863 | Acc: 51.281,76.127,91.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.848 | Acc: 51.292,76.119,91.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.851 | Acc: 51.377,76.052,91.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.855 | Acc: 51.530,75.917,91.426,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.860 | Acc: 51.590,76.008,91.340,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.864 | Acc: 51.577,76.019,91.367,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.859 | Acc: 51.770,76.045,91.411,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.869 | Acc: 51.881,75.948,91.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.876 | Acc: 51.902,75.894,91.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.878 | Acc: 51.977,75.921,91.189,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.886 | Acc: 51.871,75.811,91.092,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.887 | Acc: 51.879,75.873,91.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.888 | Acc: 52.019,75.857,91.017,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.888 | Acc: 52.127,75.859,90.968,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.890 | Acc: 52.094,75.873,90.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.905 | Acc: 51.913,75.723,90.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.912 | Acc: 51.784,75.638,90.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.709 | Acc: 39.062,60.938,65.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 6.023 | Acc: 37.984,54.874,64.360,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.974 | Acc: 38.072,54.992,63.986,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.990 | Acc: 38.038,54.675,63.768,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 135
Batch: 0 | Loss: 2.682 | Acc: 53.906,79.688,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.881 | Acc: 52.195,75.818,92.113,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.896 | Acc: 51.067,75.495,91.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.867 | Acc: 51.242,76.101,91.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.856 | Acc: 51.543,76.254,91.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.853 | Acc: 51.663,76.392,91.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.860 | Acc: 51.911,76.408,91.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.849 | Acc: 52.189,76.690,91.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.848 | Acc: 52.125,76.761,91.338,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.865 | Acc: 51.921,76.403,91.152,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.867 | Acc: 51.936,76.294,91.107,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.877 | Acc: 51.891,76.227,91.077,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.883 | Acc: 51.896,76.135,91.017,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.890 | Acc: 51.868,75.979,90.945,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.893 | Acc: 51.852,75.965,90.950,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.905 | Acc: 51.760,75.771,90.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.909 | Acc: 51.709,75.740,90.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.912 | Acc: 51.727,75.692,90.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.910 | Acc: 51.779,75.708,90.722,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.911 | Acc: 51.825,75.748,90.684,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.672 | Acc: 42.969,64.844,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.162 | Acc: 42.485,59.635,68.155,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.226 | Acc: 42.511,59.242,67.073,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.261 | Acc: 41.983,58.940,66.483,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 136
Batch: 0 | Loss: 2.824 | Acc: 53.906,73.438,87.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.775 | Acc: 52.641,77.902,91.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.785 | Acc: 52.229,77.877,92.035,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.803 | Acc: 52.561,77.318,91.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.807 | Acc: 52.382,77.122,91.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.803 | Acc: 52.367,77.135,91.971,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.790 | Acc: 52.395,77.266,92.020,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.792 | Acc: 52.366,77.200,91.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.794 | Acc: 52.509,77.101,91.955,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.807 | Acc: 52.421,77.016,91.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.817 | Acc: 52.317,76.889,91.729,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.818 | Acc: 52.372,76.881,91.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.830 | Acc: 52.302,76.783,91.529,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.842 | Acc: 52.095,76.625,91.454,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.844 | Acc: 52.196,76.501,91.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.844 | Acc: 52.331,76.547,91.321,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.851 | Acc: 52.336,76.404,91.231,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.867 | Acc: 52.199,76.214,91.086,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.880 | Acc: 52.142,76.060,90.896,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.893 | Acc: 52.112,75.958,90.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.790 | Acc: 41.406,60.938,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.122 | Acc: 44.010,58.705,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.161 | Acc: 43.293,59.013,66.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.216 | Acc: 43.161,58.607,66.304,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 137
Batch: 0 | Loss: 2.887 | Acc: 51.562,78.125,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.859 | Acc: 51.042,76.302,92.150,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.826 | Acc: 51.791,76.258,92.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.813 | Acc: 51.895,76.537,92.021,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.798 | Acc: 52.431,76.514,92.091,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.816 | Acc: 52.483,76.300,91.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.828 | Acc: 52.305,76.175,91.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.841 | Acc: 52.166,75.909,91.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.841 | Acc: 52.150,75.903,91.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.846 | Acc: 52.020,75.958,91.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.839 | Acc: 52.204,76.131,91.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.840 | Acc: 52.153,76.188,91.488,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.842 | Acc: 52.178,76.206,91.426,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.846 | Acc: 52.140,76.218,91.397,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.857 | Acc: 52.088,76.129,91.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.861 | Acc: 52.118,76.139,91.217,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.868 | Acc: 52.066,76.068,91.134,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.878 | Acc: 52.021,75.951,91.024,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.890 | Acc: 51.872,75.863,90.932,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.895 | Acc: 51.786,75.824,90.863,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.117 | Acc: 42.188,63.281,66.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.075 | Acc: 44.568,59.598,66.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.098 | Acc: 43.998,59.642,66.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.083 | Acc: 43.776,60.054,66.726,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 138
Batch: 0 | Loss: 2.688 | Acc: 55.469,75.781,88.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.798 | Acc: 53.311,78.869,91.109,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.749 | Acc: 54.135,78.468,91.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.771 | Acc: 53.240,78.151,91.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.794 | Acc: 52.652,77.778,91.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.783 | Acc: 52.854,77.684,92.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.803 | Acc: 52.518,77.253,91.994,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.802 | Acc: 52.599,77.150,91.910,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.809 | Acc: 52.504,77.028,91.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.809 | Acc: 52.547,77.128,91.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.831 | Acc: 52.383,76.831,91.500,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.845 | Acc: 52.287,76.630,91.396,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.860 | Acc: 52.217,76.332,91.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.871 | Acc: 52.200,76.137,91.107,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.887 | Acc: 51.993,75.995,90.998,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.892 | Acc: 52.043,75.963,90.892,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.896 | Acc: 52.059,75.901,90.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.901 | Acc: 52.094,75.829,90.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.906 | Acc: 52.093,75.773,90.727,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.908 | Acc: 52.102,75.783,90.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.678 | Acc: 39.844,68.750,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.250 | Acc: 43.750,59.115,65.327,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.243 | Acc: 43.559,59.508,65.568,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.252 | Acc: 43.340,59.375,65.561,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 139
Batch: 0 | Loss: 2.839 | Acc: 53.906,71.094,89.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.779 | Acc: 53.385,76.972,90.737,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.777 | Acc: 53.144,77.591,91.273,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.765 | Acc: 53.317,77.754,91.457,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.785 | Acc: 53.154,77.431,91.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.793 | Acc: 52.932,77.328,91.337,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.809 | Acc: 52.854,77.092,91.187,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.821 | Acc: 52.504,76.912,91.196,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.826 | Acc: 52.446,76.810,91.222,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.832 | Acc: 52.417,76.580,91.320,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.843 | Acc: 52.515,76.493,91.321,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.853 | Acc: 52.552,76.368,91.261,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.863 | Acc: 52.480,76.216,91.111,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.865 | Acc: 52.434,76.215,91.098,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.874 | Acc: 52.383,76.137,90.973,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.876 | Acc: 52.396,76.145,90.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.886 | Acc: 52.285,76.061,90.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.894 | Acc: 52.163,76.008,90.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.905 | Acc: 52.019,75.833,90.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.915 | Acc: 52.034,75.732,90.492,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.617 | Acc: 44.531,64.844,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.942 | Acc: 45.945,61.384,66.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.996 | Acc: 44.474,61.833,67.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.054 | Acc: 44.109,61.181,66.829,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 140
Batch: 0 | Loss: 2.704 | Acc: 52.344,77.344,96.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.695 | Acc: 53.906,78.497,92.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.716 | Acc: 53.030,78.296,92.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.728 | Acc: 53.189,78.509,92.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.724 | Acc: 53.463,78.463,92.178,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.729 | Acc: 53.326,78.179,92.218,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.735 | Acc: 53.241,78.106,92.355,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.755 | Acc: 53.064,77.765,92.204,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.769 | Acc: 52.941,77.620,92.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.768 | Acc: 53.047,77.655,92.041,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.768 | Acc: 53.078,77.717,91.974,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.781 | Acc: 53.019,77.496,91.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.798 | Acc: 52.856,77.259,91.704,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.808 | Acc: 52.577,77.092,91.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.814 | Acc: 52.508,76.929,91.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.830 | Acc: 52.393,76.809,91.388,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.838 | Acc: 52.409,76.694,91.292,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.847 | Acc: 52.300,76.533,91.166,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.862 | Acc: 52.065,76.337,91.069,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.871 | Acc: 52.057,76.202,90.965,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 5.187 | Acc: 41.406,61.719,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.349 | Acc: 38.653,60.900,65.960,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.392 | Acc: 38.796,59.947,65.968,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.417 | Acc: 38.858,59.772,66.022,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 141
Batch: 0 | Loss: 2.473 | Acc: 56.250,85.938,93.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.887 | Acc: 51.339,76.897,90.513,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.883 | Acc: 51.791,76.753,91.044,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.870 | Acc: 52.049,76.537,91.329,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.861 | Acc: 51.823,76.726,91.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.862 | Acc: 51.895,76.655,91.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.856 | Acc: 52.040,76.685,91.555,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.851 | Acc: 52.111,76.723,91.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.857 | Acc: 52.082,76.562,91.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.861 | Acc: 52.055,76.511,91.454,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.866 | Acc: 51.994,76.426,91.336,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.864 | Acc: 51.990,76.435,91.279,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.869 | Acc: 52.033,76.280,91.290,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.875 | Acc: 52.047,76.161,91.275,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.880 | Acc: 52.099,76.126,91.234,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.886 | Acc: 52.110,76.051,91.160,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.888 | Acc: 52.103,76.044,91.080,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.893 | Acc: 52.105,75.912,90.976,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.902 | Acc: 52.047,75.820,90.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.906 | Acc: 52.038,75.744,90.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.911 | Acc: 46.875,61.719,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.976 | Acc: 46.987,61.979,68.378,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.999 | Acc: 45.655,61.147,67.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.016 | Acc: 45.210,60.848,67.111,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 142
Batch: 0 | Loss: 2.757 | Acc: 51.562,76.562,92.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.758 | Acc: 53.051,77.976,91.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.803 | Acc: 52.820,77.287,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.811 | Acc: 52.536,77.446,91.381,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.822 | Acc: 52.353,77.141,91.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.821 | Acc: 52.336,76.918,91.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.827 | Acc: 52.234,76.801,91.516,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.827 | Acc: 52.211,76.695,91.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.826 | Acc: 52.417,76.664,91.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.830 | Acc: 52.387,76.649,91.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.830 | Acc: 52.379,76.776,91.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.838 | Acc: 52.280,76.718,91.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.841 | Acc: 52.243,76.653,91.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.852 | Acc: 52.305,76.452,91.427,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.853 | Acc: 52.308,76.385,91.409,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.856 | Acc: 52.276,76.316,91.383,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.856 | Acc: 52.322,76.380,91.319,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.859 | Acc: 52.293,76.329,91.255,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.868 | Acc: 52.184,76.240,91.179,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.874 | Acc: 52.210,76.210,91.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.954 | Acc: 45.312,57.812,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.218 | Acc: 45.052,58.854,67.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.245 | Acc: 44.588,58.155,66.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.255 | Acc: 44.429,58.543,66.253,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 143
Batch: 0 | Loss: 2.488 | Acc: 59.375,83.594,95.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.748 | Acc: 54.055,77.455,92.522,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.754 | Acc: 53.468,77.534,92.473,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.756 | Acc: 53.202,77.510,92.495,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.763 | Acc: 53.173,77.778,92.178,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.771 | Acc: 53.071,77.576,92.071,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.782 | Acc: 52.822,77.402,91.903,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.796 | Acc: 52.826,77.189,91.722,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.803 | Acc: 52.669,76.951,91.717,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.813 | Acc: 52.672,76.869,91.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.821 | Acc: 52.530,76.815,91.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.828 | Acc: 52.453,76.718,91.417,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.842 | Acc: 52.376,76.585,91.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.841 | Acc: 52.341,76.583,91.254,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.852 | Acc: 52.302,76.485,91.195,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.862 | Acc: 52.172,76.370,91.116,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.870 | Acc: 52.061,76.290,91.051,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.877 | Acc: 52.023,76.198,91.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.888 | Acc: 51.948,76.115,90.941,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.897 | Acc: 51.901,76.007,90.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.778 | Acc: 41.406,66.406,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.117 | Acc: 43.750,61.533,67.150,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.167 | Acc: 42.797,61.052,66.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.184 | Acc: 42.392,60.745,66.522,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 144
Batch: 0 | Loss: 2.841 | Acc: 53.125,75.781,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.625 | Acc: 54.427,80.692,93.006,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.707 | Acc: 53.201,79.306,92.378,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.716 | Acc: 53.010,79.009,92.226,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.737 | Acc: 53.009,78.463,92.429,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.742 | Acc: 53.063,78.458,92.373,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.749 | Acc: 52.989,78.416,92.271,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.749 | Acc: 53.025,78.241,92.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.764 | Acc: 52.853,77.921,92.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.766 | Acc: 53.013,77.866,92.153,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.778 | Acc: 52.927,77.624,92.009,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.793 | Acc: 52.733,77.436,91.880,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.805 | Acc: 52.681,77.295,91.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.814 | Acc: 52.556,77.218,91.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.822 | Acc: 52.602,77.013,91.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.826 | Acc: 52.616,76.960,91.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.834 | Acc: 52.553,76.818,91.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.844 | Acc: 52.543,76.643,91.351,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.855 | Acc: 52.502,76.556,91.214,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.858 | Acc: 52.551,76.538,91.117,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.495 | Acc: 42.969,60.156,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.033 | Acc: 45.610,60.379,67.485,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.081 | Acc: 45.122,60.080,67.054,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.072 | Acc: 44.659,60.246,66.919,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 145
Batch: 0 | Loss: 2.779 | Acc: 57.812,74.219,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.756 | Acc: 52.902,78.385,91.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.796 | Acc: 53.011,77.687,91.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.772 | Acc: 52.920,77.472,91.726,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.770 | Acc: 52.942,77.575,91.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.780 | Acc: 53.024,77.522,91.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.802 | Acc: 52.692,77.150,91.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.803 | Acc: 52.421,77.061,91.728,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.809 | Acc: 52.557,76.936,91.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.821 | Acc: 52.495,76.895,91.579,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.824 | Acc: 52.480,76.835,91.488,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.829 | Acc: 52.460,76.760,91.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.838 | Acc: 52.409,76.611,91.393,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.845 | Acc: 52.332,76.509,91.284,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.852 | Acc: 52.413,76.501,91.150,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.857 | Acc: 52.380,76.381,91.090,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.867 | Acc: 52.315,76.270,91.007,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.877 | Acc: 52.231,76.097,90.909,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.879 | Acc: 52.233,76.112,90.889,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.888 | Acc: 52.145,76.044,90.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.967 | Acc: 42.969,64.844,69.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.337 | Acc: 42.708,58.966,66.109,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.320 | Acc: 42.912,59.604,66.178,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.324 | Acc: 42.802,59.977,66.291,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 146
Batch: 0 | Loss: 2.223 | Acc: 59.375,80.469,96.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.788 | Acc: 53.199,77.493,91.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.788 | Acc: 53.296,77.458,91.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.790 | Acc: 52.613,77.664,92.136,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.802 | Acc: 52.392,77.498,92.101,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.788 | Acc: 52.591,77.491,92.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.791 | Acc: 52.725,77.344,92.097,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.781 | Acc: 52.831,77.477,92.121,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.789 | Acc: 52.601,77.213,92.061,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.812 | Acc: 52.378,76.916,91.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.828 | Acc: 52.328,76.664,91.686,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.834 | Acc: 52.439,76.566,91.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.840 | Acc: 52.376,76.443,91.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.842 | Acc: 52.511,76.254,91.385,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.852 | Acc: 52.458,76.173,91.289,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.865 | Acc: 52.406,76.028,91.097,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.866 | Acc: 52.417,76.017,91.068,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.870 | Acc: 52.392,75.965,91.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.875 | Acc: 52.396,75.928,90.950,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.884 | Acc: 52.377,75.830,90.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.895 | Acc: 42.188,67.188,67.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.313 | Acc: 40.588,60.454,66.555,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.375 | Acc: 40.949,59.375,66.311,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.398 | Acc: 40.779,59.375,65.843,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 147
Batch: 0 | Loss: 2.862 | Acc: 55.469,76.562,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.704 | Acc: 53.571,78.423,92.039,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.713 | Acc: 53.030,78.277,91.997,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.721 | Acc: 53.522,77.959,92.008,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.700 | Acc: 53.665,78.260,92.236,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.716 | Acc: 53.574,78.140,92.195,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.759 | Acc: 53.022,77.589,91.897,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.775 | Acc: 53.020,77.388,91.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.780 | Acc: 53.106,77.300,91.727,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.787 | Acc: 52.948,77.158,91.687,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.790 | Acc: 52.997,77.114,91.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.799 | Acc: 53.012,77.004,91.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.799 | Acc: 53.106,77.007,91.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.809 | Acc: 52.889,76.874,91.439,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.817 | Acc: 52.755,76.735,91.340,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.837 | Acc: 52.575,76.524,91.227,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.845 | Acc: 52.504,76.441,91.158,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.854 | Acc: 52.438,76.301,91.056,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.872 | Acc: 52.270,76.156,90.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.878 | Acc: 52.231,76.042,90.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.977 | Acc: 36.719,62.500,71.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.086 | Acc: 43.899,61.607,67.001,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.146 | Acc: 43.598,60.595,66.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.159 | Acc: 43.379,60.207,65.830,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 148
Batch: 0 | Loss: 2.843 | Acc: 50.781,82.031,89.062,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.854 | Acc: 52.567,76.711,91.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.830 | Acc: 52.630,76.677,91.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.803 | Acc: 52.600,76.550,91.983,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.804 | Acc: 52.922,76.813,91.696,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.808 | Acc: 52.707,77.003,91.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.808 | Acc: 52.531,77.182,91.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.806 | Acc: 52.626,77.227,91.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.793 | Acc: 52.708,77.218,91.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.799 | Acc: 52.698,77.145,91.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.806 | Acc: 52.635,76.982,91.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.821 | Acc: 52.453,76.750,91.555,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.834 | Acc: 52.256,76.588,91.426,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.844 | Acc: 52.257,76.521,91.346,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.843 | Acc: 52.363,76.546,91.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.852 | Acc: 52.307,76.492,91.217,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.858 | Acc: 52.307,76.356,91.158,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.859 | Acc: 52.344,76.310,91.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.865 | Acc: 52.320,76.221,91.051,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.872 | Acc: 52.256,76.126,90.951,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.916 | Acc: 39.062,69.531,70.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.126 | Acc: 43.787,59.673,67.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.240 | Acc: 43.159,58.880,67.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.275 | Acc: 42.905,58.952,67.098,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 149
Batch: 0 | Loss: 2.717 | Acc: 52.344,76.562,91.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.738 | Acc: 52.604,78.311,91.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.759 | Acc: 53.201,77.553,91.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.769 | Acc: 53.061,77.818,91.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.754 | Acc: 53.212,77.961,91.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.764 | Acc: 53.063,77.839,91.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.786 | Acc: 52.744,77.370,91.510,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.789 | Acc: 52.504,77.443,91.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.810 | Acc: 52.373,77.121,91.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.819 | Acc: 52.357,76.964,91.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.819 | Acc: 52.355,76.912,91.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.828 | Acc: 52.245,76.785,91.505,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.829 | Acc: 52.308,76.828,91.452,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.838 | Acc: 52.134,76.697,91.391,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.855 | Acc: 52.030,76.588,91.251,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.856 | Acc: 52.014,76.518,91.227,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.863 | Acc: 52.020,76.356,91.085,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.871 | Acc: 52.060,76.228,90.982,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.877 | Acc: 52.097,76.110,90.887,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.881 | Acc: 52.153,76.033,90.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.996 | Acc: 40.625,60.938,67.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 5.326 | Acc: 40.365,59.189,67.374,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 5.360 | Acc: 41.120,59.489,66.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 5.398 | Acc: 41.304,59.209,65.984,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 150
Batch: 0 | Loss: 2.545 | Acc: 52.344,80.469,94.531,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.650 | Acc: 54.092,78.385,91.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.591 | Acc: 54.707,79.573,92.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.532 | Acc: 55.200,80.853,93.327,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.496 | Acc: 55.401,81.568,93.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.466 | Acc: 55.546,81.931,94.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.451 | Acc: 55.682,82.315,94.583,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.436 | Acc: 55.546,82.563,94.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.405 | Acc: 55.808,82.846,95.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.390 | Acc: 55.719,83.102,95.269,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.370 | Acc: 55.993,83.186,95.363,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.352 | Acc: 56.176,83.413,95.532,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.339 | Acc: 56.331,83.526,95.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.333 | Acc: 56.358,83.603,95.696,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.325 | Acc: 56.375,83.772,95.785,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.317 | Acc: 56.486,83.921,95.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.311 | Acc: 56.418,83.969,95.933,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.302 | Acc: 56.500,84.070,95.995,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.290 | Acc: 56.672,84.213,96.048,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.285 | Acc: 56.720,84.297,96.096,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.755 | Acc: 53.125,71.875,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.911 | Acc: 51.302,69.643,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.997 | Acc: 51.220,69.036,73.952,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.017 | Acc: 51.332,68.699,73.732,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 151
Batch: 0 | Loss: 2.306 | Acc: 59.375,85.156,96.094,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.093 | Acc: 59.003,87.054,98.214,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.141 | Acc: 57.908,86.890,97.942,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.084 | Acc: 58.824,87.359,98.079,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.082 | Acc: 58.738,87.375,97.984,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.105 | Acc: 58.609,87.106,97.927,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.104 | Acc: 58.413,87.190,97.940,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.094 | Acc: 58.583,87.240,97.961,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.093 | Acc: 58.424,87.257,97.972,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.102 | Acc: 58.132,87.202,98.032,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.107 | Acc: 57.937,87.127,98.002,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.102 | Acc: 58.067,87.097,98.031,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.102 | Acc: 58.082,87.062,98.042,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.102 | Acc: 58.115,87.102,98.021,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.095 | Acc: 58.271,87.169,98.048,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.091 | Acc: 58.376,87.222,98.030,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.094 | Acc: 58.299,87.213,98.033,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.086 | Acc: 58.514,87.303,98.027,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.087 | Acc: 58.501,87.275,98.024,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.084 | Acc: 58.547,87.305,98.056,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.614 | Acc: 53.906,72.656,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.889 | Acc: 52.790,70.312,76.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.956 | Acc: 51.905,69.836,74.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.981 | Acc: 51.883,69.544,74.232,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 152
Batch: 0 | Loss: 1.866 | Acc: 60.156,89.844,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 2.008 | Acc: 58.222,88.765,98.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 2.003 | Acc: 58.575,88.548,98.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 2.014 | Acc: 58.350,88.448,98.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 2.004 | Acc: 58.709,88.571,98.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 2.000 | Acc: 58.787,88.475,98.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 2.004 | Acc: 58.962,88.514,98.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 2.016 | Acc: 58.766,88.370,98.548,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 2.011 | Acc: 58.967,88.490,98.525,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 2.011 | Acc: 58.939,88.402,98.537,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 2.011 | Acc: 58.912,88.340,98.511,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 2.018 | Acc: 58.760,88.257,98.529,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 2.019 | Acc: 58.801,88.171,98.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 2.020 | Acc: 58.806,88.173,98.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 2.018 | Acc: 58.772,88.195,98.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 2.019 | Acc: 58.737,88.170,98.536,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 2.019 | Acc: 58.786,88.140,98.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 2.021 | Acc: 58.701,88.130,98.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 2.021 | Acc: 58.713,88.091,98.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 2.020 | Acc: 58.692,88.125,98.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.587 | Acc: 51.562,72.656,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.877 | Acc: 52.158,70.350,76.153,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.958 | Acc: 51.582,69.627,74.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.979 | Acc: 51.691,69.442,74.385,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 153
Batch: 0 | Loss: 2.036 | Acc: 58.594,89.062,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.943 | Acc: 59.747,89.435,99.070,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.932 | Acc: 59.642,89.615,98.933,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.948 | Acc: 59.618,89.421,98.860,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.964 | Acc: 59.153,89.169,98.804,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.984 | Acc: 58.872,88.931,98.685,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.988 | Acc: 58.607,89.011,98.709,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.987 | Acc: 58.561,89.018,98.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.988 | Acc: 58.671,88.946,98.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.987 | Acc: 58.667,88.898,98.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.990 | Acc: 58.598,88.802,98.690,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.984 | Acc: 58.636,88.826,98.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.983 | Acc: 58.581,88.803,98.674,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.985 | Acc: 58.522,88.766,98.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.979 | Acc: 58.619,88.787,98.685,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.980 | Acc: 58.664,88.831,98.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.977 | Acc: 58.742,88.865,98.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.978 | Acc: 58.722,88.865,98.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.979 | Acc: 58.717,88.846,98.712,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.978 | Acc: 58.780,88.835,98.725,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.626 | Acc: 50.781,73.438,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.884 | Acc: 52.790,70.610,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.959 | Acc: 51.886,69.741,74.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.981 | Acc: 51.691,69.326,74.398,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 154
Batch: 0 | Loss: 2.183 | Acc: 57.812,90.625,98.438,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.985 | Acc: 59.077,88.728,98.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.950 | Acc: 59.165,89.386,98.895,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.959 | Acc: 58.927,89.395,98.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.957 | Acc: 58.941,89.371,98.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.950 | Acc: 59.104,89.573,98.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.954 | Acc: 59.143,89.547,98.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.949 | Acc: 59.281,89.589,98.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.954 | Acc: 59.074,89.528,98.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.956 | Acc: 59.069,89.451,98.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.958 | Acc: 58.986,89.447,98.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.957 | Acc: 58.986,89.398,98.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.956 | Acc: 59.038,89.338,98.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.952 | Acc: 59.040,89.383,98.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.951 | Acc: 59.114,89.349,98.866,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.948 | Acc: 59.147,89.351,98.863,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.950 | Acc: 59.173,89.303,98.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.949 | Acc: 59.144,89.271,98.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.946 | Acc: 59.293,89.262,98.877,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.945 | Acc: 59.291,89.251,98.870,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.630 | Acc: 52.344,71.875,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.899 | Acc: 52.939,70.499,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.969 | Acc: 52.229,69.741,74.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.995 | Acc: 52.024,69.326,74.398,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 155
Batch: 0 | Loss: 1.868 | Acc: 53.125,91.406,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.877 | Acc: 59.598,90.885,99.182,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.898 | Acc: 59.680,90.396,99.143,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.921 | Acc: 59.042,90.177,98.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.919 | Acc: 59.086,89.834,98.920,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.914 | Acc: 59.097,89.975,98.971,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.914 | Acc: 59.394,89.973,98.941,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.913 | Acc: 59.259,90.054,98.936,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.909 | Acc: 59.404,90.038,98.947,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.904 | Acc: 59.574,90.016,98.977,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.906 | Acc: 59.492,90.073,98.978,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.906 | Acc: 59.361,90.084,98.964,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.905 | Acc: 59.427,90.119,98.969,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.903 | Acc: 59.504,90.056,98.988,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.903 | Acc: 59.536,89.974,99.013,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.903 | Acc: 59.536,89.979,99.027,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.901 | Acc: 59.548,89.924,99.014,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.907 | Acc: 59.471,89.821,99.001,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.903 | Acc: 59.568,89.872,99.002,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.906 | Acc: 59.578,89.823,99.020,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.552 | Acc: 52.344,71.094,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.908 | Acc: 52.939,70.015,75.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.983 | Acc: 51.982,69.588,74.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.007 | Acc: 51.908,69.314,74.372,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 156
Batch: 0 | Loss: 1.920 | Acc: 60.938,91.406,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.873 | Acc: 60.082,89.993,99.182,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.870 | Acc: 59.851,90.072,99.085,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.877 | Acc: 59.567,90.330,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.878 | Acc: 59.703,90.278,99.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.875 | Acc: 59.870,90.370,99.188,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.885 | Acc: 59.575,90.251,99.161,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.890 | Acc: 59.469,90.176,99.163,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.887 | Acc: 59.477,90.193,99.151,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.887 | Acc: 59.513,90.042,99.141,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.885 | Acc: 59.562,90.015,99.137,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.882 | Acc: 59.580,90.013,99.145,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.883 | Acc: 59.696,89.983,99.154,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.883 | Acc: 59.776,89.999,99.162,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.886 | Acc: 59.734,89.999,99.163,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.885 | Acc: 59.723,90.028,99.162,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.890 | Acc: 59.665,89.965,99.148,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.889 | Acc: 59.712,89.935,99.152,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.891 | Acc: 59.652,89.885,99.160,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.888 | Acc: 59.711,89.926,99.161,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.556 | Acc: 53.125,75.781,79.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.884 | Acc: 52.381,70.461,76.376,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.967 | Acc: 52.134,69.989,75.210,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.990 | Acc: 52.049,69.864,74.872,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 157
Batch: 0 | Loss: 1.840 | Acc: 59.375,90.625,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.865 | Acc: 59.375,90.997,99.144,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.839 | Acc: 60.842,91.139,99.162,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.842 | Acc: 60.592,90.881,99.155,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.848 | Acc: 60.253,90.779,99.199,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.850 | Acc: 60.342,90.741,99.242,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.855 | Acc: 60.279,90.748,99.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.857 | Acc: 60.289,90.730,99.269,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.860 | Acc: 59.991,90.741,99.253,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.864 | Acc: 59.971,90.685,99.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.865 | Acc: 59.900,90.703,99.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.869 | Acc: 59.842,90.586,99.229,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.867 | Acc: 59.832,90.619,99.225,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.864 | Acc: 59.833,90.691,99.216,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.863 | Acc: 59.786,90.656,99.222,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.862 | Acc: 59.842,90.669,99.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.864 | Acc: 59.874,90.642,99.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.864 | Acc: 59.925,90.595,99.187,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.864 | Acc: 59.931,90.538,99.197,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.862 | Acc: 60.009,90.543,99.200,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.548 | Acc: 52.344,75.000,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.898 | Acc: 52.083,70.089,75.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.973 | Acc: 51.334,69.569,75.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.993 | Acc: 51.639,69.531,74.539,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 158
Batch: 0 | Loss: 1.816 | Acc: 61.719,92.188,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.816 | Acc: 59.338,91.220,99.256,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.836 | Acc: 59.699,90.911,99.143,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.829 | Acc: 60.131,91.035,99.206,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.815 | Acc: 60.561,91.011,99.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.815 | Acc: 60.713,90.934,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.827 | Acc: 60.550,90.683,99.238,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.824 | Acc: 60.544,90.664,99.246,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.836 | Acc: 60.229,90.654,99.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.831 | Acc: 60.135,90.793,99.206,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.836 | Acc: 59.985,90.676,99.195,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.839 | Acc: 59.926,90.621,99.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.845 | Acc: 59.819,90.631,99.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.843 | Acc: 59.914,90.601,99.189,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.842 | Acc: 59.967,90.619,99.183,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.840 | Acc: 59.951,90.635,99.190,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.841 | Acc: 59.927,90.620,99.207,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.845 | Acc: 59.968,90.598,99.221,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.851 | Acc: 59.886,90.530,99.217,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.851 | Acc: 59.865,90.510,99.208,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.532 | Acc: 53.906,74.219,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.888 | Acc: 52.976,70.647,76.525,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.978 | Acc: 52.306,69.970,75.229,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.003 | Acc: 52.228,69.634,74.641,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 159
Batch: 0 | Loss: 1.813 | Acc: 65.625,92.188,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.825 | Acc: 60.826,91.332,99.107,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.789 | Acc: 61.109,91.597,99.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.799 | Acc: 61.104,91.406,99.283,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.819 | Acc: 60.918,91.223,99.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.826 | Acc: 60.435,91.344,99.257,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.827 | Acc: 60.408,91.374,99.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.813 | Acc: 60.699,91.395,99.285,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.806 | Acc: 60.787,91.450,99.277,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.812 | Acc: 60.709,91.316,99.262,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.814 | Acc: 60.669,91.262,99.277,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.812 | Acc: 60.722,91.251,99.289,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.819 | Acc: 60.425,91.238,99.303,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.822 | Acc: 60.333,91.206,99.294,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.826 | Acc: 60.318,91.125,99.280,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.829 | Acc: 60.232,91.097,99.278,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.831 | Acc: 60.161,91.022,99.289,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.829 | Acc: 60.152,91.044,99.294,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.830 | Acc: 60.135,90.999,99.277,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.830 | Acc: 60.164,90.986,99.282,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.551 | Acc: 53.125,74.219,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.879 | Acc: 53.237,70.833,76.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.965 | Acc: 52.611,69.874,75.210,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 3.987 | Acc: 52.369,69.749,74.936,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 160
Batch: 0 | Loss: 1.912 | Acc: 56.250,92.969,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.831 | Acc: 59.561,91.071,99.442,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.809 | Acc: 59.280,91.368,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.828 | Acc: 59.349,91.176,99.347,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.819 | Acc: 59.606,91.213,99.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.816 | Acc: 59.947,91.143,99.319,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.815 | Acc: 59.982,91.277,99.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.820 | Acc: 59.929,91.212,99.368,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.820 | Acc: 60.030,91.241,99.350,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.824 | Acc: 59.867,91.199,99.335,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.824 | Acc: 59.935,91.274,99.335,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.829 | Acc: 59.785,91.187,99.314,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.828 | Acc: 59.900,91.173,99.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.823 | Acc: 59.962,91.224,99.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.827 | Acc: 59.917,91.112,99.327,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.827 | Acc: 59.951,91.084,99.299,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.820 | Acc: 60.081,91.153,99.284,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.820 | Acc: 60.092,91.157,99.276,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.819 | Acc: 60.055,91.151,99.279,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.818 | Acc: 60.111,91.121,99.264,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.647 | Acc: 53.906,75.000,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.904 | Acc: 53.348,70.350,76.153,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.984 | Acc: 52.325,69.741,75.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.009 | Acc: 52.203,69.442,74.769,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 161
Batch: 0 | Loss: 1.526 | Acc: 67.188,92.969,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.756 | Acc: 61.830,91.369,99.256,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.781 | Acc: 61.109,91.273,99.200,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.787 | Acc: 61.117,90.984,99.270,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.804 | Acc: 60.561,91.040,99.267,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.802 | Acc: 60.535,90.965,99.281,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.795 | Acc: 60.757,91.058,99.316,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.796 | Acc: 60.771,91.057,99.318,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.792 | Acc: 60.753,91.198,99.330,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.790 | Acc: 60.778,91.281,99.335,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.792 | Acc: 60.774,91.290,99.312,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.793 | Acc: 60.768,91.205,99.307,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.793 | Acc: 60.808,91.183,99.322,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.795 | Acc: 60.779,91.191,99.306,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.797 | Acc: 60.740,91.159,99.305,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.799 | Acc: 60.696,91.165,99.304,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.801 | Acc: 60.667,91.207,99.314,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.798 | Acc: 60.630,91.266,99.315,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.800 | Acc: 60.602,91.190,99.320,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.803 | Acc: 60.538,91.175,99.317,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.705 | Acc: 53.125,72.656,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.951 | Acc: 53.274,69.531,76.079,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.008 | Acc: 52.477,68.845,75.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.036 | Acc: 52.241,69.224,74.449,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 162
Batch: 0 | Loss: 1.765 | Acc: 56.250,95.312,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.781 | Acc: 59.598,91.109,99.256,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.776 | Acc: 60.156,91.216,99.447,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.782 | Acc: 60.425,90.868,99.488,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.794 | Acc: 60.021,91.040,99.489,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.788 | Acc: 60.334,91.228,99.420,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.779 | Acc: 60.737,91.400,99.400,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.768 | Acc: 60.893,91.567,99.407,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.772 | Acc: 60.748,91.552,99.403,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.779 | Acc: 60.584,91.497,99.404,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.780 | Acc: 60.506,91.507,99.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.782 | Acc: 60.535,91.565,99.417,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.787 | Acc: 60.448,91.627,99.426,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.788 | Acc: 60.441,91.601,99.419,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.790 | Acc: 60.390,91.520,99.427,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.792 | Acc: 60.307,91.474,99.429,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.796 | Acc: 60.263,91.379,99.428,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.796 | Acc: 60.273,91.409,99.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.796 | Acc: 60.232,91.359,99.407,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.796 | Acc: 60.253,91.314,99.418,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.587 | Acc: 53.125,73.438,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.910 | Acc: 53.088,70.350,76.376,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 3.998 | Acc: 52.287,69.760,75.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.023 | Acc: 51.895,69.659,74.680,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 163
Batch: 0 | Loss: 1.631 | Acc: 59.375,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.720 | Acc: 62.426,91.853,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.718 | Acc: 61.909,92.035,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.746 | Acc: 61.399,91.970,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.754 | Acc: 61.275,91.956,99.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.763 | Acc: 60.883,91.932,99.520,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.761 | Acc: 60.750,91.852,99.542,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.763 | Acc: 60.793,91.822,99.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.768 | Acc: 60.671,91.756,99.520,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.767 | Acc: 60.618,91.821,99.512,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.764 | Acc: 60.611,91.830,99.510,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.759 | Acc: 60.563,91.855,99.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.763 | Acc: 60.474,91.815,99.449,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.765 | Acc: 60.527,91.816,99.440,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.768 | Acc: 60.520,91.829,99.411,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.771 | Acc: 60.481,91.819,99.406,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.771 | Acc: 60.458,91.815,99.416,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.768 | Acc: 60.555,91.819,99.402,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.767 | Acc: 60.589,91.872,99.405,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.770 | Acc: 60.577,91.843,99.399,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.608 | Acc: 52.344,71.094,79.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.919 | Acc: 53.013,70.201,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.012 | Acc: 51.963,69.379,74.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.040 | Acc: 51.678,69.326,74.603,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 164
Batch: 0 | Loss: 1.753 | Acc: 60.156,91.406,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.708 | Acc: 62.091,92.374,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.725 | Acc: 61.643,92.321,99.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.742 | Acc: 61.309,92.136,99.436,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.747 | Acc: 61.053,92.072,99.450,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.757 | Acc: 60.938,92.002,99.443,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.757 | Acc: 61.202,91.929,99.425,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.751 | Acc: 61.275,91.955,99.413,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.756 | Acc: 61.054,91.955,99.447,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.760 | Acc: 60.773,92.058,99.448,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.755 | Acc: 60.875,92.110,99.460,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.759 | Acc: 60.750,92.039,99.459,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.760 | Acc: 60.756,92.051,99.488,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.763 | Acc: 60.662,92.020,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.764 | Acc: 60.646,92.021,99.475,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.762 | Acc: 60.787,92.034,99.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.761 | Acc: 60.748,92.049,99.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.759 | Acc: 60.750,92.045,99.452,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.759 | Acc: 60.771,92.047,99.459,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.758 | Acc: 60.817,92.071,99.436,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.593 | Acc: 52.344,73.438,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.923 | Acc: 53.051,70.350,76.265,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.007 | Acc: 52.325,69.474,75.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.028 | Acc: 52.344,69.544,74.834,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 165
Batch: 0 | Loss: 1.887 | Acc: 60.938,90.625,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.701 | Acc: 62.351,92.894,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.708 | Acc: 61.585,92.454,99.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.716 | Acc: 61.578,92.623,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.716 | Acc: 61.699,92.515,99.498,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.710 | Acc: 61.742,92.574,99.497,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.703 | Acc: 61.783,92.594,99.509,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.710 | Acc: 61.453,92.570,99.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.717 | Acc: 61.442,92.464,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.724 | Acc: 61.348,92.360,99.517,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.726 | Acc: 61.264,92.335,99.499,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.735 | Acc: 61.082,92.230,99.509,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.731 | Acc: 61.203,92.265,99.501,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.730 | Acc: 61.237,92.283,99.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.736 | Acc: 61.135,92.182,99.516,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.737 | Acc: 61.187,92.190,99.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.740 | Acc: 61.127,92.178,99.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.741 | Acc: 61.086,92.169,99.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.744 | Acc: 61.078,92.129,99.487,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.747 | Acc: 60.935,92.105,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.622 | Acc: 53.906,75.781,80.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.996 | Acc: 52.716,70.089,75.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.062 | Acc: 51.905,69.322,74.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.080 | Acc: 51.793,69.506,74.360,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 166
Batch: 0 | Loss: 1.957 | Acc: 53.125,90.625,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.712 | Acc: 61.793,92.671,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.711 | Acc: 61.757,92.283,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.741 | Acc: 60.886,92.264,99.565,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.743 | Acc: 60.735,92.544,99.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.733 | Acc: 61.046,92.574,99.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.740 | Acc: 60.976,92.452,99.503,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.734 | Acc: 61.165,92.437,99.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.733 | Acc: 61.214,92.420,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.722 | Acc: 61.373,92.490,99.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.727 | Acc: 61.237,92.479,99.537,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.724 | Acc: 61.298,92.499,99.505,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.724 | Acc: 61.213,92.502,99.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.723 | Acc: 61.204,92.457,99.482,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.727 | Acc: 61.271,92.432,99.491,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.732 | Acc: 61.104,92.377,99.476,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.734 | Acc: 61.059,92.348,99.467,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.739 | Acc: 60.921,92.302,99.478,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.739 | Acc: 60.877,92.272,99.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.745 | Acc: 60.776,92.243,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.579 | Acc: 52.344,74.219,80.469,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.980 | Acc: 53.013,70.238,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.067 | Acc: 52.115,69.379,74.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.077 | Acc: 51.934,69.672,74.488,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 167
Batch: 0 | Loss: 1.527 | Acc: 57.812,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.682 | Acc: 61.049,92.597,99.368,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.702 | Acc: 61.242,92.569,99.352,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.719 | Acc: 61.117,92.316,99.424,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.723 | Acc: 61.044,92.351,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.723 | Acc: 61.100,92.311,99.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.726 | Acc: 61.202,92.362,99.503,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.723 | Acc: 61.209,92.359,99.523,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.722 | Acc: 61.214,92.454,99.505,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.721 | Acc: 61.024,92.520,99.517,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.726 | Acc: 60.980,92.444,99.526,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.727 | Acc: 60.980,92.506,99.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.725 | Acc: 61.051,92.499,99.511,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.726 | Acc: 61.015,92.502,99.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.732 | Acc: 60.974,92.441,99.494,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.729 | Acc: 61.065,92.447,99.512,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.733 | Acc: 61.001,92.390,99.511,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.733 | Acc: 61.061,92.394,99.498,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.732 | Acc: 61.018,92.376,99.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.735 | Acc: 60.878,92.323,99.508,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.599 | Acc: 53.906,72.656,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.966 | Acc: 53.348,70.647,76.265,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.055 | Acc: 52.210,69.455,75.267,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.072 | Acc: 52.254,69.672,74.808,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 168
Batch: 0 | Loss: 1.493 | Acc: 60.156,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.685 | Acc: 62.202,92.560,99.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.700 | Acc: 61.776,92.740,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.714 | Acc: 61.668,92.546,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.731 | Acc: 60.976,92.400,99.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.735 | Acc: 60.783,92.404,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.737 | Acc: 60.595,92.420,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.732 | Acc: 60.644,92.442,99.557,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.730 | Acc: 60.748,92.391,99.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.726 | Acc: 60.916,92.412,99.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.726 | Acc: 60.903,92.495,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.722 | Acc: 60.998,92.576,99.533,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.724 | Acc: 61.032,92.560,99.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.724 | Acc: 61.099,92.505,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.727 | Acc: 61.051,92.516,99.511,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.729 | Acc: 61.036,92.470,99.504,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.728 | Acc: 61.086,92.458,99.499,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.725 | Acc: 61.112,92.453,99.496,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.727 | Acc: 61.022,92.441,99.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.726 | Acc: 61.050,92.405,99.498,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.611 | Acc: 52.344,74.219,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.996 | Acc: 53.497,69.048,76.116,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.084 | Acc: 52.477,68.617,74.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.093 | Acc: 52.190,68.916,74.539,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 169
Batch: 0 | Loss: 1.601 | Acc: 58.594,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.663 | Acc: 61.458,93.341,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.668 | Acc: 62.043,93.121,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.686 | Acc: 61.539,92.918,99.475,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.680 | Acc: 61.458,93.075,99.527,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.681 | Acc: 61.270,92.992,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.685 | Acc: 61.105,92.975,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.688 | Acc: 61.242,92.969,99.579,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.686 | Acc: 61.413,92.964,99.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.689 | Acc: 61.283,92.921,99.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.697 | Acc: 61.050,92.907,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.701 | Acc: 60.952,92.845,99.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.705 | Acc: 60.908,92.790,99.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.706 | Acc: 60.932,92.759,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.711 | Acc: 60.840,92.691,99.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.711 | Acc: 60.816,92.704,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.712 | Acc: 60.879,92.699,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.713 | Acc: 60.862,92.696,99.565,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.715 | Acc: 60.853,92.622,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.714 | Acc: 60.890,92.589,99.571,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.687 | Acc: 53.125,75.781,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.979 | Acc: 52.976,69.866,76.376,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.091 | Acc: 52.096,68.598,75.210,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.106 | Acc: 52.049,69.019,74.757,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 170
Batch: 0 | Loss: 1.505 | Acc: 67.969,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.630 | Acc: 63.430,92.411,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.653 | Acc: 61.909,93.026,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.654 | Acc: 61.988,93.110,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.679 | Acc: 61.680,92.969,99.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.705 | Acc: 61.077,92.760,99.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.709 | Acc: 61.092,92.698,99.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.708 | Acc: 61.159,92.697,99.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.709 | Acc: 61.287,92.682,99.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.706 | Acc: 61.300,92.762,99.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.702 | Acc: 61.206,92.872,99.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.699 | Acc: 61.270,92.898,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.700 | Acc: 61.190,92.836,99.598,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.700 | Acc: 61.240,92.855,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.699 | Acc: 61.224,92.821,99.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.699 | Acc: 61.197,92.810,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.700 | Acc: 61.174,92.794,99.579,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.702 | Acc: 61.160,92.804,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.701 | Acc: 61.156,92.787,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.698 | Acc: 61.220,92.815,99.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.624 | Acc: 51.562,73.438,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.954 | Acc: 53.013,69.606,76.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.049 | Acc: 52.458,69.245,75.400,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.074 | Acc: 52.190,69.326,74.910,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 171
Batch: 0 | Loss: 1.653 | Acc: 60.938,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.680 | Acc: 61.496,93.415,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.686 | Acc: 61.452,93.274,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.670 | Acc: 61.668,93.379,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.672 | Acc: 61.661,93.220,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.680 | Acc: 61.812,93.007,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.689 | Acc: 61.532,92.962,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.690 | Acc: 61.458,93.019,99.584,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.684 | Acc: 61.563,93.051,99.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.689 | Acc: 61.585,92.977,99.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.685 | Acc: 61.598,93.035,99.553,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.686 | Acc: 61.616,92.962,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.688 | Acc: 61.631,92.917,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.694 | Acc: 61.575,92.870,99.557,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.692 | Acc: 61.619,92.874,99.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.692 | Acc: 61.584,92.901,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.696 | Acc: 61.488,92.888,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.695 | Acc: 61.494,92.905,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.699 | Acc: 61.437,92.858,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.697 | Acc: 61.382,92.883,99.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.663 | Acc: 51.562,72.656,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 3.981 | Acc: 53.237,70.015,76.376,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.079 | Acc: 51.791,68.864,75.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.088 | Acc: 51.844,69.109,74.641,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 172
Batch: 0 | Loss: 1.744 | Acc: 60.156,91.406,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.711 | Acc: 60.045,93.304,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.698 | Acc: 61.166,93.331,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.687 | Acc: 61.142,93.750,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.689 | Acc: 61.323,93.547,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.695 | Acc: 61.139,93.410,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.689 | Acc: 61.092,93.447,99.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.689 | Acc: 61.176,93.412,99.579,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.686 | Acc: 61.054,93.430,99.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.693 | Acc: 60.843,93.357,99.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.686 | Acc: 60.965,93.420,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.684 | Acc: 61.082,93.404,99.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.684 | Acc: 61.106,93.322,99.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.684 | Acc: 61.120,93.292,99.557,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.685 | Acc: 61.182,93.236,99.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.687 | Acc: 61.124,93.236,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.685 | Acc: 61.213,93.227,99.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.688 | Acc: 61.144,93.168,99.530,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.691 | Acc: 61.141,93.088,99.537,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.690 | Acc: 61.190,93.036,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.676 | Acc: 50.000,71.875,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.026 | Acc: 52.939,69.010,75.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.097 | Acc: 51.867,68.350,74.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.104 | Acc: 51.972,68.635,74.449,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 173
Batch: 0 | Loss: 1.662 | Acc: 59.375,94.531,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.661 | Acc: 60.826,93.415,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.669 | Acc: 61.643,93.045,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.674 | Acc: 61.693,93.122,99.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.682 | Acc: 61.516,93.065,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.693 | Acc: 61.301,93.046,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.692 | Acc: 61.080,93.053,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.683 | Acc: 61.392,93.102,99.535,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.681 | Acc: 61.403,93.168,99.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.686 | Acc: 61.296,93.111,99.568,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.684 | Acc: 61.338,93.101,99.584,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.682 | Acc: 61.383,93.039,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.686 | Acc: 61.323,93.004,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.691 | Acc: 61.081,92.951,99.599,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.688 | Acc: 61.174,93.016,99.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.690 | Acc: 61.140,93.031,99.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.684 | Acc: 61.298,93.081,99.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.681 | Acc: 61.327,93.086,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.682 | Acc: 61.347,93.029,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.683 | Acc: 61.307,93.008,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.518 | Acc: 54.688,75.000,79.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.000 | Acc: 53.423,69.978,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.078 | Acc: 52.344,69.169,75.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.098 | Acc: 52.318,69.198,74.603,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 174
Batch: 0 | Loss: 1.613 | Acc: 62.500,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.693 | Acc: 61.607,92.894,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.699 | Acc: 61.566,93.083,99.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.687 | Acc: 61.719,93.263,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.687 | Acc: 61.699,93.355,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.678 | Acc: 61.564,93.394,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.671 | Acc: 61.719,93.427,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.666 | Acc: 61.686,93.362,99.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.667 | Acc: 61.641,93.347,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.665 | Acc: 61.740,93.331,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.662 | Acc: 61.828,93.319,99.584,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.663 | Acc: 61.733,93.326,99.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.664 | Acc: 61.767,93.273,99.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.666 | Acc: 61.749,93.220,99.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.672 | Acc: 61.658,93.188,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.674 | Acc: 61.524,93.171,99.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.676 | Acc: 61.485,93.166,99.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.678 | Acc: 61.409,93.143,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.679 | Acc: 61.383,93.131,99.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.680 | Acc: 61.403,93.110,99.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.511 | Acc: 54.688,73.438,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.027 | Acc: 53.162,69.680,76.302,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.120 | Acc: 52.325,68.998,75.267,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.147 | Acc: 52.126,68.865,74.667,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 175
Batch: 0 | Loss: 1.579 | Acc: 59.375,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.663 | Acc: 61.421,93.824,99.442,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.639 | Acc: 61.261,93.750,99.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.633 | Acc: 61.885,93.955,99.385,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.630 | Acc: 61.902,93.711,99.373,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.648 | Acc: 61.757,93.518,99.443,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.646 | Acc: 61.783,93.563,99.471,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.644 | Acc: 61.807,93.600,99.474,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.642 | Acc: 61.796,93.614,99.481,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.649 | Acc: 61.701,93.539,99.482,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.659 | Acc: 61.556,93.466,99.499,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.662 | Acc: 61.485,93.414,99.502,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.662 | Acc: 61.592,93.439,99.507,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.664 | Acc: 61.551,93.451,99.518,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.662 | Acc: 61.560,93.494,99.541,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.667 | Acc: 61.516,93.402,99.538,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.668 | Acc: 61.526,93.380,99.550,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.669 | Acc: 61.492,93.335,99.553,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.667 | Acc: 61.582,93.313,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.669 | Acc: 61.542,93.303,99.541,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.670 | Acc: 53.906,73.438,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.040 | Acc: 53.162,69.792,75.967,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.122 | Acc: 51.963,68.941,74.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.144 | Acc: 52.139,68.929,74.449,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 176
Batch: 0 | Loss: 1.639 | Acc: 58.594,93.750,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.634 | Acc: 62.463,92.820,99.516,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.638 | Acc: 62.157,93.674,99.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.654 | Acc: 62.065,93.584,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.648 | Acc: 62.124,93.769,99.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.652 | Acc: 61.843,93.897,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.645 | Acc: 61.874,93.731,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.638 | Acc: 62.073,93.872,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.643 | Acc: 61.811,93.808,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.642 | Acc: 61.727,93.832,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.645 | Acc: 61.734,93.793,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.649 | Acc: 61.779,93.729,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.650 | Acc: 61.771,93.763,99.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.652 | Acc: 61.755,93.771,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.656 | Acc: 61.624,93.719,99.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.657 | Acc: 61.651,93.646,99.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.662 | Acc: 61.568,93.611,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.663 | Acc: 61.570,93.583,99.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.663 | Acc: 61.589,93.508,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.664 | Acc: 61.528,93.512,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.723 | Acc: 52.344,69.531,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.028 | Acc: 52.641,69.122,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.114 | Acc: 51.829,68.388,74.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.132 | Acc: 52.280,68.852,74.065,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 177
Batch: 0 | Loss: 1.386 | Acc: 64.844,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.659 | Acc: 62.314,93.564,99.442,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.656 | Acc: 61.909,93.883,99.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.665 | Acc: 61.296,93.840,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.653 | Acc: 61.709,93.654,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.647 | Acc: 61.742,93.789,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.657 | Acc: 61.635,93.627,99.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.652 | Acc: 61.769,93.761,99.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.653 | Acc: 61.801,93.750,99.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.662 | Acc: 61.572,93.603,99.594,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.659 | Acc: 61.606,93.602,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.657 | Acc: 61.588,93.633,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.654 | Acc: 61.735,93.575,99.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.656 | Acc: 61.692,93.511,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.655 | Acc: 61.719,93.539,99.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.662 | Acc: 61.623,93.423,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.661 | Acc: 61.617,93.431,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.662 | Acc: 61.552,93.429,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.660 | Acc: 61.619,93.438,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.661 | Acc: 61.629,93.406,99.545,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.741 | Acc: 52.344,71.094,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.040 | Acc: 52.753,68.899,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.130 | Acc: 52.268,68.521,74.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.140 | Acc: 52.024,68.840,74.436,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 178
Batch: 0 | Loss: 1.354 | Acc: 67.969,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.651 | Acc: 59.970,93.490,99.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.622 | Acc: 60.804,93.864,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.627 | Acc: 61.245,93.840,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.620 | Acc: 61.728,93.895,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.638 | Acc: 61.672,93.688,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.649 | Acc: 61.473,93.698,99.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.651 | Acc: 61.541,93.623,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.645 | Acc: 61.694,93.653,99.612,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.647 | Acc: 61.619,93.694,99.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.650 | Acc: 61.594,93.641,99.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.649 | Acc: 61.666,93.619,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.652 | Acc: 61.579,93.588,99.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.651 | Acc: 61.800,93.534,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.651 | Acc: 61.841,93.519,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.650 | Acc: 61.893,93.553,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.650 | Acc: 61.819,93.529,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.653 | Acc: 61.762,93.546,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.653 | Acc: 61.738,93.531,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.654 | Acc: 61.682,93.537,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.640 | Acc: 53.906,71.094,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.059 | Acc: 53.237,69.085,76.228,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.151 | Acc: 52.496,68.216,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.152 | Acc: 52.459,68.763,74.731,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 179
Batch: 0 | Loss: 1.592 | Acc: 60.938,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.549 | Acc: 63.653,94.978,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.583 | Acc: 63.014,94.531,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.603 | Acc: 62.538,94.301,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.597 | Acc: 62.799,94.300,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.597 | Acc: 62.817,94.253,99.528,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.615 | Acc: 62.442,94.092,99.542,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.625 | Acc: 62.267,94.005,99.540,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.634 | Acc: 62.005,93.997,99.549,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.633 | Acc: 61.904,94.005,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.633 | Acc: 62.018,94.026,99.565,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.634 | Acc: 62.030,93.941,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.637 | Acc: 61.916,93.935,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.642 | Acc: 61.898,93.888,99.560,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.642 | Acc: 61.880,93.839,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.646 | Acc: 61.789,93.781,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.645 | Acc: 61.792,93.774,99.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.648 | Acc: 61.762,93.725,99.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.648 | Acc: 61.730,93.735,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.650 | Acc: 61.713,93.686,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.724 | Acc: 50.000,73.438,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.035 | Acc: 53.534,69.457,75.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.135 | Acc: 52.268,68.502,74.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.129 | Acc: 52.190,68.968,74.552,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 180
Batch: 0 | Loss: 1.818 | Acc: 53.906,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.606 | Acc: 62.574,93.973,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.606 | Acc: 62.005,94.417,99.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.609 | Acc: 62.013,94.416,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.622 | Acc: 61.709,94.329,99.537,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.637 | Acc: 61.494,94.191,99.536,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.635 | Acc: 61.667,94.131,99.522,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.628 | Acc: 61.763,94.160,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.633 | Acc: 61.617,94.095,99.539,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.637 | Acc: 61.581,94.022,99.538,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.635 | Acc: 61.688,94.003,99.534,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.637 | Acc: 61.705,94.019,99.544,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.643 | Acc: 61.657,93.925,99.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.647 | Acc: 61.554,93.864,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.647 | Acc: 61.630,93.797,99.550,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.648 | Acc: 61.636,93.771,99.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.651 | Acc: 61.544,93.677,99.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.650 | Acc: 61.533,93.668,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.646 | Acc: 61.606,93.629,99.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.649 | Acc: 61.532,93.586,99.549,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.710 | Acc: 53.125,73.438,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.037 | Acc: 53.423,69.829,75.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.139 | Acc: 52.572,68.921,74.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.151 | Acc: 52.254,68.968,74.424,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 181
Batch: 0 | Loss: 1.727 | Acc: 57.031,97.656,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.613 | Acc: 60.938,94.829,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.603 | Acc: 62.195,94.550,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.616 | Acc: 61.860,94.326,99.565,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.626 | Acc: 61.622,94.367,99.585,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.619 | Acc: 61.827,94.338,99.544,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.609 | Acc: 62.093,94.325,99.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.611 | Acc: 62.140,94.265,99.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.614 | Acc: 62.155,94.187,99.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.616 | Acc: 62.021,94.212,99.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.616 | Acc: 61.964,94.104,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.618 | Acc: 61.934,94.089,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.623 | Acc: 61.916,93.980,99.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.625 | Acc: 61.830,93.980,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.628 | Acc: 61.799,93.931,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.631 | Acc: 61.732,93.916,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.630 | Acc: 61.780,93.889,99.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.632 | Acc: 61.778,93.830,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.637 | Acc: 61.712,93.769,99.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.634 | Acc: 61.809,93.771,99.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.710 | Acc: 53.906,74.219,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.035 | Acc: 53.981,69.494,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.129 | Acc: 52.744,68.807,74.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.139 | Acc: 52.536,69.134,74.667,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 182
Batch: 0 | Loss: 1.728 | Acc: 64.844,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.551 | Acc: 63.467,94.717,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.551 | Acc: 64.139,94.703,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.583 | Acc: 63.204,94.775,99.718,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.596 | Acc: 62.799,94.686,99.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.583 | Acc: 63.018,94.740,99.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.591 | Acc: 62.842,94.602,99.671,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.594 | Acc: 62.777,94.576,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.596 | Acc: 62.718,94.497,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.606 | Acc: 62.573,94.410,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.605 | Acc: 62.519,94.419,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.605 | Acc: 62.443,94.408,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.608 | Acc: 62.442,94.324,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.609 | Acc: 62.539,94.259,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.613 | Acc: 62.383,94.239,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.613 | Acc: 62.365,94.196,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.616 | Acc: 62.291,94.144,99.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.621 | Acc: 62.207,94.130,99.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.619 | Acc: 62.210,94.135,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.623 | Acc: 62.121,94.078,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.819 | Acc: 53.906,73.438,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.103 | Acc: 53.125,68.601,75.149,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.174 | Acc: 52.458,68.083,74.447,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.181 | Acc: 52.267,68.455,74.129,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 183
Batch: 0 | Loss: 1.508 | Acc: 63.281,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.612 | Acc: 63.244,95.089,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.635 | Acc: 62.119,94.455,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.598 | Acc: 62.641,94.698,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.609 | Acc: 62.365,94.493,99.633,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.622 | Acc: 62.191,94.253,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.615 | Acc: 62.351,94.241,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.626 | Acc: 62.068,94.066,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.625 | Acc: 62.107,94.027,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.624 | Acc: 62.090,93.996,99.633,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.628 | Acc: 62.034,93.929,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.631 | Acc: 61.987,93.881,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.629 | Acc: 61.933,93.880,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.627 | Acc: 61.991,93.930,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.629 | Acc: 61.897,93.906,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.626 | Acc: 61.926,93.932,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.628 | Acc: 61.848,93.928,99.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.625 | Acc: 61.936,93.965,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.624 | Acc: 62.011,93.969,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.627 | Acc: 61.963,93.939,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.733 | Acc: 49.219,71.875,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.097 | Acc: 53.274,69.234,75.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.159 | Acc: 52.191,68.407,74.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.171 | Acc: 52.024,68.737,74.654,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 184
Batch: 0 | Loss: 1.590 | Acc: 59.375,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.634 | Acc: 61.644,93.899,99.516,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.614 | Acc: 62.481,94.207,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.608 | Acc: 62.436,94.237,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.610 | Acc: 62.076,94.242,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.616 | Acc: 61.912,94.199,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.613 | Acc: 62.184,94.144,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.612 | Acc: 62.190,94.077,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.608 | Acc: 62.262,94.065,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.606 | Acc: 62.319,94.009,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.604 | Acc: 62.286,93.999,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.608 | Acc: 62.168,93.976,99.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.612 | Acc: 62.056,93.961,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.610 | Acc: 62.144,94.001,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.610 | Acc: 62.111,93.939,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.611 | Acc: 62.116,93.932,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.612 | Acc: 62.137,93.937,99.630,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.614 | Acc: 62.037,93.901,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.614 | Acc: 62.013,93.930,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.614 | Acc: 61.953,93.937,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.637 | Acc: 54.688,73.438,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.104 | Acc: 52.790,69.196,75.967,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.159 | Acc: 52.058,68.617,75.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.165 | Acc: 52.164,68.981,74.641,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 185
Batch: 0 | Loss: 2.042 | Acc: 53.125,89.062,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.616 | Acc: 61.496,94.382,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.598 | Acc: 62.062,94.550,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.609 | Acc: 61.872,94.173,99.731,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.602 | Acc: 61.728,94.242,99.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.594 | Acc: 61.796,94.400,99.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.597 | Acc: 61.674,94.460,99.697,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.601 | Acc: 61.741,94.348,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.599 | Acc: 61.966,94.230,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.595 | Acc: 62.120,94.216,99.633,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.599 | Acc: 61.983,94.174,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.607 | Acc: 61.888,94.160,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.610 | Acc: 61.826,94.110,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.607 | Acc: 62.033,94.145,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.606 | Acc: 61.991,94.150,99.630,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.608 | Acc: 61.932,94.113,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.607 | Acc: 61.935,94.105,99.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.606 | Acc: 61.978,94.121,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.609 | Acc: 61.974,94.064,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.614 | Acc: 61.883,94.019,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.756 | Acc: 52.344,73.438,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.122 | Acc: 53.683,68.676,74.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.199 | Acc: 52.515,68.464,74.295,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.202 | Acc: 52.497,68.840,74.116,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 186
Batch: 0 | Loss: 1.344 | Acc: 65.625,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.549 | Acc: 63.504,94.606,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.575 | Acc: 62.424,94.341,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.571 | Acc: 62.462,94.531,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.582 | Acc: 62.346,94.309,99.585,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.593 | Acc: 62.260,94.067,99.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.599 | Acc: 62.003,94.105,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.603 | Acc: 61.891,94.199,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.604 | Acc: 61.898,94.235,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.605 | Acc: 61.969,94.216,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.607 | Acc: 61.909,94.150,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.610 | Acc: 61.941,94.065,99.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.604 | Acc: 62.121,94.126,99.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.602 | Acc: 62.156,94.136,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.605 | Acc: 62.091,94.106,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.605 | Acc: 62.080,94.137,99.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.604 | Acc: 62.196,94.098,99.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.605 | Acc: 62.188,94.112,99.592,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.606 | Acc: 62.188,94.131,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.607 | Acc: 62.158,94.129,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.781 | Acc: 51.562,73.438,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.100 | Acc: 53.460,69.308,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.152 | Acc: 52.630,68.693,74.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.161 | Acc: 52.485,68.737,74.757,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 187
Batch: 0 | Loss: 1.691 | Acc: 62.500,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.533 | Acc: 63.207,95.089,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.544 | Acc: 63.053,94.970,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.555 | Acc: 63.064,94.672,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.552 | Acc: 63.301,94.715,99.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.560 | Acc: 63.258,94.632,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.568 | Acc: 63.049,94.654,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.580 | Acc: 62.821,94.559,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.582 | Acc: 62.878,94.517,99.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.584 | Acc: 62.694,94.441,99.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.584 | Acc: 62.799,94.419,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.587 | Acc: 62.670,94.379,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.589 | Acc: 62.597,94.356,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.590 | Acc: 62.569,94.391,99.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.593 | Acc: 62.578,94.426,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.593 | Acc: 62.617,94.409,99.564,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.595 | Acc: 62.546,94.388,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.597 | Acc: 62.509,94.357,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.599 | Acc: 62.494,94.326,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.599 | Acc: 62.449,94.261,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.836 | Acc: 49.219,71.875,79.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.137 | Acc: 52.344,68.415,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.216 | Acc: 51.810,67.645,74.276,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.227 | Acc: 51.665,68.186,74.065,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 188
Batch: 0 | Loss: 1.408 | Acc: 67.188,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.603 | Acc: 61.868,94.568,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.568 | Acc: 63.186,94.360,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.577 | Acc: 63.038,94.275,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.578 | Acc: 63.108,94.300,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.589 | Acc: 62.717,94.222,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.593 | Acc: 62.571,94.241,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.599 | Acc: 62.605,94.099,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.597 | Acc: 62.655,94.211,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.597 | Acc: 62.629,94.212,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.597 | Acc: 62.593,94.232,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.597 | Acc: 62.588,94.227,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.595 | Acc: 62.575,94.207,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.596 | Acc: 62.443,94.214,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.597 | Acc: 62.350,94.228,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.596 | Acc: 62.362,94.230,99.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.597 | Acc: 62.352,94.244,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.600 | Acc: 62.292,94.178,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.601 | Acc: 62.316,94.159,99.630,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.601 | Acc: 62.320,94.154,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.843 | Acc: 53.125,73.438,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.102 | Acc: 53.795,69.196,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.167 | Acc: 53.106,68.712,74.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.184 | Acc: 52.766,68.942,74.718,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 189
Batch: 0 | Loss: 1.623 | Acc: 60.938,92.969,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.584 | Acc: 63.058,94.568,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.566 | Acc: 62.405,94.855,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.589 | Acc: 61.872,94.531,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.591 | Acc: 61.950,94.435,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.584 | Acc: 62.283,94.353,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.581 | Acc: 62.448,94.415,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.581 | Acc: 62.522,94.382,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.583 | Acc: 62.340,94.391,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.589 | Acc: 62.263,94.389,99.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.593 | Acc: 62.224,94.368,99.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.594 | Acc: 62.263,94.362,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.590 | Acc: 62.422,94.411,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.591 | Acc: 62.386,94.403,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.594 | Acc: 62.444,94.326,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.597 | Acc: 62.396,94.277,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.597 | Acc: 62.381,94.261,99.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.600 | Acc: 62.392,94.201,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.603 | Acc: 62.320,94.191,99.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.600 | Acc: 62.369,94.205,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.947 | Acc: 50.000,71.875,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.131 | Acc: 53.497,68.973,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.196 | Acc: 52.401,68.369,74.962,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.201 | Acc: 52.267,68.558,74.744,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 190
Batch: 0 | Loss: 1.578 | Acc: 67.969,92.969,96.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.554 | Acc: 63.616,94.903,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.583 | Acc: 62.348,94.836,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.576 | Acc: 62.436,94.967,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.575 | Acc: 62.297,94.907,99.585,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.579 | Acc: 62.152,94.995,99.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.576 | Acc: 62.306,95.003,99.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.573 | Acc: 62.284,95.002,99.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.571 | Acc: 62.432,94.997,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.572 | Acc: 62.319,94.851,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.569 | Acc: 62.418,94.854,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.571 | Acc: 62.443,94.835,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.577 | Acc: 62.344,94.745,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.575 | Acc: 62.404,94.723,99.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.580 | Acc: 62.272,94.679,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.584 | Acc: 62.222,94.596,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.585 | Acc: 62.225,94.563,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.588 | Acc: 62.193,94.531,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.587 | Acc: 62.271,94.518,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.587 | Acc: 62.272,94.494,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.852 | Acc: 51.562,71.094,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.116 | Acc: 53.237,68.862,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.177 | Acc: 52.306,68.312,75.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.196 | Acc: 52.305,68.635,74.821,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 191
Batch: 0 | Loss: 1.734 | Acc: 54.688,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.606 | Acc: 62.351,94.308,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.580 | Acc: 62.900,94.474,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.570 | Acc: 62.923,94.493,99.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.571 | Acc: 62.973,94.522,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.584 | Acc: 62.330,94.299,99.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.587 | Acc: 62.287,94.325,99.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.583 | Acc: 62.422,94.315,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.585 | Acc: 62.398,94.357,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.585 | Acc: 62.345,94.341,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.587 | Acc: 62.247,94.349,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.589 | Acc: 62.192,94.319,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.585 | Acc: 62.312,94.376,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.585 | Acc: 62.362,94.370,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.586 | Acc: 62.342,94.373,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.586 | Acc: 62.292,94.360,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.587 | Acc: 62.264,94.344,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.585 | Acc: 62.356,94.357,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.591 | Acc: 62.279,94.306,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.590 | Acc: 62.354,94.316,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.861 | Acc: 52.344,72.656,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.181 | Acc: 52.307,68.564,75.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.258 | Acc: 51.753,67.988,74.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.267 | Acc: 51.665,68.238,74.462,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 192
Batch: 0 | Loss: 1.747 | Acc: 60.938,91.406,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.520 | Acc: 64.323,94.048,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.550 | Acc: 63.148,94.398,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.563 | Acc: 62.718,94.634,99.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.574 | Acc: 62.577,94.637,99.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.576 | Acc: 62.237,94.562,99.559,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.572 | Acc: 62.287,94.531,99.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.580 | Acc: 62.096,94.537,99.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.587 | Acc: 62.015,94.541,99.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.586 | Acc: 62.245,94.505,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.588 | Acc: 62.228,94.422,99.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.591 | Acc: 62.040,94.393,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.589 | Acc: 62.046,94.327,99.605,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.587 | Acc: 62.213,94.334,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.590 | Acc: 62.155,94.320,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.589 | Acc: 62.222,94.331,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.592 | Acc: 62.274,94.305,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.594 | Acc: 62.305,94.252,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.595 | Acc: 62.301,94.187,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.596 | Acc: 62.295,94.158,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.752 | Acc: 53.125,73.438,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.191 | Acc: 53.311,68.304,75.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.248 | Acc: 52.268,67.969,74.409,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.250 | Acc: 52.203,68.404,74.257,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 193
Batch: 0 | Loss: 1.817 | Acc: 57.812,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.583 | Acc: 62.537,94.606,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.571 | Acc: 62.633,94.417,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.569 | Acc: 62.756,94.557,99.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.561 | Acc: 62.867,94.724,99.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.560 | Acc: 62.910,94.717,99.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.565 | Acc: 62.810,94.667,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.559 | Acc: 62.971,94.770,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.563 | Acc: 62.854,94.701,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.569 | Acc: 62.621,94.669,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.574 | Acc: 62.574,94.609,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.573 | Acc: 62.518,94.662,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.578 | Acc: 62.377,94.577,99.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.576 | Acc: 62.419,94.537,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.575 | Acc: 62.464,94.515,99.583,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.574 | Acc: 62.495,94.542,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.576 | Acc: 62.454,94.497,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.577 | Acc: 62.514,94.495,99.601,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.579 | Acc: 62.444,94.501,99.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.582 | Acc: 62.363,94.494,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.800 | Acc: 54.688,74.219,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.135 | Acc: 53.609,69.196,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.214 | Acc: 52.229,68.464,74.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.241 | Acc: 52.036,68.417,73.988,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 194
Batch: 0 | Loss: 1.488 | Acc: 59.375,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.514 | Acc: 63.244,95.015,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.521 | Acc: 63.453,94.779,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.540 | Acc: 62.961,94.826,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.554 | Acc: 62.394,94.869,99.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.552 | Acc: 62.492,94.879,99.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.548 | Acc: 62.726,94.906,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.553 | Acc: 62.467,94.808,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.562 | Acc: 62.262,94.779,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.562 | Acc: 62.383,94.782,99.663,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.570 | Acc: 62.259,94.652,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.572 | Acc: 62.200,94.634,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.571 | Acc: 62.211,94.590,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.569 | Acc: 62.287,94.618,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.571 | Acc: 62.233,94.626,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.576 | Acc: 62.113,94.562,99.663,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.573 | Acc: 62.232,94.536,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.573 | Acc: 62.145,94.579,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.573 | Acc: 62.197,94.572,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.573 | Acc: 62.235,94.548,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.992 | Acc: 54.688,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.172 | Acc: 52.976,68.118,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.231 | Acc: 52.306,67.664,74.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.248 | Acc: 52.049,67.994,74.513,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 195
Batch: 0 | Loss: 1.311 | Acc: 65.625,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.576 | Acc: 62.165,95.238,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.556 | Acc: 63.091,94.779,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.560 | Acc: 62.743,94.864,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.554 | Acc: 62.944,94.840,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.570 | Acc: 62.423,94.802,99.598,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.567 | Acc: 62.448,94.770,99.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.574 | Acc: 62.306,94.697,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.573 | Acc: 62.384,94.691,99.578,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.566 | Acc: 62.578,94.605,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.568 | Acc: 62.597,94.570,99.607,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.565 | Acc: 62.673,94.637,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.567 | Acc: 62.607,94.609,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.570 | Acc: 62.659,94.558,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.574 | Acc: 62.670,94.542,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.576 | Acc: 62.653,94.479,99.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.579 | Acc: 62.605,94.446,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.579 | Acc: 62.603,94.460,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.580 | Acc: 62.530,94.468,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.579 | Acc: 62.521,94.476,99.610,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.898 | Acc: 52.344,69.531,79.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.139 | Acc: 53.646,68.862,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.206 | Acc: 52.572,68.388,74.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.221 | Acc: 52.613,68.648,74.347,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 196
Batch: 0 | Loss: 1.525 | Acc: 65.625,93.750,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.535 | Acc: 62.277,95.536,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.544 | Acc: 62.214,95.065,99.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.547 | Acc: 62.244,95.031,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.543 | Acc: 62.741,95.014,99.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.542 | Acc: 62.856,95.042,99.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.554 | Acc: 62.661,94.925,99.690,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.556 | Acc: 62.666,94.786,99.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.561 | Acc: 62.573,94.720,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.556 | Acc: 62.543,94.743,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.553 | Acc: 62.609,94.831,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.552 | Acc: 62.723,94.786,99.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.555 | Acc: 62.552,94.794,99.686,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.557 | Acc: 62.569,94.741,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.559 | Acc: 62.578,94.729,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.560 | Acc: 62.606,94.666,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.563 | Acc: 62.556,94.653,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.563 | Acc: 62.596,94.639,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.564 | Acc: 62.630,94.670,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.565 | Acc: 62.646,94.624,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.930 | Acc: 52.344,68.750,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.148 | Acc: 53.757,68.824,75.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.230 | Acc: 52.363,68.026,74.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.238 | Acc: 52.241,68.494,74.372,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 197
Batch: 0 | Loss: 1.709 | Acc: 61.719,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.493 | Acc: 64.583,96.057,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.512 | Acc: 63.624,95.636,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.540 | Acc: 62.910,95.530,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.540 | Acc: 62.693,95.399,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.531 | Acc: 62.887,95.529,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.532 | Acc: 62.971,95.416,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.537 | Acc: 62.766,95.340,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.544 | Acc: 62.670,95.245,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.550 | Acc: 62.629,95.174,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.546 | Acc: 62.636,95.211,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.547 | Acc: 62.638,95.097,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.546 | Acc: 62.707,95.086,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.553 | Acc: 62.722,94.953,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.557 | Acc: 62.647,94.940,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.557 | Acc: 62.671,94.965,99.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.558 | Acc: 62.714,94.947,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.558 | Acc: 62.741,94.914,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.561 | Acc: 62.675,94.875,99.613,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.566 | Acc: 62.547,94.814,99.615,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.810 | Acc: 53.906,72.656,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.239 | Acc: 53.199,68.192,74.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.290 | Acc: 51.658,67.645,74.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.298 | Acc: 51.819,67.751,74.296,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 198
Batch: 0 | Loss: 1.750 | Acc: 58.594,94.531,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.539 | Acc: 62.760,94.940,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.535 | Acc: 63.624,95.312,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.543 | Acc: 62.923,95.146,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.542 | Acc: 62.693,95.197,99.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.541 | Acc: 62.833,95.166,99.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.548 | Acc: 62.771,95.138,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.538 | Acc: 62.921,95.213,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.542 | Acc: 62.849,95.152,99.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.548 | Acc: 62.789,95.114,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.552 | Acc: 62.659,95.002,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.557 | Acc: 62.560,94.938,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.555 | Acc: 62.526,94.979,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.557 | Acc: 62.479,95.001,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.559 | Acc: 62.486,94.971,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.564 | Acc: 62.404,94.905,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.561 | Acc: 62.517,94.894,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.559 | Acc: 62.585,94.845,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.559 | Acc: 62.621,94.854,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.557 | Acc: 62.705,94.849,99.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.897 | Acc: 54.688,67.188,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.190 | Acc: 52.865,68.452,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.279 | Acc: 51.848,67.873,74.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.304 | Acc: 51.755,68.122,74.270,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 199
Batch: 0 | Loss: 1.549 | Acc: 63.281,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.552 | Acc: 63.244,95.238,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.512 | Acc: 64.005,95.198,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.525 | Acc: 63.461,95.300,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.529 | Acc: 63.561,95.120,99.585,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.518 | Acc: 63.761,95.173,99.606,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.527 | Acc: 63.585,95.119,99.561,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.531 | Acc: 63.447,95.063,99.546,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.535 | Acc: 63.276,95.031,99.554,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.539 | Acc: 63.091,95.045,99.551,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.542 | Acc: 63.091,95.021,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.544 | Acc: 63.062,94.970,99.576,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.549 | Acc: 62.983,94.920,99.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.554 | Acc: 62.880,94.864,99.584,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.557 | Acc: 62.881,94.823,99.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.557 | Acc: 62.801,94.830,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.559 | Acc: 62.751,94.801,99.584,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.558 | Acc: 62.695,94.822,99.585,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.558 | Acc: 62.721,94.789,99.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.561 | Acc: 62.738,94.779,99.586,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.912 | Acc: 50.781,71.094,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.186 | Acc: 53.125,68.824,75.372,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.256 | Acc: 51.905,68.121,74.505,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.276 | Acc: 51.434,68.366,74.193,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 200
Batch: 0 | Loss: 1.409 | Acc: 67.969,95.312,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.549 | Acc: 62.686,95.312,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.520 | Acc: 62.710,95.751,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.509 | Acc: 63.128,95.607,99.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.516 | Acc: 63.194,95.312,99.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.530 | Acc: 63.041,95.243,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.536 | Acc: 62.997,95.216,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.538 | Acc: 62.849,95.102,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.542 | Acc: 62.893,95.060,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.543 | Acc: 62.932,95.066,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.540 | Acc: 62.998,95.052,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.542 | Acc: 62.832,95.065,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.544 | Acc: 62.795,95.021,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.545 | Acc: 62.898,94.965,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.544 | Acc: 62.970,94.965,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.546 | Acc: 62.946,94.936,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.547 | Acc: 62.863,94.940,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.551 | Acc: 62.809,94.921,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.556 | Acc: 62.714,94.867,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.559 | Acc: 62.662,94.835,99.653,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.766 | Acc: 54.688,73.438,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.189 | Acc: 53.125,69.234,75.149,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.277 | Acc: 52.382,68.388,74.276,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.286 | Acc: 52.331,68.353,74.244,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 201
Batch: 0 | Loss: 1.856 | Acc: 60.938,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.477 | Acc: 64.174,95.796,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.505 | Acc: 63.224,95.541,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.490 | Acc: 63.653,95.556,99.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.506 | Acc: 63.349,95.486,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.522 | Acc: 63.196,95.227,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.532 | Acc: 62.797,95.164,99.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.527 | Acc: 62.977,95.013,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.530 | Acc: 62.917,95.026,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.541 | Acc: 62.815,94.959,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.547 | Acc: 62.718,94.955,99.604,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.549 | Acc: 62.680,94.973,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.547 | Acc: 62.766,95.021,99.630,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.547 | Acc: 62.733,94.950,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.547 | Acc: 62.720,94.954,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.550 | Acc: 62.656,94.910,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.549 | Acc: 62.678,94.872,99.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.551 | Acc: 62.729,94.834,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.551 | Acc: 62.701,94.815,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.552 | Acc: 62.652,94.814,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.880 | Acc: 56.250,71.875,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.187 | Acc: 53.199,68.118,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.263 | Acc: 52.096,67.588,74.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.274 | Acc: 51.960,68.007,74.168,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 202
Batch: 0 | Loss: 1.348 | Acc: 64.844,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.535 | Acc: 62.500,95.610,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.528 | Acc: 62.938,95.427,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.535 | Acc: 62.590,95.223,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.536 | Acc: 62.616,95.091,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.539 | Acc: 62.608,95.019,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.539 | Acc: 62.700,94.944,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.540 | Acc: 62.672,94.947,99.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.537 | Acc: 62.752,94.939,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.542 | Acc: 62.772,94.825,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.543 | Acc: 62.683,94.838,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.544 | Acc: 62.755,94.832,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.543 | Acc: 62.795,94.878,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.545 | Acc: 62.701,94.864,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.545 | Acc: 62.686,94.862,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.547 | Acc: 62.666,94.825,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.548 | Acc: 62.675,94.870,99.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.549 | Acc: 62.674,94.884,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.550 | Acc: 62.719,94.839,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.550 | Acc: 62.775,94.859,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.953 | Acc: 54.688,71.094,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.186 | Acc: 52.976,69.271,75.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.274 | Acc: 51.886,68.312,74.238,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.282 | Acc: 51.985,68.609,74.065,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 203
Batch: 0 | Loss: 1.390 | Acc: 67.969,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.498 | Acc: 64.583,94.903,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.482 | Acc: 64.425,95.408,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.501 | Acc: 64.357,95.428,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.519 | Acc: 64.034,95.206,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.515 | Acc: 63.977,95.119,99.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.521 | Acc: 63.682,95.138,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.520 | Acc: 63.569,95.191,99.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.528 | Acc: 63.344,95.065,99.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.524 | Acc: 63.385,95.092,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.524 | Acc: 63.324,95.130,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.526 | Acc: 63.249,95.051,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.532 | Acc: 63.236,95.001,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.530 | Acc: 63.287,94.968,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.531 | Acc: 63.226,94.948,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.536 | Acc: 63.123,94.957,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.537 | Acc: 63.123,94.923,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.540 | Acc: 63.082,94.914,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.540 | Acc: 63.132,94.899,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.544 | Acc: 63.119,94.855,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.959 | Acc: 51.562,71.094,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.220 | Acc: 53.088,68.266,75.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.300 | Acc: 52.058,67.873,74.295,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.314 | Acc: 51.806,67.905,74.168,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 204
Batch: 0 | Loss: 1.451 | Acc: 64.844,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.502 | Acc: 62.946,95.685,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.511 | Acc: 62.824,95.465,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.497 | Acc: 63.332,95.594,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.514 | Acc: 62.992,95.380,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.518 | Acc: 63.072,95.367,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.520 | Acc: 63.184,95.319,99.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.521 | Acc: 63.320,95.257,99.612,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.525 | Acc: 63.291,95.288,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.529 | Acc: 63.415,95.252,99.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.528 | Acc: 63.390,95.254,99.631,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.533 | Acc: 63.257,95.136,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.535 | Acc: 63.197,95.115,99.630,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.538 | Acc: 63.188,95.079,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.542 | Acc: 63.062,95.040,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.544 | Acc: 63.042,95.032,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.547 | Acc: 63.004,94.996,99.608,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.547 | Acc: 63.013,94.978,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.546 | Acc: 62.985,94.973,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.550 | Acc: 62.892,94.960,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.885 | Acc: 51.562,71.875,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.172 | Acc: 53.906,69.085,76.228,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.260 | Acc: 52.477,68.178,74.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.277 | Acc: 52.062,68.225,74.398,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 205
Batch: 0 | Loss: 1.483 | Acc: 67.969,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.466 | Acc: 65.997,96.205,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.496 | Acc: 65.511,96.018,99.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.499 | Acc: 64.882,95.735,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.509 | Acc: 64.468,95.544,99.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.501 | Acc: 64.565,95.506,99.575,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.511 | Acc: 64.269,95.390,99.574,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.513 | Acc: 64.096,95.268,99.596,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.517 | Acc: 63.922,95.235,99.573,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.524 | Acc: 63.782,95.123,99.577,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.527 | Acc: 63.569,95.126,99.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.530 | Acc: 63.483,95.118,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.534 | Acc: 63.366,95.121,99.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.539 | Acc: 63.275,95.058,99.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.544 | Acc: 63.164,95.004,99.580,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.545 | Acc: 63.136,94.998,99.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.551 | Acc: 63.057,94.913,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.552 | Acc: 63.002,94.905,99.588,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.551 | Acc: 63.015,94.873,99.597,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.552 | Acc: 63.027,94.868,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.113 | Acc: 51.562,68.750,72.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.169 | Acc: 53.237,68.564,75.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.256 | Acc: 52.153,68.216,74.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.272 | Acc: 52.216,68.494,73.924,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 206
Batch: 0 | Loss: 1.319 | Acc: 69.531,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.487 | Acc: 63.988,95.610,99.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.489 | Acc: 64.024,95.922,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.503 | Acc: 63.627,95.812,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.500 | Acc: 63.474,95.920,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.498 | Acc: 63.544,95.846,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.510 | Acc: 63.494,95.745,99.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.521 | Acc: 63.348,95.529,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.528 | Acc: 63.179,95.390,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.526 | Acc: 63.169,95.334,99.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.527 | Acc: 63.192,95.274,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.528 | Acc: 63.214,95.281,99.731,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.526 | Acc: 63.229,95.312,99.731,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.528 | Acc: 63.188,95.286,99.743,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.529 | Acc: 63.184,95.240,99.747,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.531 | Acc: 63.227,95.191,99.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.529 | Acc: 63.325,95.179,99.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.526 | Acc: 63.375,95.207,99.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.530 | Acc: 63.327,95.146,99.712,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.536 | Acc: 63.197,95.087,99.713,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.972 | Acc: 51.562,68.750,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.253 | Acc: 52.530,67.894,75.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.296 | Acc: 52.020,67.645,74.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.300 | Acc: 52.036,68.084,74.270,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 207
Batch: 0 | Loss: 1.342 | Acc: 64.844,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.527 | Acc: 62.500,94.717,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.524 | Acc: 63.377,94.760,99.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.533 | Acc: 63.102,94.967,99.513,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.533 | Acc: 62.973,95.110,99.556,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.534 | Acc: 62.763,95.057,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.531 | Acc: 62.803,95.067,99.593,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.535 | Acc: 62.611,95.124,99.618,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.539 | Acc: 62.582,95.036,99.617,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.536 | Acc: 62.742,95.002,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.537 | Acc: 62.753,94.990,99.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.535 | Acc: 62.776,95.026,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.534 | Acc: 62.889,95.001,99.653,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.536 | Acc: 62.892,94.950,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.536 | Acc: 62.942,94.904,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.537 | Acc: 62.991,94.905,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.536 | Acc: 63.033,94.908,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.537 | Acc: 63.048,94.921,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.535 | Acc: 63.052,94.953,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.540 | Acc: 63.002,94.900,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.929 | Acc: 53.125,73.438,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.250 | Acc: 52.344,68.341,75.186,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.301 | Acc: 51.829,68.007,74.371,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.309 | Acc: 51.857,68.161,74.168,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 208
Batch: 0 | Loss: 1.728 | Acc: 55.469,91.406,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.487 | Acc: 63.839,95.796,99.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.504 | Acc: 63.434,95.675,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.511 | Acc: 63.281,95.556,99.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.512 | Acc: 63.272,95.611,99.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.502 | Acc: 63.281,95.398,99.698,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.507 | Acc: 63.326,95.274,99.684,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.501 | Acc: 63.553,95.335,99.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.507 | Acc: 63.553,95.249,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.507 | Acc: 63.635,95.222,99.663,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.506 | Acc: 63.600,95.281,99.689,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.513 | Acc: 63.529,95.249,99.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.518 | Acc: 63.521,95.193,99.663,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.522 | Acc: 63.368,95.166,99.671,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.525 | Acc: 63.348,95.126,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.526 | Acc: 63.294,95.092,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.528 | Acc: 63.264,95.057,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.529 | Acc: 63.217,95.047,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.532 | Acc: 63.156,95.025,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.532 | Acc: 63.136,95.017,99.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.036 | Acc: 53.906,73.438,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.259 | Acc: 52.976,68.862,74.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.310 | Acc: 52.153,68.102,74.390,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.324 | Acc: 51.921,68.084,74.168,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 209
Batch: 0 | Loss: 1.580 | Acc: 58.594,97.656,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.517 | Acc: 63.281,95.759,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.510 | Acc: 63.396,95.636,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.512 | Acc: 63.256,95.569,99.718,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.510 | Acc: 63.243,95.737,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.511 | Acc: 63.560,95.599,99.737,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.509 | Acc: 63.662,95.603,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.511 | Acc: 63.564,95.606,99.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.514 | Acc: 63.529,95.526,99.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.517 | Acc: 63.579,95.472,99.681,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.518 | Acc: 63.526,95.421,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.518 | Acc: 63.511,95.362,99.671,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.517 | Acc: 63.476,95.267,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.516 | Acc: 63.422,95.262,99.671,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.520 | Acc: 63.320,95.212,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.520 | Acc: 63.346,95.224,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.521 | Acc: 63.259,95.213,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.525 | Acc: 63.180,95.180,99.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.528 | Acc: 63.117,95.124,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.530 | Acc: 63.101,95.087,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.021 | Acc: 50.000,72.656,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.317 | Acc: 52.939,67.634,75.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.360 | Acc: 51.696,67.588,74.085,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.380 | Acc: 51.524,67.764,74.014,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 210
Batch: 0 | Loss: 1.398 | Acc: 61.719,96.875,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.517 | Acc: 62.835,95.647,99.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.535 | Acc: 62.405,95.484,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.528 | Acc: 62.615,95.415,99.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.518 | Acc: 62.992,95.505,99.711,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.504 | Acc: 63.304,95.599,99.698,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.506 | Acc: 63.378,95.551,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.511 | Acc: 63.314,95.423,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.511 | Acc: 63.378,95.361,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.513 | Acc: 63.255,95.287,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.514 | Acc: 63.242,95.223,99.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.517 | Acc: 63.214,95.224,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.519 | Acc: 63.152,95.176,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.522 | Acc: 63.060,95.139,99.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.521 | Acc: 63.164,95.115,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.524 | Acc: 63.146,95.066,99.655,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.524 | Acc: 63.186,95.055,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.527 | Acc: 63.107,95.051,99.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.528 | Acc: 63.125,95.018,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.530 | Acc: 63.121,95.015,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.812 | Acc: 56.250,71.094,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.206 | Acc: 52.976,67.969,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.301 | Acc: 51.639,67.283,74.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.324 | Acc: 51.486,67.533,74.641,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 211
Batch: 0 | Loss: 1.488 | Acc: 59.375,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.506 | Acc: 63.504,95.945,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.517 | Acc: 63.472,95.427,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.505 | Acc: 63.665,95.325,99.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.496 | Acc: 64.082,95.544,99.730,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.505 | Acc: 63.946,95.459,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.508 | Acc: 63.953,95.358,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.500 | Acc: 64.151,95.390,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.497 | Acc: 64.174,95.380,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.504 | Acc: 64.050,95.308,99.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.504 | Acc: 64.062,95.262,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.511 | Acc: 63.843,95.245,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.515 | Acc: 63.622,95.193,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.516 | Acc: 63.619,95.232,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.518 | Acc: 63.604,95.182,99.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.519 | Acc: 63.484,95.183,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.520 | Acc: 63.547,95.152,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.523 | Acc: 63.487,95.143,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.526 | Acc: 63.407,95.053,99.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.531 | Acc: 63.271,95.019,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.015 | Acc: 55.469,71.094,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.275 | Acc: 53.423,68.341,75.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.347 | Acc: 51.810,67.759,74.447,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.358 | Acc: 51.524,67.828,74.103,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 212
Batch: 0 | Loss: 1.486 | Acc: 65.625,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.496 | Acc: 63.132,95.536,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.479 | Acc: 63.815,95.655,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.470 | Acc: 63.461,95.940,99.705,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.482 | Acc: 63.407,95.766,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.495 | Acc: 63.405,95.591,99.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.492 | Acc: 63.649,95.538,99.722,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.490 | Acc: 63.680,95.590,99.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.498 | Acc: 63.660,95.492,99.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.502 | Acc: 63.450,95.524,99.715,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.503 | Acc: 63.452,95.499,99.712,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.504 | Acc: 63.515,95.415,99.721,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.510 | Acc: 63.450,95.303,99.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.514 | Acc: 63.437,95.232,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.516 | Acc: 63.417,95.157,99.689,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.515 | Acc: 63.367,95.203,99.681,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.516 | Acc: 63.364,95.205,99.671,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.521 | Acc: 63.288,95.141,99.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.524 | Acc: 63.234,95.118,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.526 | Acc: 63.199,95.116,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.154 | Acc: 53.906,70.312,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.238 | Acc: 53.013,68.266,75.112,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.329 | Acc: 51.944,67.816,74.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.326 | Acc: 51.755,68.020,74.436,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 213
Batch: 0 | Loss: 1.569 | Acc: 53.906,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.515 | Acc: 62.463,95.573,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.529 | Acc: 62.652,95.446,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.528 | Acc: 62.756,95.364,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.529 | Acc: 62.924,95.438,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.534 | Acc: 62.655,95.521,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.530 | Acc: 62.655,95.506,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.525 | Acc: 62.744,95.512,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.519 | Acc: 62.995,95.497,99.641,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.513 | Acc: 63.065,95.498,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.513 | Acc: 63.036,95.522,99.674,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.512 | Acc: 63.041,95.496,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.514 | Acc: 63.174,95.458,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.515 | Acc: 63.206,95.444,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.513 | Acc: 63.312,95.404,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.514 | Acc: 63.273,95.354,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.515 | Acc: 63.230,95.325,99.637,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.517 | Acc: 63.206,95.285,99.647,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.519 | Acc: 63.164,95.258,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.517 | Acc: 63.154,95.253,99.653,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.037 | Acc: 52.344,69.531,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.232 | Acc: 53.051,67.932,75.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.333 | Acc: 52.210,67.130,74.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.336 | Acc: 51.998,67.649,74.385,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 214
Batch: 0 | Loss: 1.481 | Acc: 65.625,96.875,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.510 | Acc: 65.030,95.015,99.628,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.510 | Acc: 64.196,95.560,99.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.519 | Acc: 63.794,95.556,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.525 | Acc: 63.532,95.583,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.526 | Acc: 63.150,95.645,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.526 | Acc: 63.191,95.487,99.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.521 | Acc: 63.259,95.462,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.515 | Acc: 63.417,95.555,99.622,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.523 | Acc: 63.307,95.472,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.527 | Acc: 63.266,95.394,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.528 | Acc: 63.143,95.401,99.639,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.527 | Acc: 63.194,95.387,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.523 | Acc: 63.323,95.393,99.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.523 | Acc: 63.256,95.329,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.524 | Acc: 63.229,95.305,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.524 | Acc: 63.194,95.249,99.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.527 | Acc: 63.185,95.223,99.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.524 | Acc: 63.255,95.193,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.524 | Acc: 63.296,95.194,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.049 | Acc: 50.781,69.531,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.215 | Acc: 53.013,68.564,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.310 | Acc: 51.963,67.702,74.638,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.332 | Acc: 51.908,67.700,74.219,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 215
Batch: 0 | Loss: 1.588 | Acc: 60.156,92.969,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.471 | Acc: 63.839,95.461,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.488 | Acc: 63.262,95.884,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.480 | Acc: 63.589,95.940,99.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.497 | Acc: 63.021,95.978,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.501 | Acc: 63.127,95.893,99.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.498 | Acc: 63.223,95.797,99.722,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.501 | Acc: 63.303,95.734,99.717,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.501 | Acc: 63.398,95.575,99.699,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.500 | Acc: 63.519,95.571,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.504 | Acc: 63.425,95.499,99.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.509 | Acc: 63.352,95.383,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.507 | Acc: 63.372,95.410,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.502 | Acc: 63.533,95.414,99.686,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.502 | Acc: 63.565,95.407,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.505 | Acc: 63.502,95.380,99.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.508 | Acc: 63.369,95.354,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.514 | Acc: 63.176,95.310,99.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.517 | Acc: 63.125,95.263,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.520 | Acc: 63.136,95.241,99.649,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.051 | Acc: 51.562,69.531,81.250,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.255 | Acc: 53.125,69.196,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.346 | Acc: 51.677,68.026,74.333,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.350 | Acc: 51.524,68.135,74.027,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 216
Batch: 0 | Loss: 1.541 | Acc: 61.719,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.457 | Acc: 64.286,95.759,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.480 | Acc: 63.910,95.484,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.465 | Acc: 63.934,95.671,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.461 | Acc: 64.024,95.814,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.468 | Acc: 64.163,95.815,99.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.472 | Acc: 64.172,95.764,99.690,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.473 | Acc: 64.046,95.739,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.488 | Acc: 63.669,95.618,99.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.495 | Acc: 63.467,95.533,99.698,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.501 | Acc: 63.378,95.565,99.685,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.508 | Acc: 63.175,95.486,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.509 | Acc: 63.200,95.462,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.510 | Acc: 63.168,95.426,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.513 | Acc: 63.190,95.374,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.514 | Acc: 63.224,95.292,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.515 | Acc: 63.272,95.274,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.513 | Acc: 63.325,95.271,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.511 | Acc: 63.400,95.271,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.511 | Acc: 63.443,95.263,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.164 | Acc: 50.000,65.625,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.282 | Acc: 52.046,67.820,75.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.333 | Acc: 51.353,67.511,74.486,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.335 | Acc: 51.409,68.033,74.078,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 217
Batch: 0 | Loss: 1.657 | Acc: 58.594,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.520 | Acc: 63.207,95.052,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.491 | Acc: 63.720,95.103,99.600,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.490 | Acc: 63.717,95.479,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.474 | Acc: 63.821,95.554,99.662,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.476 | Acc: 63.854,95.645,99.644,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.478 | Acc: 63.817,95.655,99.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.474 | Acc: 63.996,95.601,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.479 | Acc: 63.883,95.541,99.626,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.488 | Acc: 63.583,95.502,99.642,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.497 | Acc: 63.534,95.437,99.627,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.502 | Acc: 63.401,95.408,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.505 | Acc: 63.385,95.384,99.634,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.503 | Acc: 63.488,95.378,99.653,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.505 | Acc: 63.431,95.379,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.504 | Acc: 63.440,95.354,99.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.509 | Acc: 63.318,95.295,99.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.511 | Acc: 63.302,95.292,99.672,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.509 | Acc: 63.348,95.284,99.673,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.515 | Acc: 63.314,95.251,99.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.965 | Acc: 54.688,72.656,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.250 | Acc: 53.534,68.936,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.335 | Acc: 51.944,68.007,74.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.349 | Acc: 51.921,67.828,74.360,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 218
Batch: 0 | Loss: 1.650 | Acc: 64.062,97.656,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.468 | Acc: 63.876,95.871,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.470 | Acc: 63.967,95.884,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.478 | Acc: 63.614,95.838,99.629,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.476 | Acc: 64.014,95.756,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.474 | Acc: 64.024,95.823,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.477 | Acc: 64.004,95.752,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.482 | Acc: 63.824,95.695,99.640,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.486 | Acc: 63.655,95.623,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.495 | Acc: 63.458,95.567,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.499 | Acc: 63.371,95.588,99.658,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.502 | Acc: 63.313,95.503,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.501 | Acc: 63.343,95.536,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.501 | Acc: 63.386,95.501,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.499 | Acc: 63.381,95.485,99.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.502 | Acc: 63.323,95.468,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.504 | Acc: 63.284,95.439,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.508 | Acc: 63.261,95.422,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.513 | Acc: 63.134,95.397,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.512 | Acc: 63.105,95.421,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.036 | Acc: 56.250,67.969,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.245 | Acc: 52.307,68.304,75.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.294 | Acc: 52.001,67.988,74.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.317 | Acc: 51.691,67.649,74.155,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 219
Batch: 0 | Loss: 1.316 | Acc: 70.312,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.489 | Acc: 64.397,95.126,99.591,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.495 | Acc: 64.062,94.874,99.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.514 | Acc: 63.858,95.056,99.603,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.506 | Acc: 64.005,95.216,99.547,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.499 | Acc: 64.148,95.320,99.582,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.494 | Acc: 64.269,95.338,99.587,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.501 | Acc: 63.896,95.362,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.507 | Acc: 63.742,95.322,99.602,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.509 | Acc: 63.791,95.317,99.616,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.509 | Acc: 63.806,95.274,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.513 | Acc: 63.755,95.238,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.513 | Acc: 63.774,95.199,99.611,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.514 | Acc: 63.715,95.166,99.620,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.515 | Acc: 63.615,95.190,99.614,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.512 | Acc: 63.735,95.196,99.621,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.514 | Acc: 63.685,95.164,99.625,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.514 | Acc: 63.701,95.132,99.624,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.517 | Acc: 63.656,95.096,99.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.520 | Acc: 63.603,95.089,99.623,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.956 | Acc: 51.562,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.259 | Acc: 52.046,68.936,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.331 | Acc: 51.258,67.854,74.581,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.352 | Acc: 51.255,67.841,74.334,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 220
Batch: 0 | Loss: 1.371 | Acc: 70.312,92.969,98.438,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.498 | Acc: 63.318,95.164,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.463 | Acc: 63.605,95.751,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.455 | Acc: 64.165,95.978,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.471 | Acc: 63.773,95.891,99.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.475 | Acc: 63.753,95.862,99.698,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.481 | Acc: 63.540,95.706,99.684,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.497 | Acc: 63.342,95.501,99.651,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.496 | Acc: 63.437,95.579,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.499 | Acc: 63.501,95.628,99.663,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.509 | Acc: 63.258,95.460,99.666,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.513 | Acc: 63.214,95.411,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.513 | Acc: 63.122,95.368,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.515 | Acc: 63.120,95.327,99.650,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.510 | Acc: 63.290,95.293,99.661,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.510 | Acc: 63.331,95.250,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.509 | Acc: 63.403,95.249,99.667,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.510 | Acc: 63.348,95.205,99.666,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.513 | Acc: 63.331,95.139,99.669,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.515 | Acc: 63.316,95.140,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.040 | Acc: 52.344,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.292 | Acc: 53.199,67.969,75.186,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.361 | Acc: 51.982,67.740,74.009,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.370 | Acc: 51.742,67.687,73.758,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 221
Batch: 0 | Loss: 1.624 | Acc: 65.625,92.969,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.526 | Acc: 63.132,95.387,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.503 | Acc: 63.853,95.408,99.676,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.506 | Acc: 64.267,95.287,99.693,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.497 | Acc: 64.284,95.428,99.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.501 | Acc: 63.900,95.552,99.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.504 | Acc: 63.765,95.526,99.722,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.502 | Acc: 63.747,95.468,99.712,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.503 | Acc: 63.776,95.380,99.723,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.506 | Acc: 63.635,95.325,99.711,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.505 | Acc: 63.584,95.301,99.716,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.504 | Acc: 63.606,95.291,99.692,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.506 | Acc: 63.605,95.238,99.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.506 | Acc: 63.602,95.214,99.698,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.511 | Acc: 63.495,95.190,99.691,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.509 | Acc: 63.541,95.146,99.694,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.510 | Acc: 63.573,95.101,99.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.512 | Acc: 63.520,95.079,99.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.513 | Acc: 63.508,95.092,99.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.514 | Acc: 63.480,95.068,99.680,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.050 | Acc: 53.125,70.312,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.307 | Acc: 52.455,68.564,75.223,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.362 | Acc: 51.543,67.893,74.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.365 | Acc: 51.255,68.071,74.411,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 222
Batch: 0 | Loss: 1.466 | Acc: 63.281,93.750,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.471 | Acc: 64.174,95.461,99.479,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.483 | Acc: 63.319,95.694,99.524,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.475 | Acc: 63.794,95.581,99.590,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.482 | Acc: 63.590,95.505,99.643,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.473 | Acc: 63.722,95.637,99.636,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.476 | Acc: 63.843,95.493,99.645,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.477 | Acc: 63.752,95.501,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.486 | Acc: 63.461,95.526,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.488 | Acc: 63.532,95.485,99.646,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.492 | Acc: 63.444,95.460,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.491 | Acc: 63.486,95.514,99.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.492 | Acc: 63.476,95.539,99.660,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.492 | Acc: 63.464,95.561,99.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.495 | Acc: 63.401,95.513,99.664,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.497 | Acc: 63.421,95.468,99.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.498 | Acc: 63.379,95.480,99.659,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.500 | Acc: 63.371,95.443,99.652,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.498 | Acc: 63.446,95.425,99.654,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.499 | Acc: 63.449,95.409,99.656,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.258 | Acc: 52.344,69.531,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.334 | Acc: 51.935,67.857,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.415 | Acc: 51.315,67.207,74.238,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.442 | Acc: 51.358,67.098,73.847,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 223
Batch: 0 | Loss: 1.310 | Acc: 65.625,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.479 | Acc: 63.244,96.168,99.442,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.484 | Acc: 63.681,95.694,99.543,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.475 | Acc: 63.973,95.671,99.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.477 | Acc: 64.178,95.592,99.566,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.489 | Acc: 63.931,95.521,99.520,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.495 | Acc: 63.972,95.506,99.548,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.501 | Acc: 63.852,95.440,99.557,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.497 | Acc: 63.800,95.473,99.563,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.489 | Acc: 63.907,95.528,99.568,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.488 | Acc: 64.148,95.538,99.569,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.489 | Acc: 64.133,95.539,99.548,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.492 | Acc: 64.098,95.475,99.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.494 | Acc: 63.970,95.450,99.545,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.495 | Acc: 63.951,95.429,99.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.499 | Acc: 63.865,95.388,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.501 | Acc: 63.763,95.351,99.572,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.502 | Acc: 63.751,95.345,99.567,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.503 | Acc: 63.705,95.315,99.552,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.504 | Acc: 63.646,95.269,99.555,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.088 | Acc: 52.344,71.875,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.271 | Acc: 53.199,68.304,76.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.374 | Acc: 51.963,67.264,74.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.386 | Acc: 51.819,67.405,74.488,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 224
Batch: 0 | Loss: 1.400 | Acc: 65.625,96.875,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.491 | Acc: 63.579,95.573,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.455 | Acc: 64.691,95.846,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.462 | Acc: 64.639,95.530,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.467 | Acc: 64.824,95.399,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.462 | Acc: 64.720,95.483,99.729,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.466 | Acc: 64.786,95.526,99.690,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.469 | Acc: 64.678,95.512,99.679,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.465 | Acc: 64.766,95.550,99.675,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.471 | Acc: 64.580,95.494,99.668,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.484 | Acc: 64.078,95.472,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.483 | Acc: 64.055,95.493,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.487 | Acc: 63.939,95.449,99.682,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.491 | Acc: 63.814,95.399,99.674,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.491 | Acc: 63.787,95.399,99.677,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.491 | Acc: 63.743,95.393,99.686,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.491 | Acc: 63.707,95.381,99.688,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.491 | Acc: 63.689,95.363,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.493 | Acc: 63.643,95.334,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.495 | Acc: 63.615,95.290,99.674,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.165 | Acc: 51.562,70.312,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.254 | Acc: 53.534,68.118,75.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.339 | Acc: 52.268,67.340,74.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.364 | Acc: 51.883,67.495,74.027,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 225
Batch: 0 | Loss: 1.398 | Acc: 69.531,92.969,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.447 | Acc: 64.174,95.275,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.429 | Acc: 64.939,96.189,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.422 | Acc: 64.652,96.273,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.428 | Acc: 64.545,96.258,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.415 | Acc: 64.921,96.372,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.409 | Acc: 65.289,96.436,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.411 | Acc: 65.132,96.459,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.410 | Acc: 65.140,96.550,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.415 | Acc: 65.146,96.525,99.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.417 | Acc: 65.271,96.587,99.747,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.414 | Acc: 65.325,96.592,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.413 | Acc: 65.165,96.642,99.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.411 | Acc: 65.152,96.653,99.755,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.412 | Acc: 65.127,96.692,99.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.412 | Acc: 65.101,96.737,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.415 | Acc: 65.009,96.714,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.415 | Acc: 64.984,96.696,99.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.416 | Acc: 64.965,96.691,99.738,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.411 | Acc: 65.016,96.699,99.735,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.070 | Acc: 53.906,70.312,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.138 | Acc: 54.055,69.866,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.209 | Acc: 52.973,68.483,74.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.229 | Acc: 52.741,68.609,74.347,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 226
Batch: 0 | Loss: 1.423 | Acc: 67.969,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.371 | Acc: 66.034,97.284,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.367 | Acc: 66.120,97.409,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.375 | Acc: 65.894,97.349,99.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.380 | Acc: 65.664,97.242,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.391 | Acc: 65.377,97.115,99.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.389 | Acc: 65.380,97.153,99.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.397 | Acc: 65.143,97.135,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.395 | Acc: 65.266,97.137,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.395 | Acc: 65.211,97.160,99.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.391 | Acc: 65.326,97.225,99.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.392 | Acc: 65.360,97.239,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.394 | Acc: 65.346,97.241,99.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.397 | Acc: 65.203,97.219,99.770,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.396 | Acc: 65.227,97.192,99.766,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.393 | Acc: 65.282,97.236,99.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.390 | Acc: 65.323,97.238,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.390 | Acc: 65.332,97.237,99.775,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.388 | Acc: 65.372,97.275,99.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.388 | Acc: 65.322,97.283,99.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.011 | Acc: 53.906,70.312,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.141 | Acc: 54.278,69.308,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.224 | Acc: 53.030,68.445,74.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.243 | Acc: 52.856,68.635,74.501,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 227
Batch: 0 | Loss: 1.432 | Acc: 64.062,94.531,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.398 | Acc: 65.141,96.838,99.665,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.396 | Acc: 65.530,96.894,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.396 | Acc: 65.471,97.144,99.731,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.395 | Acc: 65.268,97.213,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.401 | Acc: 65.200,97.231,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.398 | Acc: 65.270,97.243,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.397 | Acc: 65.071,97.285,99.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.392 | Acc: 65.106,97.283,99.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.385 | Acc: 65.435,97.268,99.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.384 | Acc: 65.473,97.283,99.736,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.386 | Acc: 65.505,97.246,99.756,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.386 | Acc: 65.392,97.232,99.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.387 | Acc: 65.421,97.204,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.387 | Acc: 65.483,97.239,99.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.387 | Acc: 65.513,97.223,99.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.383 | Acc: 65.664,97.225,99.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.382 | Acc: 65.650,97.248,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.381 | Acc: 65.623,97.286,99.762,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.380 | Acc: 65.668,97.277,99.754,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.030 | Acc: 52.344,71.094,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.133 | Acc: 54.055,69.717,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.205 | Acc: 53.258,68.845,74.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.224 | Acc: 53.023,69.057,74.513,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 228
Batch: 0 | Loss: 1.372 | Acc: 67.969,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.365 | Acc: 66.369,97.247,99.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.349 | Acc: 66.425,97.504,99.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.362 | Acc: 66.419,97.387,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.361 | Acc: 66.213,97.425,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.362 | Acc: 66.182,97.486,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.364 | Acc: 66.251,97.469,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.369 | Acc: 66.102,97.479,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.366 | Acc: 66.164,97.525,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.372 | Acc: 65.940,97.548,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.371 | Acc: 65.893,97.571,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.373 | Acc: 65.894,97.536,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.371 | Acc: 65.914,97.520,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.372 | Acc: 65.835,97.540,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.374 | Acc: 65.792,97.495,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.373 | Acc: 65.794,97.524,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.373 | Acc: 65.837,97.542,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.373 | Acc: 65.740,97.549,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.371 | Acc: 65.740,97.557,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.372 | Acc: 65.721,97.554,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.953 | Acc: 53.125,70.312,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.129 | Acc: 53.869,69.568,76.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.198 | Acc: 53.144,68.559,75.286,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.219 | Acc: 53.023,68.660,74.731,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 229
Batch: 0 | Loss: 1.254 | Acc: 68.750,96.094,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.344 | Acc: 65.774,97.173,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.333 | Acc: 66.425,97.713,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.332 | Acc: 66.483,97.618,99.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.349 | Acc: 65.992,97.569,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.354 | Acc: 65.857,97.602,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.358 | Acc: 65.903,97.656,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.354 | Acc: 65.902,97.640,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.359 | Acc: 65.712,97.714,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.359 | Acc: 65.746,97.730,99.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.361 | Acc: 65.847,97.664,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.364 | Acc: 65.855,97.677,99.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.364 | Acc: 65.897,97.702,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.366 | Acc: 65.820,97.650,99.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.368 | Acc: 65.761,97.626,99.755,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.370 | Acc: 65.724,97.578,99.751,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.373 | Acc: 65.713,97.566,99.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.375 | Acc: 65.636,97.565,99.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.375 | Acc: 65.616,97.561,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.375 | Acc: 65.572,97.556,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.032 | Acc: 53.125,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.141 | Acc: 53.683,69.457,75.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.211 | Acc: 53.011,68.426,74.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.228 | Acc: 52.741,68.545,74.398,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 230
Batch: 0 | Loss: 1.325 | Acc: 68.750,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.364 | Acc: 66.183,97.210,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.364 | Acc: 65.701,97.313,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.348 | Acc: 66.163,97.362,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.354 | Acc: 65.866,97.338,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.367 | Acc: 65.540,97.440,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.368 | Acc: 65.625,97.469,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.377 | Acc: 65.486,97.462,99.751,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.368 | Acc: 65.649,97.545,99.743,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.362 | Acc: 65.819,97.574,99.754,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.364 | Acc: 65.792,97.559,99.747,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.360 | Acc: 65.883,97.554,99.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.363 | Acc: 65.875,97.546,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.360 | Acc: 65.888,97.548,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.362 | Acc: 65.808,97.576,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.362 | Acc: 65.807,97.578,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.364 | Acc: 65.739,97.527,99.766,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.367 | Acc: 65.756,97.484,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.365 | Acc: 65.727,97.505,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.367 | Acc: 65.650,97.519,99.770,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.968 | Acc: 53.906,67.969,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.132 | Acc: 54.315,69.754,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.204 | Acc: 53.201,68.617,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.227 | Acc: 52.946,68.660,74.577,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 231
Batch: 0 | Loss: 1.282 | Acc: 63.281,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.343 | Acc: 66.220,97.731,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.353 | Acc: 66.044,97.771,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.375 | Acc: 65.471,97.772,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.374 | Acc: 65.326,97.608,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.369 | Acc: 65.563,97.571,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.371 | Acc: 65.560,97.527,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.366 | Acc: 65.669,97.584,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.364 | Acc: 65.785,97.583,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.366 | Acc: 65.767,97.613,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.361 | Acc: 65.819,97.621,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.355 | Acc: 65.897,97.607,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.359 | Acc: 65.823,97.608,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.358 | Acc: 65.861,97.563,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.357 | Acc: 65.859,97.581,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.360 | Acc: 65.817,97.558,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.362 | Acc: 65.773,97.535,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.362 | Acc: 65.746,97.551,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.357 | Acc: 65.906,97.555,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.357 | Acc: 65.926,97.543,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.007 | Acc: 53.906,68.750,73.438,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.128 | Acc: 53.906,69.568,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.210 | Acc: 53.106,68.693,75.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.223 | Acc: 52.882,68.852,74.693,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 232
Batch: 0 | Loss: 1.562 | Acc: 61.719,95.312,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.385 | Acc: 65.551,97.693,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.349 | Acc: 66.482,97.790,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.340 | Acc: 66.637,97.951,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.348 | Acc: 66.705,97.762,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.350 | Acc: 66.553,97.811,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.349 | Acc: 66.516,97.811,99.761,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.353 | Acc: 66.356,97.756,99.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.353 | Acc: 66.411,97.700,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.353 | Acc: 66.277,97.686,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.352 | Acc: 66.251,97.691,99.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.357 | Acc: 66.116,97.617,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.357 | Acc: 66.037,97.630,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.360 | Acc: 65.930,97.602,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.360 | Acc: 65.861,97.620,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.364 | Acc: 65.835,97.615,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.360 | Acc: 65.864,97.639,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.360 | Acc: 65.921,97.643,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.362 | Acc: 65.863,97.624,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.362 | Acc: 65.890,97.617,99.785,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.977 | Acc: 53.125,67.969,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.143 | Acc: 54.018,69.234,75.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.209 | Acc: 52.992,68.464,74.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.225 | Acc: 52.946,68.635,74.513,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 233
Batch: 0 | Loss: 1.295 | Acc: 66.406,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.390 | Acc: 65.216,97.917,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.387 | Acc: 65.454,97.656,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.364 | Acc: 65.881,97.707,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.365 | Acc: 65.721,97.782,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.362 | Acc: 66.004,97.625,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.359 | Acc: 65.941,97.669,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.351 | Acc: 66.102,97.712,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.348 | Acc: 66.251,97.729,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.345 | Acc: 66.337,97.691,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.347 | Acc: 66.278,97.707,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.346 | Acc: 66.290,97.720,99.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.350 | Acc: 66.127,97.737,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.351 | Acc: 66.026,97.755,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.350 | Acc: 66.061,97.756,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.349 | Acc: 66.043,97.791,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.354 | Acc: 65.907,97.795,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.356 | Acc: 65.888,97.810,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.357 | Acc: 65.841,97.806,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.357 | Acc: 65.900,97.792,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.990 | Acc: 53.125,68.750,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.144 | Acc: 54.055,69.606,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.217 | Acc: 53.030,68.540,74.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.229 | Acc: 52.920,68.750,74.667,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 234
Batch: 0 | Loss: 1.538 | Acc: 61.719,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.362 | Acc: 65.662,97.545,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.356 | Acc: 65.720,97.523,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.355 | Acc: 65.702,97.567,99.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.355 | Acc: 65.451,97.560,99.701,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.355 | Acc: 65.594,97.602,99.706,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.354 | Acc: 65.593,97.637,99.703,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.355 | Acc: 65.486,97.701,99.729,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.351 | Acc: 65.586,97.758,99.719,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.350 | Acc: 65.603,97.747,99.724,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.357 | Acc: 65.501,97.683,99.732,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.361 | Acc: 65.508,97.646,99.735,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.364 | Acc: 65.463,97.647,99.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.365 | Acc: 65.496,97.674,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.366 | Acc: 65.544,97.676,99.747,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.366 | Acc: 65.550,97.669,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.367 | Acc: 65.498,97.644,99.735,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.367 | Acc: 65.538,97.649,99.741,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.365 | Acc: 65.606,97.661,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.365 | Acc: 65.691,97.638,99.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.990 | Acc: 52.344,70.312,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.150 | Acc: 54.353,69.717,75.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.220 | Acc: 53.392,68.655,75.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.234 | Acc: 53.125,68.904,74.782,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 235
Batch: 0 | Loss: 1.211 | Acc: 72.656,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.332 | Acc: 67.522,97.619,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.318 | Acc: 67.245,97.961,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.323 | Acc: 66.970,97.951,99.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.334 | Acc: 66.532,97.840,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.351 | Acc: 65.803,97.757,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.348 | Acc: 65.974,97.869,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.355 | Acc: 65.996,97.750,99.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.359 | Acc: 65.775,97.753,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.353 | Acc: 65.884,97.777,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.351 | Acc: 65.932,97.785,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.351 | Acc: 65.964,97.840,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.349 | Acc: 66.046,97.848,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.346 | Acc: 66.113,97.914,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.348 | Acc: 66.087,97.943,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.350 | Acc: 66.056,97.900,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.353 | Acc: 65.951,97.873,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.354 | Acc: 65.907,97.876,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.355 | Acc: 65.837,97.877,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.355 | Acc: 65.840,97.876,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.881 | Acc: 53.906,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.127 | Acc: 54.836,69.345,75.372,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.201 | Acc: 53.620,68.712,74.943,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.220 | Acc: 53.368,68.865,74.603,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 236
Batch: 0 | Loss: 1.263 | Acc: 71.875,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.338 | Acc: 67.001,97.879,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.334 | Acc: 67.226,97.504,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.335 | Acc: 67.098,97.631,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.337 | Acc: 67.091,97.550,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.344 | Acc: 66.839,97.649,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.350 | Acc: 66.445,97.637,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.354 | Acc: 66.334,97.651,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.348 | Acc: 66.270,97.710,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.355 | Acc: 66.134,97.725,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.356 | Acc: 66.099,97.711,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.360 | Acc: 66.003,97.713,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.354 | Acc: 66.105,97.737,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.353 | Acc: 66.122,97.752,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.356 | Acc: 66.059,97.726,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.354 | Acc: 66.048,97.734,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.354 | Acc: 66.056,97.749,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.359 | Acc: 65.900,97.727,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.356 | Acc: 65.999,97.732,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.358 | Acc: 65.892,97.742,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.925 | Acc: 56.250,71.875,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.149 | Acc: 54.167,69.271,75.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.218 | Acc: 53.049,68.426,75.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.233 | Acc: 52.882,68.622,74.641,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 237
Batch: 0 | Loss: 1.313 | Acc: 65.625,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.376 | Acc: 65.030,97.359,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.357 | Acc: 65.396,97.485,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.364 | Acc: 65.215,97.656,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.360 | Acc: 65.442,97.685,99.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.349 | Acc: 65.687,97.695,99.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.350 | Acc: 65.786,97.798,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.354 | Acc: 65.498,97.773,99.762,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.352 | Acc: 65.635,97.734,99.753,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.355 | Acc: 65.504,97.743,99.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.353 | Acc: 65.578,97.769,99.755,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.350 | Acc: 65.618,97.829,99.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.348 | Acc: 65.810,97.841,99.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.349 | Acc: 65.930,97.806,99.761,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.349 | Acc: 65.900,97.831,99.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.352 | Acc: 65.874,97.807,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.353 | Acc: 65.854,97.778,99.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.356 | Acc: 65.792,97.769,99.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.354 | Acc: 65.839,97.758,99.764,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.356 | Acc: 65.871,97.730,99.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.003 | Acc: 51.562,67.969,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.156 | Acc: 53.646,69.494,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.222 | Acc: 53.106,68.559,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.234 | Acc: 52.907,68.776,74.744,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 238
Batch: 0 | Loss: 1.136 | Acc: 72.656,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.309 | Acc: 67.522,98.400,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.328 | Acc: 66.235,98.228,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.336 | Acc: 66.227,98.040,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.333 | Acc: 66.175,97.917,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.332 | Acc: 66.105,97.857,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.330 | Acc: 66.174,97.940,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.335 | Acc: 66.234,97.861,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.337 | Acc: 66.008,97.860,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.338 | Acc: 66.039,97.850,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.339 | Acc: 65.994,97.897,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.342 | Acc: 65.989,97.858,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.346 | Acc: 65.914,97.867,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.346 | Acc: 65.903,97.845,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.345 | Acc: 65.872,97.865,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.344 | Acc: 65.911,97.864,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.344 | Acc: 65.922,97.827,99.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.346 | Acc: 65.925,97.805,99.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.345 | Acc: 65.989,97.816,99.775,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.348 | Acc: 65.916,97.802,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.010 | Acc: 53.906,68.750,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.144 | Acc: 54.204,69.271,75.372,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.219 | Acc: 53.201,68.445,74.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.233 | Acc: 52.984,68.737,74.424,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 239
Batch: 0 | Loss: 1.519 | Acc: 62.500,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.385 | Acc: 65.327,97.768,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.379 | Acc: 65.415,97.980,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.380 | Acc: 65.651,97.772,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.372 | Acc: 65.693,97.840,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.361 | Acc: 65.594,97.904,99.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.347 | Acc: 65.974,97.915,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.347 | Acc: 66.201,97.867,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.351 | Acc: 66.037,97.860,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.354 | Acc: 65.936,97.894,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.355 | Acc: 65.917,97.901,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.348 | Acc: 66.155,97.925,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.346 | Acc: 66.173,97.912,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.351 | Acc: 66.038,97.881,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.352 | Acc: 66.073,97.865,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.352 | Acc: 66.025,97.895,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.353 | Acc: 66.002,97.885,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.353 | Acc: 66.012,97.874,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.353 | Acc: 65.986,97.875,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.353 | Acc: 65.974,97.872,99.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.958 | Acc: 53.906,71.094,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.155 | Acc: 54.427,69.792,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.225 | Acc: 53.201,68.559,75.095,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.243 | Acc: 53.151,68.750,74.808,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 240
Batch: 0 | Loss: 1.248 | Acc: 69.531,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.363 | Acc: 65.141,97.954,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.351 | Acc: 65.701,97.732,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.355 | Acc: 65.561,97.874,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.348 | Acc: 65.876,97.907,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.348 | Acc: 65.996,97.912,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.347 | Acc: 65.967,97.882,99.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.339 | Acc: 66.268,97.911,99.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.342 | Acc: 66.202,97.879,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.343 | Acc: 66.139,97.859,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.349 | Acc: 65.889,97.831,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.346 | Acc: 65.971,97.837,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.349 | Acc: 65.982,97.838,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.346 | Acc: 66.053,97.842,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.348 | Acc: 65.981,97.801,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.351 | Acc: 66.009,97.804,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.350 | Acc: 65.983,97.834,99.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.351 | Acc: 65.994,97.826,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.349 | Acc: 66.084,97.834,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.351 | Acc: 66.019,97.839,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.989 | Acc: 55.469,69.531,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.157 | Acc: 54.167,69.308,75.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.229 | Acc: 53.468,68.236,74.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.243 | Acc: 53.163,68.443,74.539,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 241
Batch: 0 | Loss: 1.181 | Acc: 75.781,98.438,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.295 | Acc: 68.118,98.214,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.335 | Acc: 67.130,97.771,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.348 | Acc: 66.265,97.759,99.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.351 | Acc: 66.165,97.946,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.355 | Acc: 65.934,97.842,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.353 | Acc: 66.096,97.831,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.349 | Acc: 66.345,97.878,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.347 | Acc: 66.421,97.933,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.347 | Acc: 66.277,97.915,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.344 | Acc: 66.239,97.917,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.345 | Acc: 66.169,97.936,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.346 | Acc: 66.059,97.942,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.349 | Acc: 66.044,97.944,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.347 | Acc: 66.050,97.943,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.348 | Acc: 66.004,97.965,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.349 | Acc: 65.988,97.953,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.347 | Acc: 65.987,97.970,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.347 | Acc: 65.969,97.957,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.346 | Acc: 65.972,97.962,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.980 | Acc: 53.125,70.312,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.141 | Acc: 53.906,69.606,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.220 | Acc: 53.106,68.579,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.240 | Acc: 52.882,68.878,74.565,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 242
Batch: 0 | Loss: 1.314 | Acc: 67.188,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.311 | Acc: 65.923,97.917,99.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.310 | Acc: 66.159,97.809,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.317 | Acc: 66.304,97.976,99.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.332 | Acc: 66.146,97.936,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.332 | Acc: 66.066,97.919,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.333 | Acc: 66.167,97.960,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.337 | Acc: 66.207,97.944,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.346 | Acc: 65.984,97.933,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.348 | Acc: 66.130,97.915,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.351 | Acc: 66.060,97.913,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.352 | Acc: 66.000,97.928,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.348 | Acc: 66.102,97.942,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.347 | Acc: 66.047,97.917,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.347 | Acc: 66.014,97.920,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.345 | Acc: 66.069,97.913,99.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.344 | Acc: 66.121,97.914,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.345 | Acc: 66.086,97.920,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.349 | Acc: 65.945,97.903,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.348 | Acc: 65.935,97.919,99.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.904 | Acc: 51.562,67.969,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.154 | Acc: 54.278,68.638,75.707,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.221 | Acc: 53.144,68.007,74.962,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.236 | Acc: 52.997,68.430,74.539,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 243
Batch: 0 | Loss: 1.280 | Acc: 63.281,100.000,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.338 | Acc: 65.699,98.326,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.346 | Acc: 66.673,98.095,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.339 | Acc: 66.714,98.040,99.872,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.340 | Acc: 66.406,98.061,99.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.333 | Acc: 66.515,98.004,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.344 | Acc: 66.309,97.921,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.342 | Acc: 66.212,97.889,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.341 | Acc: 66.251,97.923,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.341 | Acc: 66.229,97.924,99.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.339 | Acc: 66.321,97.921,99.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.337 | Acc: 66.346,97.914,99.852,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.335 | Acc: 66.358,97.909,99.857,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.335 | Acc: 66.331,97.935,99.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.339 | Acc: 66.251,97.943,99.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.344 | Acc: 66.087,97.931,99.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.344 | Acc: 66.100,97.919,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.344 | Acc: 66.145,97.917,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.345 | Acc: 66.108,97.922,99.831,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.347 | Acc: 66.021,97.908,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.012 | Acc: 53.125,67.969,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.149 | Acc: 54.315,69.792,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.220 | Acc: 53.430,68.559,75.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.236 | Acc: 53.125,68.686,74.667,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 244
Batch: 0 | Loss: 1.259 | Acc: 67.188,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.358 | Acc: 64.844,98.103,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.350 | Acc: 65.187,97.732,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.350 | Acc: 65.279,97.823,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.349 | Acc: 65.336,97.840,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.348 | Acc: 65.300,97.950,99.760,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.345 | Acc: 65.431,97.998,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.338 | Acc: 65.686,98.066,99.773,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.340 | Acc: 65.785,97.957,99.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.345 | Acc: 65.625,97.885,99.750,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.343 | Acc: 65.602,97.866,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.341 | Acc: 65.710,97.879,99.749,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.343 | Acc: 65.700,97.860,99.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.343 | Acc: 65.873,97.896,99.734,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.344 | Acc: 65.861,97.904,99.736,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.347 | Acc: 65.887,97.890,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.345 | Acc: 65.941,97.900,99.742,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.345 | Acc: 65.907,97.895,99.748,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.346 | Acc: 65.880,97.896,99.751,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.344 | Acc: 65.906,97.929,99.756,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.972 | Acc: 54.688,69.531,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.170 | Acc: 54.167,69.048,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.238 | Acc: 53.258,68.274,74.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.251 | Acc: 53.087,68.673,74.731,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 245
Batch: 0 | Loss: 1.263 | Acc: 67.969,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.320 | Acc: 66.853,97.954,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.323 | Acc: 66.425,97.980,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.332 | Acc: 66.381,97.951,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.339 | Acc: 66.011,97.926,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.338 | Acc: 66.058,98.012,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.337 | Acc: 66.122,97.998,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.335 | Acc: 66.052,98.050,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.340 | Acc: 66.154,98.010,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.338 | Acc: 66.359,98.002,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.340 | Acc: 66.212,97.998,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.347 | Acc: 66.049,97.964,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.344 | Acc: 66.140,97.938,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.343 | Acc: 66.191,97.911,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.342 | Acc: 66.267,97.920,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.344 | Acc: 66.134,97.926,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.345 | Acc: 66.107,97.919,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.346 | Acc: 66.003,97.938,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.346 | Acc: 66.030,97.933,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.344 | Acc: 66.002,97.962,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.994 | Acc: 52.344,68.750,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.167 | Acc: 54.092,69.122,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.240 | Acc: 53.201,68.483,74.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.248 | Acc: 53.074,68.814,74.552,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 246
Batch: 0 | Loss: 1.267 | Acc: 68.750,96.875,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.391 | Acc: 64.658,97.954,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.345 | Acc: 66.139,97.809,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.346 | Acc: 65.804,97.925,99.757,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.347 | Acc: 65.953,97.840,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.353 | Acc: 65.818,97.795,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.363 | Acc: 65.741,97.785,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.364 | Acc: 65.786,97.834,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.362 | Acc: 65.858,97.904,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.366 | Acc: 65.668,97.859,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.360 | Acc: 65.769,97.858,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.353 | Acc: 65.936,97.886,99.763,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.353 | Acc: 65.858,97.890,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.353 | Acc: 65.841,97.893,99.758,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.351 | Acc: 65.792,97.890,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.352 | Acc: 65.822,97.918,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.349 | Acc: 65.949,97.907,99.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.348 | Acc: 65.966,97.911,99.775,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.346 | Acc: 65.995,97.894,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.346 | Acc: 65.937,97.917,99.779,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.950 | Acc: 53.906,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.166 | Acc: 54.129,69.457,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.243 | Acc: 53.335,68.445,74.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.259 | Acc: 53.099,68.737,74.475,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 247
Batch: 0 | Loss: 1.327 | Acc: 66.406,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.354 | Acc: 65.327,98.438,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.339 | Acc: 65.854,98.247,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.338 | Acc: 65.881,98.105,99.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.341 | Acc: 65.741,98.216,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.335 | Acc: 65.996,98.167,99.729,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.340 | Acc: 65.793,98.140,99.761,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.342 | Acc: 65.941,98.105,99.762,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.337 | Acc: 66.086,98.127,99.772,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.340 | Acc: 66.031,98.062,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.341 | Acc: 66.037,98.072,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.341 | Acc: 66.032,98.052,99.770,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.339 | Acc: 66.189,98.039,99.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.339 | Acc: 66.092,98.000,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.344 | Acc: 65.961,97.995,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.344 | Acc: 65.890,98.025,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.344 | Acc: 65.834,97.997,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.344 | Acc: 65.859,98.002,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.342 | Acc: 65.826,98.007,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.344 | Acc: 65.787,98.019,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.030 | Acc: 53.125,68.750,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.153 | Acc: 54.018,68.899,75.409,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.235 | Acc: 53.144,68.216,74.466,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.250 | Acc: 52.907,68.635,74.270,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 248
Batch: 0 | Loss: 1.363 | Acc: 64.844,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.396 | Acc: 64.286,97.359,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.354 | Acc: 65.492,97.694,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.357 | Acc: 65.753,97.772,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.346 | Acc: 66.146,97.888,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.348 | Acc: 65.989,97.958,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.344 | Acc: 65.845,97.889,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.341 | Acc: 65.863,97.989,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.339 | Acc: 65.858,98.006,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.335 | Acc: 66.121,98.032,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.341 | Acc: 66.014,98.033,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.337 | Acc: 66.049,97.981,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.338 | Acc: 66.098,97.980,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.342 | Acc: 66.008,97.941,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.341 | Acc: 65.892,97.948,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.341 | Acc: 65.939,97.916,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.339 | Acc: 66.012,97.929,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.340 | Acc: 65.962,97.952,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.341 | Acc: 65.930,97.948,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.342 | Acc: 65.898,97.941,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.944 | Acc: 53.906,68.750,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.168 | Acc: 54.018,69.122,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.240 | Acc: 52.954,68.216,74.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.253 | Acc: 52.677,68.455,74.603,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 249
Batch: 0 | Loss: 1.232 | Acc: 71.094,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.324 | Acc: 67.225,97.693,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.325 | Acc: 66.787,97.923,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.312 | Acc: 67.059,97.912,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.322 | Acc: 66.599,97.811,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.336 | Acc: 66.205,97.935,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.336 | Acc: 66.109,97.960,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.336 | Acc: 66.085,97.939,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.333 | Acc: 66.071,97.967,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.335 | Acc: 66.117,97.950,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.331 | Acc: 66.123,97.967,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.330 | Acc: 66.201,97.960,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.331 | Acc: 66.264,97.948,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.333 | Acc: 66.158,97.953,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.334 | Acc: 66.187,97.915,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.339 | Acc: 66.056,97.879,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.340 | Acc: 66.092,97.863,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.338 | Acc: 66.076,97.876,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.339 | Acc: 66.088,97.877,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.342 | Acc: 66.054,97.876,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.092 | Acc: 52.344,69.531,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.167 | Acc: 53.795,69.345,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.248 | Acc: 52.992,68.426,74.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.255 | Acc: 53.023,68.596,74.462,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 250
Batch: 0 | Loss: 1.452 | Acc: 60.156,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.305 | Acc: 67.820,98.103,99.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.314 | Acc: 66.806,98.190,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.321 | Acc: 66.765,97.989,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.332 | Acc: 66.271,97.888,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.339 | Acc: 65.903,97.919,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.340 | Acc: 66.109,97.992,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.345 | Acc: 65.941,97.983,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.341 | Acc: 65.921,98.030,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.344 | Acc: 65.919,98.062,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.342 | Acc: 65.948,98.072,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.336 | Acc: 66.180,98.091,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.334 | Acc: 66.192,98.097,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.338 | Acc: 66.077,98.105,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.340 | Acc: 66.073,98.096,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.337 | Acc: 66.227,98.097,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.339 | Acc: 66.143,98.082,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.337 | Acc: 66.301,98.066,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.341 | Acc: 66.287,98.018,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.339 | Acc: 66.302,98.042,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.953 | Acc: 53.906,70.312,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.170 | Acc: 53.497,69.271,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.238 | Acc: 52.839,68.312,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.254 | Acc: 52.779,68.558,74.590,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 251
Batch: 0 | Loss: 1.302 | Acc: 67.969,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.332 | Acc: 65.476,98.698,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.320 | Acc: 66.387,98.628,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.333 | Acc: 66.048,98.476,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.323 | Acc: 66.319,98.370,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.331 | Acc: 66.190,98.321,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.332 | Acc: 66.251,98.212,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.335 | Acc: 66.246,98.138,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.338 | Acc: 66.227,98.122,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.333 | Acc: 66.165,98.127,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.334 | Acc: 66.103,98.095,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.337 | Acc: 66.063,98.080,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.339 | Acc: 66.066,98.084,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.339 | Acc: 66.086,98.075,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.340 | Acc: 66.017,98.076,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.340 | Acc: 66.074,98.103,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.340 | Acc: 66.083,98.102,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.342 | Acc: 66.012,98.082,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.338 | Acc: 66.153,98.098,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.339 | Acc: 66.136,98.091,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.040 | Acc: 56.250,71.875,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.177 | Acc: 54.278,69.457,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.242 | Acc: 53.373,68.426,74.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.263 | Acc: 53.163,68.596,74.411,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 252
Batch: 0 | Loss: 1.432 | Acc: 60.156,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.349 | Acc: 66.853,97.656,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.340 | Acc: 66.387,97.809,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.320 | Acc: 66.688,97.861,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.326 | Acc: 66.416,97.888,99.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.324 | Acc: 66.399,97.950,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.335 | Acc: 66.090,97.837,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.335 | Acc: 65.991,97.839,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.336 | Acc: 66.018,97.860,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.333 | Acc: 66.177,97.881,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.333 | Acc: 66.138,97.913,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.333 | Acc: 66.085,97.971,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.332 | Acc: 66.166,97.929,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.333 | Acc: 66.152,97.926,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.331 | Acc: 66.181,97.915,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.334 | Acc: 66.136,97.898,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.338 | Acc: 66.109,97.909,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.338 | Acc: 66.106,97.908,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.339 | Acc: 66.101,97.912,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.339 | Acc: 66.109,97.913,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.997 | Acc: 55.469,67.188,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.170 | Acc: 54.241,69.271,75.967,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.249 | Acc: 53.220,68.178,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.266 | Acc: 53.010,68.327,74.616,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 253
Batch: 0 | Loss: 1.579 | Acc: 63.281,97.656,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.315 | Acc: 66.369,98.214,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.319 | Acc: 65.796,98.247,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.320 | Acc: 65.996,98.233,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.326 | Acc: 66.020,98.071,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.328 | Acc: 65.965,98.089,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.324 | Acc: 66.122,98.134,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.321 | Acc: 66.157,98.127,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.323 | Acc: 66.139,98.083,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.329 | Acc: 66.083,98.062,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.333 | Acc: 65.948,98.006,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.331 | Acc: 66.000,98.024,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.329 | Acc: 66.124,98.013,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.327 | Acc: 66.071,98.018,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.329 | Acc: 66.134,98.007,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.328 | Acc: 66.142,97.991,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.331 | Acc: 66.073,98.026,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.333 | Acc: 66.001,98.004,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.334 | Acc: 65.969,97.998,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.334 | Acc: 66.002,98.011,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.071 | Acc: 53.906,69.531,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.167 | Acc: 54.167,69.234,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.241 | Acc: 53.182,68.464,74.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.256 | Acc: 52.946,68.737,74.641,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 254
Batch: 0 | Loss: 1.274 | Acc: 67.188,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.330 | Acc: 66.555,98.028,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.329 | Acc: 66.349,98.171,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.321 | Acc: 66.534,98.284,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.326 | Acc: 66.512,98.129,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.318 | Acc: 66.669,98.182,99.783,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.320 | Acc: 66.626,98.108,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.317 | Acc: 66.789,98.133,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.320 | Acc: 66.673,98.180,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.315 | Acc: 66.695,98.157,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.317 | Acc: 66.671,98.169,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.320 | Acc: 66.519,98.119,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.323 | Acc: 66.513,98.091,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.324 | Acc: 66.493,98.042,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.323 | Acc: 66.462,98.043,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.325 | Acc: 66.401,98.043,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.325 | Acc: 66.414,98.043,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.331 | Acc: 66.221,98.034,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.330 | Acc: 66.235,98.033,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.333 | Acc: 66.170,98.044,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.964 | Acc: 53.125,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.175 | Acc: 54.315,69.010,75.298,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.250 | Acc: 53.201,68.045,75.038,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.259 | Acc: 53.074,68.366,74.769,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 255
Batch: 0 | Loss: 1.271 | Acc: 71.094,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.323 | Acc: 66.295,97.954,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.328 | Acc: 66.273,98.056,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.342 | Acc: 66.112,97.964,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.338 | Acc: 65.895,97.965,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.337 | Acc: 65.903,97.888,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.334 | Acc: 65.974,97.889,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.333 | Acc: 65.913,97.922,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.335 | Acc: 65.950,97.972,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.332 | Acc: 66.160,97.958,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.339 | Acc: 66.041,97.948,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.337 | Acc: 66.173,97.996,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.339 | Acc: 66.118,98.013,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.341 | Acc: 65.993,98.051,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.341 | Acc: 65.992,98.026,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.341 | Acc: 65.999,98.064,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.340 | Acc: 66.075,98.080,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.339 | Acc: 66.099,98.071,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.338 | Acc: 66.103,98.057,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.334 | Acc: 66.185,98.060,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.997 | Acc: 52.344,69.531,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.190 | Acc: 53.869,69.159,75.409,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.247 | Acc: 53.239,68.274,75.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.263 | Acc: 52.997,68.571,74.718,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 256
Batch: 0 | Loss: 1.181 | Acc: 68.750,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.310 | Acc: 65.774,98.549,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.321 | Acc: 66.616,98.152,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.324 | Acc: 66.368,98.258,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.319 | Acc: 66.445,98.245,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.330 | Acc: 66.166,98.167,99.876,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.330 | Acc: 66.206,98.160,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.333 | Acc: 66.218,98.160,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.331 | Acc: 66.261,98.137,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.330 | Acc: 66.376,98.118,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.332 | Acc: 66.332,98.084,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.334 | Acc: 66.201,98.088,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.334 | Acc: 66.212,98.091,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.336 | Acc: 66.185,98.087,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.339 | Acc: 66.131,98.037,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.338 | Acc: 66.113,98.053,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.338 | Acc: 66.109,98.046,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.338 | Acc: 66.177,98.034,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.336 | Acc: 66.214,98.028,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.336 | Acc: 66.181,98.044,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.030 | Acc: 53.906,67.969,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.184 | Acc: 54.241,69.048,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.251 | Acc: 53.373,68.140,74.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.266 | Acc: 53.099,68.404,74.565,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 257
Batch: 0 | Loss: 1.328 | Acc: 65.625,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.352 | Acc: 65.960,98.065,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.332 | Acc: 66.254,97.980,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.324 | Acc: 66.483,97.887,99.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.331 | Acc: 66.233,97.840,99.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.326 | Acc: 66.399,97.881,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.329 | Acc: 66.122,97.850,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.330 | Acc: 66.240,97.883,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.327 | Acc: 66.314,97.860,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.332 | Acc: 66.039,97.937,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.337 | Acc: 65.990,97.928,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.334 | Acc: 65.993,97.957,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.334 | Acc: 66.020,97.938,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.335 | Acc: 66.050,97.920,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.333 | Acc: 66.184,97.940,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.330 | Acc: 66.251,97.950,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.330 | Acc: 66.287,97.943,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.330 | Acc: 66.388,97.922,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.337 | Acc: 66.248,97.903,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.339 | Acc: 66.189,97.894,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.029 | Acc: 51.562,71.094,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.187 | Acc: 54.167,69.308,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.250 | Acc: 53.373,68.369,74.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.260 | Acc: 53.138,68.673,74.629,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 258
Batch: 0 | Loss: 1.327 | Acc: 68.750,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.364 | Acc: 64.658,97.991,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.343 | Acc: 65.682,97.999,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.327 | Acc: 66.317,98.117,99.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.330 | Acc: 66.233,98.158,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.333 | Acc: 66.368,98.136,99.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.336 | Acc: 66.335,97.998,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.338 | Acc: 66.251,97.983,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.337 | Acc: 66.319,97.957,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.339 | Acc: 66.169,98.023,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.338 | Acc: 66.161,98.068,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.333 | Acc: 66.212,98.119,99.852,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.329 | Acc: 66.371,98.149,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.329 | Acc: 66.313,98.114,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.328 | Acc: 66.331,98.121,99.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.329 | Acc: 66.243,98.152,99.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.328 | Acc: 66.306,98.145,99.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.327 | Acc: 66.342,98.160,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.328 | Acc: 66.285,98.176,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.327 | Acc: 66.316,98.161,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.972 | Acc: 54.688,71.094,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.171 | Acc: 53.943,69.345,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.247 | Acc: 53.087,68.579,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.261 | Acc: 52.984,68.699,74.654,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 259
Batch: 0 | Loss: 1.189 | Acc: 74.219,97.656,98.438,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.286 | Acc: 68.452,98.251,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.295 | Acc: 67.778,98.304,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.308 | Acc: 67.495,98.040,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.327 | Acc: 66.628,98.081,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.324 | Acc: 66.569,98.144,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.331 | Acc: 66.316,98.057,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.328 | Acc: 66.473,98.039,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.330 | Acc: 66.348,98.044,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.330 | Acc: 66.402,98.027,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.328 | Acc: 66.426,98.064,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.327 | Acc: 66.424,98.070,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.327 | Acc: 66.406,98.081,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.331 | Acc: 66.391,98.030,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.328 | Acc: 66.409,98.037,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.330 | Acc: 66.334,98.020,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.330 | Acc: 66.377,98.019,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.331 | Acc: 66.310,98.011,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.334 | Acc: 66.203,97.985,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.332 | Acc: 66.197,97.997,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.953 | Acc: 54.688,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.191 | Acc: 54.055,69.457,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.256 | Acc: 53.106,68.464,74.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.266 | Acc: 52.843,68.558,74.590,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 260
Batch: 0 | Loss: 1.187 | Acc: 70.312,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.333 | Acc: 65.923,98.289,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.324 | Acc: 66.101,98.285,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.310 | Acc: 66.496,98.207,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.315 | Acc: 66.165,98.264,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.319 | Acc: 66.089,98.144,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.325 | Acc: 66.129,98.128,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.325 | Acc: 66.118,98.138,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.323 | Acc: 66.193,98.166,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.323 | Acc: 66.121,98.157,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.318 | Acc: 66.305,98.158,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.318 | Acc: 66.346,98.148,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.320 | Acc: 66.351,98.123,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.322 | Acc: 66.376,98.114,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.323 | Acc: 66.384,98.129,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.323 | Acc: 66.432,98.123,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.325 | Acc: 66.345,98.128,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.326 | Acc: 66.324,98.110,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.330 | Acc: 66.207,98.061,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.332 | Acc: 66.105,98.036,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.990 | Acc: 54.688,69.531,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.182 | Acc: 54.167,69.159,76.302,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.262 | Acc: 53.239,68.216,75.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.277 | Acc: 52.971,68.443,74.846,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 261
Batch: 0 | Loss: 1.274 | Acc: 61.719,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.325 | Acc: 65.513,98.214,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.338 | Acc: 65.816,97.866,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.330 | Acc: 66.509,98.053,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.326 | Acc: 66.387,98.071,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.331 | Acc: 66.553,98.043,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.333 | Acc: 66.510,98.031,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.335 | Acc: 66.373,98.027,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.339 | Acc: 66.329,98.015,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.337 | Acc: 66.298,98.015,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.331 | Acc: 66.348,98.053,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.327 | Acc: 66.470,98.102,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.327 | Acc: 66.380,98.123,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.326 | Acc: 66.424,98.069,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.328 | Acc: 66.356,98.071,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.329 | Acc: 66.375,98.056,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.330 | Acc: 66.348,98.043,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.332 | Acc: 66.253,98.041,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.331 | Acc: 66.222,98.039,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.331 | Acc: 66.279,98.001,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.996 | Acc: 53.906,69.531,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.171 | Acc: 54.092,69.308,75.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.250 | Acc: 53.296,68.159,75.057,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.263 | Acc: 53.112,68.430,74.962,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 262
Batch: 0 | Loss: 1.223 | Acc: 63.281,97.656,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.329 | Acc: 66.778,98.661,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.328 | Acc: 66.368,98.285,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.330 | Acc: 66.470,98.207,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.319 | Acc: 66.628,98.216,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.318 | Acc: 66.553,98.205,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.328 | Acc: 66.439,98.160,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.326 | Acc: 66.373,98.177,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.325 | Acc: 66.348,98.161,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.320 | Acc: 66.588,98.170,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.323 | Acc: 66.480,98.220,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.526,98.215,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.323 | Acc: 66.390,98.181,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.327 | Acc: 66.224,98.180,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.329 | Acc: 66.214,98.148,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.329 | Acc: 66.178,98.170,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.329 | Acc: 66.214,98.155,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.327 | Acc: 66.257,98.147,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.323 | Acc: 66.350,98.163,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.325 | Acc: 66.263,98.173,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.978 | Acc: 52.344,71.094,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.178 | Acc: 53.720,69.234,75.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.252 | Acc: 53.125,68.293,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.267 | Acc: 53.112,68.571,74.731,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 263
Batch: 0 | Loss: 1.473 | Acc: 59.375,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.320 | Acc: 66.443,97.805,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.324 | Acc: 66.406,98.075,99.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.310 | Acc: 66.445,98.233,99.923,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.308 | Acc: 66.493,98.148,99.923,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.305 | Acc: 66.662,98.113,99.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.315 | Acc: 66.406,98.160,99.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.309 | Acc: 66.439,98.210,99.889,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.314 | Acc: 66.207,98.205,99.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.309 | Acc: 66.393,98.222,99.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.310 | Acc: 66.286,98.228,99.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.308 | Acc: 66.459,98.257,99.866,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.311 | Acc: 66.500,98.269,99.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.311 | Acc: 66.475,98.219,99.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.311 | Acc: 66.481,98.215,99.875,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.314 | Acc: 66.422,98.222,99.870,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.315 | Acc: 66.414,98.214,99.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.314 | Acc: 66.493,98.224,99.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.316 | Acc: 66.478,98.212,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.435,98.196,99.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.990 | Acc: 53.906,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.184 | Acc: 54.018,69.531,76.042,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.262 | Acc: 53.258,68.426,75.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.272 | Acc: 53.035,68.609,74.705,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 264
Batch: 0 | Loss: 1.523 | Acc: 59.375,96.875,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.301 | Acc: 66.220,98.251,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.286 | Acc: 67.283,98.247,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.287 | Acc: 67.188,98.194,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.286 | Acc: 67.159,98.274,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.290 | Acc: 67.126,98.298,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.302 | Acc: 66.761,98.257,99.768,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.308 | Acc: 66.650,98.205,99.762,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.317 | Acc: 66.401,98.214,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.316 | Acc: 66.445,98.222,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.316 | Acc: 66.402,98.231,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.318 | Acc: 66.311,98.225,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.317 | Acc: 66.225,98.240,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.319 | Acc: 66.170,98.249,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.317 | Acc: 66.278,98.257,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.316 | Acc: 66.258,98.235,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.318 | Acc: 66.187,98.226,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.315 | Acc: 66.349,98.245,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.437,98.234,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.313 | Acc: 66.515,98.226,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.913 | Acc: 54.688,68.750,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.160 | Acc: 54.092,68.862,75.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.244 | Acc: 53.373,68.026,74.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.252 | Acc: 53.163,68.263,74.539,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 265
Batch: 0 | Loss: 1.265 | Acc: 65.625,100.000,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.297 | Acc: 66.146,98.549,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.302 | Acc: 66.197,98.438,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.308 | Acc: 66.304,98.386,99.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.320 | Acc: 66.117,98.293,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.324 | Acc: 66.151,98.236,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.324 | Acc: 66.238,98.224,99.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.320 | Acc: 66.356,98.266,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.322 | Acc: 66.319,98.292,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.322 | Acc: 66.208,98.291,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.319 | Acc: 66.274,98.305,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.320 | Acc: 66.297,98.278,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.321 | Acc: 66.348,98.275,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.320 | Acc: 66.284,98.270,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.315 | Acc: 66.409,98.279,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.317 | Acc: 66.370,98.248,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.401,98.253,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.395,98.247,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.317 | Acc: 66.389,98.258,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.316 | Acc: 66.402,98.263,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.935 | Acc: 54.688,71.094,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.173 | Acc: 54.055,69.234,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.244 | Acc: 53.220,68.331,74.657,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.258 | Acc: 53.035,68.571,74.539,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 266
Batch: 0 | Loss: 1.350 | Acc: 65.625,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.317 | Acc: 66.220,98.400,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.322 | Acc: 66.444,98.380,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.320 | Acc: 66.457,98.386,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.308 | Acc: 66.773,98.341,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.318 | Acc: 66.561,98.252,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.318 | Acc: 66.458,98.308,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.319 | Acc: 66.606,98.238,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.321 | Acc: 66.469,98.205,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.322 | Acc: 66.609,98.200,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.324 | Acc: 66.496,98.228,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.325 | Acc: 66.463,98.187,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.322 | Acc: 66.461,98.191,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.324 | Acc: 66.376,98.174,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.325 | Acc: 66.334,98.176,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.324 | Acc: 66.349,98.175,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.322 | Acc: 66.367,98.199,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.321 | Acc: 66.292,98.188,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.323 | Acc: 66.240,98.204,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.323 | Acc: 66.252,98.200,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.024 | Acc: 53.125,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.188 | Acc: 53.757,69.122,75.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.257 | Acc: 53.182,68.121,74.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.267 | Acc: 53.023,68.430,74.705,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 267
Batch: 0 | Loss: 1.427 | Acc: 68.750,96.094,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.310 | Acc: 66.853,98.326,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.332 | Acc: 66.444,98.075,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.323 | Acc: 66.457,98.220,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.320 | Acc: 66.493,98.264,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.314 | Acc: 66.731,98.329,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.313 | Acc: 66.787,98.244,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.308 | Acc: 66.772,98.210,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.309 | Acc: 66.702,98.258,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.308 | Acc: 66.920,98.273,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.307 | Acc: 66.826,98.313,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.309 | Acc: 66.742,98.342,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.308 | Acc: 66.708,98.301,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.308 | Acc: 66.739,98.279,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.309 | Acc: 66.748,98.287,99.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.311 | Acc: 66.668,98.282,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.313 | Acc: 66.620,98.267,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.312 | Acc: 66.610,98.289,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.545,98.277,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.315 | Acc: 66.503,98.276,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.981 | Acc: 53.125,67.969,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.181 | Acc: 54.055,69.457,75.260,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.253 | Acc: 53.258,68.331,74.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.263 | Acc: 53.074,68.494,74.436,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 268
Batch: 0 | Loss: 1.335 | Acc: 63.281,98.438,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.343 | Acc: 65.960,98.475,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.328 | Acc: 66.502,98.552,99.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.324 | Acc: 66.445,98.489,99.923,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.318 | Acc: 66.416,98.409,99.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.323 | Acc: 66.576,98.368,99.876,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.326 | Acc: 66.439,98.366,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.325 | Acc: 66.495,98.299,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.321 | Acc: 66.460,98.360,99.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.321 | Acc: 66.531,98.368,99.879,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.321 | Acc: 66.651,98.371,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.622,98.363,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.322 | Acc: 66.636,98.340,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.322 | Acc: 66.703,98.300,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.319 | Acc: 66.701,98.293,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.320 | Acc: 66.606,98.292,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.319 | Acc: 66.620,98.304,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.320 | Acc: 66.688,98.279,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.319 | Acc: 66.662,98.301,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.318 | Acc: 66.728,98.292,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.988 | Acc: 53.906,70.312,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.169 | Acc: 54.353,69.159,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.243 | Acc: 53.582,68.312,75.038,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.259 | Acc: 53.291,68.481,74.808,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 269
Batch: 0 | Loss: 1.279 | Acc: 67.969,98.438,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.275 | Acc: 67.336,98.028,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.309 | Acc: 66.463,98.152,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.302 | Acc: 66.445,98.245,99.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.305 | Acc: 66.397,98.274,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.307 | Acc: 66.545,98.283,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.315 | Acc: 66.406,98.289,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.312 | Acc: 66.473,98.266,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.318 | Acc: 66.173,98.268,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.316 | Acc: 66.303,98.243,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.316 | Acc: 66.375,98.220,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.315 | Acc: 66.396,98.190,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.315 | Acc: 66.426,98.198,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.316 | Acc: 66.526,98.189,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.317 | Acc: 66.515,98.196,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.317 | Acc: 66.531,98.186,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.601,98.172,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.317 | Acc: 66.580,98.144,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.504,98.152,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.540,98.173,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.027 | Acc: 52.344,68.750,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.183 | Acc: 53.832,69.159,75.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.258 | Acc: 53.011,68.121,74.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.268 | Acc: 52.869,68.417,74.565,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 270
Batch: 0 | Loss: 1.620 | Acc: 64.844,96.094,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.332 | Acc: 66.927,98.065,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.324 | Acc: 66.463,98.037,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.317 | Acc: 66.573,98.194,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.314 | Acc: 66.657,98.196,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.326 | Acc: 66.468,98.120,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.322 | Acc: 66.497,98.108,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.317 | Acc: 66.489,98.149,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.314 | Acc: 66.508,98.161,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.320 | Acc: 66.398,98.135,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.324 | Acc: 66.301,98.127,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.320 | Acc: 66.431,98.183,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.321 | Acc: 66.364,98.201,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.322 | Acc: 66.322,98.198,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.319 | Acc: 66.315,98.198,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.320 | Acc: 66.287,98.212,99.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.319 | Acc: 66.372,98.201,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.318 | Acc: 66.354,98.218,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.437,98.202,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.318 | Acc: 66.486,98.224,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.000 | Acc: 53.125,68.750,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.179 | Acc: 54.018,69.234,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.256 | Acc: 53.068,68.331,74.962,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.265 | Acc: 52.818,68.622,74.693,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 271
Batch: 0 | Loss: 1.273 | Acc: 73.438,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.376 | Acc: 66.183,98.214,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.332 | Acc: 66.921,98.056,99.733,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.333 | Acc: 66.726,97.887,99.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.328 | Acc: 66.744,98.013,99.720,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.327 | Acc: 66.553,98.035,99.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.335 | Acc: 66.342,98.037,99.735,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.335 | Acc: 66.312,98.016,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.333 | Acc: 66.314,98.025,99.767,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.334 | Acc: 66.247,98.071,99.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.334 | Acc: 66.290,98.060,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.330 | Acc: 66.389,98.116,99.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.326 | Acc: 66.497,98.143,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.323 | Acc: 66.523,98.132,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.321 | Acc: 66.548,98.165,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.320 | Acc: 66.513,98.160,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.322 | Acc: 66.511,98.133,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.322 | Acc: 66.509,98.142,99.785,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.322 | Acc: 66.543,98.132,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.323 | Acc: 66.517,98.136,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.992 | Acc: 53.906,67.969,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.180 | Acc: 54.092,69.085,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.257 | Acc: 53.201,68.197,75.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.267 | Acc: 53.023,68.507,74.693,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 272
Batch: 0 | Loss: 1.307 | Acc: 67.969,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.295 | Acc: 66.964,98.065,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.299 | Acc: 66.883,98.075,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.310 | Acc: 66.496,98.092,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.304 | Acc: 66.387,98.139,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.313 | Acc: 66.182,98.120,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.312 | Acc: 66.368,98.108,99.787,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.318 | Acc: 66.334,98.188,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.319 | Acc: 66.275,98.175,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.318 | Acc: 66.311,98.204,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.319 | Acc: 66.216,98.177,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.318 | Acc: 66.215,98.179,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.318 | Acc: 66.221,98.185,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.317 | Acc: 66.215,98.168,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.317 | Acc: 66.175,98.184,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.316 | Acc: 66.191,98.186,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.317 | Acc: 66.190,98.165,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.225,98.165,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.300,98.180,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.313 | Acc: 66.316,98.179,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.019 | Acc: 54.688,68.750,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.177 | Acc: 54.092,68.973,75.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.254 | Acc: 53.144,68.216,74.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.265 | Acc: 53.099,68.545,74.577,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 273
Batch: 0 | Loss: 1.136 | Acc: 70.312,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.318 | Acc: 66.481,98.214,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.325 | Acc: 66.006,98.418,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.332 | Acc: 65.804,98.284,99.898,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.326 | Acc: 65.953,98.254,99.894,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.320 | Acc: 66.159,98.244,99.876,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.326 | Acc: 66.071,98.270,99.877,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.321 | Acc: 66.345,98.255,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.314 | Acc: 66.503,98.258,99.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.316 | Acc: 66.389,98.222,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.317 | Acc: 66.430,98.224,99.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.382,98.201,99.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.317 | Acc: 66.403,98.214,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.314 | Acc: 66.367,98.204,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.318 | Acc: 66.295,98.157,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.319 | Acc: 66.289,98.162,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.317 | Acc: 66.341,98.177,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.319 | Acc: 66.266,98.160,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.307,98.182,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.319 | Acc: 66.279,98.212,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.983 | Acc: 53.906,70.312,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.192 | Acc: 54.092,69.122,76.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.254 | Acc: 53.182,68.197,75.019,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.266 | Acc: 53.048,68.468,74.757,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 274
Batch: 0 | Loss: 1.214 | Acc: 69.531,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.348 | Acc: 65.811,98.028,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.325 | Acc: 66.235,98.152,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.324 | Acc: 66.445,98.105,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.323 | Acc: 66.329,98.110,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.330 | Acc: 66.136,98.066,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.331 | Acc: 66.361,98.037,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.325 | Acc: 66.417,98.050,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.318 | Acc: 66.523,98.117,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.314 | Acc: 66.562,98.179,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.316 | Acc: 66.651,98.177,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.318 | Acc: 66.541,98.215,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.318 | Acc: 66.578,98.220,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.320 | Acc: 66.535,98.186,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.319 | Acc: 66.523,98.210,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.322 | Acc: 66.419,98.212,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.320 | Acc: 66.465,98.204,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.319 | Acc: 66.489,98.202,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.488,98.230,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.507,98.237,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.970 | Acc: 55.469,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.181 | Acc: 54.018,69.196,75.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.265 | Acc: 53.011,68.369,75.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.273 | Acc: 52.997,68.596,74.603,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 275
Batch: 0 | Loss: 1.356 | Acc: 60.156,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.317 | Acc: 66.406,98.363,99.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.310 | Acc: 66.521,98.209,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.320 | Acc: 65.932,98.181,99.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.323 | Acc: 65.895,98.167,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.319 | Acc: 66.081,98.229,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.310 | Acc: 66.335,98.263,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.309 | Acc: 66.395,98.232,99.839,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.318 | Acc: 66.304,98.214,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.318 | Acc: 66.268,98.213,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.320 | Acc: 66.329,98.193,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.317 | Acc: 66.403,98.197,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.319 | Acc: 66.358,98.204,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.319 | Acc: 66.319,98.228,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.316 | Acc: 66.428,98.226,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.319 | Acc: 66.383,98.225,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.320 | Acc: 66.319,98.223,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.318 | Acc: 66.310,98.224,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.319 | Acc: 66.220,98.219,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.321 | Acc: 66.236,98.214,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.016 | Acc: 52.344,69.531,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.175 | Acc: 54.129,69.122,75.446,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.249 | Acc: 53.392,68.216,74.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.260 | Acc: 53.061,68.507,74.590,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 276
Batch: 0 | Loss: 1.073 | Acc: 76.562,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.320 | Acc: 66.109,98.363,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.331 | Acc: 65.568,98.323,99.752,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.324 | Acc: 65.753,98.438,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.315 | Acc: 65.963,98.389,99.778,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.308 | Acc: 66.337,98.329,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.306 | Acc: 66.497,98.399,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.305 | Acc: 66.633,98.393,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.307 | Acc: 66.780,98.374,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.310 | Acc: 66.713,98.304,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.314 | Acc: 66.558,98.294,99.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.312 | Acc: 66.664,98.289,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.311 | Acc: 66.682,98.259,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.313 | Acc: 66.598,98.231,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.312 | Acc: 66.604,98.207,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.315 | Acc: 66.593,98.178,99.798,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.317 | Acc: 66.601,98.184,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.638,98.185,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.675,98.189,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.316 | Acc: 66.628,98.198,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.005 | Acc: 53.906,71.094,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.191 | Acc: 54.018,69.457,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.269 | Acc: 53.030,68.369,74.943,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.273 | Acc: 52.997,68.699,74.667,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 277
Batch: 0 | Loss: 1.514 | Acc: 58.594,98.438,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.349 | Acc: 65.216,98.028,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.336 | Acc: 66.197,98.190,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.329 | Acc: 66.304,98.207,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.329 | Acc: 66.387,98.148,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.316 | Acc: 66.631,98.229,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.325 | Acc: 66.213,98.192,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.321 | Acc: 66.401,98.216,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.318 | Acc: 66.562,98.209,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.318 | Acc: 66.544,98.239,99.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.320 | Acc: 66.515,98.220,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.320 | Acc: 66.445,98.183,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.320 | Acc: 66.536,98.172,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.322 | Acc: 66.499,98.165,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.318 | Acc: 66.645,98.171,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.317 | Acc: 66.642,98.173,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.318 | Acc: 66.686,98.153,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.319 | Acc: 66.564,98.128,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.319 | Acc: 66.582,98.126,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.607,98.150,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.974 | Acc: 54.688,70.312,78.906,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.189 | Acc: 54.092,69.159,75.744,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.257 | Acc: 53.335,68.350,74.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.265 | Acc: 53.112,68.481,74.603,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 278
Batch: 0 | Loss: 1.352 | Acc: 59.375,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.375 | Acc: 65.141,98.326,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.341 | Acc: 65.758,98.304,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.327 | Acc: 65.868,98.207,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.333 | Acc: 65.943,98.100,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.331 | Acc: 66.043,98.074,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.334 | Acc: 65.916,98.134,99.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.330 | Acc: 65.919,98.155,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.321 | Acc: 66.222,98.141,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.324 | Acc: 66.013,98.122,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.321 | Acc: 66.134,98.162,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.212,98.133,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.319 | Acc: 66.315,98.136,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.319 | Acc: 66.364,98.162,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.318 | Acc: 66.373,98.159,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.313 | Acc: 66.453,98.160,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.314 | Acc: 66.418,98.150,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.313 | Acc: 66.413,98.163,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.315 | Acc: 66.352,98.163,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.313 | Acc: 66.388,98.179,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.019 | Acc: 52.344,69.531,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.187 | Acc: 54.204,69.382,75.409,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.262 | Acc: 53.182,68.369,74.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.272 | Acc: 52.971,68.596,74.513,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 279
Batch: 0 | Loss: 1.213 | Acc: 69.531,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.287 | Acc: 67.671,98.549,99.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.283 | Acc: 67.740,98.399,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.283 | Acc: 67.649,98.438,99.872,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.294 | Acc: 67.207,98.360,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.299 | Acc: 66.940,98.283,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.304 | Acc: 66.729,98.308,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.307 | Acc: 66.645,98.282,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.310 | Acc: 66.518,98.355,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.313 | Acc: 66.449,98.295,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.314 | Acc: 66.398,98.290,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.312 | Acc: 66.516,98.243,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.314 | Acc: 66.520,98.178,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.315 | Acc: 66.517,98.138,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.316 | Acc: 66.498,98.143,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.317 | Acc: 66.401,98.162,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.318 | Acc: 66.440,98.153,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.489,98.172,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.317 | Acc: 66.454,98.171,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.316 | Acc: 66.496,98.179,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.970 | Acc: 53.906,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.166 | Acc: 53.795,69.420,75.893,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.241 | Acc: 53.220,68.483,75.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.256 | Acc: 53.151,68.596,74.846,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 280
Batch: 0 | Loss: 1.495 | Acc: 61.719,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.360 | Acc: 64.807,98.065,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.339 | Acc: 65.530,98.285,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.317 | Acc: 66.278,98.233,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.318 | Acc: 66.271,98.293,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.307 | Acc: 66.515,98.306,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.308 | Acc: 66.652,98.283,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.309 | Acc: 66.711,98.282,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.315 | Acc: 66.591,98.277,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.315 | Acc: 66.592,98.230,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.317 | Acc: 66.655,98.204,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.320 | Acc: 66.587,98.176,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.318 | Acc: 66.572,98.185,99.831,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.320 | Acc: 66.478,98.183,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.317 | Acc: 66.554,98.182,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.319 | Acc: 66.461,98.191,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.319 | Acc: 66.474,98.206,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.318 | Acc: 66.509,98.197,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.316 | Acc: 66.627,98.197,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.316 | Acc: 66.599,98.193,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.037 | Acc: 53.906,68.750,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.186 | Acc: 53.981,69.606,75.670,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.257 | Acc: 53.258,68.598,74.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.273 | Acc: 52.984,68.724,74.577,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 281
Batch: 0 | Loss: 1.355 | Acc: 67.188,100.000,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.314 | Acc: 66.332,98.028,99.777,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.346 | Acc: 65.492,98.133,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.335 | Acc: 65.791,98.156,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.335 | Acc: 65.972,98.061,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.326 | Acc: 66.282,98.128,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.322 | Acc: 66.548,98.115,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.319 | Acc: 66.672,98.083,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.311 | Acc: 66.702,98.127,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.308 | Acc: 66.708,98.196,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.309 | Acc: 66.845,98.181,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.312 | Acc: 66.788,98.130,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.312 | Acc: 66.795,98.120,99.789,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.311 | Acc: 66.765,98.132,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.312 | Acc: 66.737,98.137,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.313 | Acc: 66.710,98.152,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.314 | Acc: 66.669,98.148,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.315 | Acc: 66.612,98.147,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.316 | Acc: 66.588,98.148,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.503,98.124,99.805,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.973 | Acc: 53.906,70.312,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.164 | Acc: 53.832,69.457,75.893,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.237 | Acc: 53.163,68.369,75.171,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.255 | Acc: 52.894,68.686,74.821,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 282
Batch: 0 | Loss: 1.412 | Acc: 62.500,97.656,98.438,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.262 | Acc: 67.113,98.363,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.330 | Acc: 65.911,97.980,99.695,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.319 | Acc: 66.291,98.092,99.731,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.307 | Acc: 66.725,98.148,99.759,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.309 | Acc: 66.538,98.128,99.776,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.311 | Acc: 66.484,98.179,99.800,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.305 | Acc: 66.539,98.260,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.307 | Acc: 66.566,98.214,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.305 | Acc: 66.739,98.187,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.306 | Acc: 66.636,98.150,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.306 | Acc: 66.625,98.179,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.310 | Acc: 66.633,98.149,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.307 | Acc: 66.700,98.168,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.312 | Acc: 66.584,98.154,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.312 | Acc: 66.500,98.196,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.314 | Acc: 66.496,98.189,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.315 | Acc: 66.486,98.183,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.319 | Acc: 66.393,98.152,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.320 | Acc: 66.347,98.140,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.954 | Acc: 53.906,68.750,73.438,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.159 | Acc: 54.092,69.308,75.595,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.242 | Acc: 53.201,68.407,75.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.251 | Acc: 52.997,68.712,74.757,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 283
Batch: 0 | Loss: 1.365 | Acc: 66.406,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.334 | Acc: 66.704,97.991,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.325 | Acc: 66.292,98.228,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.326 | Acc: 66.189,98.130,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.319 | Acc: 66.522,98.206,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.316 | Acc: 66.499,98.329,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.319 | Acc: 66.380,98.250,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.317 | Acc: 66.495,98.310,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.319 | Acc: 66.392,98.268,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.321 | Acc: 66.303,98.222,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.317 | Acc: 66.426,98.263,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.317 | Acc: 66.456,98.289,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.312 | Acc: 66.510,98.295,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.312 | Acc: 66.592,98.282,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.311 | Acc: 66.590,98.262,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.309 | Acc: 66.572,98.266,99.813,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.310 | Acc: 66.552,98.267,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.313 | Acc: 66.489,98.213,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.311 | Acc: 66.517,98.215,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.311 | Acc: 66.519,98.196,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.988 | Acc: 53.906,69.531,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.182 | Acc: 54.129,69.420,75.149,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.249 | Acc: 53.373,68.464,74.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.258 | Acc: 52.959,68.737,74.731,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 284
Batch: 0 | Loss: 1.530 | Acc: 55.469,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.321 | Acc: 66.741,98.475,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.329 | Acc: 66.673,98.323,99.886,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.305 | Acc: 67.354,98.425,99.910,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.304 | Acc: 67.332,98.302,99.923,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.317 | Acc: 66.723,98.244,99.899,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.317 | Acc: 66.690,98.237,99.903,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.313 | Acc: 66.755,98.221,99.900,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.317 | Acc: 66.741,98.185,99.869,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.320 | Acc: 66.644,98.140,99.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.316 | Acc: 66.612,98.177,99.868,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.312 | Acc: 66.636,98.194,99.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.312 | Acc: 66.695,98.191,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.312 | Acc: 66.727,98.228,99.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.312 | Acc: 66.693,98.221,99.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.311 | Acc: 66.733,98.232,99.852,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.313 | Acc: 66.664,98.199,99.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.313 | Acc: 66.713,98.195,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.638,98.221,99.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.316 | Acc: 66.624,98.175,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.040 | Acc: 55.469,67.969,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.187 | Acc: 54.241,69.010,75.484,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.251 | Acc: 53.506,68.121,74.714,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.258 | Acc: 53.266,68.468,74.513,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 285
Batch: 0 | Loss: 1.309 | Acc: 64.844,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.336 | Acc: 65.179,97.656,99.702,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.309 | Acc: 66.311,97.980,99.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.313 | Acc: 66.291,98.002,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.307 | Acc: 66.580,97.984,99.769,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.305 | Acc: 66.716,98.058,99.737,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.315 | Acc: 66.471,98.069,99.722,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.316 | Acc: 66.517,98.105,99.723,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.317 | Acc: 66.416,98.112,99.738,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.312 | Acc: 66.449,98.131,99.745,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.311 | Acc: 66.433,98.169,99.755,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.312 | Acc: 66.343,98.109,99.770,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.312 | Acc: 66.406,98.104,99.780,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.309 | Acc: 66.433,98.138,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.313 | Acc: 66.334,98.173,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.315 | Acc: 66.352,98.183,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.314 | Acc: 66.365,98.189,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.317 | Acc: 66.255,98.185,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.316 | Acc: 66.300,98.197,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.355,98.196,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.997 | Acc: 53.906,70.312,78.125,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.173 | Acc: 53.869,69.196,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.244 | Acc: 53.049,68.407,75.076,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.261 | Acc: 52.766,68.622,74.705,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 286
Batch: 0 | Loss: 1.518 | Acc: 62.500,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.301 | Acc: 66.853,98.214,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.315 | Acc: 66.482,97.885,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.302 | Acc: 66.611,98.130,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.315 | Acc: 66.377,98.187,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.317 | Acc: 66.166,98.236,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.318 | Acc: 66.180,98.153,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.319 | Acc: 66.201,98.138,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.314 | Acc: 66.392,98.127,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.319 | Acc: 66.324,98.092,99.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.321 | Acc: 66.379,98.127,99.852,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.315 | Acc: 66.477,98.137,99.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.315 | Acc: 66.432,98.162,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.313 | Acc: 66.604,98.159,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.317 | Acc: 66.509,98.154,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.313 | Acc: 66.645,98.186,99.847,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.623,98.182,99.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.318 | Acc: 66.541,98.206,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.519,98.199,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.318 | Acc: 66.499,98.185,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.974 | Acc: 53.906,70.312,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.183 | Acc: 54.167,69.196,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.255 | Acc: 53.125,68.350,74.619,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.265 | Acc: 52.869,68.584,74.488,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 287
Batch: 0 | Loss: 1.300 | Acc: 67.188,99.219,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.327 | Acc: 66.518,98.103,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.327 | Acc: 66.197,98.152,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.308 | Acc: 66.675,98.335,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.299 | Acc: 67.207,98.322,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.297 | Acc: 67.389,98.360,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.305 | Acc: 67.013,98.334,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.313 | Acc: 66.750,98.310,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.312 | Acc: 66.877,98.282,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.310 | Acc: 66.924,98.286,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.308 | Acc: 66.943,98.290,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.307 | Acc: 66.876,98.275,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.309 | Acc: 66.841,98.262,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.308 | Acc: 66.864,98.258,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.314 | Acc: 66.598,98.204,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.313 | Acc: 66.552,98.225,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.312 | Acc: 66.586,98.223,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.313 | Acc: 66.612,98.218,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.608,98.225,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.313 | Acc: 66.615,98.232,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.011 | Acc: 54.688,69.531,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.178 | Acc: 53.646,68.973,75.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.250 | Acc: 52.934,68.140,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.264 | Acc: 52.805,68.417,74.693,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 288
Batch: 0 | Loss: 1.210 | Acc: 68.750,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.315 | Acc: 66.927,98.289,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.316 | Acc: 66.406,98.380,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.324 | Acc: 66.240,98.245,99.872,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.331 | Acc: 66.078,98.216,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.336 | Acc: 65.888,98.198,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.334 | Acc: 65.961,98.237,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.333 | Acc: 65.963,98.293,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.327 | Acc: 66.091,98.287,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.323 | Acc: 66.221,98.304,99.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.321 | Acc: 66.282,98.309,99.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.403,98.254,99.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.323 | Acc: 66.403,98.220,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.318 | Acc: 66.592,98.267,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.317 | Acc: 66.662,98.265,99.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.317 | Acc: 66.679,98.266,99.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.313 | Acc: 66.832,98.260,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.311 | Acc: 66.848,98.268,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.312 | Acc: 66.913,98.269,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.313 | Acc: 66.835,98.269,99.815,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.993 | Acc: 53.906,68.750,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.183 | Acc: 53.571,68.973,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.256 | Acc: 53.030,68.026,74.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.268 | Acc: 52.933,68.327,74.436,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 289
Batch: 0 | Loss: 1.250 | Acc: 67.969,100.000,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.330 | Acc: 65.848,98.326,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.307 | Acc: 66.940,98.133,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.308 | Acc: 66.790,98.156,99.782,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.314 | Acc: 66.734,98.206,99.788,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.316 | Acc: 66.530,98.221,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.319 | Acc: 66.490,98.186,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.315 | Acc: 66.672,98.227,99.812,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.316 | Acc: 66.688,98.229,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.319 | Acc: 66.609,98.230,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.325 | Acc: 66.468,98.228,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.403,98.271,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.319 | Acc: 66.520,98.279,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.315 | Acc: 66.580,98.291,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.315 | Acc: 66.531,98.298,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.314 | Acc: 66.601,98.313,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.313 | Acc: 66.633,98.316,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.315 | Acc: 66.587,98.311,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.558,98.286,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.315 | Acc: 66.527,98.265,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.991 | Acc: 54.688,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.180 | Acc: 53.943,68.936,75.558,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.258 | Acc: 53.182,68.102,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.269 | Acc: 53.023,68.391,74.526,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 290
Batch: 0 | Loss: 1.409 | Acc: 64.844,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.315 | Acc: 65.960,98.214,99.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.316 | Acc: 66.082,98.323,99.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.315 | Acc: 66.253,98.258,99.898,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.325 | Acc: 66.204,98.119,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.324 | Acc: 66.182,98.190,99.853,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.330 | Acc: 65.974,98.224,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.322 | Acc: 66.041,98.260,99.861,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.326 | Acc: 66.042,98.219,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.323 | Acc: 66.044,98.200,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.322 | Acc: 66.161,98.216,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.321 | Acc: 66.134,98.229,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.317 | Acc: 66.228,98.249,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.314 | Acc: 66.328,98.252,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.315 | Acc: 66.317,98.235,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.316 | Acc: 66.297,98.238,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.287,98.218,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.313 | Acc: 66.319,98.208,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.313 | Acc: 66.346,98.215,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.314 | Acc: 66.375,98.218,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.025 | Acc: 53.906,70.312,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.194 | Acc: 54.167,69.085,75.372,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.262 | Acc: 53.201,68.083,74.924,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.271 | Acc: 53.061,68.353,74.705,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 291
Batch: 0 | Loss: 1.239 | Acc: 67.969,100.000,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.327 | Acc: 65.551,98.512,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.309 | Acc: 65.816,98.418,99.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.310 | Acc: 66.009,98.309,99.859,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.326 | Acc: 65.731,98.274,99.855,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.324 | Acc: 65.555,98.236,99.869,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.317 | Acc: 65.838,98.244,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.317 | Acc: 66.046,98.249,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.314 | Acc: 66.173,98.243,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.320 | Acc: 66.095,98.204,99.810,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.316 | Acc: 66.344,98.181,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.316 | Acc: 66.403,98.172,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.315 | Acc: 66.419,98.185,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.318 | Acc: 66.427,98.138,99.823,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.318 | Acc: 66.448,98.132,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.318 | Acc: 66.398,98.134,99.831,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.440,98.150,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.452,98.156,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.315 | Acc: 66.441,98.182,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.404,98.179,99.828,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.012 | Acc: 53.906,69.531,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.182 | Acc: 54.018,68.936,75.335,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.258 | Acc: 53.277,68.216,74.962,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.266 | Acc: 53.048,68.494,74.757,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 292
Batch: 0 | Loss: 1.236 | Acc: 65.625,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.301 | Acc: 66.592,98.586,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.312 | Acc: 66.406,98.361,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.312 | Acc: 66.240,98.348,99.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.320 | Acc: 66.001,98.264,99.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.320 | Acc: 66.019,98.306,99.884,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.313 | Acc: 66.245,98.315,99.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.319 | Acc: 66.229,98.249,99.856,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.322 | Acc: 66.217,98.200,99.854,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.315 | Acc: 66.501,98.209,99.849,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.315 | Acc: 66.554,98.216,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.316 | Acc: 66.537,98.222,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.316 | Acc: 66.562,98.204,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.314 | Acc: 66.577,98.225,99.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.314 | Acc: 66.609,98.212,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.316 | Acc: 66.622,98.191,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.630,98.167,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.640,98.172,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.601,98.173,99.816,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.626,98.191,99.809,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.073 | Acc: 53.125,70.312,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.189 | Acc: 54.129,69.308,75.632,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.261 | Acc: 53.392,68.426,75.248,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.272 | Acc: 53.074,68.660,74.910,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 293
Batch: 0 | Loss: 1.342 | Acc: 64.844,98.438,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.305 | Acc: 66.332,98.065,99.926,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.286 | Acc: 66.845,98.133,99.905,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.298 | Acc: 66.855,98.079,99.872,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.303 | Acc: 66.802,98.032,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.316 | Acc: 66.352,98.120,99.869,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.306 | Acc: 66.665,98.212,99.858,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.307 | Acc: 66.661,98.221,99.850,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.308 | Acc: 66.484,98.224,99.864,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.316 | Acc: 66.367,98.213,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.316 | Acc: 66.379,98.189,99.845,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.319 | Acc: 66.328,98.148,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.320 | Acc: 66.218,98.133,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.327 | Acc: 66.056,98.108,99.841,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.326 | Acc: 66.070,98.121,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.325 | Acc: 66.030,98.165,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.321 | Acc: 66.182,98.172,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.322 | Acc: 66.218,98.163,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.319 | Acc: 66.266,98.171,99.827,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.319 | Acc: 66.259,98.193,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.920 | Acc: 54.688,69.531,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.177 | Acc: 54.055,69.085,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.247 | Acc: 53.087,68.159,74.981,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.262 | Acc: 52.907,68.379,74.577,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 294
Batch: 0 | Loss: 1.274 | Acc: 65.625,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.308 | Acc: 67.857,98.438,99.740,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.307 | Acc: 67.226,98.323,99.790,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.311 | Acc: 66.842,98.271,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.310 | Acc: 66.975,98.187,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.313 | Acc: 66.692,98.229,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.317 | Acc: 66.510,98.250,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.322 | Acc: 66.445,98.221,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.313 | Acc: 66.688,98.248,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.314 | Acc: 66.769,98.204,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.317 | Acc: 66.562,98.220,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.315 | Acc: 66.661,98.232,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.318 | Acc: 66.575,98.185,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.318 | Acc: 66.583,98.195,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.320 | Acc: 66.517,98.204,99.842,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.318 | Acc: 66.570,98.251,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.317 | Acc: 66.526,98.253,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.571,98.257,99.837,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.316 | Acc: 66.560,98.238,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.507,98.255,99.836,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.009 | Acc: 53.125,68.750,75.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.177 | Acc: 53.943,69.345,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.245 | Acc: 53.277,68.521,75.152,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.261 | Acc: 53.023,68.737,74.821,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 295
Batch: 0 | Loss: 1.486 | Acc: 60.938,96.875,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.328 | Acc: 65.923,98.438,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.330 | Acc: 66.311,98.285,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.313 | Acc: 66.650,98.194,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.320 | Acc: 66.368,98.196,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.320 | Acc: 66.515,98.229,99.807,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.314 | Acc: 66.716,98.244,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.311 | Acc: 66.861,98.244,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.306 | Acc: 67.052,98.282,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.302 | Acc: 67.084,98.282,99.819,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.305 | Acc: 66.912,98.282,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.302 | Acc: 66.961,98.307,99.830,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.304 | Acc: 66.912,98.305,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.308 | Acc: 66.783,98.273,99.802,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.312 | Acc: 66.734,98.248,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.311 | Acc: 66.749,98.248,99.803,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.314 | Acc: 66.628,98.255,99.808,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.314 | Acc: 66.628,98.259,99.792,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.629,98.232,99.784,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.314 | Acc: 66.710,98.220,99.774,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.980 | Acc: 53.125,68.750,77.344,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.180 | Acc: 53.981,69.048,76.004,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.249 | Acc: 53.201,68.216,75.133,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.263 | Acc: 52.971,68.366,74.859,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 296
Batch: 0 | Loss: 1.307 | Acc: 65.625,97.656,100.000,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.289 | Acc: 66.592,98.363,99.888,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.310 | Acc: 66.311,98.228,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.305 | Acc: 66.483,98.335,99.898,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.302 | Acc: 66.358,98.302,99.904,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.302 | Acc: 66.267,98.321,99.876,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.306 | Acc: 66.458,98.308,99.871,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.303 | Acc: 66.406,98.327,99.867,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.308 | Acc: 66.314,98.311,99.869,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.310 | Acc: 66.367,98.256,99.862,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.314 | Acc: 66.297,98.247,99.860,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.315 | Acc: 66.435,98.151,99.866,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.316 | Acc: 66.345,98.165,99.844,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.314 | Acc: 66.391,98.195,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.312 | Acc: 66.428,98.171,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.314 | Acc: 66.437,98.162,99.818,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.314 | Acc: 66.482,98.162,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.316 | Acc: 66.509,98.160,99.821,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.314 | Acc: 66.536,98.150,99.825,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.316 | Acc: 66.552,98.132,99.824,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 3.986 | Acc: 53.906,70.312,76.562,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.166 | Acc: 54.055,69.754,75.930,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.247 | Acc: 53.258,68.559,75.210,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.259 | Acc: 53.074,68.840,74.872,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 297
Batch: 0 | Loss: 1.496 | Acc: 60.938,99.219,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.339 | Acc: 65.290,98.251,99.851,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.328 | Acc: 66.273,97.904,99.829,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.313 | Acc: 66.560,97.951,99.846,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.317 | Acc: 66.512,97.975,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.318 | Acc: 66.491,97.997,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.321 | Acc: 66.355,98.121,99.793,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.320 | Acc: 66.462,98.100,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.320 | Acc: 66.421,98.132,99.801,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.320 | Acc: 66.445,98.127,99.797,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.325 | Acc: 66.266,98.123,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.324 | Acc: 66.314,98.116,99.806,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.319 | Acc: 66.452,98.146,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.319 | Acc: 66.487,98.156,99.796,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.320 | Acc: 66.476,98.157,99.786,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.318 | Acc: 66.507,98.152,99.785,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.316 | Acc: 66.518,98.150,99.791,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.318 | Acc: 66.525,98.135,99.794,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.318 | Acc: 66.551,98.111,99.799,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.317 | Acc: 66.501,98.118,99.795,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.021 | Acc: 54.688,69.531,75.781,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.176 | Acc: 53.795,69.420,75.521,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.246 | Acc: 53.220,68.293,74.771,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.258 | Acc: 53.010,68.596,74.552,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 298
Batch: 0 | Loss: 1.509 | Acc: 58.594,97.656,99.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 1.345 | Acc: 66.592,98.177,99.963,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 1.328 | Acc: 66.216,98.304,99.848,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 1.319 | Acc: 66.278,98.271,99.885,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 80 | Loss: 1.327 | Acc: 65.654,98.245,99.865,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 100 | Loss: 1.323 | Acc: 65.764,98.275,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 120 | Loss: 1.322 | Acc: 66.103,98.224,99.826,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 140 | Loss: 1.323 | Acc: 66.234,98.127,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 160 | Loss: 1.320 | Acc: 66.513,98.156,99.811,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 180 | Loss: 1.323 | Acc: 66.471,98.114,99.814,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 200 | Loss: 1.320 | Acc: 66.593,98.134,99.817,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 220 | Loss: 1.317 | Acc: 66.643,98.169,99.820,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 240 | Loss: 1.314 | Acc: 66.688,98.152,99.835,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 260 | Loss: 1.313 | Acc: 66.795,98.198,99.838,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 280 | Loss: 1.313 | Acc: 66.762,98.223,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 300 | Loss: 1.315 | Acc: 66.648,98.201,99.834,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 320 | Loss: 1.315 | Acc: 66.715,98.204,99.832,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 340 | Loss: 1.313 | Acc: 66.793,98.206,99.840,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 360 | Loss: 1.313 | Acc: 66.815,98.180,99.833,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 380 | Loss: 1.315 | Acc: 66.714,98.181,99.822,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 0 | Loss: 4.047 | Acc: 53.906,68.750,74.219,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 20 | Loss: 4.190 | Acc: 54.129,69.196,76.079,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 40 | Loss: 4.256 | Acc: 53.335,68.312,75.114,% | Adaptive Acc: 0.000% | clf_exit: 
Batch: 60 | Loss: 4.266 | Acc: 53.189,68.545,74.872,% | Adaptive Acc: 0.000% | clf_exit: 

Epoch: 299
main.py:228: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  confidence, idx = torch.topk(F.softmax(outputs[j][i]), k=1)
Batch: 0 | Loss: 1.363 | Acc: 57.812,98.438,100.000,% | Adaptive Acc: 88.281% | clf_exit: 0.492 0.500 0.008
Batch: 20 | Loss: 1.304 | Acc: 67.225,98.326,99.851,% | Adaptive Acc: 92.597% | clf_exit: 0.557 0.427 0.016
Batch: 40 | Loss: 1.324 | Acc: 66.502,98.056,99.809,% | Adaptive Acc: 92.550% | clf_exit: 0.548 0.438 0.014
Batch: 60 | Loss: 1.328 | Acc: 66.470,98.092,99.757,% | Adaptive Acc: 92.546% | clf_exit: 0.550 0.434 0.016
Batch: 80 | Loss: 1.329 | Acc: 66.628,98.129,99.778,% | Adaptive Acc: 92.766% | clf_exit: 0.548 0.436 0.016
Batch: 100 | Loss: 1.329 | Acc: 66.685,98.105,99.791,% | Adaptive Acc: 92.737% | clf_exit: 0.547 0.437 0.016
Batch: 120 | Loss: 1.325 | Acc: 66.516,98.147,99.793,% | Adaptive Acc: 92.801% | clf_exit: 0.548 0.435 0.016
Batch: 140 | Loss: 1.324 | Acc: 66.456,98.188,99.795,% | Adaptive Acc: 92.697% | clf_exit: 0.549 0.435 0.016
Batch: 160 | Loss: 1.322 | Acc: 66.426,98.195,99.796,% | Adaptive Acc: 92.610% | clf_exit: 0.551 0.434 0.015
Batch: 180 | Loss: 1.323 | Acc: 66.247,98.183,99.784,% | Adaptive Acc: 92.347% | clf_exit: 0.551 0.434 0.015
Batch: 200 | Loss: 1.322 | Acc: 66.212,98.177,99.794,% | Adaptive Acc: 92.277% | clf_exit: 0.551 0.433 0.016
Batch: 220 | Loss: 1.317 | Acc: 66.300,98.215,99.802,% | Adaptive Acc: 92.393% | clf_exit: 0.552 0.432 0.016
Batch: 240 | Loss: 1.317 | Acc: 66.299,98.191,99.799,% | Adaptive Acc: 92.385% | clf_exit: 0.552 0.432 0.016
Batch: 260 | Loss: 1.313 | Acc: 66.400,98.201,99.805,% | Adaptive Acc: 92.403% | clf_exit: 0.554 0.431 0.016
Batch: 280 | Loss: 1.315 | Acc: 66.281,98.210,99.800,% | Adaptive Acc: 92.446% | clf_exit: 0.552 0.432 0.016
Batch: 300 | Loss: 1.315 | Acc: 66.315,98.212,99.792,% | Adaptive Acc: 92.411% | clf_exit: 0.553 0.431 0.016
Batch: 320 | Loss: 1.318 | Acc: 66.204,98.199,99.793,% | Adaptive Acc: 92.372% | clf_exit: 0.553 0.431 0.016
Batch: 340 | Loss: 1.318 | Acc: 66.246,98.211,99.796,% | Adaptive Acc: 92.357% | clf_exit: 0.553 0.431 0.016
Batch: 360 | Loss: 1.318 | Acc: 66.287,98.219,99.794,% | Adaptive Acc: 92.339% | clf_exit: 0.553 0.431 0.015
Batch: 380 | Loss: 1.317 | Acc: 66.296,98.228,99.799,% | Adaptive Acc: 92.347% | clf_exit: 0.553 0.431 0.016
main.py:328: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  confidence, idx = torch.topk(F.softmax(outputs[j][i]), k=1)
Batch: 0 | Loss: 3.973 | Acc: 53.906,68.750,77.344,% | Adaptive Acc: 65.625% | clf_exit: 0.617 0.312 0.070
Batch: 20 | Loss: 4.183 | Acc: 53.832,69.308,75.856,% | Adaptive Acc: 67.076% | clf_exit: 0.580 0.325 0.095
Batch: 40 | Loss: 4.255 | Acc: 53.182,68.407,75.171,% | Adaptive Acc: 66.311% | clf_exit: 0.569 0.331 0.100
Batch: 60 | Loss: 4.267 | Acc: 52.946,68.571,74.667,% | Adaptive Acc: 66.048% | clf_exit: 0.571 0.330 0.099
model is save as models/modelG_2con3_cifar100_adaptive0_circles5_dropout1.00_all0clf0_vanilla0_ge1_fb111_lmbda0.0000.pt
Evaluate with different circles:
Batch: 0 | Loss: 7.371 | Acc: 53.906,46.875,30.469,% | Adaptive Acc: 53.125% | clf_exit: 0.617 0.016 0.367
Batch: 20 | Loss: 7.763 | Acc: 53.832,40.290,24.368,% | Adaptive Acc: 49.926% | clf_exit: 0.580 0.012 0.408
Batch: 40 | Loss: 7.761 | Acc: 53.182,39.405,24.562,% | Adaptive Acc: 48.990% | clf_exit: 0.569 0.012 0.419
Batch: 60 | Loss: 7.777 | Acc: 52.946,39.165,24.923,% | Adaptive Acc: 49.027% | clf_exit: 0.571 0.010 0.418
Batch: 0 | Loss: 5.494 | Acc: 53.906,65.625,63.281,% | Adaptive Acc: 60.938% | clf_exit: 0.617 0.070 0.312
Batch: 20 | Loss: 5.961 | Acc: 53.832,59.412,55.878,% | Adaptive Acc: 60.045% | clf_exit: 0.580 0.059 0.361
Batch: 40 | Loss: 5.977 | Acc: 53.182,58.956,55.526,% | Adaptive Acc: 59.261% | clf_exit: 0.569 0.057 0.373
Batch: 60 | Loss: 5.996 | Acc: 52.946,59.273,55.315,% | Adaptive Acc: 58.952% | clf_exit: 0.571 0.056 0.373
Batch: 0 | Loss: 4.535 | Acc: 53.906,70.312,69.531,% | Adaptive Acc: 65.625% | clf_exit: 0.617 0.133 0.250
Batch: 20 | Loss: 4.916 | Acc: 53.832,65.960,67.746,% | Adaptive Acc: 65.848% | clf_exit: 0.580 0.131 0.289
Batch: 40 | Loss: 4.949 | Acc: 53.182,65.396,67.492,% | Adaptive Acc: 64.939% | clf_exit: 0.569 0.131 0.300
Batch: 60 | Loss: 4.964 | Acc: 52.946,65.779,67.354,% | Adaptive Acc: 64.703% | clf_exit: 0.571 0.131 0.298
Batch: 0 | Loss: 4.018 | Acc: 53.906,69.531,73.438,% | Adaptive Acc: 67.969% | clf_exit: 0.617 0.203 0.180
Batch: 20 | Loss: 4.304 | Acc: 53.832,68.564,73.326,% | Adaptive Acc: 67.597% | clf_exit: 0.580 0.212 0.208
Batch: 40 | Loss: 4.349 | Acc: 53.182,67.835,72.961,% | Adaptive Acc: 66.940% | clf_exit: 0.569 0.212 0.218
Batch: 60 | Loss: 4.362 | Acc: 52.946,68.251,72.823,% | Adaptive Acc: 66.752% | clf_exit: 0.571 0.212 0.217
Batch: 0 | Loss: 3.824 | Acc: 53.906,69.531,76.562,% | Adaptive Acc: 67.188% | clf_exit: 0.617 0.297 0.086
Batch: 20 | Loss: 4.054 | Acc: 53.832,69.754,75.818,% | Adaptive Acc: 68.080% | clf_exit: 0.580 0.284 0.136
Batch: 40 | Loss: 4.111 | Acc: 53.182,68.731,75.057,% | Adaptive Acc: 67.016% | clf_exit: 0.569 0.288 0.143
Batch: 60 | Loss: 4.123 | Acc: 52.946,68.840,74.680,% | Adaptive Acc: 66.701% | clf_exit: 0.571 0.283 0.146
Batch: 0 | Loss: 3.973 | Acc: 53.906,68.750,77.344,% | Adaptive Acc: 65.625% | clf_exit: 0.617 0.312 0.070
Batch: 20 | Loss: 4.183 | Acc: 53.832,69.308,75.856,% | Adaptive Acc: 67.076% | clf_exit: 0.580 0.325 0.095
Batch: 40 | Loss: 4.255 | Acc: 53.182,68.407,75.171,% | Adaptive Acc: 66.311% | clf_exit: 0.569 0.331 0.100
Batch: 60 | Loss: 4.267 | Acc: 52.946,68.571,74.667,% | Adaptive Acc: 66.048% | clf_exit: 0.571 0.330 0.099







Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=0 | Loss: 11.929 |  Acc: 6.192,9.292,15.858,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=0 | Loss: 11.104 |  Acc: 8.390,13.330,19.160,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=1 | Loss: 10.205 |  Acc: 12.008,17.972,27.730,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=1 | Loss: 9.793 |  Acc: 13.450,18.940,28.500,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=2 | Loss: 9.050 |  Acc: 17.198,23.560,36.102,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=2 | Loss: 8.703 |  Acc: 18.430,24.870,38.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=3 | Loss: 8.230 |  Acc: 21.486,28.194,42.772,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=3 | Loss: 8.244 |  Acc: 20.600,27.480,43.100,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=4 | Loss: 7.634 |  Acc: 24.442,32.082,47.538,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=4 | Loss: 7.820 |  Acc: 21.520,29.410,48.080,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=5 | Loss: 7.214 |  Acc: 26.680,35.112,51.100,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=5 | Loss: 7.486 |  Acc: 22.330,34.140,51.200,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=6 | Loss: 6.840 |  Acc: 28.312,38.550,54.362,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=6 | Loss: 7.136 |  Acc: 26.520,35.240,53.730,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=7 | Loss: 6.509 |  Acc: 30.052,41.204,56.912,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=7 | Loss: 7.005 |  Acc: 26.510,37.300,54.620,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=8 | Loss: 6.224 |  Acc: 31.366,43.690,59.924,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=8 | Loss: 6.900 |  Acc: 26.010,38.340,57.020,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=9 | Loss: 5.990 |  Acc: 32.516,46.184,61.934,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=9 | Loss: 6.409 |  Acc: 28.650,43.190,59.370,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=10 | Loss: 5.760 |  Acc: 33.476,47.768,63.936,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=10 | Loss: 6.459 |  Acc: 28.360,42.590,59.150,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=11 | Loss: 5.557 |  Acc: 34.914,49.780,66.086,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=11 | Loss: 6.295 |  Acc: 31.010,44.250,59.660,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=12 | Loss: 5.403 |  Acc: 35.548,51.332,67.496,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=12 | Loss: 5.951 |  Acc: 32.710,47.670,62.770,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=13 | Loss: 5.249 |  Acc: 36.148,52.572,68.900,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=13 | Loss: 6.072 |  Acc: 31.900,46.400,62.120,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=14 | Loss: 5.106 |  Acc: 36.952,53.972,70.138,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=14 | Loss: 5.877 |  Acc: 32.030,48.750,64.300,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=15 | Loss: 4.952 |  Acc: 37.814,55.400,71.658,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=15 | Loss: 5.758 |  Acc: 33.580,50.870,64.050,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=16 | Loss: 4.825 |  Acc: 38.602,56.606,73.194,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=16 | Loss: 5.741 |  Acc: 33.500,50.930,64.890,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=17 | Loss: 4.748 |  Acc: 38.842,57.258,74.064,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=17 | Loss: 5.628 |  Acc: 34.640,51.440,65.080,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=18 | Loss: 4.621 |  Acc: 39.530,58.424,75.394,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=18 | Loss: 5.639 |  Acc: 34.610,52.450,65.000,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=19 | Loss: 4.529 |  Acc: 39.832,59.196,76.430,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=19 | Loss: 5.945 |  Acc: 33.920,49.150,64.360,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=20 | Loss: 4.462 |  Acc: 40.166,60.040,76.998,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=20 | Loss: 5.523 |  Acc: 36.540,53.660,65.200,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=21 | Loss: 4.369 |  Acc: 41.132,60.758,77.996,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=21 | Loss: 5.473 |  Acc: 37.220,53.370,65.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=22 | Loss: 4.305 |  Acc: 41.364,61.590,79.060,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=22 | Loss: 5.632 |  Acc: 35.580,51.700,64.610,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=23 | Loss: 4.236 |  Acc: 41.670,62.008,79.434,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=23 | Loss: 5.292 |  Acc: 37.970,55.220,66.370,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=24 | Loss: 4.174 |  Acc: 42.198,62.512,80.360,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=24 | Loss: 5.426 |  Acc: 37.050,54.760,66.400,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=25 | Loss: 4.119 |  Acc: 42.406,63.198,80.458,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=25 | Loss: 5.490 |  Acc: 37.740,53.960,64.660,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=26 | Loss: 4.053 |  Acc: 43.214,63.632,81.572,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=26 | Loss: 5.331 |  Acc: 38.440,56.120,65.820,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=27 | Loss: 4.000 |  Acc: 43.222,64.100,82.194,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=27 | Loss: 5.403 |  Acc: 38.200,55.160,65.600,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=28 | Loss: 3.954 |  Acc: 43.552,64.674,82.378,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=28 | Loss: 5.418 |  Acc: 37.250,55.970,66.040,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=29 | Loss: 3.918 |  Acc: 43.882,64.826,82.896,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=29 | Loss: 5.531 |  Acc: 37.360,53.440,66.030,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=30 | Loss: 3.875 |  Acc: 44.170,65.182,83.284,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=30 | Loss: 5.386 |  Acc: 37.320,56.180,66.110,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=31 | Loss: 3.830 |  Acc: 44.418,65.838,83.832,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=31 | Loss: 5.361 |  Acc: 38.300,56.710,66.300,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=32 | Loss: 3.801 |  Acc: 44.656,65.864,84.224,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=32 | Loss: 5.402 |  Acc: 37.260,55.830,65.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=33 | Loss: 3.776 |  Acc: 44.626,66.258,84.450,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=33 | Loss: 5.328 |  Acc: 38.620,56.370,66.030,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=34 | Loss: 3.725 |  Acc: 45.000,66.902,84.820,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=34 | Loss: 5.282 |  Acc: 39.060,57.310,66.830,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=35 | Loss: 3.690 |  Acc: 44.986,67.118,85.102,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=35 | Loss: 5.202 |  Acc: 40.860,57.610,65.530,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=36 | Loss: 3.648 |  Acc: 45.496,67.344,85.580,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=36 | Loss: 5.243 |  Acc: 40.320,57.200,66.110,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=37 | Loss: 3.659 |  Acc: 45.622,67.280,85.598,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=37 | Loss: 5.213 |  Acc: 39.080,59.180,66.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=38 | Loss: 3.615 |  Acc: 45.420,67.880,85.896,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=38 | Loss: 5.057 |  Acc: 40.810,59.340,67.800,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=39 | Loss: 3.614 |  Acc: 45.774,68.082,85.892,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=39 | Loss: 5.486 |  Acc: 37.780,55.490,66.320,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=40 | Loss: 3.575 |  Acc: 45.986,68.216,86.578,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=40 | Loss: 5.478 |  Acc: 37.270,56.630,66.420,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=41 | Loss: 3.549 |  Acc: 46.608,68.732,86.608,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=41 | Loss: 5.350 |  Acc: 38.860,57.340,66.330,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=42 | Loss: 3.563 |  Acc: 46.170,68.480,86.426,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=42 | Loss: 5.379 |  Acc: 38.790,57.120,66.670,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=43 | Loss: 3.501 |  Acc: 46.674,69.074,87.160,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=43 | Loss: 5.267 |  Acc: 40.660,57.790,66.450,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=44 | Loss: 3.497 |  Acc: 46.636,69.154,87.066,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=44 | Loss: 5.503 |  Acc: 37.020,55.810,66.140,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=45 | Loss: 3.475 |  Acc: 46.790,69.146,87.032,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=45 | Loss: 5.295 |  Acc: 40.320,57.530,66.680,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=46 | Loss: 3.459 |  Acc: 47.086,69.348,87.282,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=46 | Loss: 5.405 |  Acc: 39.730,55.590,66.010,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=47 | Loss: 3.457 |  Acc: 46.926,69.316,87.188,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=47 | Loss: 5.351 |  Acc: 38.930,58.350,65.940,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=48 | Loss: 3.422 |  Acc: 47.082,69.902,87.910,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=48 | Loss: 5.182 |  Acc: 40.220,57.940,67.000,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=49 | Loss: 3.410 |  Acc: 47.200,70.002,88.030,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=49 | Loss: 5.312 |  Acc: 41.720,56.910,66.440,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=50 | Loss: 3.409 |  Acc: 47.318,69.970,87.704,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=50 | Loss: 5.257 |  Acc: 39.790,58.570,66.240,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=51 | Loss: 3.378 |  Acc: 47.348,70.528,88.132,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=51 | Loss: 5.187 |  Acc: 40.570,57.280,65.730,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=52 | Loss: 3.371 |  Acc: 47.534,70.424,87.768,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=52 | Loss: 5.213 |  Acc: 41.380,57.560,67.120,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=53 | Loss: 3.349 |  Acc: 47.750,70.576,88.260,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=53 | Loss: 5.216 |  Acc: 40.500,58.720,67.000,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=54 | Loss: 3.330 |  Acc: 47.982,71.010,88.308,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=54 | Loss: 5.292 |  Acc: 41.800,56.130,66.300,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=55 | Loss: 3.336 |  Acc: 47.984,70.800,88.134,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=55 | Loss: 5.152 |  Acc: 41.940,58.680,66.310,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=56 | Loss: 3.315 |  Acc: 48.286,70.998,88.552,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=56 | Loss: 5.399 |  Acc: 39.070,57.850,64.980,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=57 | Loss: 3.290 |  Acc: 48.300,71.544,88.742,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=57 | Loss: 5.146 |  Acc: 39.470,60.020,68.400,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=58 | Loss: 3.282 |  Acc: 48.180,71.554,88.922,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=58 | Loss: 5.205 |  Acc: 40.990,58.180,66.620,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=59 | Loss: 3.304 |  Acc: 48.186,71.230,88.390,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=59 | Loss: 5.276 |  Acc: 40.130,59.030,67.220,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=60 | Loss: 3.280 |  Acc: 48.324,71.558,88.606,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=60 | Loss: 5.094 |  Acc: 40.800,59.390,67.510,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=61 | Loss: 3.258 |  Acc: 48.654,71.878,88.972,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=61 | Loss: 5.189 |  Acc: 39.360,59.330,67.110,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=62 | Loss: 3.264 |  Acc: 48.560,71.762,88.558,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=62 | Loss: 5.475 |  Acc: 38.610,58.770,66.160,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=63 | Loss: 3.243 |  Acc: 48.610,71.884,88.856,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=63 | Loss: 5.203 |  Acc: 40.610,59.450,67.400,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=64 | Loss: 3.230 |  Acc: 48.838,72.200,89.026,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=64 | Loss: 5.023 |  Acc: 42.310,60.090,67.480,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=65 | Loss: 3.218 |  Acc: 48.938,72.130,89.120,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=65 | Loss: 5.331 |  Acc: 40.570,57.770,65.540,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=66 | Loss: 3.222 |  Acc: 48.736,72.264,89.100,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=66 | Loss: 5.327 |  Acc: 40.880,57.370,66.460,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=67 | Loss: 3.216 |  Acc: 48.588,71.916,89.394,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=67 | Loss: 5.278 |  Acc: 40.730,59.090,65.130,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=68 | Loss: 3.215 |  Acc: 48.866,72.460,89.072,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=68 | Loss: 5.193 |  Acc: 42.350,59.170,66.140,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=69 | Loss: 3.188 |  Acc: 49.260,72.742,89.538,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=69 | Loss: 5.330 |  Acc: 41.530,58.430,65.220,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=70 | Loss: 3.190 |  Acc: 49.236,72.512,89.160,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=70 | Loss: 5.271 |  Acc: 39.340,58.810,66.740,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=71 | Loss: 3.181 |  Acc: 49.284,72.450,89.328,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=71 | Loss: 5.178 |  Acc: 41.270,59.410,67.270,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=72 | Loss: 3.160 |  Acc: 49.472,72.976,89.538,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=72 | Loss: 5.366 |  Acc: 41.100,58.720,65.310,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=73 | Loss: 3.174 |  Acc: 49.410,72.806,89.276,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=73 | Loss: 5.249 |  Acc: 42.030,58.570,64.470,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=74 | Loss: 3.182 |  Acc: 49.068,72.670,89.170,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=74 | Loss: 5.091 |  Acc: 42.070,60.060,67.360,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=75 | Loss: 3.151 |  Acc: 49.628,73.090,89.654,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=75 | Loss: 5.192 |  Acc: 41.790,58.690,67.470,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=76 | Loss: 3.149 |  Acc: 49.284,72.794,89.390,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=76 | Loss: 5.281 |  Acc: 40.790,58.560,66.800,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=77 | Loss: 3.148 |  Acc: 49.676,72.920,89.466,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=77 | Loss: 5.392 |  Acc: 38.990,57.730,65.410,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=78 | Loss: 3.141 |  Acc: 49.620,73.202,89.632,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=78 | Loss: 5.292 |  Acc: 42.220,58.480,65.350,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=79 | Loss: 3.122 |  Acc: 49.704,73.432,89.634,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=79 | Loss: 5.385 |  Acc: 41.190,57.890,65.430,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=80 | Loss: 3.118 |  Acc: 49.720,73.358,89.768,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=80 | Loss: 5.058 |  Acc: 43.740,59.180,66.890,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=81 | Loss: 3.112 |  Acc: 49.764,73.234,89.662,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=81 | Loss: 5.094 |  Acc: 42.330,61.010,67.440,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=82 | Loss: 3.122 |  Acc: 50.040,73.482,89.568,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=82 | Loss: 5.273 |  Acc: 40.700,59.400,66.160,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=83 | Loss: 3.097 |  Acc: 49.894,73.750,89.912,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=83 | Loss: 5.227 |  Acc: 41.580,59.790,66.810,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=84 | Loss: 3.100 |  Acc: 49.984,73.702,89.834,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=84 | Loss: 5.127 |  Acc: 42.810,60.270,66.580,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=85 | Loss: 3.093 |  Acc: 50.066,73.606,89.600,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=85 | Loss: 5.168 |  Acc: 42.670,59.090,66.680,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=86 | Loss: 3.096 |  Acc: 50.090,73.670,89.634,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=86 | Loss: 5.206 |  Acc: 43.210,58.140,66.260,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=87 | Loss: 3.078 |  Acc: 50.284,74.004,90.054,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=87 | Loss: 5.216 |  Acc: 42.470,57.690,66.280,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=88 | Loss: 3.092 |  Acc: 49.936,73.452,89.708,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=88 | Loss: 5.413 |  Acc: 40.050,59.120,65.270,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=89 | Loss: 3.060 |  Acc: 50.406,73.994,90.220,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=89 | Loss: 5.105 |  Acc: 42.250,60.770,67.060,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=90 | Loss: 3.084 |  Acc: 50.052,73.570,89.640,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=90 | Loss: 5.130 |  Acc: 41.930,61.370,67.600,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=91 | Loss: 3.055 |  Acc: 50.652,74.116,89.950,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=91 | Loss: 5.148 |  Acc: 43.600,59.940,66.300,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=92 | Loss: 3.055 |  Acc: 50.422,74.052,89.954,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=92 | Loss: 5.329 |  Acc: 41.340,58.350,66.180,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=93 | Loss: 3.064 |  Acc: 50.502,73.698,89.948,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=93 | Loss: 5.155 |  Acc: 43.400,60.010,66.680,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=94 | Loss: 3.029 |  Acc: 50.456,74.342,90.346,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=94 | Loss: 5.251 |  Acc: 41.500,58.470,66.900,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=95 | Loss: 3.043 |  Acc: 50.544,74.080,90.070,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=95 | Loss: 5.136 |  Acc: 43.270,59.080,66.160,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=96 | Loss: 3.039 |  Acc: 50.414,74.358,89.798,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=96 | Loss: 5.088 |  Acc: 42.550,59.970,66.310,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=97 | Loss: 3.038 |  Acc: 50.820,74.166,89.920,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=97 | Loss: 5.096 |  Acc: 42.540,60.250,67.360,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=98 | Loss: 3.018 |  Acc: 50.698,74.408,90.430,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=98 | Loss: 5.262 |  Acc: 40.620,59.150,66.610,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=99 | Loss: 3.029 |  Acc: 50.566,74.302,89.988,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=99 | Loss: 5.525 |  Acc: 40.190,57.880,65.370,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=100 | Loss: 3.030 |  Acc: 50.622,74.218,90.104,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=100 | Loss: 5.204 |  Acc: 41.910,60.140,66.370,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=101 | Loss: 3.010 |  Acc: 50.718,74.726,90.194,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=101 | Loss: 5.387 |  Acc: 41.860,57.820,65.270,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=102 | Loss: 3.015 |  Acc: 50.996,74.470,90.180,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=102 | Loss: 5.604 |  Acc: 39.670,56.020,65.130,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=103 | Loss: 3.012 |  Acc: 50.704,74.540,90.388,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=103 | Loss: 5.125 |  Acc: 44.410,59.680,66.520,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=104 | Loss: 3.009 |  Acc: 51.052,74.670,90.026,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=104 | Loss: 5.260 |  Acc: 42.060,58.380,65.690,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=105 | Loss: 2.992 |  Acc: 51.226,74.776,90.460,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=105 | Loss: 5.327 |  Acc: 42.630,58.280,65.770,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=106 | Loss: 3.002 |  Acc: 50.928,74.802,90.078,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=106 | Loss: 5.269 |  Acc: 41.690,59.120,66.350,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=107 | Loss: 3.002 |  Acc: 50.976,74.920,90.198,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=107 | Loss: 5.141 |  Acc: 44.110,58.850,66.930,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=108 | Loss: 2.975 |  Acc: 51.186,74.960,90.370,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=108 | Loss: 5.104 |  Acc: 41.210,61.090,67.240,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=109 | Loss: 2.985 |  Acc: 51.362,74.958,90.424,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=109 | Loss: 5.156 |  Acc: 41.130,60.290,66.480,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=110 | Loss: 2.979 |  Acc: 51.324,74.966,90.516,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=110 | Loss: 5.134 |  Acc: 43.290,59.910,67.160,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=111 | Loss: 2.980 |  Acc: 51.198,75.074,90.290,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=111 | Loss: 5.140 |  Acc: 42.570,61.290,67.500,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=112 | Loss: 2.946 |  Acc: 51.732,75.550,90.774,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=112 | Loss: 5.273 |  Acc: 40.860,58.310,67.290,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=113 | Loss: 2.967 |  Acc: 51.064,75.140,90.502,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=113 | Loss: 5.337 |  Acc: 40.440,58.430,66.550,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=114 | Loss: 2.984 |  Acc: 51.260,74.846,90.182,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=114 | Loss: 5.238 |  Acc: 41.470,58.880,66.390,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=115 | Loss: 2.958 |  Acc: 51.244,74.932,90.560,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=115 | Loss: 5.188 |  Acc: 43.230,59.060,66.580,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=116 | Loss: 2.975 |  Acc: 51.596,75.004,90.362,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=116 | Loss: 5.043 |  Acc: 44.420,59.640,67.190,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=117 | Loss: 2.949 |  Acc: 51.478,75.242,90.476,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=117 | Loss: 4.978 |  Acc: 44.240,61.540,68.090,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=118 | Loss: 2.957 |  Acc: 51.496,75.164,90.416,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=118 | Loss: 4.957 |  Acc: 43.660,61.310,68.190,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=119 | Loss: 2.946 |  Acc: 51.662,75.568,90.462,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=119 | Loss: 5.215 |  Acc: 42.390,59.140,66.600,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=120 | Loss: 2.953 |  Acc: 51.476,75.220,90.512,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=120 | Loss: 5.344 |  Acc: 44.160,58.140,64.500,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=121 | Loss: 2.931 |  Acc: 51.602,75.462,90.766,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=121 | Loss: 5.287 |  Acc: 42.940,58.840,66.100,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=122 | Loss: 2.939 |  Acc: 51.580,75.500,90.576,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=122 | Loss: 5.127 |  Acc: 44.160,58.440,66.930,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=123 | Loss: 2.934 |  Acc: 51.686,75.554,90.846,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=123 | Loss: 5.131 |  Acc: 43.080,61.440,67.820,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=124 | Loss: 2.929 |  Acc: 51.624,75.450,90.534,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=124 | Loss: 5.133 |  Acc: 42.970,60.310,67.080,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=125 | Loss: 2.933 |  Acc: 51.736,75.518,90.554,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=125 | Loss: 5.275 |  Acc: 43.010,58.130,66.670,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=126 | Loss: 2.918 |  Acc: 51.906,75.750,90.516,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=126 | Loss: 5.075 |  Acc: 43.800,60.000,67.390,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=127 | Loss: 2.922 |  Acc: 51.910,75.284,90.924,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=127 | Loss: 5.176 |  Acc: 43.000,60.910,67.110,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=128 | Loss: 2.911 |  Acc: 51.832,75.886,90.520,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=128 | Loss: 4.992 |  Acc: 43.960,61.170,67.610,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=129 | Loss: 2.931 |  Acc: 51.518,75.494,90.616,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=129 | Loss: 5.089 |  Acc: 44.350,61.100,67.020,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=130 | Loss: 2.907 |  Acc: 51.600,75.902,90.960,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=130 | Loss: 5.077 |  Acc: 43.480,60.330,67.210,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=131 | Loss: 2.907 |  Acc: 51.886,75.772,90.844,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=131 | Loss: 5.036 |  Acc: 44.470,60.750,67.410,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=132 | Loss: 2.919 |  Acc: 51.936,75.872,90.698,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=132 | Loss: 5.162 |  Acc: 43.800,60.330,66.520,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=133 | Loss: 2.916 |  Acc: 51.884,75.680,90.746,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=133 | Loss: 4.916 |  Acc: 44.670,61.900,67.440,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=134 | Loss: 2.917 |  Acc: 51.754,75.592,90.610,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=134 | Loss: 5.941 |  Acc: 38.000,55.190,64.220,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=135 | Loss: 2.911 |  Acc: 51.868,75.770,90.654,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=135 | Loss: 5.266 |  Acc: 41.970,59.150,66.550,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=136 | Loss: 2.896 |  Acc: 52.130,75.968,90.718,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=136 | Loss: 5.191 |  Acc: 43.450,58.610,66.440,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=137 | Loss: 2.899 |  Acc: 51.756,75.794,90.828,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=137 | Loss: 5.051 |  Acc: 44.150,60.630,67.160,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=138 | Loss: 2.913 |  Acc: 52.088,75.732,90.566,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=138 | Loss: 5.208 |  Acc: 43.530,59.810,65.880,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=139 | Loss: 2.913 |  Acc: 52.056,75.758,90.472,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=139 | Loss: 5.017 |  Acc: 44.320,61.540,67.110,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=140 | Loss: 2.880 |  Acc: 51.962,76.134,90.916,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=140 | Loss: 5.376 |  Acc: 38.870,60.150,66.550,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=141 | Loss: 2.908 |  Acc: 52.014,75.758,90.766,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=141 | Loss: 4.980 |  Acc: 45.210,61.210,67.670,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=142 | Loss: 2.880 |  Acc: 52.212,76.126,91.038,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=142 | Loss: 5.258 |  Acc: 44.510,58.630,66.360,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=143 | Loss: 2.898 |  Acc: 51.958,75.978,90.836,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=143 | Loss: 5.145 |  Acc: 42.610,61.180,66.850,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=144 | Loss: 2.861 |  Acc: 52.534,76.490,91.072,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=144 | Loss: 5.038 |  Acc: 44.680,60.470,67.400,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=145 | Loss: 2.892 |  Acc: 52.134,76.006,90.752,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=145 | Loss: 5.273 |  Acc: 42.820,59.960,66.660,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=146 | Loss: 2.885 |  Acc: 52.414,75.804,90.846,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=146 | Loss: 5.361 |  Acc: 41.150,59.860,66.040,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=147 | Loss: 2.884 |  Acc: 52.208,76.004,90.778,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=147 | Loss: 5.126 |  Acc: 43.920,60.600,66.350,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=148 | Loss: 2.877 |  Acc: 52.220,76.044,90.896,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=148 | Loss: 5.239 |  Acc: 43.400,59.260,67.330,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=149 | Loss: 2.887 |  Acc: 52.136,75.954,90.718,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=149 | Loss: 5.385 |  Acc: 41.510,59.660,66.140,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=150 | Loss: 2.282 |  Acc: 56.774,84.358,96.114,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=150 | Loss: 3.992 |  Acc: 51.800,68.990,73.770,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=151 | Loss: 2.084 |  Acc: 58.528,87.306,98.054,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=151 | Loss: 3.961 |  Acc: 52.140,69.840,74.510,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=152 | Loss: 2.019 |  Acc: 58.712,88.144,98.534,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=152 | Loss: 3.953 |  Acc: 52.170,69.800,74.630,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=153 | Loss: 1.979 |  Acc: 58.802,88.824,98.722,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=153 | Loss: 3.957 |  Acc: 52.280,69.720,74.620,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=154 | Loss: 1.945 |  Acc: 59.260,89.230,98.884,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=154 | Loss: 3.973 |  Acc: 52.460,69.720,74.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=155 | Loss: 1.908 |  Acc: 59.534,89.808,99.006,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=155 | Loss: 3.979 |  Acc: 52.380,69.620,74.630,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=156 | Loss: 1.888 |  Acc: 59.702,89.960,99.158,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=156 | Loss: 3.970 |  Acc: 52.560,70.160,75.070,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=157 | Loss: 1.864 |  Acc: 59.994,90.518,99.200,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=157 | Loss: 3.973 |  Acc: 52.020,69.920,74.880,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=158 | Loss: 1.854 |  Acc: 59.794,90.478,99.214,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=158 | Loss: 3.978 |  Acc: 52.560,70.020,74.860,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=159 | Loss: 1.832 |  Acc: 60.092,90.954,99.278,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=159 | Loss: 3.968 |  Acc: 52.760,70.140,75.140,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=160 | Loss: 1.818 |  Acc: 60.108,91.094,99.270,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=160 | Loss: 3.990 |  Acc: 52.680,69.710,75.060,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=161 | Loss: 1.805 |  Acc: 60.474,91.184,99.324,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=161 | Loss: 4.013 |  Acc: 52.650,69.510,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=162 | Loss: 1.797 |  Acc: 60.252,91.318,99.412,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=162 | Loss: 4.006 |  Acc: 52.250,69.910,75.030,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=163 | Loss: 1.771 |  Acc: 60.588,91.820,99.406,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=163 | Loss: 4.017 |  Acc: 52.100,69.620,74.810,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=164 | Loss: 1.757 |  Acc: 60.792,92.088,99.444,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=164 | Loss: 4.007 |  Acc: 52.610,69.910,75.130,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=165 | Loss: 1.748 |  Acc: 60.902,92.100,99.484,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=165 | Loss: 4.060 |  Acc: 52.250,69.830,74.670,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=166 | Loss: 1.747 |  Acc: 60.706,92.228,99.480,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=166 | Loss: 4.059 |  Acc: 52.390,69.910,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=167 | Loss: 1.731 |  Acc: 60.988,92.324,99.504,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=167 | Loss: 4.061 |  Acc: 52.400,69.780,74.970,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=168 | Loss: 1.725 |  Acc: 61.078,92.382,99.498,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=168 | Loss: 4.071 |  Acc: 52.520,69.420,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=169 | Loss: 1.714 |  Acc: 60.908,92.602,99.564,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=169 | Loss: 4.082 |  Acc: 52.550,69.550,75.100,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=170 | Loss: 1.700 |  Acc: 61.214,92.810,99.582,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=170 | Loss: 4.061 |  Acc: 52.510,69.600,75.040,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=171 | Loss: 1.698 |  Acc: 61.344,92.838,99.574,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=171 | Loss: 4.066 |  Acc: 52.390,69.600,74.930,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=172 | Loss: 1.690 |  Acc: 61.246,93.014,99.538,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=172 | Loss: 4.086 |  Acc: 52.220,69.140,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=173 | Loss: 1.683 |  Acc: 61.332,92.996,99.622,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=173 | Loss: 4.086 |  Acc: 52.650,69.560,74.920,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=174 | Loss: 1.683 |  Acc: 61.340,93.076,99.528,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=174 | Loss: 4.135 |  Acc: 52.520,69.170,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=175 | Loss: 1.670 |  Acc: 61.520,93.280,99.534,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=175 | Loss: 4.131 |  Acc: 52.280,69.220,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=176 | Loss: 1.665 |  Acc: 61.536,93.464,99.566,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=176 | Loss: 4.108 |  Acc: 52.520,69.360,74.510,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=177 | Loss: 1.661 |  Acc: 61.634,93.402,99.548,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=177 | Loss: 4.115 |  Acc: 52.410,69.280,74.840,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=178 | Loss: 1.653 |  Acc: 61.700,93.526,99.624,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=178 | Loss: 4.126 |  Acc: 52.790,69.140,75.000,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=179 | Loss: 1.648 |  Acc: 61.770,93.698,99.560,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=179 | Loss: 4.102 |  Acc: 52.490,69.250,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=180 | Loss: 1.649 |  Acc: 61.560,93.578,99.546,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=180 | Loss: 4.134 |  Acc: 52.480,69.230,74.610,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=181 | Loss: 1.634 |  Acc: 61.866,93.746,99.592,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=181 | Loss: 4.116 |  Acc: 52.860,69.430,74.950,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=182 | Loss: 1.623 |  Acc: 62.124,94.064,99.630,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=182 | Loss: 4.155 |  Acc: 52.560,69.010,74.480,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=183 | Loss: 1.628 |  Acc: 61.958,93.948,99.596,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=183 | Loss: 4.152 |  Acc: 52.500,69.140,74.910,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=184 | Loss: 1.617 |  Acc: 61.942,93.914,99.640,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=184 | Loss: 4.137 |  Acc: 52.400,69.340,74.920,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=185 | Loss: 1.613 |  Acc: 61.870,93.994,99.606,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=185 | Loss: 4.172 |  Acc: 52.850,69.160,74.360,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=186 | Loss: 1.608 |  Acc: 62.174,94.138,99.598,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=186 | Loss: 4.135 |  Acc: 52.750,69.120,75.060,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=187 | Loss: 1.600 |  Acc: 62.428,94.258,99.564,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=187 | Loss: 4.193 |  Acc: 52.000,68.610,74.450,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=188 | Loss: 1.604 |  Acc: 62.258,94.146,99.618,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=188 | Loss: 4.169 |  Acc: 52.860,69.150,74.850,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=189 | Loss: 1.601 |  Acc: 62.362,94.204,99.614,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=189 | Loss: 4.171 |  Acc: 52.530,68.840,74.910,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=190 | Loss: 1.586 |  Acc: 62.252,94.524,99.636,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=190 | Loss: 4.175 |  Acc: 52.420,69.020,75.100,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=191 | Loss: 1.591 |  Acc: 62.346,94.292,99.636,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=191 | Loss: 4.244 |  Acc: 51.950,68.670,74.830,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=192 | Loss: 1.595 |  Acc: 62.300,94.156,99.618,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=192 | Loss: 4.227 |  Acc: 52.340,68.660,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=193 | Loss: 1.586 |  Acc: 62.334,94.466,99.596,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=193 | Loss: 4.206 |  Acc: 52.370,68.820,74.370,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=194 | Loss: 1.573 |  Acc: 62.226,94.572,99.646,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=194 | Loss: 4.215 |  Acc: 52.470,68.400,74.790,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=195 | Loss: 1.579 |  Acc: 62.550,94.476,99.614,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=195 | Loss: 4.198 |  Acc: 52.880,69.000,74.620,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=196 | Loss: 1.567 |  Acc: 62.648,94.634,99.640,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=196 | Loss: 4.204 |  Acc: 52.710,68.810,74.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=197 | Loss: 1.567 |  Acc: 62.546,94.776,99.608,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=197 | Loss: 4.262 |  Acc: 52.240,68.210,74.650,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=198 | Loss: 1.560 |  Acc: 62.696,94.808,99.648,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=198 | Loss: 4.280 |  Acc: 52.030,68.520,74.690,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=199 | Loss: 1.562 |  Acc: 62.732,94.770,99.588,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=199 | Loss: 4.253 |  Acc: 51.780,68.670,74.560,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=200 | Loss: 1.559 |  Acc: 62.672,94.832,99.658,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=200 | Loss: 4.264 |  Acc: 52.550,68.730,74.440,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=201 | Loss: 1.550 |  Acc: 62.704,94.810,99.632,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=201 | Loss: 4.247 |  Acc: 52.430,68.400,74.410,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=202 | Loss: 1.551 |  Acc: 62.780,94.846,99.642,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=202 | Loss: 4.240 |  Acc: 52.400,69.040,74.480,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=203 | Loss: 1.545 |  Acc: 63.132,94.832,99.632,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=203 | Loss: 4.292 |  Acc: 52.280,68.200,74.450,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=204 | Loss: 1.550 |  Acc: 62.848,94.946,99.626,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=204 | Loss: 4.233 |  Acc: 52.420,68.810,74.680,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=205 | Loss: 1.550 |  Acc: 63.094,94.872,99.600,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=205 | Loss: 4.246 |  Acc: 52.550,68.910,74.370,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=206 | Loss: 1.538 |  Acc: 63.186,95.054,99.712,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=206 | Loss: 4.268 |  Acc: 52.210,68.630,74.550,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=207 | Loss: 1.542 |  Acc: 62.940,94.912,99.654,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=207 | Loss: 4.286 |  Acc: 52.030,68.380,74.510,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=208 | Loss: 1.532 |  Acc: 63.142,95.024,99.650,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=208 | Loss: 4.297 |  Acc: 52.120,68.430,74.530,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=209 | Loss: 1.531 |  Acc: 63.124,95.090,99.658,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=209 | Loss: 4.345 |  Acc: 52.100,68.430,74.380,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=210 | Loss: 1.529 |  Acc: 63.096,95.024,99.654,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=210 | Loss: 4.290 |  Acc: 51.810,68.250,74.770,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=211 | Loss: 1.532 |  Acc: 63.216,95.002,99.640,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=211 | Loss: 4.330 |  Acc: 52.000,68.140,74.330,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=212 | Loss: 1.526 |  Acc: 63.186,95.098,99.642,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=212 | Loss: 4.313 |  Acc: 52.110,68.510,74.680,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=213 | Loss: 1.517 |  Acc: 63.194,95.244,99.654,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=213 | Loss: 4.297 |  Acc: 52.270,68.090,74.590,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=214 | Loss: 1.525 |  Acc: 63.262,95.170,99.626,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=214 | Loss: 4.298 |  Acc: 52.220,68.010,74.510,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=215 | Loss: 1.520 |  Acc: 63.110,95.242,99.650,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=215 | Loss: 4.330 |  Acc: 51.810,68.460,74.260,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=216 | Loss: 1.512 |  Acc: 63.402,95.248,99.644,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=216 | Loss: 4.310 |  Acc: 51.730,68.470,74.190,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=217 | Loss: 1.515 |  Acc: 63.316,95.230,99.672,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=217 | Loss: 4.318 |  Acc: 52.030,68.370,74.600,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=218 | Loss: 1.512 |  Acc: 63.128,95.392,99.660,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=218 | Loss: 4.290 |  Acc: 52.150,68.190,74.400,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=219 | Loss: 1.521 |  Acc: 63.578,95.100,99.628,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=219 | Loss: 4.311 |  Acc: 51.760,68.310,74.670,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=220 | Loss: 1.514 |  Acc: 63.310,95.150,99.658,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=220 | Loss: 4.333 |  Acc: 52.110,67.940,74.130,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=221 | Loss: 1.516 |  Acc: 63.402,95.050,99.682,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=221 | Loss: 4.347 |  Acc: 51.640,68.500,74.530,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=222 | Loss: 1.499 |  Acc: 63.484,95.428,99.658,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=222 | Loss: 4.408 |  Acc: 51.630,67.610,74.170,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=223 | Loss: 1.506 |  Acc: 63.634,95.248,99.556,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=223 | Loss: 4.369 |  Acc: 52.100,67.960,74.660,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=224 | Loss: 1.494 |  Acc: 63.640,95.286,99.668,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=224 | Loss: 4.348 |  Acc: 52.280,67.900,74.280,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=225 | Loss: 1.411 |  Acc: 64.994,96.712,99.738,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=225 | Loss: 4.218 |  Acc: 53.170,69.060,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=226 | Loss: 1.390 |  Acc: 65.296,97.274,99.780,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=226 | Loss: 4.232 |  Acc: 53.280,69.070,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=227 | Loss: 1.381 |  Acc: 65.600,97.288,99.754,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=227 | Loss: 4.212 |  Acc: 53.460,69.370,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=228 | Loss: 1.373 |  Acc: 65.678,97.550,99.790,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=228 | Loss: 4.209 |  Acc: 53.370,69.110,74.980,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=229 | Loss: 1.374 |  Acc: 65.580,97.562,99.750,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=229 | Loss: 4.217 |  Acc: 53.140,69.120,74.670,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=230 | Loss: 1.365 |  Acc: 65.660,97.534,99.776,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=230 | Loss: 4.209 |  Acc: 53.270,69.130,74.800,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=231 | Loss: 1.359 |  Acc: 65.902,97.542,99.806,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=231 | Loss: 4.206 |  Acc: 53.400,69.260,74.930,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=232 | Loss: 1.362 |  Acc: 65.886,97.624,99.788,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=232 | Loss: 4.209 |  Acc: 53.310,69.050,74.800,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=233 | Loss: 1.358 |  Acc: 65.880,97.788,99.794,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=233 | Loss: 4.216 |  Acc: 53.330,69.260,74.880,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=234 | Loss: 1.363 |  Acc: 65.710,97.666,99.746,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=234 | Loss: 4.217 |  Acc: 53.410,69.300,75.000,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=235 | Loss: 1.357 |  Acc: 65.826,97.866,99.796,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=235 | Loss: 4.203 |  Acc: 53.710,69.330,74.800,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=236 | Loss: 1.356 |  Acc: 65.948,97.752,99.792,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=236 | Loss: 4.220 |  Acc: 53.240,69.170,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=237 | Loss: 1.355 |  Acc: 65.894,97.732,99.764,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=237 | Loss: 4.218 |  Acc: 53.310,69.240,74.920,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=238 | Loss: 1.349 |  Acc: 65.920,97.800,99.782,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=238 | Loss: 4.213 |  Acc: 53.280,69.240,74.750,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=239 | Loss: 1.354 |  Acc: 65.978,97.866,99.776,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=239 | Loss: 4.228 |  Acc: 53.400,69.180,75.070,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=240 | Loss: 1.350 |  Acc: 66.040,97.866,99.802,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=240 | Loss: 4.221 |  Acc: 53.440,69.060,74.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=241 | Loss: 1.346 |  Acc: 65.958,97.954,99.806,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=241 | Loss: 4.221 |  Acc: 53.270,69.340,74.860,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=242 | Loss: 1.349 |  Acc: 65.916,97.912,99.778,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=242 | Loss: 4.220 |  Acc: 53.370,68.990,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=243 | Loss: 1.347 |  Acc: 66.018,97.920,99.836,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=243 | Loss: 4.219 |  Acc: 53.550,69.200,74.930,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=244 | Loss: 1.347 |  Acc: 65.828,97.910,99.756,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=244 | Loss: 4.234 |  Acc: 53.460,69.140,74.920,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=245 | Loss: 1.343 |  Acc: 66.044,97.958,99.774,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=245 | Loss: 4.227 |  Acc: 53.480,69.310,74.890,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=246 | Loss: 1.345 |  Acc: 65.944,97.926,99.784,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=246 | Loss: 4.239 |  Acc: 53.430,69.280,74.730,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=247 | Loss: 1.343 |  Acc: 65.814,98.020,99.794,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=247 | Loss: 4.232 |  Acc: 53.320,69.060,74.540,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=248 | Loss: 1.345 |  Acc: 65.870,97.898,99.796,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=248 | Loss: 4.232 |  Acc: 53.080,68.980,74.850,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=249 | Loss: 1.342 |  Acc: 66.024,97.890,99.816,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=249 | Loss: 4.238 |  Acc: 53.320,69.110,74.660,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=250 | Loss: 1.340 |  Acc: 66.262,98.032,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=250 | Loss: 4.232 |  Acc: 53.110,69.100,74.850,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=251 | Loss: 1.340 |  Acc: 66.110,98.092,99.796,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=251 | Loss: 4.243 |  Acc: 53.460,69.060,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=252 | Loss: 1.339 |  Acc: 66.118,97.922,99.820,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=252 | Loss: 4.246 |  Acc: 53.350,68.900,74.850,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=253 | Loss: 1.336 |  Acc: 65.960,97.990,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=253 | Loss: 4.235 |  Acc: 53.260,69.230,74.810,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=254 | Loss: 1.334 |  Acc: 66.184,98.038,99.818,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=254 | Loss: 4.235 |  Acc: 53.350,68.900,74.940,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=255 | Loss: 1.333 |  Acc: 66.196,98.052,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=255 | Loss: 4.243 |  Acc: 53.370,69.070,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=256 | Loss: 1.334 |  Acc: 66.244,98.046,99.810,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=256 | Loss: 4.249 |  Acc: 53.390,68.950,74.820,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=257 | Loss: 1.338 |  Acc: 66.204,97.898,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=257 | Loss: 4.239 |  Acc: 53.520,69.150,74.810,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=258 | Loss: 1.331 |  Acc: 66.248,98.150,99.828,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=258 | Loss: 4.242 |  Acc: 53.390,69.150,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=259 | Loss: 1.332 |  Acc: 66.226,98.004,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=259 | Loss: 4.240 |  Acc: 53.220,68.970,74.740,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=260 | Loss: 1.332 |  Acc: 66.136,98.042,99.796,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=260 | Loss: 4.251 |  Acc: 53.390,68.960,75.060,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=261 | Loss: 1.330 |  Acc: 66.298,98.000,99.810,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=261 | Loss: 4.242 |  Acc: 53.420,68.860,75.040,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=262 | Loss: 1.325 |  Acc: 66.268,98.182,99.818,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=262 | Loss: 4.245 |  Acc: 53.460,69.060,74.840,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=263 | Loss: 1.317 |  Acc: 66.422,98.204,99.852,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=263 | Loss: 4.244 |  Acc: 53.320,69.040,74.860,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=264 | Loss: 1.314 |  Acc: 66.450,98.222,99.822,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=264 | Loss: 4.230 |  Acc: 53.500,68.780,74.690,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=265 | Loss: 1.318 |  Acc: 66.342,98.258,99.832,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=265 | Loss: 4.232 |  Acc: 53.330,69.050,74.790,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=266 | Loss: 1.323 |  Acc: 66.292,98.204,99.790,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=266 | Loss: 4.241 |  Acc: 53.430,68.920,74.930,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=267 | Loss: 1.314 |  Acc: 66.484,98.280,99.834,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=267 | Loss: 4.241 |  Acc: 53.460,68.960,74.690,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=268 | Loss: 1.317 |  Acc: 66.734,98.298,99.818,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=268 | Loss: 4.235 |  Acc: 53.610,68.960,74.980,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=269 | Loss: 1.316 |  Acc: 66.498,98.182,99.794,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=269 | Loss: 4.244 |  Acc: 53.310,68.950,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=270 | Loss: 1.317 |  Acc: 66.494,98.216,99.810,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=270 | Loss: 4.241 |  Acc: 53.170,69.150,74.840,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=271 | Loss: 1.323 |  Acc: 66.538,98.144,99.790,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=271 | Loss: 4.241 |  Acc: 53.370,68.920,75.010,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=272 | Loss: 1.314 |  Acc: 66.324,98.162,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=272 | Loss: 4.241 |  Acc: 53.440,68.990,74.840,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=273 | Loss: 1.320 |  Acc: 66.260,98.204,99.828,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=273 | Loss: 4.244 |  Acc: 53.370,69.000,74.890,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=274 | Loss: 1.318 |  Acc: 66.466,98.242,99.820,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=274 | Loss: 4.249 |  Acc: 53.340,69.070,74.770,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=275 | Loss: 1.320 |  Acc: 66.258,98.210,99.828,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=275 | Loss: 4.237 |  Acc: 53.460,69.000,74.830,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=276 | Loss: 1.317 |  Acc: 66.610,98.190,99.794,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=276 | Loss: 4.246 |  Acc: 53.290,69.140,74.920,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=277 | Loss: 1.317 |  Acc: 66.556,98.152,99.826,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=277 | Loss: 4.244 |  Acc: 53.480,68.980,74.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=278 | Loss: 1.315 |  Acc: 66.382,98.174,99.816,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=278 | Loss: 4.249 |  Acc: 53.360,69.070,74.720,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=279 | Loss: 1.317 |  Acc: 66.502,98.176,99.824,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=279 | Loss: 4.232 |  Acc: 53.410,69.070,74.990,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=280 | Loss: 1.316 |  Acc: 66.608,98.180,99.840,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=280 | Loss: 4.250 |  Acc: 53.360,69.160,74.780,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=281 | Loss: 1.316 |  Acc: 66.520,98.122,99.802,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=281 | Loss: 4.232 |  Acc: 53.260,69.130,74.990,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=282 | Loss: 1.321 |  Acc: 66.314,98.144,99.824,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=282 | Loss: 4.227 |  Acc: 53.290,69.220,74.900,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=283 | Loss: 1.312 |  Acc: 66.514,98.194,99.822,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=283 | Loss: 4.235 |  Acc: 53.320,69.070,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=284 | Loss: 1.317 |  Acc: 66.584,98.150,99.850,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=284 | Loss: 4.234 |  Acc: 53.500,68.990,74.690,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=285 | Loss: 1.315 |  Acc: 66.382,98.204,99.804,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=285 | Loss: 4.237 |  Acc: 53.220,69.130,74.890,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=286 | Loss: 1.318 |  Acc: 66.514,98.178,99.834,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=286 | Loss: 4.241 |  Acc: 53.220,69.080,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=287 | Loss: 1.313 |  Acc: 66.670,98.230,99.822,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=287 | Loss: 4.238 |  Acc: 53.130,68.910,74.830,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=288 | Loss: 1.312 |  Acc: 66.826,98.256,99.816,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=288 | Loss: 4.245 |  Acc: 53.320,68.770,74.660,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=289 | Loss: 1.315 |  Acc: 66.546,98.266,99.830,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=289 | Loss: 4.245 |  Acc: 53.300,68.860,74.710,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=290 | Loss: 1.316 |  Acc: 66.352,98.214,99.818,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=290 | Loss: 4.246 |  Acc: 53.480,68.950,74.840,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=291 | Loss: 1.317 |  Acc: 66.398,98.182,99.826,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=291 | Loss: 4.241 |  Acc: 53.350,68.960,74.910,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=292 | Loss: 1.316 |  Acc: 66.626,98.206,99.814,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=292 | Loss: 4.252 |  Acc: 53.360,69.130,74.980,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=293 | Loss: 1.320 |  Acc: 66.286,98.198,99.824,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=293 | Loss: 4.238 |  Acc: 53.220,68.850,74.840,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=294 | Loss: 1.317 |  Acc: 66.530,98.234,99.838,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=294 | Loss: 4.242 |  Acc: 53.340,69.190,74.910,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=295 | Loss: 1.315 |  Acc: 66.634,98.226,99.778,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=295 | Loss: 4.240 |  Acc: 53.350,68.840,74.940,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=296 | Loss: 1.315 |  Acc: 66.528,98.156,99.824,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=296 | Loss: 4.235 |  Acc: 53.520,69.220,74.970,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=297 | Loss: 1.318 |  Acc: 66.526,98.132,99.798,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=297 | Loss: 4.233 |  Acc: 53.400,69.070,74.870,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=298 | Loss: 1.315 |  Acc: 66.712,98.176,99.818,% | Adaptive Acc:0.000% | clf_exit:
Testing: Epoch=298 | Loss: 4.240 |  Acc: 53.480,68.990,74.950,% | Adaptive Acc:0.000% | clf_exit:

Training Setting: Namespace(T=3.0, adaptive=0, backend='modelG_2con3', batch_size=128, circles=5, dataset_name='cifar100', dropout=1.0, fb='1:1:1', flops_str='', gamma=0.9, ge=1, lmbda=0.0, lmbda_e=0.9, loss_type='cross_entropy', max_epoch=300, step_all=0, step_clf=0, threshold=0.5, vanilla=0)
Training: Epoch=299 | Loss: 1.316 |  Acc: 66.322,98.236,99.798,% | Adaptive Acc:92.340% | clf_exit: 0.553 0.431 0.016
Testing: Epoch=299 | Loss: 4.247 |  Acc: 53.270,69.070,74.800,% | Adaptive Acc:66.490% | clf_exit: 0.573 0.329 0.098

circles: 0
Testing: Epoch=299 | Loss: 7.751 |  Acc: 53.270,39.520,25.300,% | Adaptive Acc:49.560% | clf_exit: 0.573 0.010 0.417
circles: 1
Testing: Epoch=299 | Loss: 5.961 |  Acc: 53.270,59.630,55.960,% | Adaptive Acc:59.630% | clf_exit: 0.573 0.055 0.372
circles: 2
Testing: Epoch=299 | Loss: 4.928 |  Acc: 53.270,66.190,67.690,% | Adaptive Acc:65.060% | clf_exit: 0.573 0.133 0.294
circles: 3
Testing: Epoch=299 | Loss: 4.331 |  Acc: 53.270,68.680,73.130,% | Adaptive Acc:67.150% | clf_exit: 0.573 0.214 0.213
circles: 4
Testing: Epoch=299 | Loss: 4.098 |  Acc: 53.270,69.210,74.820,% | Adaptive Acc:67.050% | clf_exit: 0.573 0.283 0.144
circles: 5
Testing: Epoch=299 | Loss: 4.247 |  Acc: 53.270,69.070,74.800,% | Adaptive Acc:66.490% | clf_exit: 0.573 0.329 0.098

